{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from monai.data import create_test_image_3d\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from datetime import datetime, timedelta\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# from pathlib import Path\n",
    "# from monai.transforms import Compose\n",
    "# from monai.data import ThreadDataLoader, create_test_image_3d\n",
    "# from monai.utils import set_determinism\n",
    "# from monai.config import print_config\n",
    "\n",
    "# print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate-dataset",
   "metadata": {},
   "source": [
    "## Simulate a special dataset\n",
    "\n",
    "It is well known that AI takes time to train. To provide the \"Hello World!\" experience of Auto3D in this notebook, we will simulate a small dataset and run training only for multiple epochs. Due to the nature of AI, the performance shouldn't be highly expected, but the entire pipeline will be completed within minutes!\n",
    "\n",
    "`sim_datalist` provides the information of the simulated datasets. It lists 12 training and 2 testing images and labels. The training data are split into 3 folds. Each fold will use 8 images to train and 4 images to validate. The size of the dimension is defined by the `sim_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "define-datalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_datalist = {\n",
    "    \"training\": [\n",
    "        {\"image\": \"tr_image_001.nii.gz\", \"label\": \"tr_label_001.nii.gz\"},\n",
    "        {\"image\": \"tr_image_002.nii.gz\", \"label\": \"tr_label_002.nii.gz\"},\n",
    "        {\"image\": \"tr_image_003.nii.gz\", \"label\": \"tr_label_003.nii.gz\"},\n",
    "        {\"image\": \"tr_image_004.nii.gz\", \"label\": \"tr_label_004.nii.gz\"},\n",
    "        {\"image\": \"tr_image_005.nii.gz\", \"label\": \"tr_label_005.nii.gz\"},\n",
    "        {\"image\": \"tr_image_006.nii.gz\", \"label\": \"tr_label_006.nii.gz\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "sim_dim = (128, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-images-labels",
   "metadata": {},
   "source": [
    "## Generate images and labels\n",
    "\n",
    "Now we can use MONAI `create_test_image_3d` and `nib.Nifti1Image` functions to generate the 3D simulated images under the work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "generate-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated simulated images and labels.\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"./helloworld_work_dir\"\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "dataroot_dir = os.path.join(work_dir, \"sim_dataroot\")\n",
    "if not os.path.isdir(dataroot_dir):\n",
    "    os.makedirs(dataroot_dir)\n",
    "\n",
    "datalist_file = os.path.join(work_dir, \"sim_datalist.json\")\n",
    "with open(datalist_file, \"w\") as f:\n",
    "    json.dump(sim_datalist, f)\n",
    "\n",
    "for d in sim_datalist[\"training\"]:\n",
    "    im, seg = create_test_image_3d(\n",
    "        sim_dim[0], sim_dim[1], sim_dim[2], rad_max=10, num_seg_classes=1, random_state=np.random.RandomState(42)\n",
    "    )\n",
    "    image_fpath = os.path.join(dataroot_dir, d[\"image\"])\n",
    "    label_fpath = os.path.join(dataroot_dir, d[\"label\"])\n",
    "    nib.save(nib.Nifti1Image(im, affine=np.eye(4)), image_fpath)\n",
    "    nib.save(nib.Nifti1Image(seg, affine=np.eye(4)), label_fpath)\n",
    "\n",
    "print(\"Generated simulated images and labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "import-updated-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the updated functions (definitions provided earlier)\n",
    "from scripts.diff_model_create_training_data import diff_model_create_training_data\n",
    "from scripts.diff_model_train import diff_model_train\n",
    "from scripts.diff_model_infer import diff_model_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "setup-directories",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data_base_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Set up directories based on configurations\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dataroot \u001b[38;5;241m=\u001b[39m \u001b[43menv_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_base_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m filenames_filepath \u001b[38;5;241m=\u001b[39m env_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_data_list\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m output_root_embedding \u001b[38;5;241m=\u001b[39m env_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data_base_dir'"
     ]
    }
   ],
   "source": [
    "# Set up directories and configurations\n",
    "env_config_path = \"./configs/environment_maisi_diff_model_train.json\"\n",
    "model_config_path = \"./configs/config_maisi_diff_model_train.json\"\n",
    "ckpt_filepath = None\n",
    "output_prefix = \"unet_3d\"\n",
    "amp = True\n",
    "a_min = -1000\n",
    "a_max = 1000\n",
    "b_min = 0\n",
    "b_max = 1\n",
    "\n",
    "# Load environment and model configurations\n",
    "with open(env_config_path, \"r\") as f:\n",
    "    env_config = json.load(f)\n",
    "\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Set up directories based on configurations\n",
    "dataroot = env_config[\"data_base_dir\"][0]\n",
    "filenames_filepath = env_config[\"json_data_list\"][0]\n",
    "output_root_embedding = env_config[\"output_dir\"]\n",
    "autoencoder_root = env_config[\"trained_autoencoder_path\"]\n",
    "list_filepath = filenames_filepath\n",
    "output_dir = env_config[\"output_dir\"]\n",
    "pl_root = env_config[\"tfevent_path\"]\n",
    "ckpt_folder = env_config[\"model_dir\"]\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(dataroot, exist_ok=True)\n",
    "os.makedirs(output_root_embedding, exist_ok=True)\n",
    "os.makedirs(autoencoder_root, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(pl_root, exist_ok=True)\n",
    "os.makedirs(ckpt_folder, exist_ok=True)\n",
    "os.makedirs(\"./predictions\", exist_ok=True)\n",
    "\n",
    "# Create pseudo filenames.txt\n",
    "filenames = [f\"image_{i:03d}_image.nii.gz\" for i in range(10)]\n",
    "with open(filenames_filepath, \"w\") as f:\n",
    "    f.write(\"\\n\".join(filenames))\n",
    "\n",
    "print(\"Created pseudo filenames.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-training-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Training Data\n",
    "print(\"Creating training data...\")\n",
    "diff_model_create_training_data(env_config, model_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train the Model\n",
    "print(\"Training the model...\")\n",
    "diff_model_train(env_config_path, model_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infer-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Infer using the Trained Model\n",
    "print(\"Running inference...\")\n",
    "diff_model_infer(env_config_path, model_config_path, ckpt_filepath, amp)\n",
    "\n",
    "print(\"Completed all steps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
