{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Algorithm Ensemble in Auto3DSeg\n",
    "\n",
    "In this notebook, we will provide a brief example of how to customize your ensemble pipeline by defining a new ensemble class.\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, yaml, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from monai.apps.auto3dseg import (\n",
    "    AlgoEnsemble,\n",
    "    AlgoEnsembleBuilder,\n",
    "    DataAnalyzer,\n",
    "    BundleGen,\n",
    ")\n",
    "\n",
    "from monai.bundle.config_parser import ConfigParser\n",
    "from monai.data import create_test_image_3d\n",
    "from monai.utils.enums import AlgoEnsembleKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a special dataset\n",
    "\n",
    "It is well known that AI takes time to train. To provide the \"Hello World!\" experience of Auto3D in this notebook, we will simulate a small dataset and run training only for multiple epochs. Due to the nature of AI, the performance shouldn't be highly expected, but the entire pipeline will be completed within minutes!\n",
    "\n",
    "`sim_datalist` provides the information of the simulated datasets. It lists 12 training and 2 testing images and labels. The training data are split into 3 folds. Each fold will use 8 images to train and 4 images to validate. The size of the dimension is defined by the `sim_dim` .\n",
    "\n",
    "> NOTE: Each validation set only has 4 images in one fold of training.\n",
    "> Therefore, we need to set a limit on the total number of GPUs we're using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_datalist = {\n",
    "    \"testing\": [\n",
    "        {\"image\": \"test_image_001.nii.gz\", \"label\": \"test_label_001.nii.gz\"},\n",
    "        {\"image\": \"test_image_002.nii.gz\", \"label\": \"test_label_002.nii.gz\"},\n",
    "    ],\n",
    "    \"training\": [\n",
    "        {\"fold\": 0, \"image\": \"tr_image_001.nii.gz\", \"label\": \"tr_label_001.nii.gz\"},\n",
    "        {\"fold\": 0, \"image\": \"tr_image_002.nii.gz\", \"label\": \"tr_label_002.nii.gz\"},\n",
    "        {\"fold\": 0, \"image\": \"tr_image_003.nii.gz\", \"label\": \"tr_label_003.nii.gz\"},\n",
    "        {\"fold\": 0, \"image\": \"tr_image_004.nii.gz\", \"label\": \"tr_label_004.nii.gz\"},\n",
    "        {\"fold\": 1, \"image\": \"tr_image_005.nii.gz\", \"label\": \"tr_label_005.nii.gz\"},\n",
    "        {\"fold\": 1, \"image\": \"tr_image_006.nii.gz\", \"label\": \"tr_label_006.nii.gz\"},\n",
    "        {\"fold\": 1, \"image\": \"tr_image_007.nii.gz\", \"label\": \"tr_label_007.nii.gz\"},\n",
    "        {\"fold\": 1, \"image\": \"tr_image_008.nii.gz\", \"label\": \"tr_label_008.nii.gz\"},\n",
    "        {\"fold\": 2, \"image\": \"tr_image_009.nii.gz\", \"label\": \"tr_label_009.nii.gz\"},\n",
    "        {\"fold\": 2, \"image\": \"tr_image_010.nii.gz\", \"label\": \"tr_label_010.nii.gz\"},\n",
    "        {\"fold\": 2, \"image\": \"tr_image_011.nii.gz\", \"label\": \"tr_label_011.nii.gz\"},\n",
    "        {\"fold\": 2, \"image\": \"tr_image_012.nii.gz\", \"label\": \"tr_label_012.nii.gz\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "sim_dim = (64, 64, 64)\n",
    "\n",
    "if torch.cuda.device_count() > 4:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images and labels\n",
    "\n",
    "Now we can use MONAI `create_test_image_3d` and `nib.Nifti1Image` functions to generate the 3D simulated images under the work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = str(Path(\"./ensemble_byoc_work_dir\"))\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "dataroot_dir = os.path.join(work_dir, \"sim_dataroot\")\n",
    "if not os.path.isdir(dataroot_dir):\n",
    "    os.makedirs(dataroot_dir)\n",
    "\n",
    "datalist_file = os.path.join(work_dir, \"sim_datalist.json\")\n",
    "with open(datalist_file, 'w') as f:\n",
    "    json.dump(sim_datalist, f)\n",
    "\n",
    "for d in sim_datalist[\"testing\"] + sim_datalist[\"training\"]:\n",
    "    im, seg = create_test_image_3d(\n",
    "        sim_dim[0], sim_dim[1], sim_dim[2], rad_max=10, num_seg_classes=1, random_state=np.random.RandomState(42)\n",
    "    )\n",
    "    image_fpath = os.path.join(dataroot_dir, d[\"image\"])\n",
    "    label_fpath = os.path.join(dataroot_dir, d[\"label\"])\n",
    "    nib.save(nib.Nifti1Image(im, affine=np.eye(4)), image_fpath)\n",
    "    nib.save(nib.Nifti1Image(seg, affine=np.eye(4)), label_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new class that inherit the AlgoEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlgoEnsemble(AlgoEnsemble):\n",
    "    \"\"\"\n",
    "    Randomly select N models to do ensemble\n",
    "    \"\"\"\n",
    "    def __init__(self, n_models=3):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_models = n_models\n",
    "\n",
    "    def collect_algos(self):\n",
    "        \"\"\"\n",
    "        collect_algos defines the method to collect the target algos from the self.algos list\n",
    "        \"\"\"\n",
    "        n = len(self.algos)\n",
    "        if self.n_models > n:\n",
    "            raise ValueError(f\"Number of loaded Algo is {n}, but {self.n_models} algos are requested.\")\n",
    "\n",
    "        indexes = list(range(n))\n",
    "        random.shuffle(indexes)\n",
    "        indexes = indexes[0:self.n_models]\n",
    "        self.algo_ensemble = []\n",
    "        for idx in indexes:\n",
    "            self.algo_ensemble.append(deepcopy(self.algos[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Auto3DSeg data analyzer, algo generation, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DataAnalyzer(datalist_file, dataroot_dir)\n",
    "da.get_all_case_stats()\n",
    "\n",
    "input = {\n",
    "    \"modality\": \"MRI\",\n",
    "    \"datalist\": datalist_file,\n",
    "    \"dataroot\": dataroot_dir,\n",
    "}\n",
    "\n",
    "input_cfg = \"input.yaml\"\n",
    "ConfigParser.export_config_file(input, input_cfg)\n",
    "\n",
    "bundle_generator = BundleGen(\n",
    "    algo_path=work_dir, data_stats_filename='data_stats.yaml', data_src_cfg_name=input_cfg\n",
    ")\n",
    "bundle_generator.generate(work_dir, num_fold=2)\n",
    "history = bundle_generator.get_history()\n",
    "\n",
    "max_epochs = 2\n",
    "\n",
    "# safeguard to ensure max_epochs is greater or equal to 2\n",
    "max_epochs = max(max_epochs, 2)\n",
    "\n",
    "train_param = {\n",
    "    \"CUDA_VISIBLE_DEVICES\": [0],  # use only 1 gpu\n",
    "    \"num_iterations\": 4 * max_epochs,\n",
    "    \"num_iterations_per_validation\": 2 * max_epochs,\n",
    "    \"num_images_per_batch\": 2,\n",
    "    \"num_epochs\": max_epochs,\n",
    "    \"num_warmup_iterations\": 2 * max_epochs,\n",
    "}\n",
    "\n",
    "for h in history:\n",
    "    for _, algo in h.items():\n",
    "        algo.train(train_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply MyAlgoEnsemble\n",
    "\n",
    "As we defined earlier in this notebook, `MyAlgoEnsemble` randomly shuffles the trained models, and picks three for ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = AlgoEnsembleBuilder(history, input_cfg)\n",
    "builder.set_ensemble_method(MyAlgoEnsemble())\n",
    "ensemble = builder.get_ensemble()\n",
    "preds = ensemble()\n",
    "\n",
    "print('The ensemble randomly picks the following models:')\n",
    "for algo in ensemble.get_algo_ensemble():\n",
    "    print(algo[AlgoEnsembleKeys.ID])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
