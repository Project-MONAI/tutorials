{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# MAISI VAE Training Tutorial\n",
    "\n",
    "This tutorial illustrates how to train the VAE model in MAISI. The VAE model is used for latent feature compression, which significantly reduce the memory usage of the diffusion model.\n",
    "\n",
    "## Setup environment\n",
    "\n",
    "To run this tutorial, please install `xformers` by following the [official guide](https://github.com/facebookresearch/xformers#installing-xformers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "# TODO: remove print statement after generative dir is renamed to generation\n",
    "!python -c \"import xformers\" || pip install -q xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -c \"import generative; print(generative.__version__)\" || pip install -q \"monai-generative\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2019e-1556-41a6-95e8-5d1a65f8b3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.nn import L1Loss, MSELoss\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_url\n",
    "from monai.config import print_config\n",
    "from monai.transforms import LoadImage, Orientation\n",
    "from monai.utils import set_determinism\n",
    "from monai.data import load_decathlon_datalist\n",
    "from monai.apps import download_and_extract\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.losses.perceptual import PerceptualLoss\n",
    "from monai.losses.adversarial_loss import PatchAdversarialLoss\n",
    "from monai.inferers.inferer import SlidingWindowInferer, SimpleInferer\n",
    "\n",
    "from generative.networks.nets import PatchDiscriminator\n",
    "\n",
    "from scripts.utils import define_instance, load_autoencoder_ckpt, KL_loss, dynamic_infer\n",
    "from scripts.utils_plot import find_label_center_loc, get_xyz_plot, show_image\n",
    "from scripts.transforms import VAE_Transform\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.\n",
    "This allows you to save results and reuse downloads.\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c12dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "root_dir = '/mnt/drive1/canz/datasets'\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9719f-159b-4ecf-9689-dce848e0cf11",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "This tutorial shows how to train a VAE with both CT and MRI data. We use MSD 09 Spleen segmentation and MSD 01 Brats16&17 Brain Tumor segmentation as examples. Users can choose their own training datasets.\n",
    "\n",
    "Downloads and extracts the dataset.\n",
    "The dataset comes from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f0c29-1989-45c6-bd64-35682533794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSD Spleen CT data\n",
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_path_1 = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_path_1):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)\n",
    "    \n",
    "train_images_1 = sorted(glob.glob(os.path.join(data_path_1, \"imagesTr\", \"*.nii.gz\")))\n",
    "data_dicts_1 = [{\"image\": image_name} for image_name in train_images_1]\n",
    "len_train= int(0.8*len(data_dicts_1))\n",
    "train_files_1, val_files_1 = data_dicts_1[:len_train], data_dicts_1[len_train:]\n",
    "\n",
    "# MSD Brats MRI data\n",
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task01_BrainTumour.tar\"\n",
    "md5 = \"240a19d752f0d9e9101544901065d872\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task01_BrainTumour.tar\")\n",
    "data_path_2 = os.path.join(root_dir, \"Task01_BrainTumour\")\n",
    "if not os.path.exists(data_path_2):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)\n",
    "    \n",
    "train_images_2 = sorted(glob.glob(os.path.join(data_path_2, \"imagesTr\", \"*.nii.gz\")))\n",
    "data_dicts_2 = [{\"image\": image_name} for image_name in train_images_2]\n",
    "len_train= int(0.8*len(data_dicts_2))\n",
    "train_files_2, val_files_2 = data_dicts_2[:len_train], data_dicts_2[len_train:]\n",
    "\n",
    "# Expandable to more datasets\n",
    "datasets = {\n",
    "    1: {\n",
    "        'data_name': \"Dataset 1 MSD09 Spleen Abdomen CT\",\n",
    "        'train_files': train_files_1,\n",
    "        'val_files': val_files_1,\n",
    "        \"modality\": 'ct'\n",
    "    },\n",
    "    2: {\n",
    "        'data_name': \"Dataset 2 MSD01 Brats Brain MRI\",\n",
    "        'train_files': train_files_2,\n",
    "        'val_files': val_files_2,\n",
    "        \"modality\": 'mri'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f164998-6c7d-44cd-93ee-e9d36c26ef96",
   "metadata": {},
   "source": [
    "## Read in environment setting, including data directory, model directory, and output directory\n",
    "\n",
    "The information for data directory, model directory, and output directory are saved in ./configs/environment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b4c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "environment_file = \"./configs/environment_maisi_vae_train.json\"\n",
    "env_dict = json.load(open(environment_file, \"r\"))\n",
    "for k, v in env_dict.items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# model path\n",
    "Path(args.model_dir).mkdir(parents=True, exist_ok=True)\n",
    "trained_g_path = os.path.join(args.model_dir,\"autoencoder.pt\")\n",
    "trained_d_path = os.path.join(args.model_dir,\"discriminator.pt\")\n",
    "print(f\"Trained model will be saved as {trained_g_path} and {trained_d_path}.\")\n",
    "\n",
    "# initialize tensorboard writer\n",
    "Path(args.tfevent_path).mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_path = os.path.join(args.tfevent_path, \"autoencoder\")\n",
    "Path(tensorboard_path).mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_writer = SummaryWriter(tensorboard_path)\n",
    "print(f\"Tensorboard event will be saved as {tensorboard_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98233f-6492-40ed-9ba5-7ab4ae0f8ffd",
   "metadata": {},
   "source": [
    "## Read in configuration setting, including network definition, body region and anatomy to generate, etc.\n",
    "\n",
    "The information used for both training and inference, like network definition, is stored in `\"./configs/config_maisi.json\"`. Training and inference should use the same \"./configs/config_maisi.json\".\n",
    "\n",
    "The information for the training hyperparameters and data processing pparameters, like learning rate and patch size, are stored in `\"./configs/config_maisi_vae_train\"`. Please feel free to play with it.\n",
    "\n",
    "Dataset preprocessing:\n",
    "- `\"random_aug\"`: bool, whether to add random data augmentation for training data.\n",
    "- `\"spacing_type\"`: choose from `\"original\"` (no resampling involved), `\"fixed\"` (all images resampled to same voxel size), and `\"rand\"` (images randomly resampled).\n",
    "- `\"spacing\"`: None or list of three floats. If `\"spacing_type\"` is `\"fixed\"`, all the images will be interpolated to the voxel size of `\"spacing\"`.\n",
    "- `\"select_channel\"`: int, if multi-channel MRI, which chennel it will select.\n",
    "\n",
    "Training configs:\n",
    "- `\"patch_size\"`: training patch size. For the released model, we first trained the autoencoder with small patch size ([128,128,128]), then increased to larger patch size ([256,256,128]).\n",
    "- `\"val_patch_size\"`: Size of validation patches. If None, will use the whole volume for validation. If given, will central crop a patch for validation.\n",
    "- `\"val_sliding_window_patch_size\"`: if the validation patch is too large, will use sliding window inference. Choose small value if GPU memory is small.\n",
    "- `\"perceptual_weight\"`: perceptual loss weight.\n",
    "- `\"kl_weight\"`: KL loss weight, important hyper-parameter. If too large, decoder cannot recon good results from latent space. If too small, latent space will not be regularized enough for the diffusion model.\n",
    "- `\"adv_weight\"`: adversavarial loss weight.\n",
    "- `\"recon_loss\"`: choose from 'l1' and l2'.\n",
    "- `\"val_interval\"`:int, do validation every `\"val_interval\"` epoches.\n",
    "- `\"cache\"`: float between 0 and 1, dataloader cache, choose small value if CPU memory is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533414f3-bef5-49f7-b082-f803b5e494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./configs/config_maisi.json\"\n",
    "config_dict = json.load(open(config_file, \"r\"))\n",
    "for k, v in config_dict.items():\n",
    "    setattr(args, k, v)\n",
    "\n",
    "# check the format of inference inputs\n",
    "config_train_file = \"./configs/config_maisi_vae_train.json\"\n",
    "config_train_dict = json.load(open(config_train_file, \"r\"))\n",
    "for k, v in config_train_dict[\"data_option\"].items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "for k, v in config_train_dict[\"autoencoder_train\"].items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"Network definition and training hyperparameters have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba613d-a2f5-4afc-95df-65ad21fafedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02476bc7-c980-4b0b-8d7a-c780a5ccc11f",
   "metadata": {},
   "source": [
    "## Initialize networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "args.autoencoder_def[\"num_splits\"] = 1\n",
    "autoencoder = define_instance(args, \"autoencoder_def\").to(device)\n",
    "discriminator_norm = \"INSTANCE\"\n",
    "discriminator = PatchDiscriminator(\n",
    "    spatial_dims=args.spatial_dims,\n",
    "    num_layers_d=3,\n",
    "    num_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    norm=discriminator_norm,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "## Build training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize file lists\n",
    "train_files = {'ct': [], 'mri': []}\n",
    "val_files = {'ct': [], 'mri': []}\n",
    "\n",
    "# Function to add assigned class to datalist\n",
    "def add_assigned_class_to_datalist(datalist, classname):\n",
    "    for item in datalist:\n",
    "        item[\"class\"] = classname\n",
    "    return datalist\n",
    "\n",
    "# Process datasets\n",
    "for data_id, dataset in datasets.items(): \n",
    "    train_files_i = dataset[\"train_files\"]\n",
    "    val_files_i = dataset[\"val_files\"]\n",
    "    print(f\"{dataset['data_name']}: number of training data is {len(train_files_i)}.\")\n",
    "    print(f\"{dataset['data_name']}: number of val data is {len(val_files_i)}.\")\n",
    "    \n",
    "    # attach modality to each file\n",
    "    modality = dataset['modality']\n",
    "    train_files[modality] += add_assigned_class_to_datalist(train_files_i, modality)\n",
    "    val_files[modality] += add_assigned_class_to_datalist(val_files_i, modality)\n",
    "\n",
    "# Print total numbers for each modality\n",
    "for modality in train_files.keys():\n",
    "    print(f\"Total number of training data for {modality} is {len(train_files[modality])}.\")\n",
    "    print(f\"Total number of val data for {modality} is {len(val_files[modality])}.\")\n",
    "\n",
    "# Combine the data\n",
    "train_files_combined = train_files['ct'] + train_files['mri']\n",
    "val_files_combined = val_files['ct'] + val_files['mri']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1e997-c239-463b-8905-a05de5c0afff",
   "metadata": {},
   "source": [
    "## Define data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c314e-6998-417b-97cc-8a86cc3a9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = VAE_Transform(\n",
    "    is_train=True, \n",
    "    random_aug=args.random_aug,  # whether apply random data augmentation for training\n",
    "    k=4,  # patches should be divisible by k\n",
    "    patch_size=args.patch_size,\n",
    "    val_patch_size=args.val_patch_size,\n",
    "    output_dtype=torch.float16,  # final data type\n",
    "    spacing_type=args.spacing_type,\n",
    "    spacing=args.spacing,\n",
    "    image_keys=[\"image\"],\n",
    "    label_keys=[],\n",
    "    additional_keys=[],\n",
    "    select_channel=0)\n",
    "val_transform = VAE_Transform(\n",
    "    is_train=False, \n",
    "    random_aug=False,\n",
    "    k=4,  # patches should be divisible by k\n",
    "    val_patch_size=args.val_patch_size, # if None, will validate on whole image volume\n",
    "    output_dtype=torch.float16,  # final data type\n",
    "    image_keys=[\"image\"],\n",
    "    label_keys=[],\n",
    "    additional_keys=[],\n",
    "    select_channel=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b67b9d-fe54-4d6f-95c3-cb84122f1e3e",
   "metadata": {},
   "source": [
    "## Build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126468a-273b-40fc-ae18-b8c1b5302e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of training data is {len(train_files_combined)}.\")\n",
    "dataset_train = CacheDataset(\n",
    "    data=train_files_combined, transform=train_transform, cache_rate=args.cache, num_workers=8\n",
    ")\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Total number of validation data is {len(val_files_combined)}.\")\n",
    "dataset_val = CacheDataset(\n",
    "    data=val_files_combined, transform=val_transform, cache_rate=args.cache, num_workers=8\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.val_batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bb8d7-17db-48b5-8d08-d8af61fc763a",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0453d9f-1614-4c84-aef1-77b6339d8c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_vis_img = dataset_train[0][\"image\"]\n",
    "print(f\"Train image shape {example_vis_img.shape}\")\n",
    "center_loc_axis = find_label_center_loc(example_vis_img.squeeze(0))\n",
    "vis_image = get_xyz_plot(example_vis_img, center_loc_axis, mask_bool=False)\n",
    "show_image(vis_image, title=\"training image\")\n",
    "\n",
    "example_vis_img = dataset_val[0][\"image\"]\n",
    "print(f\"Val image shape {example_vis_img.shape}\")\n",
    "center_loc_axis = find_label_center_loc(example_vis_img.squeeze(0))\n",
    "vis_image = get_xyz_plot(example_vis_img, center_loc_axis, mask_bool=False)\n",
    "show_image(vis_image, title=\"validation image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe658ac2-6a90-40eb-89fc-7ec35a41dc4b",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f91bf-1c55-46e2-ae56-8677cd8eb81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# config loss and loss weight\n",
    "if args.recon_loss == \"l2\":\n",
    "    intensity_loss = MSELoss()\n",
    "    print(\"Use l2 loss\")\n",
    "else:\n",
    "    intensity_loss = L1Loss(reduction='mean')\n",
    "    print(\"Use l1 loss\")\n",
    "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "\n",
    "loss_perceptual = PerceptualLoss(spatial_dims=3, network_type=\"squeeze\", is_fake_3d=True,\n",
    "    fake_3d_ratio=0.2).eval().to(device)\n",
    "\n",
    "# config optimizer and lr scheduler\n",
    "optimizer_g = torch.optim.Adam(params=autoencoder.parameters(), \n",
    "    lr=args.lr, eps=1e-06 if args.amp else 1e-08)\n",
    "optimizer_d = torch.optim.Adam(params=discriminator.parameters(), \n",
    "    lr=args.lr, eps=1e-06 if args.amp else 1e-08)\n",
    "\n",
    "def warmup_rule(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.01\n",
    "    elif epoch < 20:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "scheduler_g = lr_scheduler.LambdaLR(optimizer_g, lr_lambda=warmup_rule)\n",
    "\n",
    "# set AMP scaler\n",
    "if args.amp:\n",
    "    # test use mean reduction for everything\n",
    "    scaler_g = GradScaler(init_scale=2. ** 8, growth_factor=1.5)\n",
    "    scaler_d = GradScaler(init_scale=2. ** 8, growth_factor=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c00aa4-a6ee-4adc-8933-51757cb79628",
   "metadata": {},
   "source": [
    "## Training\n",
    "**Time cost:** \n",
    "\n",
    "**GPU memory:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251a32-390f-46dd-a613-75b12a7884c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "val_interval = args.val_interval\n",
    "best_val_recon_epoch_loss = 10000000.0\n",
    "total_step = 0\n",
    "start_epoch = 0\n",
    "\n",
    "# Setup validation inferer\n",
    "val_inferer = SlidingWindowInferer(\n",
    "    roi_size=args.val_sliding_window_patch_size,\n",
    "    sw_batch_size=1, progress=False, overlap=0.0,\n",
    "    device=torch.device('cpu'), sw_device=device\n",
    ") if args.val_sliding_window_patch_size else SimpleInferer()\n",
    "\n",
    "def loss_weighted_sum(losses):\n",
    "    return losses['recons_loss'] + args.kl_weight*losses['kl_loss'] + args.perceptual_weight*losses['p_loss']\n",
    "\n",
    "# Training and validation loops\n",
    "for epoch in range(start_epoch, args.n_epochs):\n",
    "    print(\"lr:\", scheduler_g.get_lr())\n",
    "    autoencoder.train()\n",
    "    discriminator.train()\n",
    "    train_epoch_losses = {\n",
    "        'recons_loss': 0,\n",
    "        'kl_loss': 0,\n",
    "        'p_loss': 0\n",
    "    }\n",
    "\n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        images = batch[\"image\"].to(device).contiguous()\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        optimizer_d.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=args.amp):\n",
    "            # Train Generator\n",
    "            reconstruction, z_mu, z_sigma = autoencoder(images)\n",
    "            losses = {\n",
    "                'recons_loss': intensity_loss(reconstruction, images),\n",
    "                'kl_loss': KL_loss(z_mu, z_sigma),\n",
    "                'p_loss': loss_perceptual(reconstruction.float(), images.float())\n",
    "            }\n",
    "            logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
    "            generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "            loss_g = loss_weighted_sum(losses)+ args.adv_weight * generator_loss\n",
    "\n",
    "            if args.amp:\n",
    "                scaler_g.scale(loss_g).backward()\n",
    "                scaler_g.unscale_(optimizer_g)\n",
    "                scaler_g.step(optimizer_g)\n",
    "                scaler_g.update()\n",
    "            else:\n",
    "                loss_g.backward()\n",
    "                optimizer_g.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "            loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "            logits_real = discriminator(images.contiguous().detach())[-1]\n",
    "            loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "            loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
    "\n",
    "            if args.amp:\n",
    "                scaler_d.scale(loss_d).backward()\n",
    "                scaler_d.step(optimizer_d)\n",
    "                scaler_d.update()\n",
    "            else:\n",
    "                loss_d.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "        # Log training loss\n",
    "        total_step += 1\n",
    "        for loss_name, loss_value in losses.items():\n",
    "            tensorboard_writer.add_scalar(f\"train_{loss_name}_iter\", loss_value.item(), total_step)\n",
    "            train_epoch_losses[loss_name] += loss_value.item()\n",
    "        tensorboard_writer.add_scalar(\"train_adv_loss_iter\", generator_loss, total_step)\n",
    "        tensorboard_writer.add_scalar(\"train_fake_loss_iter\", loss_d_fake, total_step)\n",
    "        tensorboard_writer.add_scalar(\"train_real_loss_iter\", loss_d_real, total_step)\n",
    "\n",
    "    scheduler_g.step()\n",
    "    print(f\"Epoch {epoch} train_vae_loss {loss_weighted_sum(train_epoch_losses)}: {train_epoch_losses}.\")\n",
    "    for loss_name, loss_value in train_epoch_losses.items():\n",
    "        tensorboard_writer.add_scalar(f\"train_{loss_name}_epoch\", loss_value, epoch)\n",
    "\n",
    "    # Validation\n",
    "    if epoch % val_interval == 0:\n",
    "        autoencoder.eval()\n",
    "        val_epoch_losses = {\n",
    "            'recons_loss': 0,\n",
    "            'kl_loss': 0,\n",
    "            'p_loss': 0\n",
    "        }\n",
    "        val_loader_iter = iter(dataloader_val)\n",
    "        for step in range(len(dataloader_val)):\n",
    "            try:\n",
    "                batch = next(val_loader_iter)\n",
    "            except:\n",
    "                print(\"Error for one data in val_loader, skip\")\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=args.amp):\n",
    "                    images = batch[\"image\"]\n",
    "                    reconstruction, _, _ = dynamic_infer(val_inferer, autoencoder, images)\n",
    "                    reconstruction = reconstruction.to(device)\n",
    "                    val_epoch_losses['recons_loss'] += intensity_loss(reconstruction, images.to(device)).item()\n",
    "                    val_epoch_losses['kl_loss'] += KL_loss(z_mu, z_sigma).item()\n",
    "                    val_epoch_losses['p_loss'] += loss_perceptual(reconstruction, images.to(device)).item()\n",
    "\n",
    "        for key in val_epoch_losses:\n",
    "            val_epoch_losses[key] /= (step + 1)\n",
    "        \n",
    "        torch.save(autoencoder.state_dict(), trained_g_path)\n",
    "        torch.save(discriminator.state_dict(), trained_d_path)\n",
    "        val_loss_g = loss_weighted_sum(val_epoch_losses)\n",
    "        print(f\"Epoch {epoch} val_vae_loss {val_loss_g}: {val_epoch_losses}.\")\n",
    "\n",
    "        if val_loss_g < best_val_recon_epoch_loss:\n",
    "            best_val_recon_epoch_loss = val_loss_g\n",
    "            trained_g_path_epoch = f\"{trained_g_path[:-3]}_epoch{epoch}.pt\"\n",
    "            torch.save(autoencoder.state_dict(), trained_g_path_epoch)\n",
    "            print(\"Got best val vae loss.\")\n",
    "            print(\"Save trained autoencoder to\", trained_g_path)\n",
    "            print(\"Save trained discriminator to\", trained_d_path)\n",
    "\n",
    "        for loss_name, loss_value in val_epoch_losses.items():\n",
    "            tensorboard_writer.add_scalar(loss_name, loss_value, epoch)\n",
    "        \n",
    "        # Monitor scale_factor\n",
    "        # We'd like to tune kl_weights in order to make scale_factor close to 1.\n",
    "        scale_factor_sample = 1. / z_mu.flatten().std()\n",
    "        tensorboard_writer.add_scalar(\"val_one_sample_scale_factor\", scale_factor_sample, epoch)\n",
    "        \n",
    "        # Monitor reconstruction result \n",
    "        center_loc_axis = find_label_center_loc(images[0,0,...])\n",
    "        vis_image = get_xyz_plot(images[0,...], center_loc_axis, mask_bool=False)\n",
    "        vis_recon_image = get_xyz_plot(reconstruction[0,...], center_loc_axis, mask_bool=False)\n",
    "        \n",
    "        tensorboard_writer.add_image(\n",
    "            \"val_orig_img\",\n",
    "            vis_image.transpose([2, 0, 1]),\n",
    "            epoch,\n",
    "        )\n",
    "        tensorboard_writer.add_image(\n",
    "            \"val_recon_img\",\n",
    "            vis_recon_image.transpose([2, 0, 1]),\n",
    "            epoch,\n",
    "        )\n",
    "\n",
    "        show_image(vis_image, title=\"val image\")\n",
    "        show_image(vis_recon_image, title=\"cal recon result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321ad70-2c2f-41f5-9c9a-94f9029297c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
