{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# Fast Inference with MONAI Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating Model Inference with MONAI\n",
    "\n",
    "In this tutorial, we explore three powerful features that can accelerate model inference using MONAI. These features are designed to optimize the data handling and computational efficiency of your inference pipeline, particularly when working with NVIDIA GPUs. The tutorial will guide you through the following features and provide a comprehensive benchmarking strategy to evaluate the performance improvements offered by each feature:\n",
    "\n",
    "1. **TensorRT Inference**: Utilize NVIDIA's TensorRT to optimize and execute models for high-performance inference on NVIDIA GPUs.\n",
    "\n",
    "2. **GPU-Based Preprocessing**: Leverage the computational power of GPUs to perform data preprocessing directly on the GPU. This can significantly reduce the time spent on data preparation, enabling faster inference.\n",
    "\n",
    "3. **Direct GPU Data Loading**: Minimize data transfer times by loading data directly from disk into GPU memory. This feature supports NIfTI and DICOM file formats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data directly from disk to GPU memory requires the `kvikio` library. In addition, this tutorial requires many other dependencies such as `monai`, `torch`, `torch_tensorrt`, `numpy`, `ignite`, `pandas`, `matplotlib`, etc. We recommend using the [MONAI Docker](https://docs.monai.io/en/latest/installation.html#from-dockerhub) image to run this tutorial, which includes pre-configured dependencies and allows you to skip manual installation.\n",
    "\n",
    "If not using MONAI Docker, install `kvikio` using one of these methods:\n",
    "\n",
    "- **PyPI Installation**  \n",
    "  Use the appropriate package for your CUDA version:\n",
    "  ```bash\n",
    "  pip install kvikio-cu12  # For CUDA 12\n",
    "  pip install kvikio-cu11  # For CUDA 11\n",
    "  ```\n",
    "\n",
    "- **Conda/Mamba Installation**  \n",
    "  Follow the official [KvikIO installation guide](https://docs.rapids.ai/api/kvikio/nightly/install/) for Conda/Mamba installations.\n",
    "\n",
    "For convenience, we provide the cell below to install all the dependencies (please modify the cell based on your actual CUDA version, and please note that only CUDA 11 and CUDA 12 are supported for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, pydicom, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import torch_tensorrt\" || pip install torch_tensorrt\n",
    "!python -c \"import kvikio\" || pip install kvikio-cu12\n",
    "!python -c \"import pandas\" || pip install pandas\n",
    "!python -c \"import requests\" || pip install requests\n",
    "!python -c \"import onnx\" || pip install onnx\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch_tensorrt\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    NormalizeIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    Compose,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SegResNet\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction on Fast Inference Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TensorRT Inference\n",
    "\n",
    "`monai.networks.utils.convert_to_trt` is a function that converts a PyTorch model to a TensorRT engine-based TorchScript model. Except the loading method (need to use `torch.jit.load` to load the model), the usage of the converted TorchScriptmodel is the same as the original model.\n",
    "\n",
    "`monai.data.torchscript_utils.save_net_with_metadata` is a function that saves the converted TorchScript model and its metadata.\n",
    "\n",
    "example:\n",
    "\n",
    "```python\n",
    "\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.networks.utils import convert_to_trt\n",
    "from monai.data.torchscript_utils import save_net_with_metadata\n",
    "\n",
    "model = SegResNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=105,\n",
    "    init_filters=32,\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    dropout_prob=0.2,\n",
    ")\n",
    "weights = torch.load(\"model.pt\")\n",
    "model.load_state_dict(weights)\n",
    "torchscript_model = convert_to_trt(\n",
    "    model=model,\n",
    "    precision=\"fp16\",\n",
    "    input_shape=[1, 1, 96, 96, 96],\n",
    "    dynamic_batchsize=[1, 1, 1],\n",
    "    use_trace=True,\n",
    "    verify=False,\n",
    ")\n",
    "\n",
    "save_net_with_metadata(torchscript_model, \"segresnet_trt\")\n",
    "\n",
    "model = torch.jit.load(\"segresnet_trt.ts\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GPU-Based Preprocessing\n",
    "\n",
    "`monai.transforms.EnsureTyped` transform allows you to specify the `device` and `dtype` for the output tensor. Therefore, in order to perform GPU-based preprocessing, you can insert the `EnsureTyped` transform at the beginning of your preprocessing transforms. For example:\n",
    "\n",
    "```python\n",
    "preprocess_transforms = [\n",
    "    EnsureTyped(keys=\"image\", device=torch.device(\"cuda:0\"), track_meta=True),\n",
    "    Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "    ScaleIntensityRanged(\n",
    "        keys=[\"image\"],\n",
    "        a_min=-57,\n",
    "        a_max=164,\n",
    "        b_min=0.0,\n",
    "        b_max=1.0,\n",
    "        clip=True,\n",
    "    ),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Direct GPU Data Loading\n",
    "\n",
    "Starting with MONAI `1.4.1rc1`, `monai.data.PydicomReader` and `monai.data.NibabelReader` added the `to_gpu` argument to enable direct GPU data loading. To use this feature, you can set the `to_gpu` argument to `True` when initializing the `LoadImaged` transform. For example:\n",
    "\n",
    "```python\n",
    "loader = LoadImaged(keys=\"image\", reader=\"NibabelReader\", to_gpu=True)\n",
    "```\n",
    "\n",
    "Please note that only NIfTI (.nii, for compressed \".nii.gz\" files, this feature also supports but the acceleration is not significant) and DICOM (.dcm) files are supported for direct GPU data loading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Strategy\n",
    "\n",
    "In this section, we will benchmark the acceleration performance on each feature. Specifically, we will benchmark the following inference workflows:\n",
    "\n",
    "- Original inference workflow\n",
    "- TensorRT inference workflow\n",
    "- TensorRT inference workflow with GPU-based preprocessing\n",
    "- TensorRT inference workflow with direct GPU data loading and GPU-based preprocessing\n",
    "\n",
    "For each benchmark type, `timeit.default_timer` is used to measure the time taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Benchmark Type\n",
    "\n",
    "A variable `benchmark_type` is used to specify the type of benchmark to run. To have a fair comparison, each benchmark type should be run after restarting the notebook kernel. `benchmark_type` can be one of the following:\n",
    "\n",
    "- `\"original\"`: benchmark the original model inference.\n",
    "- `\"trt\"`: benchmark the TensorRT accelerated model inference.\n",
    "- `\"trt_gpu_transforms\"`: benchmark the model inference with GPU transforms.\n",
    "- `\"trt_gds_gpu_transforms\"`: benchmark the model inference with GPU data loading and GPU transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please uncomment the expected benchmark type to run\n",
    "\n",
    "benchmark_type = \"original\"\n",
    "# benchmark_type = \"trt\"\n",
    "# benchmark_type = \"trt_gpu_transforms\"\n",
    "# benchmark_type = \"trt_gds_gpu_transforms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Model\n",
    "\n",
    "The [Medical Segmentation Decathlon Task03 Liver dataset](http://medicaldecathlon.com/) is used as an example to benchmark the acceleration performance.A helper script, [`prepare_data.py`](./prepare_data.py), is used to download and extract the dataset. In addition, the script also prepares the model weights and TensorRT engine-based TorchScript model.\n",
    "\n",
    "The script automatically checks for existing data. This ensures that repeated executions of the notebook do not result in redundant operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_test_datalist, prepare_model_weights, prepare_tensorrt_model\n",
    "\n",
    "root_dir = \".\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch_tensorrt.runtime.set_multi_device_safe_mode(True)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train_files = prepare_test_datalist(root_dir)\n",
    "# since the dataset is too large, the smallest 21 files are used for warm up (1 file) and benchmarking (11 files)\n",
    "train_files = sorted(train_files, key=lambda x: os.path.getsize(x), reverse=False)[:21]\n",
    "weights_path = prepare_model_weights(root_dir=root_dir, bundle_name=\"wholeBody_ct_segmentation\")\n",
    "trt_model_name = \"model_trt.ts\"\n",
    "trt_model_path = prepare_tensorrt_model(root_dir, weights_path, trt_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inference Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(device, gpu_loading_flag=False, gpu_transforms_flag=False):\n",
    "    preprocess_transforms = [\n",
    "        LoadImaged(keys=\"image\", reader=\"NibabelReader\", to_gpu=gpu_loading_flag),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 1.5), mode=\"bilinear\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True),\n",
    "        ScaleIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            minv=-1.0,\n",
    "            maxv=1.0,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if gpu_transforms_flag and not gpu_loading_flag:\n",
    "        preprocess_transforms.insert(1, EnsureTyped(keys=\"image\", device=device, track_meta=True))\n",
    "    infer_transforms = Compose(preprocess_transforms)\n",
    "\n",
    "    return infer_transforms\n",
    "\n",
    "\n",
    "def get_model(device, weights_path, trt_model_path, trt_flag=False):\n",
    "    if not trt_flag:\n",
    "        model = SegResNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=105,\n",
    "            init_filters=32,\n",
    "            blocks_down=[1, 2, 2, 4],\n",
    "            blocks_up=[1, 1, 1],\n",
    "            dropout_prob=0.2,\n",
    "        )\n",
    "        weights = torch.load(weights_path)\n",
    "        model.load_state_dict(weights)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = torch.jit.load(trt_model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inference Workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(data_list, infer_transforms, model, device, benchmark_type):\n",
    "    total_time_dict = {}\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 1\n",
    "\n",
    "    for idx, sample in enumerate(data_list[:10]):\n",
    "        start = timer()\n",
    "        data = infer_transforms({\"image\": sample})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_image = (\n",
    "                data[\"image\"].unsqueeze(0).to(device)\n",
    "                if benchmark_type in [\"trt\", \"original\"]\n",
    "                else data[\"image\"].unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            output_image = sliding_window_inference(input_image, roi_size, sw_batch_size, model)\n",
    "            output_image = output_image.cpu()\n",
    "\n",
    "            end = timer()\n",
    "\n",
    "        print(output_image.mean())\n",
    "\n",
    "        del data\n",
    "        del input_image\n",
    "        del output_image\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        sample_name = sample.split(\"/\")[-1]\n",
    "        if idx > 0:\n",
    "            total_time_dict[sample_name] = end - start\n",
    "            print(end - start)\n",
    "    return total_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Benchmark\n",
    "\n",
    "The cell below will execute the benchmark based on the `benchmark_type` variable.\n",
    "\n",
    "#### Optional: Using the Python Script\n",
    "\n",
    "For convenience, a Python script, [`run_benchmark.py`](./run_benchmark.py), is available to run the benchmark. You can open a terminal and execute the following command to run the benchmark for all benchmark types:\n",
    "\n",
    "\n",
    "```bash\n",
    "for benchmark_type in \"original\" \"trt\" \"trt_gpu_transforms\" \"trt_gds_gpu_transforms\"; do\n",
    "    python run_benchmark.py --benchmark_type \"$benchmark_type\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_transforms_flag = False\n",
    "gpu_loading_flag = False\n",
    "trt_flag = False\n",
    "\n",
    "if \"trt\" in benchmark_type:\n",
    "    trt_flag = True\n",
    "if \"gpu_transforms\" in benchmark_type:\n",
    "    gpu_transforms_flag = True\n",
    "if \"gds\" in benchmark_type:\n",
    "    gpu_loading_flag = True\n",
    "\n",
    "infer_transforms = get_transforms(device, gpu_loading_flag, gpu_transforms_flag)\n",
    "model = get_model(device, weights_path, trt_model_path, trt_flag)\n",
    "\n",
    "total_time_dict = run_inference(train_files, infer_transforms, model, device, benchmark_type)\n",
    "\n",
    "df = pd.DataFrame(list(total_time_dict.items()), columns=[\"file_name\", \"time\"])\n",
    "df.to_csv(os.path.join(root_dir, f\"time_{benchmark_type}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Visualize the Results\n",
    "\n",
    "In this section, we will analyze and visualize the results.\n",
    "All cell outputs presented in this section were obtained by a NVIDIA RTX A6000 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect benchmark results\n",
    "all_df = pd.read_csv(os.path.join(root_dir, f\"time_original.csv\"))\n",
    "all_df.columns = [\"file_name\", \"original_time\"]\n",
    "\n",
    "for benchmark_type in [\"trt\", \"trt_gpu_transforms\", \"trt_gds_gpu_transforms\"]:\n",
    "    df = pd.read_csv(os.path.join(root_dir, f\"time_{benchmark_type}.csv\"))\n",
    "    df.columns = [\"file_name\", f\"{benchmark_type}_time\"]\n",
    "    all_df = pd.merge(all_df, df, on=\"file_name\", how=\"left\")\n",
    "\n",
    "# for each file, add it's size\n",
    "all_df[\"file_size\"] = all_df[\"file_name\"].apply(\n",
    "    lambda x: os.path.getsize(os.path.join(root_dir, \"Task03_Liver\", \"imagesTs_nii\", x))\n",
    ")\n",
    "# sort by file size\n",
    "all_df = all_df.sort_values(by=\"file_size\", ascending=True)\n",
    "# convert file size to MB\n",
    "all_df[\"file_size\"] = all_df[\"file_size\"] / 1024 / 1024\n",
    "# get the average time for each benchmark type\n",
    "average_time = all_df.mean(numeric_only=True)\n",
    "del average_time[\"file_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Average Inference Time for Each Benchmark Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "average_time.plot(kind=\"bar\", color=[\"skyblue\", \"orange\", \"green\", \"red\"])\n",
    "plt.title(\"Average Inference Time for Each Benchmark Type\")\n",
    "plt.xlabel(\"Benchmark Type\")\n",
    "plt.ylabel(\"Average Time (seconds)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvikio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
