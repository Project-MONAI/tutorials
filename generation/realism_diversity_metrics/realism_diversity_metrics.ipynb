{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6161aec",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e837e16",
   "metadata": {},
   "source": [
    "# Evaluate Realism and Diversity of the generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfe817",
   "metadata": {},
   "source": [
    "This notebook illustrates how to use MONAI to compute the most common metrics to evaluate the performance of a generative model. The metrics that we will analyse on this tutorial are:\n",
    "\n",
    "- Frechet Inception Distance (FID) [1] and Maximum Mean Discrepancy (MMD) [2], two metrics commonly used to assess the realism of generated image\n",
    "\n",
    "- the MS-SSIM [3] and SSIM [4] used to evaluate the image diversity\n",
    "\n",
    "Note: We are using the RadImageNet [5] to compute the feature space necessary to compute the FID. So we need to transform the images in the same way they were transformed when the network was trained before computing the FID.\n",
    "\n",
    "[1] - Heusel et al., \"Gans trained by a two time-scale update rule converge to a local nash equilibrium\", https://arxiv.org/pdf/1706.08500.pdf\n",
    "\n",
    "[2] - Gretton et al., \"A Kernel Two-Sample Test\", https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf\n",
    "\n",
    "[3] - Wang et al., \"Multiscale structural similarity for image quality assessment\", https://ieeexplore.ieee.org/document/1292216\n",
    "\n",
    "[4] - Wang et al., \"Image quality assessment: from error visibility to structural similarity\", https://ieeexplore.ieee.org/document/1284395\n",
    "\n",
    "[5] - Mei et al., \"RadImageNet: An Open Radiologic Deep Learning Research Dataset for Effective Transfer Learning, https://pubs.rsna.org/doi/10.1148/ryai.210315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80769612",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c60fc",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import set_determinism\n",
    "import urllib.request\n",
    "\n",
    "from monai.inferers import LatentDiffusionInferer\n",
    "from monai.metrics import FIDMetric, MMDMetric, MultiScaleSSIMMetric, SSIMMetric\n",
    "from monai.networks.nets import AutoencoderKL, DiffusionModelUNet\n",
    "from monai.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620df5c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The transformations defined below are necessary in order to transform the input images in the same way that the images were\n",
    "processed for the RadImageNet train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(x: torch.Tensor) -> torch.Tensor:\n",
    "    mean = [0.406, 0.456, 0.485]\n",
    "    x[:, 0, :, :] -= mean[0]\n",
    "    x[:, 1, :, :] -= mean[1]\n",
    "    x[:, 2, :, :] -= mean[2]\n",
    "    return x\n",
    "\n",
    "\n",
    "def spatial_average(x: torch.Tensor, keepdim: bool = True) -> torch.Tensor:\n",
    "    return x.mean([2, 3], keepdim=keepdim)\n",
    "\n",
    "\n",
    "def get_features(image):\n",
    "    # If input has just 1 channel, repeat channel to have 3 channels\n",
    "    if image.shape[1]:\n",
    "        image = image.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # Change order from 'RGB' to 'BGR'\n",
    "    image = image[:, [2, 1, 0], ...]\n",
    "\n",
    "    # Subtract mean used during training\n",
    "    image = subtract_mean(image)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        feature_image = radnet.forward(image)\n",
    "        # flattens the image spatially\n",
    "        feature_image = spatial_average(feature_image, keepdim=False)\n",
    "\n",
    "    return feature_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbd59a",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the MONAI_DATA_DIRECTORY environment variable.\n",
    "This allows you to save results and reuse downloads.\n",
    "\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b189f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79c501",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5a5d1",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195db858",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderkl = AutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(128, 128, 256),\n",
    "    latent_channels=3,\n",
    "    num_res_blocks=2,\n",
    "    attention_levels=(False, False, False),\n",
    "    with_encoder_nonlocal_attn=False,\n",
    "    with_decoder_nonlocal_attn=False,\n",
    ")\n",
    "autoencoderkl = autoencoderkl.to(device)\n",
    "autoencoderkl.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2424564",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=2,\n",
    "    channels=(128, 256, 512),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_head_channels=(0, 256, 512),\n",
    ")\n",
    "unet = unet.to(device)\n",
    "unet.eval()\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\", beta_start=0.0015, beta_end=0.0195)\n",
    "\n",
    "inferer = LatentDiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d9e13",
   "metadata": {},
   "source": [
    "## Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "unet_url = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/tutorial_generation_unet.pth\"\n",
    "unet_path = unet_url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(unet_url, unet_path)\n",
    "unet.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "print(\"UNet weights loaded\")\n",
    "autoencoderkl_url = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/tutorial_generation_autoencoderkl.pth\"\n",
    "autoencoderkl_path = autoencoderkl_url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(autoencoderkl_url, autoencoderkl_path)\n",
    "autoencoderkl.load_state_dict(torch.load(autoencoderkl_path, weights_only=True))\n",
    "print(\"AutoencoderKL weights loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c187146",
   "metadata": {},
   "source": [
    "## Get the real images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b42415",
   "metadata": {},
   "source": [
    "Similar to the 2D LDM tutorial, we will use the MedNISTDataset, which contains images from different body parts. For easiness, here we will use only the `Hand` class. The first part of the code will get the real images from the MedNISTDataset and apply some transformations to scale the intensity of the image. Because we are evaluating the performance of the trained network, we will only use the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"Hand\"]\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=180, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6facbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some synthetic data for visualisation\n",
    "n_synthetic_images = 3\n",
    "noise = torch.randn((n_synthetic_images, 3, 16, 16))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    syn_images = inferer.sample(input_noise=noise, diffusion_model=unet, scheduler=scheduler, autoencoder_model=autoencoderkl)\n",
    "\n",
    "# Plot 3 examples from the synthetic data\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(syn_images[image_n, 0, :, :].cpu(), cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676aa62",
   "metadata": {},
   "source": [
    "## Compute FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98452f3f",
   "metadata": {},
   "source": [
    "The FID measures the distance between the feature vectors from the real images and those obtained from generated images. In order to compute the FID the images need to be passed into a pre-trained network to get the desired feature vectors. Although the FID is commonly computed using the Inception network, here, we used a pre-trained version of the RadImageNet to calculate the feature space. Lower FID scores indicate that the images are more similar, with a perfect score being 0 indicating that the two groups of images are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "radnet = torch.hub.load(\"Warvito/radimagenet-models\", model=\"radimagenet_resnet50\", verbose=True)\n",
    "radnet.to(device)\n",
    "radnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9faca46",
   "metadata": {},
   "source": [
    "Here, we will load the real and generate synthetic images from noise and compute the FID of these two groups of images. Because we are generating the synthetic images on this code snippet the entire cell will take about 15 mins run and most of this time is spent in generating the images. The loading bars show how long it will take to complete the image generation for each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48d18c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "synth_features = []\n",
    "real_features = []\n",
    "\n",
    "for step, x in enumerate(val_loader):\n",
    "    # Get the real images\n",
    "    real_images = x[\"image\"].to(device)\n",
    "\n",
    "    # Generate some synthetic images using the defined model\n",
    "    n_synthetic_images = len(x[\"image\"])\n",
    "    noise = torch.randn((n_synthetic_images, 3, 64, 64))\n",
    "    noise = noise.to(device)\n",
    "    scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        syn_images = inferer.sample(input_noise=noise, diffusion_model=unet, scheduler=scheduler, autoencoder_model=autoencoderkl)\n",
    "\n",
    "        # Get the features for the real data\n",
    "        real_eval_feats = get_features(real_images)\n",
    "        real_features.append(real_eval_feats)\n",
    "\n",
    "        # Get the features for the synthetic data\n",
    "        synth_eval_feats = get_features(syn_images)\n",
    "        synth_features.append(synth_eval_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_features = torch.vstack(synth_features)\n",
    "real_features = torch.vstack(real_features)\n",
    "\n",
    "fid = FIDMetric()\n",
    "fid_res = fid(synth_features, real_features)\n",
    "\n",
    "print(f\"FID Score: {fid_res.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the synthetic data\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(syn_images[image_n, 0, :, :].cpu(), cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4e62d",
   "metadata": {},
   "source": [
    "# Compute MMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa01253",
   "metadata": {},
   "source": [
    "Because the realism of the LDMs will depend on the realism of the autoencoder reconstructions, we will compute the MMD betweeen the original images and the reconstructed images to evaluate the performance of the autoencoder.\n",
    "\n",
    "MMD (Maximum Mean Discrepancy) is a distance metric used to measure the similarity between two probability distributions. This metric maps the samples from each distribution to a high-dimensional feature space and calculates the distance between the mean of the features of each distribution. A smaller MMD value indicates a better match between the real and generated distributions. It is often used in combination with other evaluation metrics to assess the performance of a generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d92309",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_scores = []\n",
    "\n",
    "mmd = MMDMetric()\n",
    "\n",
    "for step, x in list(enumerate(val_loader)):\n",
    "    image = x[\"image\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_recon = autoencoderkl.reconstruct(image)\n",
    "\n",
    "    mmd_scores.append(mmd(image, image_recon))\n",
    "\n",
    "mmd_scores = torch.stack(mmd_scores)\n",
    "print(f\"MS-SSIM score: {mmd_scores.mean().item():.4f} +- {mmd_scores.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f914c",
   "metadata": {},
   "source": [
    "# Compute MultiScaleSSIMMetric and SSIMMetric\n",
    "\n",
    "SSIM measures the similarity between two images based on three components: luminance, contrast, and structure. In addition, MS-SSIM is an extension of SSIM that computes the structural similarity measure at multiple scales. Both metrics can assume values between 0 and 1, where 1 indicates perfect similarity between the images.\n",
    "\n",
    "There are two ways to compute the MS-SSIM and SSIM, and in this notebook we will look at both ways:\n",
    "1. Use the reconstructions of the autoencoder and the real images. By using the metric this way we can assess the performance of the autoencoder.\n",
    "2. Compute the MS-SSIM and SSIM between pairs of synthetic images. This second way of computing the MS-SSIM can be used as a metric to evaluate the diversity of the synthetic images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd139cb5",
   "metadata": {},
   "source": [
    "In this section we will compute the MS-SSIM and SSIM Meteric between the real images and those reconstructed by the AutoencoderKL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_ssim_recon_scores = []\n",
    "ssim_recon_scores = []\n",
    "\n",
    "ms_ssim = MultiScaleSSIMMetric(spatial_dims=2, data_range=1.0, kernel_size=4)\n",
    "ssim = SSIMMetric(spatial_dims=2, data_range=1.0, kernel_size=4)\n",
    "\n",
    "for step, x in list(enumerate(val_loader)):\n",
    "    image = x[\"image\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_recon = autoencoderkl.reconstruct(image)\n",
    "\n",
    "    ms_ssim_recon_scores.append(ms_ssim(image, image_recon))\n",
    "    ssim_recon_scores.append(ssim(image, image_recon))\n",
    "\n",
    "ms_ssim_recon_scores = torch.cat(ms_ssim_recon_scores, dim=0)\n",
    "ssim_recon_scores = torch.cat(ssim_recon_scores, dim=0)\n",
    "\n",
    "print(f\"MS-SSIM Metric: {ms_ssim_recon_scores.mean():.7f} +- {ms_ssim_recon_scores.std():.7f}\")\n",
    "print(f\"SSIM Metric: {ssim_recon_scores.mean():.7f} +- {ssim_recon_scores.std():.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad94fd",
   "metadata": {},
   "source": [
    "Compute the SSIM and MS-SSIM between pairs of synthetic images, the results of the MS-SSIM and SSIM can be used to evaluate the diversity of the synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e189159",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ms_ssim_scores = []\n",
    "ssim_scores = []\n",
    "\n",
    "# How many synthetic images we want to generate\n",
    "n_synthetic_images = 100\n",
    "\n",
    "# Generate some synthetic images using the defined model\n",
    "noise = torch.randn((n_synthetic_images, 3, 64, 64))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=25)\n",
    "\n",
    "with torch.no_grad():\n",
    "    syn_images = inferer.sample(input_noise=noise, diffusion_model=unet, scheduler=scheduler)\n",
    "\n",
    "    idx_pairs = list(combinations(range(n_synthetic_images), 2))\n",
    "    for idx_a, idx_b in idx_pairs:\n",
    "        ms_ssim_scores.append(ms_ssim(syn_images[[idx_a]], syn_images[[idx_b]]))\n",
    "        ssim_scores.append(ssim(syn_images[[idx_a]], syn_images[[idx_b]]))\n",
    "\n",
    "\n",
    "ms_ssim_scores = torch.cat(ms_ssim_scores, dim=0)\n",
    "ssim_scores = torch.cat(ssim_scores, dim=0)\n",
    "\n",
    "print(f\"MS-SSIM Metric: {ms_ssim_scores.mean():.4f} +- {ms_ssim_scores.std():.4f}\")\n",
    "print(f\"SSIM Metric: {ssim_scores.mean():.4f} +- {ssim_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd99f0d",
   "metadata": {},
   "source": [
    "# Clean-up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
