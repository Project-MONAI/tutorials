{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepEdit Inference Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepEdit is an algorithm that combines the power of two models in one single architecture. It allows the user to perform inference, as a standard segmentation method (i.e. UNet), and also to interactively segment part of an image using clicks (Sakinis et al.). DeepEdit aims to facilitate the user experience and at the same time the development of new active learning techniques.\n",
    "\n",
    "\n",
    "This Notebooks shows the performance of a model trained to segment the spleen. \n",
    "\n",
    "**We recommend importing the pretrained model into the [DeepEdit App in MONAI Label](https://github.com/Project-MONAI/MONAILabel/tree/main/sample-apps/radiology#deepedit) for full experience.**\n",
    "\n",
    "Sakinis et al., Interactive segmentation of medical images through fully convolutional neural networks. (2019) https://arxiv.org/abs/1903.08205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using library versions:\n",
    "\n",
    "monai==0.8.1 nibabel==3.2.2 numpy==1.22.3 pytorch-ignite==0.4.8 scikit-image==0.19.2 scipy==1.8.0 tensorboard==2.8.0 torch==1.11.0 tqdm==4.64.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import jit\n",
    "\n",
    "import monai\n",
    "\n",
    "from monai.apps.deepedit.transforms import (\n",
    "    AddGuidanceSignalCustomd,\n",
    "    AddGuidanceFromPointsCustomd,\n",
    "    ResizeGuidanceMultipleLabelCustomd,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    SqueezeDimd,\n",
    "    ToNumpyd,\n",
    "    ToTensord,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(guidance, slice_idx):\n",
    "    if guidance is None:\n",
    "        return\n",
    "    for p in guidance:\n",
    "        p1 = p[1]\n",
    "        p2 = p[0]\n",
    "        plt.plot(p1, p2, \"r+\", 'MarkerSize', 30)\n",
    "\n",
    "\n",
    "def show_image(image, label, guidance=None, slice_idx=None):\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    if label is not None:\n",
    "        masked = np.ma.masked_where(label == 0, label)\n",
    "        plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "\n",
    "    draw_points(guidance, slice_idx)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if label is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"label\")\n",
    "        plt.imshow(label)\n",
    "        plt.colorbar()\n",
    "        # draw_points(guidance, slice_idx)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_data(data):\n",
    "    for k in data:\n",
    "        v = data[k]\n",
    "\n",
    "        d = type(v)\n",
    "        if type(v) in (int, float, bool, str, dict, tuple):\n",
    "            d = v\n",
    "        elif hasattr(v, 'shape'):\n",
    "            d = v.shape\n",
    "\n",
    "        if k in ('image_meta_dict', 'label_meta_dict'):\n",
    "            for m in data[k]:\n",
    "                print('{} Meta:: {} => {}'.format(k, m, data[k][m]))\n",
    "        else:\n",
    "            print('Data key: {} = {}'.format(k, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and model\n",
    "\n",
    "resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/_image.nii.gz\"\n",
    "dst = \"_image.nii.gz\"\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    monai.apps.download_url(resource, dst)\n",
    "\n",
    "resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/pretrained_deepedit_dynunet-final.ts\"\n",
    "dst = \"pretrained_deepedit_dynunet-final.ts\"\n",
    "if not os.path.exists(dst):\n",
    "    monai.apps.download_url(resource, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels = {'spleen': 1,\n",
    "          'background': 0\n",
    "          }\n",
    "\n",
    "# Pre Processing\n",
    "spatial_size = [128, 128, 128]\n",
    "\n",
    "data = {\n",
    "    'image': '_image.nii.gz',\n",
    "    'guidance': {'spleen': [[66, 180, 105], [66, 180, 145]], 'background': []},\n",
    "}\n",
    "\n",
    "slice_idx = original_slice_idx = data['guidance']['spleen'][0][2]\n",
    "\n",
    "pre_transforms = [\n",
    "                # Loading the image\n",
    "                LoadImaged(keys=\"image\", reader=\"ITKReader\"),\n",
    "                # Ensure channel first\n",
    "                EnsureChannelFirstd(keys=\"image\"),\n",
    "                # Change image orientation\n",
    "                Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
    "                # Scaling image intensity - works well for CT images\n",
    "                ScaleIntensityRanged(keys=\"image\", a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "                # DeepEdit Tranforms for Inference #\n",
    "                # Add guidance (points) in the form of tensors based on the user input\n",
    "                AddGuidanceFromPointsCustomd(ref_image=\"image\", guidance=\"guidance\", label_names=labels),\n",
    "                # Resize the image\n",
    "                Resized(keys=\"image\", spatial_size=spatial_size, mode=\"area\"),\n",
    "                # Resize the guidance based on the image resizing\n",
    "                ResizeGuidanceMultipleLabelCustomd(guidance=\"guidance\", ref_image=\"image\"),\n",
    "                # Add the guidance to the input image\n",
    "                AddGuidanceSignalCustomd(keys=\"image\", guidance=\"guidance\"),\n",
    "                # Convert image to tensor \n",
    "                ToTensord(keys=\"image\"),\n",
    "                ]\n",
    "\n",
    "original_image = None\n",
    "\n",
    "# Going through each of the pre_transforms\n",
    "for t in pre_transforms:\n",
    "    \n",
    "    tname = type(t).__name__\n",
    "    data = t(data)         \n",
    "    image = data['image']\n",
    "    label = data.get('label')\n",
    "    guidance = data.get('guidance')\n",
    "\n",
    "    print(\"{} => image shape: {}\".format(tname, image.shape))\n",
    "    \n",
    "    if tname == 'LoadImaged':\n",
    "        original_image = data['image']\n",
    "        label = None\n",
    "        tmp_image = image[:, :, slice_idx]  \n",
    "        show_image(tmp_image, label, [guidance['spleen'][0]], slice_idx)\n",
    "          \n",
    "transformed_image = data['image']\n",
    "guidance = data.get('guidance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_path = 'pretrained_deepedit_dynunet-final.ts'\n",
    "model = jit.load(model_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "inputs = data['image'][None].cuda()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "outputs = outputs[0]\n",
    "data['pred'] = outputs\n",
    "\n",
    "post_transforms = [\n",
    "                    EnsureTyped(keys=\"pred\"),\n",
    "                    Activationsd(keys=\"pred\", softmax=True),\n",
    "                    AsDiscreted(keys=\"pred\", argmax=True),\n",
    "                    SqueezeDimd(keys=\"pred\", dim=0),\n",
    "                    ToNumpyd(keys=\"pred\"),\n",
    "]\n",
    "\n",
    "pred = None\n",
    "for t in post_transforms:\n",
    "    \n",
    "    tname = type(t).__name__\n",
    "    data = t(data)\n",
    "    image = data['image']\n",
    "    label = data['pred']\n",
    "    print(\"{} => image shape: {}, pred shape: {}\".format(tname, image.shape, label.shape))\n",
    "    \n",
    "for i in range(data['pred'].shape[0]):\n",
    "    image = transformed_image[0, :, :, i]  # Taking the first channel which is the main image\n",
    "    label = data['pred'][:, :, i]\n",
    "    if np.sum(label) == 0:\n",
    "        continue\n",
    "\n",
    "    print(\"Final PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format(\n",
    "        i, image.shape, label.shape, np.min(label), np.max(label), np.sum(label)))\n",
    "    show_image(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove downloaded files\n",
    "os.remove('_image.nii.gz')\n",
    "os.remove('pretrained_deepedit_dynunet-final.ts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
