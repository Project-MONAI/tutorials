{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0+63.g5feb3530\n",
      "Numpy version: 1.25.1\n",
      "Pytorch version: 2.0.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 5feb353030e0bb204e21e1de338cd81b5972bb8a\n",
      "MONAI __file__: /Users/hung.nh/codespace/yauangon/MONAI/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.5\n",
      "pandas version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import BasicUnetPlusPlus\n",
    "import monai\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "monai.config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['INSTANCE', 'BATCH', 'GROUP', 'LAYER', 'LOCALRESPONSE', 'SYNCBATCH', 'INSTANCE_NVFUSER'])\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "Exception layer: __init__() missing 1 required positional argument: 'num_groups'\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "Exception layer: __init__() missing 1 required positional argument: 'normalized_shape'\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "Exception layer: __init__() missing 1 required positional argument: 'size'\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hung.nh/codespace/yauangon/MONAI/monai/networks/layers/factories.py:270: UserWarning: `apex.normalization.InstanceNorm3dNVFuser` is not installed properly, use nn.InstanceNorm3d instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int = 3,\n",
    "#         in_channels: int = 1,\n",
    "#         out_channels: int = 2,\n",
    "#         features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "#         deep_supervision: bool = False,\n",
    "#         act: str | tuple = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "#         norm: str | tuple = (\"instance\", {\"affine\": True}),\n",
    "#         bias: bool = True,\n",
    "#         dropout: float | tuple = 0.0,\n",
    "#         upsample: str = \"deconv\",\n",
    "#     ):\n",
    "print(monai.networks.layers.factories.Norm.factories.keys())\n",
    "for norm_layer in monai.networks.layers.factories.Norm.factories.keys():\n",
    "    try:\n",
    "        model = BasicUnetPlusPlus(  \n",
    "            spatial_dims=3,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            features=(32, 32, 64, 128, 256, 32),\n",
    "            # norm='localresponse',\n",
    "            norm=norm_layer,\n",
    "        )\n",
    "    except Exception as msg:\n",
    "        print(f\"Exception layer: {msg}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "UNET++ use the same `TwoConv`, `Down`, and `UpCat` as UNet. Therefore, you can referred to the `modules/UNet_input_size_constrains.ipynb` for break down analysis. For summary, the constraints for these types of normalization are:\n",
    "\n",
    "- Instance Norm: the product of spatial dimension must > 1 (not include channel and batch)\n",
    "- Batch Norm: the product of spatial dimension must > 1 (not include channel and batch). For training best interested, `batch_size` should be larger than 1\n",
    "- Local Response Norm: No constraint.\n",
    "- Other Normalization: please referred to `modules/UNet_input_size_constrains.ipynb`\n",
    "\n",
    "As for UNET++ have 4 down-sampling blocks with 2x kernel size, with no argument to change this behavior, the smallest edge we can have is `2**4 = 16`, and after the last down-sampling block, the `vector.shape  = [..., ..., 1, 1]` or (`[..., ..., 1, 1, 1]` for 3D), which will cause error for the Normalization layer.\n",
    "\n",
    "See the test code below for examples of batch norm and instance norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "========== USING NORM LAYER: INSTANCE ==========\n",
      "__________ Changing the H dimension of 2D input __________\n",
      ">> Using Input.shape=torch.Size([1, 3, 16, 16])\n",
      ">>>> Exception: Expected more than 1 spatial element when training, got input size torch.Size([1, 256, 1, 1])\n",
      "\n",
      ">> Using Input.shape=torch.Size([1, 3, 32, 16])\n",
      "__________ Changing the batch size __________\n",
      ">> Input.shape=torch.Size([1, 3, 16, 16])\n",
      ">> Exception: Expected more than 1 spatial element when training, got input size torch.Size([1, 256, 1, 1])\n",
      "\n",
      ">> Input.shape=torch.Size([2, 3, 16, 16])\n",
      ">> Exception: Expected more than 1 spatial element when training, got input size torch.Size([2, 256, 1, 1])\n",
      "\n",
      "========== USING NORM LAYER: BATCH ==========\n",
      "__________ Changing the H dimension of 2D input __________\n",
      ">> Using Input.shape=torch.Size([1, 3, 16, 16])\n",
      ">>>> Exception: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "\n",
      ">> Using Input.shape=torch.Size([1, 3, 32, 16])\n",
      "__________ Changing the batch size __________\n",
      ">> Input.shape=torch.Size([1, 3, 16, 16])\n",
      ">> Exception: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n",
      "\n",
      ">> Input.shape=torch.Size([2, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def make_model_with_layer(layer_norm):\n",
    "    return BasicUnetPlusPlus(\n",
    "        spatial_dims=2,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        features=(32, 32, 64, 128, 256, 32),\n",
    "        norm=layer_norm\n",
    "    )\n",
    "\n",
    "def test_min_dim():\n",
    "    MIN_EDGE = 16\n",
    "    batch_size, spatial_dim, H, W = 1, 3, MIN_EDGE, MIN_EDGE\n",
    "    MODEL_BY_NORM_LAYER: Dict[str, BasicUnetPlusPlus] = {}\n",
    "    print(\"Prepare model\")\n",
    "    for norm_layer in ['instance', 'batch']:\n",
    "        MODEL_BY_NORM_LAYER[norm_layer] = make_model_with_layer(norm_layer)\n",
    "        \n",
    "    # print(f\"Input dimension {(batch_size, spatial_dim, H, W)} that will cause error\")\n",
    "    for norm_layer in ['instance', 'batch']:\n",
    "        print(\"=\"*10 + f\" USING NORM LAYER: {norm_layer.upper()} \" + \"=\"*10)\n",
    "        model = MODEL_BY_NORM_LAYER[norm_layer]\n",
    "        print(\"_\" * 10 + \" Changing the H dimension of 2D input \" + \"_\" * 10)\n",
    "        for _H_temp in [H, H*2]:\n",
    "            try:\n",
    "                x = torch.ones(batch_size, spatial_dim, _H_temp, W)\n",
    "                print(f\">> Using Input.shape={x.shape}\")\n",
    "                model(x)\n",
    "            except Exception as msg:\n",
    "                print(f\">>>> Exception: {msg}\\n\")\n",
    "\n",
    "        # print(\"Changing the batch size\")\n",
    "        print(\"_\" * 10 + \" Changing the batch size \" + \"_\" * 10)\n",
    "        for batch_size_tmp in [1, 2]:\n",
    "            try:\n",
    "                x = torch.ones(batch_size_tmp, spatial_dim, H, W)\n",
    "                print(f\">> Input.shape={x.shape}\")\n",
    "                model(x)\n",
    "            except Exception as msg:\n",
    "                print(f\">> Exception: {msg}\\n\")\n",
    "    pass\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_min_dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling (down-sampling) and Up-sampling\n",
    "\n",
    "While all the internal padding is handled to ensure the input spatial shape is the same as the output spatial shape, you still need to aware about these implicit processing:\n",
    "\n",
    "- For down-scale path: MaxPooling with `kernel_size` and `strides` = 2. Any odd size will have the last element ignored. See below code block for the pooling with odd shape\n",
    "- For up-scale path, depend on the up-samping mode, using `UpCat` module with auto padding (`replicate` mode) the up-sampled path to the same input shape as input path. See this code of the `UpCat`\n",
    "\n",
    "```[python]\n",
    "def forward(self, x: torch.Tensor, x_e: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: features to be upsampled.\n",
    "            x_e: optional features from the encoder, if None, this branch is not in use.\n",
    "        \"\"\"\n",
    "        x_0 = self.upsample(x)\n",
    "\n",
    "        if x_e is not None and torch.jit.isinstance(x_e, torch.Tensor):\n",
    "            if self.is_pad: # alway True\n",
    "                # handling spatial shapes due to the 2x max-pooling with odd edge lengths.\n",
    "                dimensions = len(x.shape) - 2\n",
    "                sp = [0] * (dimensions * 2)\n",
    "                for i in range(dimensions):\n",
    "                    if x_e.shape[-i - 1] != x_0.shape[-i - 1]:\n",
    "                        sp[i * 2 + 1] = 1\n",
    "                x_0 = torch.nn.functional.pad(x_0, sp, \"replicate\")\n",
    "            x = self.convs(torch.cat([x_e, x_0], dim=1))  # input channels: (cat_chns + up_chns)\n",
    "        else:\n",
    "            x = self.convs(x_0)\n",
    "\n",
    "        return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_pool.shape=torch.Size([1, 1, 5, 5]), pooled.shape=torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[ 1.,  2.,  3.,  4.,  5.],\n",
      "          [ 2.,  4.,  6.,  8., 10.],\n",
      "          [ 3.,  6.,  9., 12., 15.],\n",
      "          [ 4.,  8., 12., 16., 20.],\n",
      "          [ 5., 10., 15., 20., 25.]]]])\n",
      "tensor([[[[ 4.,  8.],\n",
      "          [ 8., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example about the pooling with odd shape\n",
    "un_pool = torch.Tensor(\n",
    "    [[1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5]]\n",
    ") * torch.Tensor([1, 2, 3, 4, 5])[..., None]\n",
    "un_pool = un_pool[None, None, ...]\n",
    "\n",
    "pooled = nn.MaxPool2d(kernel_size=2)(un_pool)\n",
    "print(f\"{un_pool.shape=}, {pooled.shape=}\")\n",
    "print(un_pool)\n",
    "print(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNetPlusPlus features: (32, 32, 64, 128, 256, 32).\n",
      "Input x.shape=torch.Size([2, 3, 33, 17, 17])\n",
      "x_1_0.shape=torch.Size([2, 32, 16, 8, 8])\n",
      "x_0_1.shape=torch.Size([2, 32, 33, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# # @torch.no_grad()\n",
    "# def test():\n",
    "#     model = BasicUnetPlusPlus(\n",
    "#         spatial_dims=3,\n",
    "#         in_channels=3,\n",
    "#         out_channels=3,\n",
    "#         features=(32, 32, 64, 128, 256, 32),\n",
    "#         deep_supervision=True,\n",
    "#     )\n",
    "#     batch_size, spatial_dim, D, H, W = 2, 3, 16 * 2 + 1, 16 + 1, 16 + 1\n",
    "#     x = torch.ones(\n",
    "#         [batch_size, spatial_dim, D, H, W]\n",
    "#     )\n",
    "#     # y = model.forward(x)\n",
    "#     # print(f\"{x.shape=}\")  \n",
    "#     # print(f\"{[o.shape for o in y]=}\")\n",
    "    \n",
    "#     return model, x\n",
    "\n",
    "# # Loss edge info. if input dim is not divisible by 2 when pooling\n",
    "# # Ensure the lowest image dimension\n",
    "# # As the lowest dim before the norm is [1x1x1] \n",
    "# # -> instance and batch norm don't allow that, so make it at least\n",
    "# # [2x1x1]\n",
    "# from monai.networks.blocks import ADN\n",
    "# with torch.no_grad():\n",
    "#     model, x = test()\n",
    "#     print(f\"Input {x.shape=}\")\n",
    "#     model.eval()\n",
    "#     # 2 conv\n",
    "#     x_0_0 = model.conv_0_0(x)     \n",
    "#     # down conv\n",
    "#     x_1_0 = model.conv_1_0(x_0_0) \n",
    "#     print(f\"{x_1_0.shape=}\")\n",
    "#     x_0_1 = model.upcat_0_1(x_1_0, x_0_0)\n",
    "#     print(f\"{x_0_1.shape=}\")\n",
    "#     # x_2_0 = model.conv_2_0(x_1_0) \n",
    "#     # print(f\"{x_2_0.shape=}\")\n",
    "#     # x_3_0 = model.conv_3_0(x_2_0) \n",
    "#     # print(f\"{x_3_0.shape=}\") # 2 x 2 -> 1 x 1\n",
    "#     # pooled = nn.MaxPool3d(kernel_size=2)(x_3_0)\n",
    "#     # print(f\"{pooled.shape=}\") # 2 x 2 -> 1 x 1\n",
    "#     # conved1 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)(pooled)\n",
    "#     # conved2 = nn.Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)(conved1)\n",
    "#     # print(f\"{conved1.shape=}\") # 2 x 2 -> 1 x 1\n",
    "#     # print(f\"{conved2.shape=}\") # 2 x 2 -> 1 x 1\n",
    "    \n",
    "#     # normed = ADN(\n",
    "#     #     ordering='NDA',\n",
    "#     #     in_channels=256,\n",
    "#     #     norm='batch',\n",
    "#     #     norm_dim=3,\n",
    "#     #     # dropout=dropout,\n",
    "#     #     # dropout_dim=dropout_dim,\n",
    "#     # )(conved2)\n",
    "#     # print(f\"{normed.shape=}\") # 2 x 2 -> 1 x 1\n",
    "\n",
    "#     # x_4_0 = model.conv_4_0(x_3_0) \n",
    "#     # print(f\"{x_4_0.shape=}\")\n",
    "\n",
    "#     # Up path\n",
    "#     # x_3_0 = model.conv_3_0(x_2_0)\n",
    "#     # print(f\"{x_1_0.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_pool.shape=torch.Size([1, 1, 5, 5]), pooled.shape=torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[ 1.,  2.,  3.,  4.,  5.],\n",
      "          [ 2.,  4.,  6.,  8., 10.],\n",
      "          [ 3.,  6.,  9., 12., 15.],\n",
      "          [ 4.,  8., 12., 16., 20.],\n",
      "          [ 5., 10., 15., 20., 25.]]]])\n",
      "tensor([[[[ 4.,  8.],\n",
      "          [ 8., 16.]]]])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
