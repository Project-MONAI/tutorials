{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ],
   "id": "acf7c55ccff728d3"
  },
  {
   "cell_type": "markdown",
   "id": "63d95da6",
   "metadata": {},
   "source": [
    "# Diffusion Models for Medical Anomaly Detection with Classifier Guidance\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a 2D gradient-guided anomaly detection using DDIMs [1].\n",
    "\n",
    "We train a diffusion model on 2D slices of brain MR images. A classification model is trained to predict whether the given slice shows a tumor or not.\\\n",
    "We then translate an input slice to its healthy reconstruction using DDIMs.\\\n",
    "Anomaly detection is performed by taking the difference between input and output, as proposed in [1].\n",
    "\n",
    "[1] - Wolleb et al. \"Diffusion Models for Medical Anomaly Detection\" https://arxiv.org/abs/2203.04306\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "75f2d5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:19.862069Z",
     "start_time": "2024-09-05T17:10:12.949753Z"
    }
   },
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import seaborn\" || pip install -q seaborn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6b766027",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "972ed3f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:22.562556Z",
     "start_time": "2024-09-05T17:10:19.865516Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.inferers import DiffusionInferer\n",
    "from monai.networks.nets.diffusion_model_unet import DiffusionModelEncoder, DiffusionModelUNet\n",
    "from monai.networks.schedulers.ddim import DDIMScheduler\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "print_config()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2434\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.4.0+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: dc611d231ba670004b1da1b011fe140375fb91af\n",
      "MONAI __file__: /home/<username>/monaigen-tutorials-v2/venv/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.17.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7d4ff515",
   "metadata": {},
   "source": [
    "## Setup data directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b4323e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:22.569073Z",
     "start_time": "2024-09-05T17:10:22.564517Z"
    }
   },
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "99175d50",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "id": "34ea510f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:22.592212Z",
     "start_time": "2024-09-05T17:10:22.571614Z"
    }
   },
   "source": [
    "set_determinism(42)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "c3f70dd1-236a-47ff-a244-575729ad92ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing of the BRATS Dataset in 2D slices for training\n",
    "We download the BRATS training dataset from the Decathlon dataset. \\\n",
    "We slice the volumes in axial 2D slices, and assign slice-wise labels (0 for healthy, 1 for diseased) to all slices.\n",
    "Here we use transforms to augment the training dataset:\n",
    "\n",
    "1. `LoadImaged` loads the brain MR images from files.\n",
    "2. `EnsureChannelFirstd` ensures the original data to construct \"channel first\" shape.\n",
    "2. `ScaleIntensityRangePercentilesd` takes the lower and upper intensity percentiles and scales them to [0, 1].\n",
    "3. The first `Lambdad` transform selects a pre-defined channel, in this case, channel zero, representing the FLAIR images.\n",
    "4. The `CopyItemsd` transform creates a copy of the label called slice_label\n",
    "5. The final `Lambdad` transform sets the slice_label variable to one if there are no non-zero pixels in it, and zero otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c68d2d91-9a0b-4ac1-ae49-f4a64edbd82a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:27.136634Z",
     "start_time": "2024-09-05T17:10:27.123272Z"
    }
   },
   "source": [
    "channel = 0  # 0 = Flair\n",
    "assert channel in [0, 1, 2, 3], \"Choose a valid channel\"\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        transforms.Lambdad(keys=[\"image\"], func=lambda x: x[channel, :, :, :]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        transforms.EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        transforms.Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        transforms.Spacingd(keys=[\"image\", \"label\"], pixdim=(3.0, 3.0, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(64, 64, 44)),\n",
    "        transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
    "        transforms.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=(64, 64, 1), random_size=False),\n",
    "        transforms.Lambdad(keys=[\"image\", \"label\"], func=lambda x: x.squeeze(-1)),\n",
    "        transforms.CopyItemsd(keys=[\"label\"], times=1, names=[\"slice_label\"]),\n",
    "        transforms.Lambdad(keys=[\"slice_label\"], func=lambda x: 0.0 if x.sum() > 0 else 1.0),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "da1927b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-05T17:36:21.846929Z",
     "start_time": "2024-09-05T17:10:28.093771Z"
    }
   },
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    section=\"training\",  # validation\n",
    "    cache_rate=1.0,  # you may need a few Gb of RAM... Set to 0 otherwise\n",
    "    num_workers=4,\n",
    "    download=True,  # Set download to True if the dataset hasnt been downloaded yet\n",
    "    seed=0,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "print(f\"Length of training data: {len(train_ds)}\")  # this gives the number of patients in the training set\n",
    "print(f'Train image shape {train_ds[0][\"image\"].shape}')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, persistent_workers=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task01_BrainTumour.tar: 7.09GB [22:27, 5.65MB/s]                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05 18:32:55,570 - INFO - Downloaded: /tmp/tmp91o789q_/Task01_BrainTumour.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05 18:33:05,864 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2024-09-05 18:33:05,865 - INFO - Writing into directory: /tmp/tmp91o789q_.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 388/388 [03:08<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 388\n",
      "Train image shape torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fac55e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing of the BRATS Dataset in 2D slices for validation\n",
    "We download the BRATS validation dataset from the Decathlon dataset, and define the dataloader to load 2D slices for validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73d72110-a8b3-4e03-91cc-1dab4d5a7b87",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-09-05T17:41:38.893866Z",
     "start_time": "2024-09-05T17:40:52.303514Z"
    }
   },
   "source": [
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    section=\"validation\",\n",
    "    cache_rate=1.0,  # you may need a few Gb of RAM... Set to 0 otherwise\n",
    "    num_workers=4,\n",
    "    download=False,  # Set download to True if the dataset hasnt been downloaded yet\n",
    "    seed=0,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "print(f\"Length of training data: {len(val_ds)}\")\n",
    "print(f'Validation Image shape {val_ds[0][\"image\"].shape}')\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True, persistent_workers=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96/96 [00:46<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 96\n",
      "Validation Image shape torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "08428bc6",
   "metadata": {},
   "source": [
    "## Define network, scheduler, optimizer, and inferer\n",
    "At this step, we instantiate the MONAI components to create a DDIM, the UNET, the noise scheduler, and the inferer used for training and sampling. We are using\n",
    "the deterministic DDIM scheduler containing 1000 timesteps, and a 2D UNET with attention mechanisms\n",
    "in the 3rd level (`num_head_channels=64`).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bee5913e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-09-05T17:41:44.233076Z",
     "start_time": "2024-09-05T17:41:43.838660Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(64, 64, 64),\n",
    "    attention_levels=(False, False, True),\n",
    "    num_res_blocks=1,\n",
    "    num_head_channels=64,\n",
    "    with_conditioning=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2a4d3ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training of the diffusion model\n",
    "We train our diffusion model for 2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "id": "6c0ed909",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "is_executing": true
    },
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "start_time": "2024-09-05T17:41:45.486475Z"
    }
   },
   "source": [
    "max_epochs = 2000\n",
    "val_interval = 20\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        classes = data[\"slice_label\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        timesteps = torch.randint(0, 1000, (len(images),)).to(device)  # pick a random time step t\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "\n",
    "        for step, data in enumerate(val_loader):\n",
    "            images = data[\"image\"].to(device)\n",
    "            classes = data[\"slice_label\"].to(device)\n",
    "            timesteps = torch.randint(0, 1000, (len(images),)).to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    noise = torch.randn_like(images).to(device)\n",
    "                    noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        print(\"Epoch\", epoch, \"Validation loss\", val_epoch_loss / (step + 1))\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train diffusion completed, total time: {total_time}.\")\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.title(\"Learning Curves Diffusion Model\", fontsize=20)\n",
    "plt.plot(np.linspace(1, max_epochs, max_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, max_epochs, int(max_epochs / val_interval)),\n",
    "    val_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2597919/631147426.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_2597919/631147426.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "/tmp/ipykernel_2597919/631147426.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation loss 0.984978199005127\n",
      "Epoch 20 Validation loss 0.4565010666847229\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "326101ed-333b-44a9-933f-55760b5d93a4",
   "metadata": {},
   "source": [
    "## Check the performance of the diffusion model\n",
    "\n",
    "We generate a random image from noise to check whether our diffusion model works properly for an image generation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f7a9e99-a8a4-4c8f-a42f-17ef91b18585",
   "metadata": {
    "lines_to_next_cell": 2,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model.eval()\n",
    "noise = torch.randn((1, 1, 64, 64))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "with autocast(enabled=True):\n",
    "    image, intermediates = inferer.sample(\n",
    "        input_noise=noise, diffusion_model=model, scheduler=scheduler, save_intermediates=True, intermediate_steps=100\n",
    "    )\n",
    "\n",
    "chain = torch.cat(intermediates, dim=-1)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "546f9983-c2e2-4c24-b03a-ebe34627638a",
   "metadata": {},
   "source": [
    "## Define the classification model\n",
    "First, we define the classification model. It follows the encoder architecture of the diffusion model, combined with linear layers for binary classification between healthy and diseased slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "44cc6928-2525-4e61-8805-15b409097bbb",
   "metadata": {
    "lines_to_next_cell": 2,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "device = torch.device(\"cuda\")\n",
    "classifier = DiffusionModelEncoder(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    num_channels=(32, 64, 64),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_res_blocks=(1, 1, 1),\n",
    "    num_head_channels=64,\n",
    "    with_conditioning=False,\n",
    ")\n",
    "\n",
    "classifier.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45fab83a-b4c8-42cb-96c9-4e9f1e191111",
   "metadata": {},
   "source": [
    "## Model training of the classification model\n",
    "We train our classification model for 1000 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "de18d5cb-68e7-407c-afe9-8efd7a5a904a",
   "metadata": {
    "lines_to_next_cell": 0,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "max_epochs = 1000\n",
    "val_interval = 10\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "optimizer_cls = torch.optim.Adam(params=classifier.parameters(), lr=2.5e-5)\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        classes = data[\"slice_label\"].to(device)\n",
    "        # classes[classes==2]=0\n",
    "\n",
    "        optimizer_cls.zero_grad(set_to_none=True)\n",
    "        timesteps = torch.randint(0, 1000, (len(images),)).to(device)\n",
    "\n",
    "        with autocast(enabled=False):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Get model prediction\n",
    "            noisy_img = scheduler.add_noise(images, noise, timesteps)  # add t steps of noise to the input image\n",
    "            pred = classifier(noisy_img, timesteps)\n",
    "\n",
    "            loss = F.cross_entropy(pred, classes.long())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_cls.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        classifier.eval()\n",
    "        val_epoch_loss = 0\n",
    "\n",
    "        for step, data_val in enumerate(val_loader):\n",
    "            images = data_val[\"image\"].to(device)\n",
    "            classes = data_val[\"slice_label\"].to(device)\n",
    "            timesteps = torch.randint(0, 1, (len(images),)).to(\n",
    "                device\n",
    "            )  # check validation accuracy on the original images, i.e., do not add noise\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=False):\n",
    "                    noise = torch.randn_like(images).to(device)\n",
    "                    pred = classifier(images, timesteps)\n",
    "                    val_loss = F.cross_entropy(pred, classes.long(), reduction=\"mean\")\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        print(\"Epoch\", epoch, \"Validation loss\", val_epoch_loss / (step + 1))\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")\n",
    "\n",
    "## Learning curves for the Classifier\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, max_epochs, max_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, max_epochs, int(max_epochs / val_interval)),\n",
    "    val_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a676b3fe",
   "metadata": {},
   "source": [
    "# Image-to-image translation to a healthy subject\n",
    "We pick a diseased subject of the validation set as input image. We want to translate it to its healthy reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "id": "fe0d9eac-1477-4d6d-a885-d3c4acb4a781",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "idx_unhealthy = np.argwhere(data_val[\"slice_label\"].numpy() == 0).squeeze()\n",
    "idx = idx_unhealthy[4]  # Pick a random slice of the validation set to be transformed\n",
    "inputimg = data_val[\"image\"][idx]  # Pick an input slice of the validation set to be transformed\n",
    "inputlabel = data_val[\"slice_label\"][idx]  # Check whether it is healthy or diseased\n",
    "print(\"minmax\", inputimg.min(), inputimg.max())\n",
    "\n",
    "plt.figure(\"input\" + str(inputlabel))\n",
    "plt.imshow(inputimg[0, ...], vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "classifier.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cd48c2d",
   "metadata": {},
   "source": [
    "### Encoding the input image in noise with the reversed DDIM sampling scheme\n",
    "In order to sample using gradient guidance, we first need to encode the input image in noise by using the reversed DDIM sampling scheme.\\\n",
    "We define the number of steps in the noising and denoising process by L.\\\n",
    "The encoding process is presented in Equation 6 of the paper \"Diffusion Models for Medical Anomaly Detection\" (https://arxiv.org/pdf/2203.04306.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f71e4924",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "is_executing": true
    },
    "lines_to_next_cell": 2
   },
   "source": [
    "L = 200\n",
    "current_img = inputimg[None, ...].to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "progress_bar = tqdm(range(L))  # go back and forth L timesteps\n",
    "for t in progress_bar:  # go through the noising process\n",
    "    with autocast(enabled=False):\n",
    "        with torch.no_grad():\n",
    "            model_output = model(current_img, timesteps=torch.Tensor((t,)).to(current_img.device))\n",
    "    current_img, _ = scheduler.reversed_step(model_output, t, current_img)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(current_img[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7c8346a-6296-4800-b978-c10fcdf09779",
   "metadata": {},
   "source": [
    "### Denoising process using gradient guidance\n",
    "From the noisy image, we apply DDIM sampling scheme for denoising for L steps.\\\n",
    "Additionally, we apply gradient guidance using the classifier network towards the desired class label y=0 (healthy). This is presented in Algorithm 2 of https://arxiv.org/pdf/2105.05233.pdf, and in Algorithm 1 of https://arxiv.org/pdf/2203.04306.pdf. \\\n",
    "The scale s is used to amplify the gradient."
   ]
  },
  {
   "cell_type": "code",
   "id": "7ab274bd-ea60-4674-b59b-d41de98fee5b",
   "metadata": {
    "lines_to_next_cell": 2,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "y = torch.tensor(0)  # define the desired class label\n",
    "scale = 6  # define the desired gradient scale s\n",
    "progress_bar = tqdm(range(L))  # go back and forth L timesteps\n",
    "\n",
    "for i in progress_bar:  # go through the denoising process\n",
    "    t = L - i\n",
    "    with autocast(enabled=True):\n",
    "        with torch.no_grad():\n",
    "            model_output = model(\n",
    "                current_img, timesteps=torch.Tensor((t,)).to(current_img.device)\n",
    "            ).detach()  # this is supposed to be epsilon\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x_in = current_img.detach().requires_grad_(True)\n",
    "            logits = classifier(x_in, timesteps=torch.Tensor((t,)).to(current_img.device))\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            selected = log_probs[range(len(logits)), y.view(-1)]\n",
    "            a = torch.autograd.grad(selected.sum(), x_in)[0]\n",
    "            alpha_prod_t = scheduler.alphas_cumprod[t]\n",
    "            updated_noise = (\n",
    "                model_output - (1 - alpha_prod_t).sqrt() * scale * a\n",
    "            )  # update the predicted noise epsilon with the gradient of the classifier\n",
    "\n",
    "    current_img, _ = scheduler.step(updated_noise, t, current_img)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(current_img[0, 0].cpu().detach().numpy(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2e343f8-c6f3-4071-a5e6-771e2343c3bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Anomaly Detection\n",
    "To get the anomaly map, we compute the difference between the input image the output of our image-to-image translation model towards the healthy reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "id": "ecffaaf3-a7df-453e-81a9-757113d85084",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "diff = abs(inputimg.cpu() - current_img[0, 0].cpu()).detach().numpy()\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(diff[0, ...], cmap=\"jet\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
