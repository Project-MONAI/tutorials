{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "## 3D Segmentation with DAF3D\n",
    "\n",
    "This tutorial shows how to construct a training workflow of the DAF3D network based on 'Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound' <https://arxiv.org/pdf/1907.01743.pdf>. \n",
    "\n",
    "![DAF3D](../figures/DAF3D_scheme.png)\n",
    "\n",
    "It contains the following features:\n",
    "1. Transforms for dictionary format data\n",
    "1. Use of DecathlonDataset to faciliate loading and caching images from Nifti Files\n",
    "1. DAF3D model, Dice metric, customized loss function to work on multiple supervised signals\n",
    "1. Visualization of supervised signals and Attentive Maps of last validation image\n",
    "1. Sliding window inference method\n",
    "\n",
    "\n",
    "Instead of prostate scans data is obtained from Decathlon Dataset Task09:Spleen since image properties are similar (one-channeled input, single-label ground truth). The Spleen dataset can be downloaded from http://medicaldecathlon.com/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    ToTensord,\n",
    "    ScaleIntensityRanged,\n",
    "    Resized,\n",
    "    Orientationd\n",
    ")\n",
    "from monai.networks.nets.daf3d import DAF3D\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss\n",
    "from monai.data import DataLoader\n",
    "from monai.config import print_config\n",
    "from monai.apps import DecathlonDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup paths to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform spleen data from Decathlon Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_size = (128, 128, 90)\n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "    ),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    Resized(\n",
    "            keys=[\"image\", \"label\"], \n",
    "            spatial_size=spatial_size, \n",
    "            mode=[\"trilinear\", \"nearest-exact\"], \n",
    "            allow_missing_keys=True, \n",
    "            align_corners=[False, None]\n",
    "    ),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "msd_task = \"Task09_Spleen\"\n",
    "train_set = DecathlonDataset(root_dir, msd_task, \"training\", train_transforms)\n",
    "val_set = DecathlonDataset(root_dir, msd_task, \"validation\", train_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check transforms in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= DataLoader(train_set)\n",
    "val_loader = DataLoader(val_set)\n",
    "\n",
    "image = first(train_loader)[\"image\"]\n",
    "label = first(train_loader)[\"label\"]\n",
    "print(\"Image shape: \", image.shape, \", label shape: \", label.shape)\n",
    "image = torch.squeeze(image)\n",
    "label = torch.squeeze(label)\n",
    "print(f\"image shape: {image.shape},label shape: {label.shape}\")\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:,:,80], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:,:,80])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Optimizer, Dice Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if (torch.cuda.is_available()) else 'cpu'\n",
    "\n",
    "model = DAF3D(in_channels=1, out_channels=1, visual_output=True).to(device)\n",
    "\n",
    "criterion_dice = DiceLoss(smooth_nr=1, smooth_dr=1, squared_pred=True, reduction=\"none\")\n",
    "criterion_bce = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "dice_metric = DiceMetric()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom loss function\n",
    "Based on 'Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound' <https://arxiv.org/pdf/1907.01743.pdf>.\n",
    "Necessary to work on multiple supervised signals consisting of \n",
    "1. Layerwise outputs of Feature Pyramid Network (Single Layer Features / SLFs)\n",
    "2. Layerwise outputs of Attention Module (Attentive Features / Refined SLFs)\n",
    "3. Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(outputs, label):\n",
    "    criterion_dice = DiceLoss(smooth_nr=1, smooth_dr=1, squared_pred=True, reduction=\"none\")\n",
    "    criterion_bce = torch.nn.BCELoss()\n",
    "\n",
    "    dice_losses = [criterion_dice(i, label) for i in outputs]\n",
    "    bce_losses = [criterion_bce(i, label) for i in outputs]\n",
    "\n",
    "    weights = [0.4, 0.5, 0.7, 0.8] #weights for slfs & attentive bce\n",
    "    weights_special = [0.4, 0.7, 0.8, 1] #weights for attentive dice\n",
    "    loss_slf = sum([weights[i] * (dice_losses[i] + bce_losses[i]) for i in range(0, 4)])\n",
    "    loss_attentive = sum([weights[i] * bce_losses[i+4] + weights_special[i] * dice_losses[i+4] for i in range(0, 4)])\n",
    "    loss_output = dice_losses[8] + bce_losses[8]\n",
    "\n",
    "    loss = loss_slf + loss_attentive + loss_output\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute DAF3D training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 20\n",
    "numTrainingData = len(train_set)\n",
    "best_dice = -1\n",
    "best_dice_epoch = -1\n",
    "epoch_loss_values = []\n",
    "epoch_dice_values = []\n",
    "plotted_outputs = []\n",
    "\n",
    "\n",
    "for epoch in range(1, numEpochs+1):\n",
    "    print((\"-\" * 10) + \" Epoch: {} \".format(epoch) + (\"-\" * 10))\n",
    "    \n",
    "    #Start training\n",
    "    model.train()\n",
    "    epoch_loss = 0.\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "\n",
    "        image = batch_data[\"image\"].to(device)\n",
    "        label = batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        outputs = [torch.sigmoid(i) for i in outputs]\n",
    "\n",
    "        loss = loss_function(outputs, label)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = epoch_loss / (batch_idx + 1)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch} Finished ! Loss is {epoch_loss:.4f}\")\n",
    "\n",
    "    #Start validation\n",
    "    model.eval()\n",
    "    epoch_dice = 0.\n",
    "    for batch_idx, batch_data in enumerate(val_loader):\n",
    "\n",
    "        image = batch_data[\"image\"].to(device)\n",
    "        label = batch_data[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(image) #since visual_outputs=True in shape of [final_prediction,inner_outputs]\n",
    "        predict = outputs[0]\n",
    "        inner_outputs = outputs[1:]\n",
    "        predict = torch.sigmoid(predict)\n",
    "        inner_outputs = [torch.sigmoid(i) for i in inner_outputs]\n",
    "\n",
    "        dice_metric(predict > 0.5, label)\n",
    "    \n",
    "    epoch_dice = dice_metric.aggregate().item()\n",
    "    dice_metric.reset()\n",
    "    epoch_dice_values.append(epoch_dice)\n",
    "    print(f\"Epoch {epoch} Dice score: {epoch_dice:.4f}\")\n",
    "\n",
    "    if (epoch_dice > best_dice):\n",
    "        best_dice = epoch_dice\n",
    "        best_dice_epoch = epoch\n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"best_dice_model.pth\"))\n",
    "\n",
    "\n",
    "print(f\"Training completed, best dice score: {best_dice:.4f} at epoch: {best_dice_epoch}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss and metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Epoch Dice Score\")\n",
    "x = [i + 1 for i in range(len(epoch_dice_values))]\n",
    "y = epoch_dice_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all outputs of last validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 55\n",
    "\n",
    "inner_outputs_slice =[torch.squeeze(i).detach().cpu().numpy()[:,:,slice] for i in inner_outputs]   \n",
    "label_slice = torch.squeeze(label).detach().cpu().numpy()[:,:,slice]\n",
    "image_slice = torch.squeeze(image).detach().cpu().numpy()[:,:,slice]\n",
    "predict_slice = torch.squeeze(predict).detach().cpu().numpy()[:,:,slice]\n",
    "\n",
    "slfs = inner_outputs_slice[0:4]\n",
    "refined = inner_outputs_slice[4:8]\n",
    "att_maps = inner_outputs_slice[8:]\n",
    "\n",
    "plt.figure(\"Outputs\", (30,10))\n",
    "\n",
    "plt.subplot(3,7,1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image_slice, cmap=\"gray\")\n",
    "\n",
    "for i,slf in enumerate(slfs):\n",
    "    plt.subplot(3,7,i+2)\n",
    "    plt.title(\"SLF \" + str(i+1))\n",
    "    plt.imshow(slf, cmap=\"gray\")\n",
    "\n",
    "for (i, am) in enumerate(att_maps):\n",
    "    plt.subplot(3,7,i+9)\n",
    "    plt.title(\"Attentive Map \" + str(i+1))\n",
    "    plt.imshow(am)\n",
    "\n",
    "for i,rslf in enumerate(refined):\n",
    "    plt.subplot(3,7,i+16)\n",
    "    plt.title(\"Refined SLF \" + str(i+1))\n",
    "    plt.imshow(rslf, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(3,7,6)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(predict_slice, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(3,7,7)\n",
    "plt.title(\"Ground truth\")\n",
    "plt.imshow(label_slice, cmap=\"gray\")       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "    ),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    Resized(\n",
    "            keys=[\"image\"], \n",
    "            spatial_size=spatial_size, \n",
    "            mode=[\"trilinear\"]\n",
    "    ),\n",
    "    ToTensord(keys=[\"image\"]),\n",
    "])\n",
    "test_set = DecathlonDataset(root_dir, msd_task, \"test\", test_transforms)\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAF3D(in_channels=1, out_channels=1, visual_output=False)\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_dice_model.pth\"), map_location=device))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        image = test_data[\"image\"].to(device)\n",
    "        outputs = model(image)\n",
    "        roi_size = spatial_size\n",
    "        sw_batch_size = 1\n",
    "\n",
    "        prediction = sliding_window_inference(image, roi_size, sw_batch_size, model)\n",
    "\n",
    "        plt.figure(\"Prediction on test set\")\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"image\")\n",
    "        plt.imshow(torch.squeeze(image).detach().cpu()[:,:,55], cmap=\"gray\")\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"prediction\")\n",
    "        plt.imshow(torch.squeeze(prediction).detach().cpu()[:,:,55], cmap=\"gray\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai-contributions-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6104c6d6a3194d6b3091f0d4beb3e929effbf304768b258f7e7deadbaf456090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
