{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain tumor 3D segmentation with MONAI\n",
    "\n",
    "This tutorial shows how to construct a training workflow of multi-labels segmentation task.\n",
    "\n",
    "And it contains below features:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly adjust intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D UNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "1. Deterministic training for reproducibility.\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n",
    "(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n",
    "\n",
    "![image](../figures/brats_tasks.png)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/3d_segmentation/brats_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.4.0+618.g69b44596\n",
      "Numpy version: 1.20.3\n",
      "Pytorch version: 1.9.0a0+c3d40fd\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 69b4459650fb6943b9e729e724254d2db2b2a1f2\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.5\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: 0.15.0\n",
      "Pillow version: 8.3.1\n",
      "Tensorboard version: 2.5.0\n",
      "gdown version: 3.13.0\n",
      "TorchVision version: 0.10.0a0\n",
      "tqdm version: 4.53.0\n",
      "lmdb version: 1.2.1\n",
      "psutil version: 5.8.0\n",
      "pandas version: 1.1.4\n",
      "einops version: 0.3.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data/medical\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 3), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly load data with DecathlonDataset\n",
    "\n",
    "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
    "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the defaut args to cache all the items for validation, it depends on your memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "File exists: /workspace/data/medical/Task01_BrainTumour.tar, skipped downloading.\n",
      "Non-empty folder exists in /workspace/data/medical/Task01_BrainTumour, skipped extracting.\n"
     ]
    }
   ],
   "source": [
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([4, 240, 240, 155])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: torch.Size([3, 240, 240, 155])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
    "print(f\"image shape: {val_ds[2]['image'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(val_ds[2][\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_ds[2]['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(val_ds[2][\"label\"][i, :, :, 60].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "val_interval = 1\n",
    "\n",
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose(\n",
    "    [EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold_values=True)]\n",
    ")\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/300\n",
      "1/388, train_loss: 0.9785, step time: 16.5686\n",
      "2/388, train_loss: 0.9814, step time: 0.4695\n",
      "3/388, train_loss: 0.9888, step time: 0.4694\n",
      "4/388, train_loss: 0.9578, step time: 0.4705\n",
      "5/388, train_loss: 0.9840, step time: 0.4716\n",
      "6/388, train_loss: 0.9398, step time: 0.4722\n",
      "7/388, train_loss: 0.9725, step time: 0.4717\n",
      "8/388, train_loss: 0.9405, step time: 0.4698\n",
      "9/388, train_loss: 0.9852, step time: 0.4682\n",
      "10/388, train_loss: 0.9434, step time: 0.4713\n",
      "11/388, train_loss: 0.8947, step time: 0.4677\n",
      "12/388, train_loss: 0.8908, step time: 0.4718\n",
      "13/388, train_loss: 0.9646, step time: 0.4725\n",
      "14/388, train_loss: 0.9018, step time: 0.4728\n",
      "15/388, train_loss: 0.9303, step time: 0.4725\n",
      "16/388, train_loss: 0.9549, step time: 0.4697\n",
      "17/388, train_loss: 0.9709, step time: 0.4690\n",
      "18/388, train_loss: 0.9358, step time: 0.4680\n",
      "19/388, train_loss: 0.9586, step time: 0.4703\n",
      "20/388, train_loss: 0.8867, step time: 0.4716\n",
      "21/388, train_loss: 0.9415, step time: 0.4772\n",
      "22/388, train_loss: 0.9378, step time: 0.4806\n",
      "23/388, train_loss: 0.9302, step time: 0.4795\n",
      "24/388, train_loss: 0.9370, step time: 0.4730\n",
      "25/388, train_loss: 0.9515, step time: 0.4692\n",
      "26/388, train_loss: 0.9515, step time: 0.4698\n",
      "27/388, train_loss: 0.9825, step time: 0.4686\n",
      "28/388, train_loss: 0.9507, step time: 0.4732\n",
      "29/388, train_loss: 0.9554, step time: 0.4725\n",
      "30/388, train_loss: 0.9372, step time: 0.4725\n",
      "31/388, train_loss: 0.9678, step time: 0.4720\n",
      "32/388, train_loss: 0.9920, step time: 0.4694\n",
      "33/388, train_loss: 0.9330, step time: 0.4711\n",
      "34/388, train_loss: 0.9582, step time: 0.4701\n",
      "35/388, train_loss: 0.9478, step time: 0.4718\n",
      "36/388, train_loss: 0.9321, step time: 0.4684\n",
      "37/388, train_loss: 0.9020, step time: 0.4733\n",
      "38/388, train_loss: 0.9794, step time: 0.4700\n",
      "39/388, train_loss: 0.8921, step time: 0.4690\n",
      "40/388, train_loss: 0.9023, step time: 0.4721\n",
      "41/388, train_loss: 0.9879, step time: 0.4695\n",
      "42/388, train_loss: 0.9505, step time: 0.4699\n",
      "43/388, train_loss: 0.9105, step time: 0.4679\n",
      "44/388, train_loss: 0.9227, step time: 0.4691\n",
      "45/388, train_loss: 0.9633, step time: 0.4706\n",
      "46/388, train_loss: 0.9379, step time: 0.4703\n",
      "47/388, train_loss: 0.9283, step time: 0.4694\n",
      "48/388, train_loss: 0.9634, step time: 0.4695\n",
      "49/388, train_loss: 0.8597, step time: 0.4680\n",
      "50/388, train_loss: 0.9880, step time: 0.4697\n",
      "51/388, train_loss: 0.9884, step time: 0.4695\n",
      "52/388, train_loss: 0.9610, step time: 0.4684\n",
      "53/388, train_loss: 0.8128, step time: 0.4673\n",
      "54/388, train_loss: 0.9437, step time: 0.4706\n",
      "55/388, train_loss: 0.9104, step time: 0.4714\n",
      "56/388, train_loss: 0.9638, step time: 0.4760\n",
      "57/388, train_loss: 0.9522, step time: 0.4697\n",
      "58/388, train_loss: 0.9446, step time: 0.4719\n",
      "59/388, train_loss: 0.9570, step time: 0.4705\n",
      "60/388, train_loss: 0.9638, step time: 0.4699\n",
      "61/388, train_loss: 0.9407, step time: 0.4686\n",
      "62/388, train_loss: 0.9017, step time: 0.4719\n",
      "63/388, train_loss: 0.8760, step time: 0.4728\n",
      "64/388, train_loss: 0.9406, step time: 0.4681\n",
      "65/388, train_loss: 0.9601, step time: 0.4788\n",
      "66/388, train_loss: 0.9599, step time: 0.4723\n",
      "67/388, train_loss: 0.8832, step time: 0.4718\n",
      "68/388, train_loss: 0.7857, step time: 0.4697\n",
      "69/388, train_loss: 0.8907, step time: 0.4694\n",
      "70/388, train_loss: 0.9053, step time: 0.4696\n",
      "71/388, train_loss: 0.9501, step time: 0.4700\n",
      "72/388, train_loss: 0.9355, step time: 0.4682\n",
      "73/388, train_loss: 0.9171, step time: 0.4695\n",
      "74/388, train_loss: 0.9295, step time: 0.4699\n",
      "75/388, train_loss: 0.9899, step time: 0.4701\n",
      "76/388, train_loss: 0.9111, step time: 0.4673\n",
      "77/388, train_loss: 0.9360, step time: 0.4689\n",
      "78/388, train_loss: 0.8900, step time: 0.4669\n",
      "79/388, train_loss: 0.8801, step time: 0.4677\n",
      "80/388, train_loss: 0.9926, step time: 0.4694\n",
      "81/388, train_loss: 0.9836, step time: 0.4684\n",
      "82/388, train_loss: 0.9071, step time: 0.4689\n",
      "83/388, train_loss: 0.9682, step time: 0.4687\n",
      "84/388, train_loss: 0.9534, step time: 0.4691\n",
      "85/388, train_loss: 0.9167, step time: 0.4678\n",
      "86/388, train_loss: 0.9523, step time: 0.4744\n",
      "87/388, train_loss: 0.9646, step time: 0.4731\n",
      "88/388, train_loss: 0.8554, step time: 0.4688\n",
      "89/388, train_loss: 0.9413, step time: 0.4688\n",
      "90/388, train_loss: 0.9956, step time: 0.4696\n",
      "91/388, train_loss: 0.9818, step time: 0.4689\n",
      "92/388, train_loss: 0.9947, step time: 0.4701\n",
      "93/388, train_loss: 0.9648, step time: 0.4717\n",
      "94/388, train_loss: 0.9142, step time: 0.4698\n",
      "95/388, train_loss: 0.9104, step time: 0.4701\n",
      "96/388, train_loss: 0.9051, step time: 0.4699\n",
      "97/388, train_loss: 0.8303, step time: 0.4698\n",
      "98/388, train_loss: 0.7696, step time: 0.4714\n",
      "99/388, train_loss: 0.9253, step time: 0.4705\n",
      "100/388, train_loss: 0.9131, step time: 0.4694\n",
      "101/388, train_loss: 0.9673, step time: 0.4710\n",
      "102/388, train_loss: 0.8987, step time: 0.4690\n",
      "103/388, train_loss: 0.9027, step time: 0.4697\n",
      "104/388, train_loss: 0.9796, step time: 0.4684\n",
      "105/388, train_loss: 0.9148, step time: 0.4695\n",
      "106/388, train_loss: 0.9717, step time: 0.4688\n",
      "107/388, train_loss: 0.9869, step time: 0.4688\n",
      "108/388, train_loss: 0.9421, step time: 0.4695\n",
      "109/388, train_loss: 0.8254, step time: 0.4719\n",
      "110/388, train_loss: 0.9241, step time: 0.4700\n",
      "111/388, train_loss: 0.9756, step time: 0.4721\n",
      "112/388, train_loss: 0.9173, step time: 0.4714\n",
      "113/388, train_loss: 0.9702, step time: 0.4742\n",
      "114/388, train_loss: 0.9000, step time: 0.4709\n",
      "115/388, train_loss: 0.9339, step time: 0.4719\n",
      "116/388, train_loss: 0.9472, step time: 0.4708\n",
      "117/388, train_loss: 0.9032, step time: 0.4770\n",
      "118/388, train_loss: 0.9442, step time: 0.4692\n",
      "119/388, train_loss: 0.9148, step time: 0.4710\n",
      "120/388, train_loss: 0.8672, step time: 0.4855\n",
      "121/388, train_loss: 0.9378, step time: 0.4760\n",
      "122/388, train_loss: 0.9622, step time: 0.4687\n",
      "123/388, train_loss: 0.9595, step time: 0.4707\n",
      "124/388, train_loss: 0.8747, step time: 0.4676\n",
      "125/388, train_loss: 0.8712, step time: 0.4688\n",
      "126/388, train_loss: 0.9739, step time: 0.4693\n",
      "127/388, train_loss: 0.9738, step time: 0.4690\n",
      "128/388, train_loss: 0.9825, step time: 0.4697\n",
      "129/388, train_loss: 0.9426, step time: 0.4688\n",
      "130/388, train_loss: 0.8947, step time: 0.4692\n",
      "131/388, train_loss: 0.9599, step time: 0.4687\n",
      "132/388, train_loss: 0.9376, step time: 0.4684\n",
      "133/388, train_loss: 0.9279, step time: 0.4690\n",
      "134/388, train_loss: 0.9598, step time: 0.4680\n",
      "135/388, train_loss: 0.9653, step time: 0.4699\n",
      "136/388, train_loss: 0.7962, step time: 0.4693\n",
      "137/388, train_loss: 0.9411, step time: 0.4703\n",
      "138/388, train_loss: 0.9582, step time: 0.4684\n",
      "139/388, train_loss: 0.8904, step time: 0.4763\n",
      "140/388, train_loss: 0.9128, step time: 0.4719\n",
      "141/388, train_loss: 0.9414, step time: 0.4728\n",
      "142/388, train_loss: 0.8808, step time: 0.4714\n",
      "143/388, train_loss: 0.8706, step time: 0.4719\n",
      "144/388, train_loss: 0.9024, step time: 0.4708\n",
      "145/388, train_loss: 0.8903, step time: 0.4707\n",
      "146/388, train_loss: 0.9704, step time: 0.4696\n",
      "147/388, train_loss: 0.9942, step time: 0.4690\n",
      "148/388, train_loss: 0.9656, step time: 0.4696\n",
      "149/388, train_loss: 0.9360, step time: 0.4700\n",
      "150/388, train_loss: 0.9722, step time: 0.4694\n",
      "151/388, train_loss: 0.9285, step time: 0.4701\n",
      "152/388, train_loss: 0.9681, step time: 0.4700\n",
      "153/388, train_loss: 0.8472, step time: 0.4697\n",
      "154/388, train_loss: 0.8826, step time: 0.4693\n",
      "155/388, train_loss: 0.8619, step time: 0.4747\n",
      "156/388, train_loss: 0.7953, step time: 0.4683\n",
      "157/388, train_loss: 0.9089, step time: 0.4705\n",
      "158/388, train_loss: 0.9099, step time: 0.4757\n",
      "159/388, train_loss: 0.9903, step time: 0.4726\n",
      "160/388, train_loss: 0.9744, step time: 0.4712\n",
      "161/388, train_loss: 0.8835, step time: 0.4696\n",
      "162/388, train_loss: 0.9683, step time: 0.4698\n",
      "163/388, train_loss: 0.9216, step time: 0.4722\n",
      "164/388, train_loss: 0.8973, step time: 0.4710\n",
      "165/388, train_loss: 0.9239, step time: 0.4713\n",
      "166/388, train_loss: 0.9657, step time: 0.4708\n",
      "167/388, train_loss: 0.9739, step time: 0.4814\n",
      "168/388, train_loss: 0.8031, step time: 0.4735\n",
      "169/388, train_loss: 0.8603, step time: 0.4714\n",
      "170/388, train_loss: 0.8482, step time: 0.4678\n",
      "171/388, train_loss: 0.9687, step time: 0.4775\n",
      "172/388, train_loss: 0.9761, step time: 0.4721\n",
      "173/388, train_loss: 0.8112, step time: 0.4721\n",
      "174/388, train_loss: 0.9787, step time: 0.4737\n",
      "175/388, train_loss: 0.9746, step time: 0.4757\n",
      "176/388, train_loss: 0.8071, step time: 0.4717\n",
      "177/388, train_loss: 0.9908, step time: 0.4728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/388, train_loss: 0.9382, step time: 0.4732\n",
      "179/388, train_loss: 0.9766, step time: 0.4724\n",
      "180/388, train_loss: 0.9662, step time: 0.4698\n",
      "181/388, train_loss: 0.9841, step time: 0.4711\n",
      "182/388, train_loss: 0.8751, step time: 0.4717\n",
      "183/388, train_loss: 0.9415, step time: 0.4699\n",
      "184/388, train_loss: 0.9068, step time: 0.4701\n",
      "185/388, train_loss: 0.9871, step time: 0.4719\n",
      "186/388, train_loss: 0.9636, step time: 0.4709\n",
      "187/388, train_loss: 0.8016, step time: 0.4725\n",
      "188/388, train_loss: 0.9715, step time: 0.4687\n",
      "189/388, train_loss: 0.9800, step time: 0.4727\n",
      "190/388, train_loss: 0.9231, step time: 0.4720\n",
      "191/388, train_loss: 0.9332, step time: 0.4716\n",
      "192/388, train_loss: 0.9841, step time: 0.4733\n",
      "193/388, train_loss: 0.9538, step time: 0.4706\n",
      "194/388, train_loss: 0.9520, step time: 0.4708\n",
      "195/388, train_loss: 0.8938, step time: 0.4698\n",
      "196/388, train_loss: 0.9374, step time: 0.4682\n",
      "197/388, train_loss: 0.8740, step time: 0.4688\n",
      "198/388, train_loss: 0.9084, step time: 0.4698\n",
      "199/388, train_loss: 0.9616, step time: 0.4708\n",
      "200/388, train_loss: 0.8674, step time: 0.4693\n",
      "201/388, train_loss: 0.9179, step time: 0.4706\n",
      "202/388, train_loss: 0.8968, step time: 0.4712\n",
      "203/388, train_loss: 0.8341, step time: 0.4700\n",
      "204/388, train_loss: 0.8680, step time: 0.4700\n",
      "205/388, train_loss: 0.8889, step time: 0.4687\n",
      "206/388, train_loss: 0.8461, step time: 0.4682\n",
      "207/388, train_loss: 0.9753, step time: 0.4694\n",
      "208/388, train_loss: 0.9531, step time: 0.4729\n",
      "209/388, train_loss: 0.9461, step time: 0.4743\n",
      "210/388, train_loss: 0.9458, step time: 0.4708\n",
      "211/388, train_loss: 0.9280, step time: 0.4710\n",
      "212/388, train_loss: 0.9413, step time: 0.4712\n",
      "213/388, train_loss: 0.7878, step time: 0.4689\n",
      "214/388, train_loss: 0.9565, step time: 0.4676\n",
      "215/388, train_loss: 0.9069, step time: 0.4704\n",
      "216/388, train_loss: 0.8886, step time: 0.4690\n",
      "217/388, train_loss: 0.9442, step time: 0.4710\n",
      "218/388, train_loss: 0.9030, step time: 0.4717\n",
      "219/388, train_loss: 0.7579, step time: 0.4688\n",
      "220/388, train_loss: 0.9204, step time: 0.4731\n",
      "221/388, train_loss: 0.8406, step time: 0.4681\n",
      "222/388, train_loss: 0.9771, step time: 0.4708\n",
      "223/388, train_loss: 0.8047, step time: 0.4682\n",
      "224/388, train_loss: 0.9784, step time: 0.4744\n",
      "225/388, train_loss: 0.8756, step time: 0.4733\n",
      "226/388, train_loss: 0.9689, step time: 0.4706\n",
      "227/388, train_loss: 0.8602, step time: 0.4742\n",
      "228/388, train_loss: 0.8414, step time: 0.4796\n",
      "229/388, train_loss: 0.9019, step time: 0.4685\n",
      "230/388, train_loss: 0.9920, step time: 0.4757\n",
      "231/388, train_loss: 0.9082, step time: 0.4706\n",
      "232/388, train_loss: 0.8286, step time: 0.4770\n",
      "233/388, train_loss: 0.8297, step time: 0.4772\n",
      "234/388, train_loss: 0.9128, step time: 0.4737\n",
      "235/388, train_loss: 0.9838, step time: 0.4709\n",
      "236/388, train_loss: 0.9284, step time: 0.4733\n",
      "237/388, train_loss: 0.9885, step time: 0.4708\n",
      "238/388, train_loss: 0.9796, step time: 0.4685\n",
      "239/388, train_loss: 0.8838, step time: 0.4678\n",
      "240/388, train_loss: 0.8891, step time: 0.4684\n",
      "241/388, train_loss: 0.9534, step time: 0.4691\n",
      "242/388, train_loss: 0.9491, step time: 0.4688\n",
      "243/388, train_loss: 0.7921, step time: 0.4682\n",
      "244/388, train_loss: 0.9227, step time: 0.4701\n",
      "245/388, train_loss: 0.8384, step time: 0.4685\n",
      "246/388, train_loss: 0.9420, step time: 0.4689\n",
      "247/388, train_loss: 0.8879, step time: 0.4694\n",
      "248/388, train_loss: 0.9050, step time: 0.4686\n",
      "249/388, train_loss: 0.9267, step time: 0.4686\n",
      "250/388, train_loss: 0.9200, step time: 0.4703\n",
      "251/388, train_loss: 0.7704, step time: 0.4679\n",
      "252/388, train_loss: 0.8566, step time: 0.4674\n",
      "253/388, train_loss: 0.9828, step time: 0.4716\n",
      "254/388, train_loss: 0.8587, step time: 0.4717\n",
      "255/388, train_loss: 0.9418, step time: 0.4670\n",
      "256/388, train_loss: 0.9861, step time: 0.4716\n",
      "257/388, train_loss: 0.9161, step time: 0.4728\n",
      "258/388, train_loss: 0.9506, step time: 0.4711\n",
      "259/388, train_loss: 0.7877, step time: 0.4707\n",
      "260/388, train_loss: 0.8285, step time: 0.4704\n",
      "261/388, train_loss: 0.9714, step time: 0.4730\n",
      "262/388, train_loss: 0.9757, step time: 0.4726\n",
      "263/388, train_loss: 0.9141, step time: 0.4724\n",
      "264/388, train_loss: 0.9151, step time: 0.4710\n",
      "265/388, train_loss: 0.8768, step time: 0.4697\n",
      "266/388, train_loss: 0.9209, step time: 0.4702\n",
      "267/388, train_loss: 0.9329, step time: 0.4692\n",
      "268/388, train_loss: 0.8454, step time: 0.4678\n",
      "269/388, train_loss: 0.9899, step time: 0.4695\n",
      "270/388, train_loss: 0.9100, step time: 0.4692\n",
      "271/388, train_loss: 0.9640, step time: 0.4688\n",
      "272/388, train_loss: 0.8680, step time: 0.4724\n",
      "273/388, train_loss: 0.8688, step time: 0.4682\n",
      "274/388, train_loss: 0.8623, step time: 0.4696\n",
      "275/388, train_loss: 0.9502, step time: 0.4715\n",
      "276/388, train_loss: 0.9620, step time: 0.4699\n",
      "277/388, train_loss: 0.8276, step time: 0.4691\n",
      "278/388, train_loss: 0.9498, step time: 0.4690\n",
      "279/388, train_loss: 0.8787, step time: 0.4679\n",
      "280/388, train_loss: 0.9564, step time: 0.4676\n",
      "281/388, train_loss: 0.8807, step time: 0.4690\n",
      "282/388, train_loss: 0.8780, step time: 0.4752\n",
      "283/388, train_loss: 0.9325, step time: 0.4732\n",
      "284/388, train_loss: 0.9860, step time: 0.4707\n",
      "285/388, train_loss: 0.9087, step time: 0.4712\n",
      "286/388, train_loss: 0.9424, step time: 0.4716\n",
      "287/388, train_loss: 0.9626, step time: 0.4710\n",
      "288/388, train_loss: 0.8389, step time: 0.4710\n",
      "289/388, train_loss: 0.8827, step time: 0.4715\n",
      "290/388, train_loss: 0.9195, step time: 0.4749\n",
      "291/388, train_loss: 0.9710, step time: 0.4716\n",
      "292/388, train_loss: 0.9089, step time: 0.4711\n",
      "293/388, train_loss: 0.8224, step time: 0.4721\n",
      "294/388, train_loss: 0.9346, step time: 0.4739\n",
      "295/388, train_loss: 0.9011, step time: 0.4738\n",
      "296/388, train_loss: 0.9085, step time: 0.4693\n",
      "297/388, train_loss: 0.9231, step time: 0.4696\n",
      "298/388, train_loss: 0.8819, step time: 0.4708\n",
      "299/388, train_loss: 0.8956, step time: 0.4783\n",
      "300/388, train_loss: 0.8728, step time: 0.4675\n",
      "301/388, train_loss: 0.9544, step time: 0.4776\n",
      "302/388, train_loss: 0.9282, step time: 0.4707\n",
      "303/388, train_loss: 0.9864, step time: 0.4757\n",
      "304/388, train_loss: 0.9179, step time: 0.4699\n",
      "305/388, train_loss: 0.8737, step time: 0.4692\n",
      "306/388, train_loss: 0.9665, step time: 0.4693\n",
      "307/388, train_loss: 0.7788, step time: 0.4729\n",
      "308/388, train_loss: 0.9361, step time: 0.4698\n",
      "309/388, train_loss: 0.9298, step time: 0.4689\n",
      "310/388, train_loss: 0.9213, step time: 0.4696\n",
      "311/388, train_loss: 0.9649, step time: 0.4694\n",
      "312/388, train_loss: 0.9197, step time: 0.4676\n",
      "313/388, train_loss: 0.9200, step time: 0.4676\n",
      "314/388, train_loss: 0.8694, step time: 0.4674\n",
      "315/388, train_loss: 0.9947, step time: 0.4679\n",
      "316/388, train_loss: 0.9212, step time: 0.4712\n",
      "317/388, train_loss: 0.8042, step time: 0.4716\n",
      "318/388, train_loss: 0.8107, step time: 0.4693\n",
      "319/388, train_loss: 0.9134, step time: 0.4682\n",
      "320/388, train_loss: 0.9365, step time: 0.4675\n",
      "321/388, train_loss: 0.8867, step time: 0.4709\n",
      "322/388, train_loss: 0.8335, step time: 0.4720\n",
      "323/388, train_loss: 0.9434, step time: 0.4710\n",
      "324/388, train_loss: 0.9645, step time: 0.4704\n",
      "325/388, train_loss: 0.9002, step time: 0.4716\n",
      "326/388, train_loss: 0.9000, step time: 0.4707\n",
      "327/388, train_loss: 0.9026, step time: 0.4690\n",
      "328/388, train_loss: 0.9494, step time: 0.4699\n",
      "329/388, train_loss: 0.8521, step time: 0.4703\n",
      "330/388, train_loss: 0.8895, step time: 0.4693\n",
      "331/388, train_loss: 0.9733, step time: 0.4694\n",
      "332/388, train_loss: 0.9848, step time: 0.4694\n",
      "333/388, train_loss: 0.8971, step time: 0.4696\n",
      "334/388, train_loss: 0.8465, step time: 0.4703\n",
      "335/388, train_loss: 0.9287, step time: 0.4690\n",
      "336/388, train_loss: 0.9170, step time: 0.4690\n",
      "337/388, train_loss: 0.9858, step time: 0.4693\n",
      "338/388, train_loss: 0.9419, step time: 0.4692\n",
      "339/388, train_loss: 0.9697, step time: 0.4695\n",
      "340/388, train_loss: 0.8669, step time: 0.4696\n",
      "341/388, train_loss: 0.9793, step time: 0.4703\n",
      "342/388, train_loss: 0.8274, step time: 0.4710\n",
      "343/388, train_loss: 0.8569, step time: 0.4705\n",
      "344/388, train_loss: 0.9524, step time: 0.4706\n",
      "345/388, train_loss: 0.7950, step time: 0.4714\n",
      "346/388, train_loss: 0.9210, step time: 0.4685\n",
      "347/388, train_loss: 0.8295, step time: 0.4690\n",
      "348/388, train_loss: 0.9294, step time: 0.4718\n",
      "349/388, train_loss: 0.9113, step time: 0.4707\n",
      "350/388, train_loss: 0.8665, step time: 0.4710\n",
      "351/388, train_loss: 0.9413, step time: 0.4718\n",
      "352/388, train_loss: 0.9327, step time: 0.4681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/388, train_loss: 0.9311, step time: 0.4693\n",
      "354/388, train_loss: 0.8958, step time: 0.4711\n",
      "355/388, train_loss: 0.8157, step time: 0.4803\n",
      "356/388, train_loss: 0.9693, step time: 0.4726\n",
      "357/388, train_loss: 0.8517, step time: 0.4720\n",
      "358/388, train_loss: 0.8995, step time: 0.4717\n",
      "359/388, train_loss: 0.8473, step time: 0.4716\n",
      "360/388, train_loss: 0.8423, step time: 0.4733\n",
      "361/388, train_loss: 0.8731, step time: 0.4765\n",
      "362/388, train_loss: 0.9218, step time: 0.4710\n",
      "363/388, train_loss: 0.9152, step time: 0.4702\n",
      "364/388, train_loss: 0.8817, step time: 0.4705\n",
      "365/388, train_loss: 0.8435, step time: 0.4719\n",
      "366/388, train_loss: 0.8959, step time: 0.4710\n",
      "367/388, train_loss: 0.9407, step time: 0.4717\n",
      "368/388, train_loss: 0.8880, step time: 0.4684\n",
      "369/388, train_loss: 0.9850, step time: 0.4745\n",
      "370/388, train_loss: 0.8319, step time: 0.4732\n",
      "371/388, train_loss: 0.9708, step time: 0.4820\n",
      "372/388, train_loss: 0.9523, step time: 0.4753\n",
      "373/388, train_loss: 0.8892, step time: 0.4798\n",
      "374/388, train_loss: 0.9462, step time: 0.4732\n",
      "375/388, train_loss: 0.8987, step time: 0.4716\n",
      "376/388, train_loss: 0.9283, step time: 0.4750\n",
      "377/388, train_loss: 0.8840, step time: 0.4755\n",
      "378/388, train_loss: 0.9888, step time: 0.4712\n",
      "379/388, train_loss: 0.8610, step time: 0.4716\n",
      "380/388, train_loss: 0.8933, step time: 0.4708\n",
      "381/388, train_loss: 0.9333, step time: 0.4685\n",
      "382/388, train_loss: 0.8515, step time: 0.4657\n",
      "383/388, train_loss: 0.9428, step time: 0.4649\n",
      "384/388, train_loss: 0.9271, step time: 0.4594\n",
      "385/388, train_loss: 0.8381, step time: 0.4596\n",
      "386/388, train_loss: 0.9920, step time: 0.4595\n",
      "387/388, train_loss: 0.7938, step time: 0.4599\n",
      "388/388, train_loss: 0.9648, step time: 0.4600\n",
      "epoch 1 average loss: 0.9191\n",
      "saved new best metric model\n",
      "current epoch: 1 current mean dice: 0.4212 tc: 0.4280 wt: 0.6227 et: 0.2128\n",
      "best mean dice: 0.4212 at epoch: 1\n",
      "time consuming of epoch 1 is: 286.0213\n",
      "----------\n",
      "epoch 2/300\n",
      "1/388, train_loss: 0.8570, step time: 0.4803\n",
      "2/388, train_loss: 0.8246, step time: 0.4919\n",
      "3/388, train_loss: 0.8421, step time: 0.4741\n",
      "4/388, train_loss: 0.8670, step time: 0.4812\n",
      "5/388, train_loss: 0.8885, step time: 0.4657\n",
      "6/388, train_loss: 0.8834, step time: 0.4803\n",
      "7/388, train_loss: 0.8521, step time: 0.4812\n",
      "8/388, train_loss: 0.7665, step time: 0.4727\n",
      "9/388, train_loss: 0.9607, step time: 0.4848\n",
      "10/388, train_loss: 0.7826, step time: 0.4729\n",
      "11/388, train_loss: 0.9426, step time: 0.4794\n",
      "12/388, train_loss: 0.8405, step time: 0.4699\n",
      "13/388, train_loss: 0.8701, step time: 0.4837\n",
      "14/388, train_loss: 0.8854, step time: 0.4767\n",
      "15/388, train_loss: 0.8991, step time: 0.4820\n",
      "16/388, train_loss: 0.9386, step time: 0.4651\n",
      "17/388, train_loss: 0.7453, step time: 0.4889\n",
      "18/388, train_loss: 0.9499, step time: 0.4855\n",
      "19/388, train_loss: 0.9699, step time: 0.4749\n",
      "20/388, train_loss: 0.9495, step time: 0.4665\n",
      "21/388, train_loss: 0.8959, step time: 0.4734\n",
      "22/388, train_loss: 0.8118, step time: 0.4803\n",
      "23/388, train_loss: 0.9071, step time: 0.4735\n",
      "24/388, train_loss: 0.9138, step time: 0.4664\n",
      "25/388, train_loss: 0.8905, step time: 0.4704\n",
      "26/388, train_loss: 0.9532, step time: 0.4722\n",
      "27/388, train_loss: 0.8175, step time: 0.4715\n",
      "28/388, train_loss: 0.9851, step time: 0.4751\n",
      "29/388, train_loss: 0.8925, step time: 0.4742\n",
      "30/388, train_loss: 0.9203, step time: 0.4748\n",
      "31/388, train_loss: 0.8479, step time: 0.4690\n",
      "32/388, train_loss: 0.7141, step time: 0.4699\n",
      "33/388, train_loss: 0.9777, step time: 0.4835\n",
      "34/388, train_loss: 0.7527, step time: 0.4809\n",
      "35/388, train_loss: 0.8658, step time: 0.4699\n",
      "36/388, train_loss: 0.9165, step time: 0.4719\n",
      "37/388, train_loss: 0.8343, step time: 0.4695\n",
      "38/388, train_loss: 0.7485, step time: 0.4796\n",
      "39/388, train_loss: 0.8784, step time: 0.4669\n",
      "40/388, train_loss: 0.8424, step time: 0.4699\n",
      "41/388, train_loss: 0.8879, step time: 0.4727\n",
      "42/388, train_loss: 0.9096, step time: 0.4750\n",
      "43/388, train_loss: 0.8823, step time: 0.4683\n",
      "44/388, train_loss: 0.9225, step time: 0.4688\n",
      "45/388, train_loss: 0.9341, step time: 0.4703\n",
      "46/388, train_loss: 0.8253, step time: 0.4722\n",
      "47/388, train_loss: 0.9665, step time: 0.4681\n",
      "48/388, train_loss: 0.9567, step time: 0.4683\n",
      "49/388, train_loss: 0.8453, step time: 0.4714\n",
      "50/388, train_loss: 0.8389, step time: 0.4735\n",
      "51/388, train_loss: 0.9136, step time: 0.4686\n",
      "52/388, train_loss: 0.9863, step time: 0.4672\n",
      "53/388, train_loss: 0.9494, step time: 0.4921\n",
      "54/388, train_loss: 0.6805, step time: 0.4702\n",
      "55/388, train_loss: 0.9702, step time: 0.4714\n",
      "56/388, train_loss: 0.8710, step time: 0.4716\n",
      "57/388, train_loss: 0.8778, step time: 0.4755\n",
      "58/388, train_loss: 0.9592, step time: 0.4712\n",
      "59/388, train_loss: 0.8486, step time: 0.4708\n",
      "60/388, train_loss: 0.7918, step time: 0.4706\n",
      "61/388, train_loss: 0.8631, step time: 0.4766\n",
      "62/388, train_loss: 0.8779, step time: 0.4712\n",
      "63/388, train_loss: 0.8843, step time: 0.4724\n",
      "64/388, train_loss: 0.9129, step time: 0.4696\n",
      "65/388, train_loss: 0.7605, step time: 0.4704\n",
      "66/388, train_loss: 0.9294, step time: 0.4710\n",
      "67/388, train_loss: 0.9326, step time: 0.4679\n",
      "68/388, train_loss: 0.8717, step time: 0.4701\n",
      "69/388, train_loss: 0.9796, step time: 0.4733\n",
      "70/388, train_loss: 0.8493, step time: 0.4684\n",
      "71/388, train_loss: 0.8867, step time: 0.4686\n",
      "72/388, train_loss: 0.8501, step time: 0.4683\n",
      "73/388, train_loss: 0.8276, step time: 0.4683\n",
      "74/388, train_loss: 0.7481, step time: 0.4681\n",
      "75/388, train_loss: 0.8916, step time: 0.4692\n",
      "76/388, train_loss: 0.8915, step time: 0.4673\n",
      "77/388, train_loss: 0.9799, step time: 0.4673\n",
      "78/388, train_loss: 0.8885, step time: 0.4701\n",
      "79/388, train_loss: 0.9219, step time: 0.4694\n",
      "80/388, train_loss: 0.8983, step time: 0.4682\n",
      "81/388, train_loss: 0.9307, step time: 0.4747\n",
      "82/388, train_loss: 0.9111, step time: 0.4860\n",
      "83/388, train_loss: 0.7900, step time: 0.4797\n",
      "84/388, train_loss: 0.8856, step time: 0.4703\n",
      "85/388, train_loss: 0.9508, step time: 0.4763\n",
      "86/388, train_loss: 0.8563, step time: 0.4670\n",
      "87/388, train_loss: 0.8734, step time: 0.4689\n",
      "88/388, train_loss: 0.9629, step time: 0.4691\n",
      "89/388, train_loss: 0.9889, step time: 0.4699\n",
      "90/388, train_loss: 0.9447, step time: 0.4678\n",
      "91/388, train_loss: 0.8320, step time: 0.4686\n",
      "92/388, train_loss: 0.9911, step time: 0.4709\n",
      "93/388, train_loss: 0.9708, step time: 0.4699\n",
      "94/388, train_loss: 0.9365, step time: 0.4717\n",
      "95/388, train_loss: 0.9808, step time: 0.4674\n",
      "96/388, train_loss: 0.9110, step time: 0.4726\n",
      "97/388, train_loss: 0.9280, step time: 0.4683\n",
      "98/388, train_loss: 0.8231, step time: 0.4707\n",
      "99/388, train_loss: 0.8858, step time: 0.4692\n",
      "100/388, train_loss: 0.8149, step time: 0.4690\n",
      "101/388, train_loss: 0.7863, step time: 0.4686\n",
      "102/388, train_loss: 0.8422, step time: 0.4679\n",
      "103/388, train_loss: 0.8553, step time: 0.4683\n",
      "104/388, train_loss: 0.9722, step time: 0.4693\n",
      "105/388, train_loss: 0.9241, step time: 0.4694\n",
      "106/388, train_loss: 0.9908, step time: 0.4679\n",
      "107/388, train_loss: 0.8027, step time: 0.4680\n",
      "108/388, train_loss: 0.9425, step time: 0.4707\n",
      "109/388, train_loss: 0.8923, step time: 0.4721\n",
      "110/388, train_loss: 0.9196, step time: 0.4721\n",
      "111/388, train_loss: 0.9594, step time: 0.4728\n",
      "112/388, train_loss: 0.7689, step time: 0.4685\n",
      "113/388, train_loss: 0.8815, step time: 0.4717\n",
      "114/388, train_loss: 0.9097, step time: 0.4774\n",
      "115/388, train_loss: 0.9120, step time: 0.4779\n",
      "116/388, train_loss: 0.9015, step time: 0.4723\n",
      "117/388, train_loss: 0.8193, step time: 0.4726\n",
      "118/388, train_loss: 0.8274, step time: 0.4690\n",
      "119/388, train_loss: 0.8871, step time: 0.4740\n",
      "120/388, train_loss: 0.7391, step time: 0.4706\n",
      "121/388, train_loss: 0.9293, step time: 0.4711\n",
      "122/388, train_loss: 0.8938, step time: 0.4761\n",
      "123/388, train_loss: 0.9037, step time: 0.4744\n",
      "124/388, train_loss: 0.8642, step time: 0.4807\n",
      "125/388, train_loss: 0.8787, step time: 0.4914\n",
      "126/388, train_loss: 0.8384, step time: 0.4761\n",
      "127/388, train_loss: 0.9619, step time: 1.1534\n",
      "128/388, train_loss: 0.8074, step time: 0.5222\n",
      "129/388, train_loss: 0.9294, step time: 0.4922\n",
      "130/388, train_loss: 0.8840, step time: 0.4904\n",
      "131/388, train_loss: 0.9010, step time: 0.4811\n",
      "132/388, train_loss: 0.9055, step time: 0.4748\n",
      "133/388, train_loss: 0.9296, step time: 0.4772\n",
      "134/388, train_loss: 0.8559, step time: 0.4872\n",
      "135/388, train_loss: 0.8999, step time: 0.4885\n",
      "136/388, train_loss: 0.9741, step time: 0.4755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/388, train_loss: 0.9437, step time: 0.4753\n",
      "138/388, train_loss: 0.9363, step time: 0.4736\n",
      "139/388, train_loss: 0.9211, step time: 0.4701\n",
      "140/388, train_loss: 0.9173, step time: 0.4742\n",
      "141/388, train_loss: 0.7605, step time: 0.4753\n",
      "142/388, train_loss: 0.9313, step time: 0.4788\n",
      "143/388, train_loss: 0.9828, step time: 0.4730\n",
      "144/388, train_loss: 0.8692, step time: 0.4737\n",
      "145/388, train_loss: 0.9144, step time: 0.4710\n",
      "146/388, train_loss: 0.7670, step time: 0.4921\n",
      "147/388, train_loss: 0.8449, step time: 0.4914\n",
      "148/388, train_loss: 0.8968, step time: 0.5095\n",
      "149/388, train_loss: 0.9627, step time: 0.5014\n",
      "150/388, train_loss: 0.9584, step time: 0.4855\n",
      "151/388, train_loss: 0.8923, step time: 0.4878\n",
      "152/388, train_loss: 0.8402, step time: 0.4805\n",
      "153/388, train_loss: 0.9882, step time: 0.9077\n",
      "154/388, train_loss: 0.9203, step time: 0.5282\n",
      "155/388, train_loss: 0.9871, step time: 0.5003\n",
      "156/388, train_loss: 0.9040, step time: 0.4949\n",
      "157/388, train_loss: 0.9822, step time: 0.4837\n",
      "158/388, train_loss: 0.9938, step time: 0.4823\n",
      "159/388, train_loss: 0.9212, step time: 0.4903\n",
      "160/388, train_loss: 0.9079, step time: 0.4822\n",
      "161/388, train_loss: 0.8095, step time: 0.4747\n",
      "162/388, train_loss: 0.8956, step time: 0.4719\n",
      "163/388, train_loss: 0.9325, step time: 0.4716\n",
      "164/388, train_loss: 0.9317, step time: 0.4738\n",
      "165/388, train_loss: 0.9520, step time: 0.4736\n",
      "166/388, train_loss: 0.8832, step time: 0.4839\n",
      "167/388, train_loss: 0.9804, step time: 0.4831\n",
      "168/388, train_loss: 0.8620, step time: 0.4757\n",
      "169/388, train_loss: 0.7806, step time: 0.4757\n",
      "170/388, train_loss: 0.9435, step time: 1.0276\n",
      "171/388, train_loss: 0.9052, step time: 0.5270\n",
      "172/388, train_loss: 0.7783, step time: 0.5136\n",
      "173/388, train_loss: 0.8579, step time: 0.4907\n",
      "174/388, train_loss: 0.9341, step time: 0.4983\n",
      "175/388, train_loss: 0.9673, step time: 0.4785\n",
      "176/388, train_loss: 0.9714, step time: 0.4766\n",
      "177/388, train_loss: 0.7873, step time: 0.4799\n",
      "178/388, train_loss: 0.9398, step time: 0.4702\n",
      "179/388, train_loss: 0.7588, step time: 0.4716\n",
      "180/388, train_loss: 0.9149, step time: 0.6720\n",
      "181/388, train_loss: 0.9181, step time: 0.5406\n",
      "182/388, train_loss: 0.9469, step time: 0.5170\n",
      "183/388, train_loss: 0.8815, step time: 0.4942\n",
      "184/388, train_loss: 0.9245, step time: 0.4960\n",
      "185/388, train_loss: 0.9588, step time: 0.4808\n",
      "186/388, train_loss: 0.9078, step time: 0.5159\n",
      "187/388, train_loss: 0.9421, step time: 0.4999\n",
      "188/388, train_loss: 0.9499, step time: 0.5277\n",
      "189/388, train_loss: 0.9298, step time: 0.5958\n",
      "190/388, train_loss: 0.9796, step time: 0.5348\n",
      "191/388, train_loss: 0.8351, step time: 0.5037\n",
      "192/388, train_loss: 0.9445, step time: 0.4923\n",
      "193/388, train_loss: 0.8651, step time: 0.4967\n",
      "194/388, train_loss: 0.8707, step time: 0.4798\n",
      "195/388, train_loss: 0.7132, step time: 0.4726\n",
      "196/388, train_loss: 0.8192, step time: 0.4987\n",
      "197/388, train_loss: 0.6951, step time: 0.4851\n",
      "198/388, train_loss: 0.8868, step time: 0.4919\n",
      "199/388, train_loss: 0.8297, step time: 0.6759\n",
      "200/388, train_loss: 0.8057, step time: 0.5491\n",
      "201/388, train_loss: 0.7544, step time: 0.5082\n",
      "202/388, train_loss: 0.9152, step time: 0.5054\n",
      "203/388, train_loss: 0.9703, step time: 0.4859\n",
      "204/388, train_loss: 0.9475, step time: 0.5013\n",
      "205/388, train_loss: 0.9651, step time: 0.4826\n",
      "206/388, train_loss: 0.8474, step time: 0.9065\n",
      "207/388, train_loss: 0.9031, step time: 0.5277\n",
      "208/388, train_loss: 0.7978, step time: 0.5062\n",
      "209/388, train_loss: 0.8802, step time: 0.4915\n",
      "210/388, train_loss: 0.9056, step time: 0.5011\n",
      "211/388, train_loss: 0.9020, step time: 0.4830\n",
      "212/388, train_loss: 0.8315, step time: 0.6543\n",
      "213/388, train_loss: 0.9487, step time: 0.5192\n",
      "214/388, train_loss: 0.9441, step time: 0.4947\n",
      "215/388, train_loss: 0.9331, step time: 0.4932\n",
      "216/388, train_loss: 0.9643, step time: 0.4813\n",
      "217/388, train_loss: 0.8900, step time: 0.4805\n",
      "218/388, train_loss: 0.9134, step time: 0.4928\n",
      "219/388, train_loss: 0.8603, step time: 0.4838\n",
      "220/388, train_loss: 0.9660, step time: 0.4852\n",
      "221/388, train_loss: 0.8720, step time: 0.4714\n",
      "222/388, train_loss: 0.9803, step time: 0.4708\n",
      "223/388, train_loss: 0.9148, step time: 0.4749\n",
      "224/388, train_loss: 0.8342, step time: 0.4731\n",
      "225/388, train_loss: 0.9579, step time: 0.4726\n",
      "226/388, train_loss: 0.8431, step time: 0.4770\n",
      "227/388, train_loss: 0.9802, step time: 0.9551\n",
      "228/388, train_loss: 0.8872, step time: 0.5440\n",
      "229/388, train_loss: 0.7926, step time: 0.5178\n",
      "230/388, train_loss: 0.7879, step time: 0.5094\n",
      "231/388, train_loss: 0.8403, step time: 0.4969\n",
      "232/388, train_loss: 0.8254, step time: 0.4957\n",
      "233/388, train_loss: 0.8875, step time: 0.4831\n",
      "234/388, train_loss: 0.8777, step time: 0.4942\n",
      "235/388, train_loss: 0.7589, step time: 0.4820\n",
      "236/388, train_loss: 0.8133, step time: 1.0918\n",
      "237/388, train_loss: 0.9845, step time: 0.5251\n",
      "238/388, train_loss: 0.7812, step time: 0.5131\n",
      "239/388, train_loss: 0.8422, step time: 0.4863\n",
      "240/388, train_loss: 0.8902, step time: 0.4945\n",
      "241/388, train_loss: 0.9253, step time: 0.4768\n",
      "242/388, train_loss: 0.9011, step time: 0.4720\n",
      "243/388, train_loss: 0.9062, step time: 0.9964\n",
      "244/388, train_loss: 0.8479, step time: 0.5234\n",
      "245/388, train_loss: 0.9850, step time: 0.4964\n",
      "246/388, train_loss: 0.8070, step time: 0.4859\n",
      "247/388, train_loss: 0.9015, step time: 0.4841\n",
      "248/388, train_loss: 0.9587, step time: 0.4959\n",
      "249/388, train_loss: 0.8531, step time: 0.4760\n",
      "250/388, train_loss: 0.8852, step time: 0.4719\n",
      "251/388, train_loss: 0.7895, step time: 0.4837\n",
      "252/388, train_loss: 0.9522, step time: 0.9652\n",
      "253/388, train_loss: 0.9237, step time: 0.5253\n",
      "254/388, train_loss: 0.9033, step time: 0.5105\n",
      "255/388, train_loss: 0.7723, step time: 0.4881\n",
      "256/388, train_loss: 0.7654, step time: 0.4942\n",
      "257/388, train_loss: 0.9232, step time: 0.4852\n",
      "258/388, train_loss: 0.9297, step time: 0.4787\n",
      "259/388, train_loss: 0.9312, step time: 0.4884\n",
      "260/388, train_loss: 0.8986, step time: 0.4791\n",
      "261/388, train_loss: 0.7890, step time: 0.4764\n",
      "262/388, train_loss: 0.8756, step time: 0.4758\n",
      "263/388, train_loss: 0.9332, step time: 0.4874\n",
      "264/388, train_loss: 0.8294, step time: 0.4880\n",
      "265/388, train_loss: 0.9163, step time: 0.7845\n",
      "266/388, train_loss: 0.9098, step time: 0.5435\n",
      "267/388, train_loss: 0.8663, step time: 0.5137\n",
      "268/388, train_loss: 0.8306, step time: 0.4893\n",
      "269/388, train_loss: 0.8863, step time: 0.4939\n",
      "270/388, train_loss: 0.8100, step time: 0.4758\n",
      "271/388, train_loss: 0.8004, step time: 0.4776\n",
      "272/388, train_loss: 0.7469, step time: 0.4756\n",
      "273/388, train_loss: 0.8787, step time: 0.5727\n",
      "274/388, train_loss: 0.7721, step time: 0.5337\n",
      "275/388, train_loss: 0.7686, step time: 0.5079\n",
      "276/388, train_loss: 0.9276, step time: 0.4892\n",
      "277/388, train_loss: 0.8937, step time: 0.4999\n",
      "278/388, train_loss: 0.8406, step time: 0.4889\n",
      "279/388, train_loss: 0.8550, step time: 0.4797\n",
      "280/388, train_loss: 0.7776, step time: 0.5072\n",
      "281/388, train_loss: 0.9786, step time: 0.4841\n",
      "282/388, train_loss: 0.9858, step time: 0.4894\n",
      "283/388, train_loss: 0.8422, step time: 0.5073\n",
      "284/388, train_loss: 0.8565, step time: 0.4870\n",
      "285/388, train_loss: 0.6750, step time: 0.9914\n",
      "286/388, train_loss: 0.8442, step time: 0.5418\n",
      "287/388, train_loss: 0.8458, step time: 0.5202\n",
      "288/388, train_loss: 0.8932, step time: 0.4984\n",
      "289/388, train_loss: 0.8248, step time: 0.4818\n",
      "290/388, train_loss: 0.7935, step time: 0.4753\n",
      "291/388, train_loss: 0.7166, step time: 0.4888\n",
      "292/388, train_loss: 0.8411, step time: 0.9655\n",
      "293/388, train_loss: 0.8356, step time: 0.5616\n",
      "294/388, train_loss: 0.9525, step time: 0.5146\n",
      "295/388, train_loss: 0.8826, step time: 0.5111\n",
      "296/388, train_loss: 0.7983, step time: 0.4900\n",
      "297/388, train_loss: 0.9545, step time: 0.4949\n",
      "298/388, train_loss: 0.8211, step time: 0.4788\n",
      "299/388, train_loss: 0.9244, step time: 0.4791\n",
      "300/388, train_loss: 0.9202, step time: 0.4947\n",
      "301/388, train_loss: 0.8284, step time: 0.4803\n",
      "302/388, train_loss: 0.8475, step time: 0.4861\n",
      "303/388, train_loss: 0.6800, step time: 0.4891\n",
      "304/388, train_loss: 0.9406, step time: 0.5041\n",
      "305/388, train_loss: 0.8751, step time: 0.4880\n",
      "306/388, train_loss: 0.7359, step time: 0.5042\n",
      "307/388, train_loss: 0.8782, step time: 0.4895\n",
      "308/388, train_loss: 0.9248, step time: 0.4971\n",
      "309/388, train_loss: 0.7597, step time: 0.4819\n",
      "310/388, train_loss: 0.7153, step time: 0.4754\n",
      "311/388, train_loss: 0.8555, step time: 0.4799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/388, train_loss: 0.7459, step time: 0.4783\n",
      "313/388, train_loss: 0.8728, step time: 0.9291\n",
      "314/388, train_loss: 0.8397, step time: 0.5456\n",
      "315/388, train_loss: 0.9520, step time: 0.5146\n",
      "316/388, train_loss: 0.8157, step time: 0.4983\n",
      "317/388, train_loss: 0.9547, step time: 0.4828\n",
      "318/388, train_loss: 0.9158, step time: 0.4880\n",
      "319/388, train_loss: 0.9411, step time: 0.4913\n",
      "320/388, train_loss: 0.7223, step time: 0.8701\n",
      "321/388, train_loss: 0.8759, step time: 0.5421\n",
      "322/388, train_loss: 0.9295, step time: 0.5109\n",
      "323/388, train_loss: 0.8214, step time: 0.4928\n",
      "324/388, train_loss: 0.8531, step time: 0.5495\n",
      "325/388, train_loss: 0.7397, step time: 0.5279\n",
      "326/388, train_loss: 0.9218, step time: 0.5045\n",
      "327/388, train_loss: 0.9518, step time: 0.5103\n",
      "328/388, train_loss: 0.9629, step time: 0.4852\n",
      "329/388, train_loss: 0.8271, step time: 0.4862\n",
      "330/388, train_loss: 0.9789, step time: 0.9721\n",
      "331/388, train_loss: 0.9128, step time: 0.5276\n",
      "332/388, train_loss: 0.6410, step time: 0.5068\n",
      "333/388, train_loss: 0.8856, step time: 0.4825\n",
      "334/388, train_loss: 0.7426, step time: 0.4838\n",
      "335/388, train_loss: 0.8931, step time: 0.4902\n",
      "336/388, train_loss: 0.9766, step time: 0.4728\n",
      "337/388, train_loss: 0.8497, step time: 0.6220\n",
      "338/388, train_loss: 0.8585, step time: 0.5447\n",
      "339/388, train_loss: 0.9912, step time: 0.5167\n",
      "340/388, train_loss: 0.9711, step time: 0.4945\n",
      "341/388, train_loss: 0.8638, step time: 0.4923\n",
      "342/388, train_loss: 0.9368, step time: 0.4909\n",
      "343/388, train_loss: 0.9597, step time: 0.4831\n",
      "344/388, train_loss: 0.8846, step time: 0.4934\n",
      "345/388, train_loss: 0.7852, step time: 0.4802\n",
      "346/388, train_loss: 0.7017, step time: 0.5382\n",
      "347/388, train_loss: 0.8543, step time: 0.5231\n",
      "348/388, train_loss: 0.9684, step time: 0.5039\n",
      "349/388, train_loss: 0.9209, step time: 0.4941\n",
      "350/388, train_loss: 0.7178, step time: 0.4893\n",
      "351/388, train_loss: 0.6036, step time: 0.4863\n",
      "352/388, train_loss: 0.9506, step time: 0.5552\n",
      "353/388, train_loss: 0.9725, step time: 0.5346\n",
      "354/388, train_loss: 0.7785, step time: 0.5038\n",
      "355/388, train_loss: 0.8617, step time: 0.4997\n",
      "356/388, train_loss: 0.9529, step time: 0.4978\n",
      "357/388, train_loss: 0.5972, step time: 0.5067\n",
      "358/388, train_loss: 0.7784, step time: 0.5001\n",
      "359/388, train_loss: 0.7967, step time: 0.5498\n",
      "360/388, train_loss: 0.9373, step time: 0.5229\n",
      "361/388, train_loss: 0.9356, step time: 0.5043\n",
      "362/388, train_loss: 0.9430, step time: 0.5011\n",
      "363/388, train_loss: 0.7798, step time: 0.4816\n",
      "364/388, train_loss: 0.8835, step time: 0.4912\n",
      "365/388, train_loss: 0.9041, step time: 0.4989\n",
      "366/388, train_loss: 0.7071, step time: 0.4871\n",
      "367/388, train_loss: 0.7923, step time: 1.1595\n",
      "368/388, train_loss: 0.8998, step time: 0.5500\n",
      "369/388, train_loss: 0.8578, step time: 0.5212\n",
      "370/388, train_loss: 0.8153, step time: 0.4987\n",
      "371/388, train_loss: 0.6339, step time: 0.5005\n",
      "372/388, train_loss: 0.8792, step time: 0.5221\n",
      "373/388, train_loss: 0.9477, step time: 0.4996\n",
      "374/388, train_loss: 0.8101, step time: 0.4886\n",
      "375/388, train_loss: 0.9237, step time: 0.4946\n",
      "376/388, train_loss: 0.9877, step time: 0.4811\n",
      "377/388, train_loss: 0.8063, step time: 0.4914\n",
      "378/388, train_loss: 0.8245, step time: 0.4777\n",
      "379/388, train_loss: 0.8023, step time: 1.0428\n",
      "380/388, train_loss: 0.9338, step time: 0.5417\n",
      "381/388, train_loss: 0.8816, step time: 0.5076\n",
      "382/388, train_loss: 0.8621, step time: 0.4852\n",
      "383/388, train_loss: 0.9376, step time: 0.4759\n",
      "384/388, train_loss: 0.8997, step time: 0.4743\n",
      "385/388, train_loss: 0.7685, step time: 0.4748\n",
      "386/388, train_loss: 0.9318, step time: 0.5300\n",
      "387/388, train_loss: 0.7655, step time: 0.4993\n",
      "388/388, train_loss: 0.7951, step time: 0.4753\n",
      "epoch 2 average loss: 0.8765\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.4588 tc: 0.4740 wt: 0.6635 et: 0.2389\n",
      "best mean dice: 0.4588 at epoch: 2\n",
      "time consuming of epoch 2 is: 287.9808\n",
      "----------\n",
      "epoch 3/300\n",
      "1/388, train_loss: 0.8779, step time: 0.4700\n",
      "2/388, train_loss: 0.8485, step time: 0.8409\n",
      "3/388, train_loss: 0.6851, step time: 0.5442\n",
      "4/388, train_loss: 0.7897, step time: 0.5146\n",
      "5/388, train_loss: 0.7084, step time: 0.5076\n",
      "6/388, train_loss: 0.9062, step time: 0.7791\n",
      "7/388, train_loss: 0.6676, step time: 0.5555\n",
      "8/388, train_loss: 0.9072, step time: 0.5367\n",
      "9/388, train_loss: 0.9431, step time: 0.5059\n",
      "10/388, train_loss: 0.8760, step time: 0.4980\n",
      "11/388, train_loss: 0.7452, step time: 0.4875\n",
      "12/388, train_loss: 0.7425, step time: 0.5418\n",
      "13/388, train_loss: 0.8677, step time: 0.4990\n",
      "14/388, train_loss: 0.9136, step time: 0.8296\n",
      "15/388, train_loss: 0.8289, step time: 0.5665\n",
      "16/388, train_loss: 0.8156, step time: 0.5385\n",
      "17/388, train_loss: 0.8932, step time: 0.5106\n",
      "18/388, train_loss: 0.7084, step time: 0.5222\n",
      "19/388, train_loss: 0.6477, step time: 0.4952\n",
      "20/388, train_loss: 0.6349, step time: 0.8512\n",
      "21/388, train_loss: 0.9059, step time: 0.5454\n",
      "22/388, train_loss: 0.8145, step time: 0.5131\n",
      "23/388, train_loss: 0.9451, step time: 0.5251\n",
      "24/388, train_loss: 0.8265, step time: 0.5148\n",
      "25/388, train_loss: 0.8004, step time: 0.5055\n",
      "26/388, train_loss: 0.9231, step time: 0.4971\n",
      "27/388, train_loss: 0.9476, step time: 0.5066\n",
      "28/388, train_loss: 0.5983, step time: 1.2581\n",
      "29/388, train_loss: 0.8328, step time: 0.5366\n",
      "30/388, train_loss: 0.8633, step time: 0.5252\n",
      "31/388, train_loss: 0.7840, step time: 0.5090\n",
      "32/388, train_loss: 0.9817, step time: 0.5756\n",
      "33/388, train_loss: 0.9389, step time: 0.5867\n",
      "34/388, train_loss: 0.9363, step time: 0.5261\n",
      "35/388, train_loss: 0.8271, step time: 0.5085\n",
      "36/388, train_loss: 0.9859, step time: 0.5002\n",
      "37/388, train_loss: 0.8942, step time: 0.5661\n",
      "38/388, train_loss: 0.8141, step time: 0.5397\n",
      "39/388, train_loss: 0.9723, step time: 0.5209\n",
      "40/388, train_loss: 0.9865, step time: 0.5023\n",
      "41/388, train_loss: 0.8115, step time: 1.2154\n",
      "42/388, train_loss: 0.7947, step time: 0.5397\n",
      "43/388, train_loss: 0.8329, step time: 0.5049\n",
      "44/388, train_loss: 0.8154, step time: 0.4861\n",
      "45/388, train_loss: 0.9314, step time: 0.4939\n",
      "46/388, train_loss: 0.9076, step time: 0.4974\n",
      "47/388, train_loss: 0.7582, step time: 0.4803\n",
      "48/388, train_loss: 0.7422, step time: 0.4926\n",
      "49/388, train_loss: 0.9035, step time: 0.4824\n",
      "50/388, train_loss: 0.6692, step time: 0.4856\n",
      "51/388, train_loss: 0.7409, step time: 0.5387\n",
      "52/388, train_loss: 0.8071, step time: 0.5094\n",
      "53/388, train_loss: 0.6468, step time: 0.5163\n",
      "54/388, train_loss: 0.8882, step time: 1.0873\n",
      "55/388, train_loss: 0.8525, step time: 0.5561\n",
      "56/388, train_loss: 0.7059, step time: 0.5208\n",
      "57/388, train_loss: 0.8304, step time: 0.5071\n",
      "58/388, train_loss: 0.9034, step time: 0.4893\n",
      "59/388, train_loss: 0.8074, step time: 0.5206\n",
      "60/388, train_loss: 0.8448, step time: 0.4991\n",
      "61/388, train_loss: 0.9275, step time: 0.4898\n",
      "62/388, train_loss: 0.8262, step time: 0.4930\n",
      "63/388, train_loss: 0.7366, step time: 0.4884\n",
      "64/388, train_loss: 0.8883, step time: 0.5070\n",
      "65/388, train_loss: 0.6905, step time: 0.8216\n",
      "66/388, train_loss: 0.6704, step time: 0.5445\n",
      "67/388, train_loss: 0.9476, step time: 0.5200\n",
      "68/388, train_loss: 0.8921, step time: 0.4939\n",
      "69/388, train_loss: 0.9709, step time: 0.5175\n",
      "70/388, train_loss: 0.6988, step time: 0.4897\n",
      "71/388, train_loss: 0.8540, step time: 0.4918\n",
      "72/388, train_loss: 0.7506, step time: 0.5074\n",
      "73/388, train_loss: 0.7789, step time: 0.5755\n",
      "74/388, train_loss: 0.8027, step time: 0.5770\n",
      "75/388, train_loss: 0.5849, step time: 0.5292\n",
      "76/388, train_loss: 0.9712, step time: 0.5073\n",
      "77/388, train_loss: 0.8519, step time: 0.5895\n",
      "78/388, train_loss: 0.8114, step time: 0.5428\n",
      "79/388, train_loss: 0.8325, step time: 0.5192\n",
      "80/388, train_loss: 0.8658, step time: 0.4966\n",
      "81/388, train_loss: 0.9712, step time: 1.0798\n",
      "82/388, train_loss: 0.7856, step time: 0.5342\n",
      "83/388, train_loss: 0.9380, step time: 0.5090\n",
      "84/388, train_loss: 0.8084, step time: 0.4892\n",
      "85/388, train_loss: 0.9075, step time: 0.4838\n",
      "86/388, train_loss: 0.8168, step time: 1.0094\n",
      "87/388, train_loss: 0.8515, step time: 0.5463\n",
      "88/388, train_loss: 0.7348, step time: 0.5158\n",
      "89/388, train_loss: 0.8978, step time: 0.4875\n",
      "90/388, train_loss: 0.8354, step time: 0.4892\n",
      "91/388, train_loss: 0.7938, step time: 0.5221\n",
      "92/388, train_loss: 0.9210, step time: 0.5181\n",
      "93/388, train_loss: 0.8177, step time: 0.5535\n",
      "94/388, train_loss: 0.8034, step time: 0.5538\n",
      "95/388, train_loss: 0.9887, step time: 0.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/388, train_loss: 0.7386, step time: 0.5004\n",
      "97/388, train_loss: 0.7469, step time: 0.5011\n",
      "98/388, train_loss: 0.9132, step time: 0.4837\n",
      "99/388, train_loss: 0.7620, step time: 1.0359\n",
      "100/388, train_loss: 0.8052, step time: 0.5410\n",
      "101/388, train_loss: 0.7891, step time: 0.5075\n",
      "102/388, train_loss: 0.8534, step time: 0.4976\n",
      "103/388, train_loss: 0.7563, step time: 0.4989\n",
      "104/388, train_loss: 0.6630, step time: 0.5016\n",
      "105/388, train_loss: 0.7081, step time: 0.5131\n",
      "106/388, train_loss: 0.7810, step time: 0.5012\n",
      "107/388, train_loss: 0.8375, step time: 0.4993\n",
      "108/388, train_loss: 0.8075, step time: 0.4892\n",
      "109/388, train_loss: 0.6484, step time: 0.5381\n",
      "110/388, train_loss: 0.9849, step time: 0.5184\n",
      "111/388, train_loss: 0.9784, step time: 0.4939\n",
      "112/388, train_loss: 0.7709, step time: 0.4941\n",
      "113/388, train_loss: 0.9465, step time: 0.4864\n",
      "114/388, train_loss: 0.8808, step time: 0.7968\n",
      "115/388, train_loss: 0.8922, step time: 0.5483\n",
      "116/388, train_loss: 0.8511, step time: 0.5199\n",
      "117/388, train_loss: 0.7344, step time: 0.5007\n",
      "118/388, train_loss: 0.7641, step time: 0.4923\n",
      "119/388, train_loss: 0.7830, step time: 0.4905\n",
      "120/388, train_loss: 0.9099, step time: 1.1775\n",
      "121/388, train_loss: 0.6837, step time: 0.5421\n",
      "122/388, train_loss: 0.8716, step time: 0.5102\n",
      "123/388, train_loss: 0.5861, step time: 0.4979\n",
      "124/388, train_loss: 0.9190, step time: 0.4917\n",
      "125/388, train_loss: 0.9332, step time: 0.4811\n",
      "126/388, train_loss: 0.5327, step time: 0.4886\n",
      "127/388, train_loss: 0.9489, step time: 0.9455\n",
      "128/388, train_loss: 0.8539, step time: 0.5513\n",
      "129/388, train_loss: 0.6667, step time: 0.5095\n",
      "130/388, train_loss: 0.8076, step time: 0.4996\n",
      "131/388, train_loss: 0.7053, step time: 0.5095\n",
      "132/388, train_loss: 0.9583, step time: 0.4992\n",
      "133/388, train_loss: 0.8017, step time: 0.4815\n",
      "134/388, train_loss: 0.7797, step time: 0.4805\n",
      "135/388, train_loss: 0.8322, step time: 0.8283\n",
      "136/388, train_loss: 0.7991, step time: 0.5371\n",
      "137/388, train_loss: 0.7811, step time: 0.5075\n",
      "138/388, train_loss: 0.8278, step time: 0.4882\n",
      "139/388, train_loss: 0.8931, step time: 0.4925\n",
      "140/388, train_loss: 0.8110, step time: 0.4901\n",
      "141/388, train_loss: 0.8287, step time: 0.4889\n",
      "142/388, train_loss: 0.6948, step time: 0.4946\n",
      "143/388, train_loss: 0.8787, step time: 0.4847\n",
      "144/388, train_loss: 0.7240, step time: 1.0592\n",
      "145/388, train_loss: 0.8297, step time: 0.5414\n",
      "146/388, train_loss: 0.5291, step time: 0.5203\n",
      "147/388, train_loss: 0.7726, step time: 0.5020\n",
      "148/388, train_loss: 0.8511, step time: 0.5011\n",
      "149/388, train_loss: 0.5675, step time: 0.4912\n",
      "150/388, train_loss: 0.6887, step time: 0.4945\n",
      "151/388, train_loss: 0.9661, step time: 0.4803\n",
      "152/388, train_loss: 0.7617, step time: 0.4923\n",
      "153/388, train_loss: 0.7936, step time: 0.4760\n",
      "154/388, train_loss: 0.8994, step time: 0.4979\n",
      "155/388, train_loss: 0.8841, step time: 0.5021\n",
      "156/388, train_loss: 0.7510, step time: 0.4939\n",
      "157/388, train_loss: 0.7403, step time: 0.4930\n",
      "158/388, train_loss: 0.6309, step time: 1.1779\n",
      "159/388, train_loss: 0.9354, step time: 0.5356\n",
      "160/388, train_loss: 0.8574, step time: 0.4980\n",
      "161/388, train_loss: 0.8810, step time: 0.4968\n",
      "162/388, train_loss: 0.9318, step time: 0.4845\n",
      "163/388, train_loss: 0.8278, step time: 0.4878\n",
      "164/388, train_loss: 0.9004, step time: 0.5108\n",
      "165/388, train_loss: 0.9139, step time: 0.5000\n",
      "166/388, train_loss: 0.8455, step time: 0.4845\n",
      "167/388, train_loss: 0.8182, step time: 0.4927\n",
      "168/388, train_loss: 0.9486, step time: 0.4873\n",
      "169/388, train_loss: 0.7802, step time: 0.4814\n",
      "170/388, train_loss: 0.9043, step time: 1.1848\n",
      "171/388, train_loss: 0.7497, step time: 0.5453\n",
      "172/388, train_loss: 0.7722, step time: 0.5110\n",
      "173/388, train_loss: 0.7262, step time: 0.4962\n",
      "174/388, train_loss: 0.8379, step time: 0.5027\n",
      "175/388, train_loss: 0.6777, step time: 0.4961\n",
      "176/388, train_loss: 0.7699, step time: 0.4793\n",
      "177/388, train_loss: 0.6433, step time: 0.4839\n",
      "178/388, train_loss: 0.9234, step time: 0.4910\n",
      "179/388, train_loss: 0.7985, step time: 0.4832\n",
      "180/388, train_loss: 0.8041, step time: 0.4951\n",
      "181/388, train_loss: 0.9282, step time: 0.4816\n",
      "182/388, train_loss: 0.8504, step time: 0.4893\n",
      "183/388, train_loss: 0.7342, step time: 0.4940\n",
      "184/388, train_loss: 0.8717, step time: 1.1590\n",
      "185/388, train_loss: 0.7739, step time: 0.5254\n",
      "186/388, train_loss: 0.9645, step time: 0.4949\n",
      "187/388, train_loss: 0.9514, step time: 0.4872\n",
      "188/388, train_loss: 0.7512, step time: 0.4944\n",
      "189/388, train_loss: 0.8132, step time: 0.4770\n",
      "190/388, train_loss: 0.5281, step time: 0.4860\n",
      "191/388, train_loss: 0.8985, step time: 0.8641\n",
      "192/388, train_loss: 0.7481, step time: 0.5481\n",
      "193/388, train_loss: 0.6166, step time: 0.5044\n",
      "194/388, train_loss: 0.8035, step time: 0.4962\n",
      "195/388, train_loss: 0.6727, step time: 0.4819\n",
      "196/388, train_loss: 0.8003, step time: 0.4897\n",
      "197/388, train_loss: 0.8042, step time: 0.4800\n",
      "198/388, train_loss: 0.9738, step time: 0.4715\n",
      "199/388, train_loss: 0.5434, step time: 0.4948\n",
      "200/388, train_loss: 0.7344, step time: 0.4892\n",
      "201/388, train_loss: 0.9658, step time: 0.5015\n",
      "202/388, train_loss: 0.7326, step time: 0.4814\n",
      "203/388, train_loss: 0.8847, step time: 0.5276\n",
      "204/388, train_loss: 0.6488, step time: 0.5278\n",
      "205/388, train_loss: 0.6608, step time: 0.5680\n",
      "206/388, train_loss: 0.7594, step time: 0.5291\n",
      "207/388, train_loss: 0.7942, step time: 0.5228\n",
      "208/388, train_loss: 0.8675, step time: 0.5049\n",
      "209/388, train_loss: 0.9079, step time: 0.4969\n",
      "210/388, train_loss: 0.6965, step time: 0.4795\n",
      "211/388, train_loss: 0.9234, step time: 0.4958\n",
      "212/388, train_loss: 0.7456, step time: 0.4892\n",
      "213/388, train_loss: 0.8901, step time: 0.4971\n",
      "214/388, train_loss: 0.8264, step time: 0.4893\n",
      "215/388, train_loss: 0.9180, step time: 1.1070\n",
      "216/388, train_loss: 0.8169, step time: 0.5403\n",
      "217/388, train_loss: 0.6832, step time: 0.5000\n",
      "218/388, train_loss: 0.8519, step time: 0.4877\n",
      "219/388, train_loss: 0.9602, step time: 0.4938\n",
      "220/388, train_loss: 0.4904, step time: 0.4776\n",
      "221/388, train_loss: 0.6752, step time: 0.4996\n",
      "222/388, train_loss: 0.6678, step time: 0.4839\n",
      "223/388, train_loss: 0.8363, step time: 0.5070\n",
      "224/388, train_loss: 0.7525, step time: 0.4924\n",
      "225/388, train_loss: 0.7712, step time: 0.4980\n",
      "226/388, train_loss: 0.7203, step time: 0.4908\n",
      "227/388, train_loss: 0.8645, step time: 0.4874\n",
      "228/388, train_loss: 0.7389, step time: 0.5094\n",
      "229/388, train_loss: 0.7606, step time: 0.5000\n",
      "230/388, train_loss: 0.7164, step time: 0.4873\n",
      "231/388, train_loss: 0.8225, step time: 0.5018\n",
      "232/388, train_loss: 0.9232, step time: 0.4904\n",
      "233/388, train_loss: 0.7143, step time: 0.4975\n",
      "234/388, train_loss: 0.9472, step time: 0.4969\n",
      "235/388, train_loss: 0.8260, step time: 0.4868\n",
      "236/388, train_loss: 0.9141, step time: 1.2110\n",
      "237/388, train_loss: 0.6605, step time: 0.5378\n",
      "238/388, train_loss: 0.8273, step time: 0.5086\n",
      "239/388, train_loss: 0.5997, step time: 0.4953\n",
      "240/388, train_loss: 0.8077, step time: 0.4952\n",
      "241/388, train_loss: 0.8784, step time: 0.4791\n",
      "242/388, train_loss: 0.9241, step time: 0.4779\n",
      "243/388, train_loss: 0.7900, step time: 0.4789\n",
      "244/388, train_loss: 0.8931, step time: 0.4777\n",
      "245/388, train_loss: 0.9014, step time: 0.4892\n",
      "246/388, train_loss: 0.7863, step time: 0.4931\n",
      "247/388, train_loss: 0.8623, step time: 1.1367\n",
      "248/388, train_loss: 0.8210, step time: 0.5220\n",
      "249/388, train_loss: 0.9879, step time: 0.4995\n",
      "250/388, train_loss: 0.8365, step time: 0.4991\n",
      "251/388, train_loss: 0.6632, step time: 0.6128\n",
      "252/388, train_loss: 0.6422, step time: 0.5521\n",
      "253/388, train_loss: 0.5872, step time: 0.5245\n",
      "254/388, train_loss: 0.8669, step time: 0.5002\n",
      "255/388, train_loss: 0.8773, step time: 0.5059\n",
      "256/388, train_loss: 0.8562, step time: 0.4869\n",
      "257/388, train_loss: 0.9444, step time: 0.4929\n",
      "258/388, train_loss: 0.6786, step time: 0.4795\n",
      "259/388, train_loss: 0.9534, step time: 0.4728\n",
      "260/388, train_loss: 0.8115, step time: 0.4843\n",
      "261/388, train_loss: 0.7087, step time: 0.4791\n",
      "262/388, train_loss: 0.7906, step time: 0.4951\n",
      "263/388, train_loss: 0.8111, step time: 0.5335\n",
      "264/388, train_loss: 0.8719, step time: 0.5250\n",
      "265/388, train_loss: 0.7575, step time: 0.4952\n",
      "266/388, train_loss: 0.9815, step time: 0.4959\n",
      "267/388, train_loss: 0.9413, step time: 0.4778\n",
      "268/388, train_loss: 0.6742, step time: 0.9553\n",
      "269/388, train_loss: 0.7014, step time: 0.5478\n",
      "270/388, train_loss: 0.8788, step time: 0.5243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/388, train_loss: 0.6899, step time: 0.5028\n",
      "272/388, train_loss: 0.8879, step time: 0.4942\n",
      "273/388, train_loss: 0.9152, step time: 0.4965\n",
      "274/388, train_loss: 0.7558, step time: 0.4836\n",
      "275/388, train_loss: 0.6629, step time: 0.4915\n",
      "276/388, train_loss: 0.9150, step time: 0.4898\n",
      "277/388, train_loss: 0.8356, step time: 1.1975\n",
      "278/388, train_loss: 0.8054, step time: 0.5428\n",
      "279/388, train_loss: 0.8535, step time: 0.5100\n",
      "280/388, train_loss: 0.7478, step time: 0.4940\n",
      "281/388, train_loss: 0.8829, step time: 0.4998\n",
      "282/388, train_loss: 0.6064, step time: 0.5099\n",
      "283/388, train_loss: 0.7425, step time: 0.5006\n",
      "284/388, train_loss: 0.6771, step time: 0.4866\n",
      "285/388, train_loss: 0.5852, step time: 0.4918\n",
      "286/388, train_loss: 0.8509, step time: 1.1668\n",
      "287/388, train_loss: 0.9346, step time: 0.5257\n",
      "288/388, train_loss: 0.6564, step time: 0.4975\n",
      "289/388, train_loss: 0.9021, step time: 0.4934\n",
      "290/388, train_loss: 0.5174, step time: 0.4810\n",
      "291/388, train_loss: 0.8575, step time: 0.4902\n",
      "292/388, train_loss: 0.5942, step time: 0.5065\n",
      "293/388, train_loss: 0.6150, step time: 0.4967\n",
      "294/388, train_loss: 0.6529, step time: 0.4804\n",
      "295/388, train_loss: 0.7191, step time: 0.6941\n",
      "296/388, train_loss: 0.7089, step time: 0.5393\n",
      "297/388, train_loss: 0.8092, step time: 0.5198\n",
      "298/388, train_loss: 0.8965, step time: 0.4964\n",
      "299/388, train_loss: 0.6430, step time: 0.4968\n",
      "300/388, train_loss: 0.8301, step time: 0.4842\n",
      "301/388, train_loss: 0.8895, step time: 0.4847\n",
      "302/388, train_loss: 0.8276, step time: 0.4730\n",
      "303/388, train_loss: 0.8456, step time: 0.4757\n",
      "304/388, train_loss: 0.9477, step time: 0.4814\n",
      "305/388, train_loss: 0.7356, step time: 0.4726\n",
      "306/388, train_loss: 0.5937, step time: 0.5089\n",
      "307/388, train_loss: 0.9471, step time: 0.8913\n",
      "308/388, train_loss: 0.7977, step time: 0.5415\n",
      "309/388, train_loss: 0.8565, step time: 0.5243\n",
      "310/388, train_loss: 0.6113, step time: 0.4964\n",
      "311/388, train_loss: 0.6296, step time: 0.5034\n",
      "312/388, train_loss: 0.6347, step time: 0.4937\n",
      "313/388, train_loss: 0.8325, step time: 0.5395\n",
      "314/388, train_loss: 0.5312, step time: 0.5110\n",
      "315/388, train_loss: 0.6838, step time: 0.5055\n",
      "316/388, train_loss: 0.7131, step time: 0.4825\n",
      "317/388, train_loss: 0.8673, step time: 1.0536\n",
      "318/388, train_loss: 0.8336, step time: 0.5341\n",
      "319/388, train_loss: 0.7348, step time: 0.5108\n",
      "320/388, train_loss: 0.8449, step time: 0.4935\n",
      "321/388, train_loss: 0.8867, step time: 0.5013\n",
      "322/388, train_loss: 0.7748, step time: 0.4832\n",
      "323/388, train_loss: 0.6409, step time: 0.5106\n",
      "324/388, train_loss: 0.7542, step time: 0.4876\n",
      "325/388, train_loss: 0.8156, step time: 0.4818\n",
      "326/388, train_loss: 0.7680, step time: 0.5093\n",
      "327/388, train_loss: 0.8167, step time: 0.4874\n",
      "328/388, train_loss: 0.7139, step time: 0.9088\n",
      "329/388, train_loss: 0.5634, step time: 0.5423\n",
      "330/388, train_loss: 0.6482, step time: 0.5115\n",
      "331/388, train_loss: 0.6578, step time: 0.5038\n",
      "332/388, train_loss: 0.7580, step time: 0.4877\n",
      "333/388, train_loss: 0.6095, step time: 0.4910\n",
      "334/388, train_loss: 0.8513, step time: 0.4871\n",
      "335/388, train_loss: 0.7955, step time: 0.8750\n",
      "336/388, train_loss: 0.9561, step time: 0.5315\n",
      "337/388, train_loss: 0.6803, step time: 0.5111\n",
      "338/388, train_loss: 0.8388, step time: 0.4876\n",
      "339/388, train_loss: 0.8879, step time: 0.4936\n",
      "340/388, train_loss: 0.6764, step time: 0.4782\n",
      "341/388, train_loss: 0.7897, step time: 0.4799\n",
      "342/388, train_loss: 0.8694, step time: 0.6680\n",
      "343/388, train_loss: 0.6200, step time: 0.5457\n",
      "344/388, train_loss: 0.8626, step time: 0.5227\n",
      "345/388, train_loss: 0.9265, step time: 0.5093\n",
      "346/388, train_loss: 0.6326, step time: 0.4943\n",
      "347/388, train_loss: 0.6479, step time: 0.5137\n",
      "348/388, train_loss: 0.4780, step time: 1.0763\n",
      "349/388, train_loss: 0.6776, step time: 0.5242\n",
      "350/388, train_loss: 0.9689, step time: 0.5041\n",
      "351/388, train_loss: 0.7080, step time: 0.4836\n",
      "352/388, train_loss: 0.7430, step time: 0.5926\n",
      "353/388, train_loss: 0.7800, step time: 0.5341\n",
      "354/388, train_loss: 0.8792, step time: 0.5113\n",
      "355/388, train_loss: 0.9533, step time: 0.4985\n",
      "356/388, train_loss: 0.6158, step time: 0.4838\n",
      "357/388, train_loss: 0.9036, step time: 0.4849\n",
      "358/388, train_loss: 0.9185, step time: 0.4964\n",
      "359/388, train_loss: 0.9016, step time: 0.4886\n",
      "360/388, train_loss: 0.9191, step time: 0.4786\n",
      "361/388, train_loss: 0.9583, step time: 0.4851\n",
      "362/388, train_loss: 0.5328, step time: 0.4867\n",
      "363/388, train_loss: 0.6083, step time: 0.5053\n",
      "364/388, train_loss: 0.8820, step time: 0.4808\n",
      "365/388, train_loss: 0.6113, step time: 0.4902\n",
      "366/388, train_loss: 0.6182, step time: 0.5038\n",
      "367/388, train_loss: 0.6153, step time: 0.4820\n",
      "368/388, train_loss: 0.7388, step time: 1.0869\n",
      "369/388, train_loss: 0.9334, step time: 0.5320\n",
      "370/388, train_loss: 0.7183, step time: 0.5108\n",
      "371/388, train_loss: 0.7600, step time: 0.5022\n",
      "372/388, train_loss: 0.6942, step time: 0.5312\n",
      "373/388, train_loss: 0.7396, step time: 0.5168\n",
      "374/388, train_loss: 0.6771, step time: 0.4954\n",
      "375/388, train_loss: 0.7143, step time: 0.4909\n",
      "376/388, train_loss: 0.8945, step time: 0.4939\n",
      "377/388, train_loss: 0.5924, step time: 0.4975\n",
      "378/388, train_loss: 0.6250, step time: 0.4816\n",
      "379/388, train_loss: 0.8252, step time: 0.5175\n",
      "380/388, train_loss: 0.7924, step time: 0.4976\n",
      "381/388, train_loss: 0.7826, step time: 0.4953\n",
      "382/388, train_loss: 0.7565, step time: 0.4781\n",
      "383/388, train_loss: 0.7636, step time: 0.5216\n",
      "384/388, train_loss: 0.5447, step time: 0.5056\n",
      "385/388, train_loss: 0.7371, step time: 0.5314\n",
      "386/388, train_loss: 0.9601, step time: 0.5827\n",
      "387/388, train_loss: 0.7722, step time: 0.5433\n",
      "388/388, train_loss: 0.7500, step time: 0.5059\n",
      "epoch 3 average loss: 0.7963\n",
      "saved new best metric model\n",
      "current epoch: 3 current mean dice: 0.5624 tc: 0.5871 wt: 0.7824 et: 0.3177\n",
      "best mean dice: 0.5624 at epoch: 3\n",
      "time consuming of epoch 3 is: 303.9389\n",
      "----------\n",
      "epoch 4/300\n",
      "1/388, train_loss: 0.6825, step time: 0.4775\n",
      "2/388, train_loss: 0.7671, step time: 0.4871\n",
      "3/388, train_loss: 0.7389, step time: 0.5168\n",
      "4/388, train_loss: 0.5127, step time: 0.5137\n",
      "5/388, train_loss: 0.7428, step time: 0.5067\n",
      "6/388, train_loss: 0.6236, step time: 0.4938\n",
      "7/388, train_loss: 0.6949, step time: 0.8004\n",
      "8/388, train_loss: 0.7099, step time: 0.6600\n",
      "9/388, train_loss: 0.7529, step time: 0.5530\n",
      "10/388, train_loss: 0.7146, step time: 0.5161\n",
      "11/388, train_loss: 0.9712, step time: 0.4969\n",
      "12/388, train_loss: 0.8570, step time: 0.4995\n",
      "13/388, train_loss: 0.6735, step time: 0.4905\n",
      "14/388, train_loss: 0.6756, step time: 0.9332\n",
      "15/388, train_loss: 0.7299, step time: 0.5756\n",
      "16/388, train_loss: 0.6898, step time: 0.5524\n",
      "17/388, train_loss: 0.5941, step time: 0.4985\n",
      "18/388, train_loss: 0.7805, step time: 0.4923\n",
      "19/388, train_loss: 0.7524, step time: 0.9333\n",
      "20/388, train_loss: 0.7390, step time: 0.5695\n",
      "21/388, train_loss: 0.6506, step time: 0.5699\n",
      "22/388, train_loss: 0.6325, step time: 0.7415\n",
      "23/388, train_loss: 0.6854, step time: 0.5787\n",
      "24/388, train_loss: 0.6200, step time: 0.5412\n",
      "25/388, train_loss: 0.6479, step time: 0.4991\n",
      "26/388, train_loss: 0.8575, step time: 0.4900\n",
      "27/388, train_loss: 0.8327, step time: 0.5148\n",
      "28/388, train_loss: 0.6894, step time: 0.6950\n",
      "29/388, train_loss: 0.8751, step time: 0.6178\n",
      "30/388, train_loss: 0.7551, step time: 0.5518\n",
      "31/388, train_loss: 0.6741, step time: 0.5339\n",
      "32/388, train_loss: 0.8123, step time: 0.5130\n",
      "33/388, train_loss: 0.6005, step time: 0.9193\n",
      "34/388, train_loss: 0.8798, step time: 0.5691\n",
      "35/388, train_loss: 0.7693, step time: 0.5345\n",
      "36/388, train_loss: 0.6436, step time: 0.5059\n",
      "37/388, train_loss: 0.6547, step time: 0.4907\n",
      "38/388, train_loss: 0.9690, step time: 0.5541\n",
      "39/388, train_loss: 0.4034, step time: 0.5406\n",
      "40/388, train_loss: 0.9781, step time: 0.5005\n",
      "41/388, train_loss: 0.6804, step time: 1.2919\n",
      "42/388, train_loss: 0.8136, step time: 0.5358\n",
      "43/388, train_loss: 0.8257, step time: 0.4975\n",
      "44/388, train_loss: 0.9341, step time: 0.4866\n",
      "45/388, train_loss: 0.5545, step time: 0.5529\n",
      "46/388, train_loss: 0.6528, step time: 0.5501\n",
      "47/388, train_loss: 0.7251, step time: 0.5201\n",
      "48/388, train_loss: 0.5850, step time: 0.5045\n",
      "49/388, train_loss: 0.8122, step time: 0.4844\n",
      "50/388, train_loss: 0.5115, step time: 0.5233\n",
      "51/388, train_loss: 0.5652, step time: 0.6084\n",
      "52/388, train_loss: 0.7290, step time: 0.6071\n",
      "53/388, train_loss: 0.8936, step time: 0.5456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/388, train_loss: 0.8162, step time: 0.5364\n",
      "55/388, train_loss: 0.8909, step time: 0.5528\n",
      "56/388, train_loss: 0.8992, step time: 0.6518\n",
      "57/388, train_loss: 0.6988, step time: 0.5308\n",
      "58/388, train_loss: 0.8532, step time: 0.5024\n",
      "59/388, train_loss: 0.8011, step time: 0.5264\n",
      "60/388, train_loss: 0.9144, step time: 0.6224\n",
      "61/388, train_loss: 0.9606, step time: 0.5555\n",
      "62/388, train_loss: 0.7695, step time: 0.5362\n",
      "63/388, train_loss: 0.8115, step time: 0.5311\n",
      "64/388, train_loss: 0.5665, step time: 0.5167\n",
      "65/388, train_loss: 0.4196, step time: 0.5478\n",
      "66/388, train_loss: 0.5573, step time: 0.5391\n",
      "67/388, train_loss: 0.9241, step time: 0.6384\n",
      "68/388, train_loss: 0.6139, step time: 0.5588\n",
      "69/388, train_loss: 0.8910, step time: 0.5199\n",
      "70/388, train_loss: 0.6904, step time: 0.4932\n",
      "71/388, train_loss: 0.7129, step time: 1.1642\n",
      "72/388, train_loss: 0.5417, step time: 0.5354\n",
      "73/388, train_loss: 0.5998, step time: 0.5008\n",
      "74/388, train_loss: 0.7665, step time: 0.4940\n",
      "75/388, train_loss: 0.5879, step time: 0.8069\n",
      "76/388, train_loss: 0.8227, step time: 0.5727\n",
      "77/388, train_loss: 0.6003, step time: 0.5308\n",
      "78/388, train_loss: 0.7778, step time: 0.5109\n",
      "79/388, train_loss: 0.7638, step time: 0.4923\n",
      "80/388, train_loss: 0.8525, step time: 0.5113\n",
      "81/388, train_loss: 0.9392, step time: 0.4909\n",
      "82/388, train_loss: 0.8231, step time: 0.4946\n",
      "83/388, train_loss: 0.7352, step time: 0.5155\n",
      "84/388, train_loss: 0.7170, step time: 0.5183\n",
      "85/388, train_loss: 0.5453, step time: 0.5007\n",
      "86/388, train_loss: 0.8542, step time: 1.1091\n",
      "87/388, train_loss: 0.6808, step time: 0.5441\n",
      "88/388, train_loss: 0.8047, step time: 0.5038\n",
      "89/388, train_loss: 0.6527, step time: 0.4818\n",
      "90/388, train_loss: 0.8960, step time: 0.5044\n",
      "91/388, train_loss: 0.7370, step time: 0.4890\n",
      "92/388, train_loss: 0.7133, step time: 0.4901\n",
      "93/388, train_loss: 0.9345, step time: 0.5444\n",
      "94/388, train_loss: 0.7398, step time: 0.5100\n",
      "95/388, train_loss: 0.7150, step time: 0.5067\n",
      "96/388, train_loss: 0.9082, step time: 0.4871\n",
      "97/388, train_loss: 0.5300, step time: 1.1589\n",
      "98/388, train_loss: 0.5770, step time: 0.5298\n",
      "99/388, train_loss: 0.8782, step time: 0.5167\n",
      "100/388, train_loss: 0.7381, step time: 0.4958\n",
      "101/388, train_loss: 0.7487, step time: 0.5020\n",
      "102/388, train_loss: 0.8573, step time: 0.4838\n",
      "103/388, train_loss: 0.5586, step time: 0.4784\n",
      "104/388, train_loss: 0.9560, step time: 0.6730\n",
      "105/388, train_loss: 0.6348, step time: 0.5572\n",
      "106/388, train_loss: 0.7485, step time: 0.5106\n",
      "107/388, train_loss: 0.4926, step time: 0.4917\n",
      "108/388, train_loss: 0.4851, step time: 1.1806\n",
      "109/388, train_loss: 0.8302, step time: 0.5390\n",
      "110/388, train_loss: 0.6131, step time: 0.5158\n",
      "111/388, train_loss: 0.9093, step time: 0.4897\n",
      "112/388, train_loss: 0.7933, step time: 0.4895\n",
      "113/388, train_loss: 0.7505, step time: 0.4911\n",
      "114/388, train_loss: 0.7070, step time: 0.4804\n",
      "115/388, train_loss: 0.5482, step time: 0.5025\n",
      "116/388, train_loss: 0.8304, step time: 0.4966\n",
      "117/388, train_loss: 0.7721, step time: 0.9007\n",
      "118/388, train_loss: 0.8478, step time: 0.5529\n",
      "119/388, train_loss: 0.7443, step time: 0.5260\n",
      "120/388, train_loss: 0.4950, step time: 0.5116\n",
      "121/388, train_loss: 0.6384, step time: 0.4967\n",
      "122/388, train_loss: 0.7221, step time: 0.4914\n",
      "123/388, train_loss: 0.9765, step time: 0.4827\n",
      "124/388, train_loss: 0.5613, step time: 0.4943\n",
      "125/388, train_loss: 0.7973, step time: 0.4977\n",
      "126/388, train_loss: 0.5550, step time: 0.4825\n",
      "127/388, train_loss: 0.8784, step time: 0.5029\n",
      "128/388, train_loss: 0.7097, step time: 0.5518\n",
      "129/388, train_loss: 0.5740, step time: 0.5391\n",
      "130/388, train_loss: 0.6055, step time: 0.5292\n",
      "131/388, train_loss: 0.9252, step time: 0.6087\n",
      "132/388, train_loss: 0.9018, step time: 0.5429\n",
      "133/388, train_loss: 0.6035, step time: 0.5383\n",
      "134/388, train_loss: 0.4743, step time: 0.5316\n",
      "135/388, train_loss: 0.6213, step time: 0.5062\n",
      "136/388, train_loss: 0.6688, step time: 0.5223\n",
      "137/388, train_loss: 0.7698, step time: 0.5106\n",
      "138/388, train_loss: 0.8878, step time: 0.4912\n",
      "139/388, train_loss: 0.7345, step time: 0.4917\n",
      "140/388, train_loss: 0.5745, step time: 0.4834\n",
      "141/388, train_loss: 0.5149, step time: 0.6463\n",
      "142/388, train_loss: 0.5900, step time: 0.5657\n",
      "143/388, train_loss: 0.5801, step time: 0.5302\n",
      "144/388, train_loss: 0.8124, step time: 0.4967\n",
      "145/388, train_loss: 0.8222, step time: 0.4992\n",
      "146/388, train_loss: 0.8733, step time: 0.4890\n",
      "147/388, train_loss: 0.7666, step time: 0.4828\n",
      "148/388, train_loss: 0.9517, step time: 1.1240\n",
      "149/388, train_loss: 0.6394, step time: 0.5273\n",
      "150/388, train_loss: 0.7092, step time: 0.5145\n",
      "151/388, train_loss: 0.7401, step time: 0.4909\n",
      "152/388, train_loss: 0.7031, step time: 0.4958\n",
      "153/388, train_loss: 0.6039, step time: 0.4974\n",
      "154/388, train_loss: 0.5804, step time: 0.4932\n",
      "155/388, train_loss: 0.5240, step time: 0.4949\n",
      "156/388, train_loss: 0.7839, step time: 0.4838\n",
      "157/388, train_loss: 0.6249, step time: 0.4854\n",
      "158/388, train_loss: 0.6680, step time: 1.2010\n",
      "159/388, train_loss: 0.8074, step time: 0.5247\n",
      "160/388, train_loss: 0.3783, step time: 0.5019\n",
      "161/388, train_loss: 0.4479, step time: 0.4927\n",
      "162/388, train_loss: 0.9537, step time: 0.4805\n",
      "163/388, train_loss: 0.3930, step time: 0.4858\n",
      "164/388, train_loss: 0.6838, step time: 0.9756\n",
      "165/388, train_loss: 0.5689, step time: 0.5406\n",
      "166/388, train_loss: 0.6016, step time: 0.5021\n",
      "167/388, train_loss: 0.8171, step time: 0.4938\n",
      "168/388, train_loss: 0.7298, step time: 0.4938\n",
      "169/388, train_loss: 0.6501, step time: 0.4793\n",
      "170/388, train_loss: 0.4174, step time: 0.4822\n",
      "171/388, train_loss: 0.7036, step time: 0.4769\n",
      "172/388, train_loss: 0.4783, step time: 0.4903\n",
      "173/388, train_loss: 0.4516, step time: 0.4971\n",
      "174/388, train_loss: 0.5077, step time: 0.9501\n",
      "175/388, train_loss: 0.7805, step time: 0.5480\n",
      "176/388, train_loss: 0.5104, step time: 0.5252\n",
      "177/388, train_loss: 0.5693, step time: 0.5025\n",
      "178/388, train_loss: 0.7517, step time: 0.5503\n",
      "179/388, train_loss: 0.6845, step time: 0.5165\n",
      "180/388, train_loss: 0.5449, step time: 0.5048\n",
      "181/388, train_loss: 0.5033, step time: 0.4961\n",
      "182/388, train_loss: 0.7923, step time: 0.4941\n",
      "183/388, train_loss: 0.6604, step time: 0.4819\n",
      "184/388, train_loss: 0.5665, step time: 0.5257\n",
      "185/388, train_loss: 0.7713, step time: 0.5010\n",
      "186/388, train_loss: 0.5949, step time: 0.4853\n",
      "187/388, train_loss: 0.9128, step time: 0.5025\n",
      "188/388, train_loss: 0.7192, step time: 0.8255\n",
      "189/388, train_loss: 0.8219, step time: 0.5656\n",
      "190/388, train_loss: 0.4452, step time: 0.5202\n",
      "191/388, train_loss: 0.5670, step time: 0.4979\n",
      "192/388, train_loss: 0.7438, step time: 0.5266\n",
      "193/388, train_loss: 0.5078, step time: 0.5266\n",
      "194/388, train_loss: 0.5128, step time: 0.5869\n",
      "195/388, train_loss: 0.8121, step time: 0.5345\n",
      "196/388, train_loss: 0.6027, step time: 0.5218\n",
      "197/388, train_loss: 0.7404, step time: 0.5124\n",
      "198/388, train_loss: 0.5859, step time: 0.4960\n",
      "199/388, train_loss: 0.7635, step time: 0.5001\n",
      "200/388, train_loss: 0.7504, step time: 0.4890\n",
      "201/388, train_loss: 0.5536, step time: 0.5491\n",
      "202/388, train_loss: 0.6846, step time: 0.5198\n",
      "203/388, train_loss: 0.6168, step time: 0.5047\n",
      "204/388, train_loss: 0.9053, step time: 0.4914\n",
      "205/388, train_loss: 0.3830, step time: 0.5000\n",
      "206/388, train_loss: 0.8561, step time: 0.4880\n",
      "207/388, train_loss: 0.5943, step time: 0.9998\n",
      "208/388, train_loss: 0.4792, step time: 0.5543\n",
      "209/388, train_loss: 0.5222, step time: 0.5211\n",
      "210/388, train_loss: 0.5987, step time: 0.5004\n",
      "211/388, train_loss: 0.6739, step time: 0.5032\n",
      "212/388, train_loss: 0.5957, step time: 0.4882\n",
      "213/388, train_loss: 0.7818, step time: 0.4982\n",
      "214/388, train_loss: 0.4843, step time: 0.5424\n",
      "215/388, train_loss: 0.6019, step time: 0.5166\n",
      "216/388, train_loss: 0.4966, step time: 0.4964\n",
      "217/388, train_loss: 0.4520, step time: 0.4980\n",
      "218/388, train_loss: 0.5893, step time: 0.4845\n",
      "219/388, train_loss: 0.6523, step time: 0.4956\n",
      "220/388, train_loss: 0.4147, step time: 0.4820\n",
      "221/388, train_loss: 0.7280, step time: 1.0126\n",
      "222/388, train_loss: 0.8144, step time: 0.5304\n",
      "223/388, train_loss: 0.6936, step time: 0.5168\n",
      "224/388, train_loss: 0.5681, step time: 0.4869\n",
      "225/388, train_loss: 0.5904, step time: 0.5099\n",
      "226/388, train_loss: 0.6659, step time: 0.4870\n",
      "227/388, train_loss: 0.8080, step time: 0.5058\n",
      "228/388, train_loss: 0.4640, step time: 0.5188\n",
      "229/388, train_loss: 0.6330, step time: 0.5072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/388, train_loss: 0.5986, step time: 0.4893\n",
      "231/388, train_loss: 0.3263, step time: 0.4910\n",
      "232/388, train_loss: 0.4347, step time: 0.4834\n",
      "233/388, train_loss: 0.8448, step time: 0.4914\n",
      "234/388, train_loss: 0.6220, step time: 0.5069\n",
      "235/388, train_loss: 0.5913, step time: 0.4831\n",
      "236/388, train_loss: 0.9413, step time: 0.4845\n",
      "237/388, train_loss: 0.7110, step time: 1.1940\n",
      "238/388, train_loss: 0.6222, step time: 0.5332\n",
      "239/388, train_loss: 0.3578, step time: 0.5026\n",
      "240/388, train_loss: 0.8939, step time: 0.4960\n",
      "241/388, train_loss: 0.6604, step time: 0.4844\n",
      "242/388, train_loss: 0.4940, step time: 0.4773\n",
      "243/388, train_loss: 0.3954, step time: 0.4788\n",
      "244/388, train_loss: 0.5196, step time: 0.4863\n",
      "245/388, train_loss: 0.7862, step time: 0.4892\n",
      "246/388, train_loss: 0.6588, step time: 1.1276\n",
      "247/388, train_loss: 0.6727, step time: 0.5451\n",
      "248/388, train_loss: 0.6676, step time: 0.5037\n",
      "249/388, train_loss: 0.6121, step time: 0.4988\n",
      "250/388, train_loss: 0.7620, step time: 0.4952\n",
      "251/388, train_loss: 0.7375, step time: 0.5053\n",
      "252/388, train_loss: 0.5991, step time: 0.4955\n",
      "253/388, train_loss: 0.5221, step time: 0.4923\n",
      "254/388, train_loss: 0.6652, step time: 0.4726\n",
      "255/388, train_loss: 0.8149, step time: 0.4759\n",
      "256/388, train_loss: 0.4290, step time: 0.4843\n",
      "257/388, train_loss: 0.4994, step time: 0.4827\n",
      "258/388, train_loss: 0.5533, step time: 1.0104\n",
      "259/388, train_loss: 0.9145, step time: 0.5215\n",
      "260/388, train_loss: 0.5691, step time: 0.4992\n",
      "261/388, train_loss: 0.6020, step time: 0.4952\n",
      "262/388, train_loss: 0.7083, step time: 0.5128\n",
      "263/388, train_loss: 0.4772, step time: 0.5003\n",
      "264/388, train_loss: 0.4853, step time: 0.4866\n",
      "265/388, train_loss: 0.5490, step time: 0.4956\n",
      "266/388, train_loss: 0.8998, step time: 0.4802\n",
      "267/388, train_loss: 0.4942, step time: 0.9885\n",
      "268/388, train_loss: 0.5950, step time: 0.5276\n",
      "269/388, train_loss: 0.3973, step time: 0.4989\n",
      "270/388, train_loss: 0.3213, step time: 0.4973\n",
      "271/388, train_loss: 0.5418, step time: 0.4830\n",
      "272/388, train_loss: 0.4441, step time: 0.4784\n",
      "273/388, train_loss: 0.9671, step time: 0.4795\n",
      "274/388, train_loss: 0.5010, step time: 0.4747\n",
      "275/388, train_loss: 0.4901, step time: 0.4746\n",
      "276/388, train_loss: 0.7149, step time: 0.8516\n",
      "277/388, train_loss: 0.8326, step time: 0.5460\n",
      "278/388, train_loss: 0.5947, step time: 0.5200\n",
      "279/388, train_loss: 0.9624, step time: 0.5001\n",
      "280/388, train_loss: 0.6269, step time: 0.4993\n",
      "281/388, train_loss: 0.4791, step time: 0.5191\n",
      "282/388, train_loss: 0.5949, step time: 0.4997\n",
      "283/388, train_loss: 0.4425, step time: 0.4819\n",
      "284/388, train_loss: 0.8249, step time: 0.5207\n",
      "285/388, train_loss: 0.8606, step time: 0.5140\n",
      "286/388, train_loss: 0.4803, step time: 0.5061\n",
      "287/388, train_loss: 0.6076, step time: 0.4898\n",
      "288/388, train_loss: 0.4181, step time: 0.4971\n",
      "289/388, train_loss: 0.8933, step time: 0.4889\n",
      "290/388, train_loss: 0.4929, step time: 0.4783\n",
      "291/388, train_loss: 0.5171, step time: 0.4835\n",
      "292/388, train_loss: 0.6563, step time: 0.5495\n",
      "293/388, train_loss: 0.8043, step time: 0.5282\n",
      "294/388, train_loss: 0.5089, step time: 0.5091\n",
      "295/388, train_loss: 0.8898, step time: 0.4972\n",
      "296/388, train_loss: 0.3139, step time: 0.4821\n",
      "297/388, train_loss: 0.3872, step time: 0.5189\n",
      "298/388, train_loss: 0.4461, step time: 0.4957\n",
      "299/388, train_loss: 0.9024, step time: 0.4928\n",
      "300/388, train_loss: 0.5217, step time: 0.4845\n",
      "301/388, train_loss: 0.6943, step time: 0.4802\n",
      "302/388, train_loss: 0.8193, step time: 0.9351\n",
      "303/388, train_loss: 0.8917, step time: 0.5475\n",
      "304/388, train_loss: 0.7625, step time: 0.5122\n",
      "305/388, train_loss: 0.7678, step time: 0.5349\n",
      "306/388, train_loss: 0.6998, step time: 0.5210\n",
      "307/388, train_loss: 0.9087, step time: 0.5005\n",
      "308/388, train_loss: 0.7522, step time: 0.4950\n",
      "309/388, train_loss: 0.6234, step time: 0.4789\n",
      "310/388, train_loss: 0.6450, step time: 0.4794\n",
      "311/388, train_loss: 0.4229, step time: 0.4892\n",
      "312/388, train_loss: 0.2816, step time: 1.1649\n",
      "313/388, train_loss: 0.7240, step time: 0.5375\n",
      "314/388, train_loss: 0.4434, step time: 0.5040\n",
      "315/388, train_loss: 0.6108, step time: 0.4998\n",
      "316/388, train_loss: 0.5283, step time: 0.4843\n",
      "317/388, train_loss: 0.7896, step time: 0.4813\n",
      "318/388, train_loss: 0.5415, step time: 0.7252\n",
      "319/388, train_loss: 0.7062, step time: 0.5759\n",
      "320/388, train_loss: 0.5768, step time: 0.5235\n",
      "321/388, train_loss: 0.2993, step time: 0.4957\n",
      "322/388, train_loss: 0.7969, step time: 0.4979\n",
      "323/388, train_loss: 0.5023, step time: 0.4894\n",
      "324/388, train_loss: 0.7242, step time: 0.4824\n",
      "325/388, train_loss: 0.5480, step time: 0.4908\n",
      "326/388, train_loss: 0.7857, step time: 0.4794\n",
      "327/388, train_loss: 0.5568, step time: 0.4800\n",
      "328/388, train_loss: 0.5102, step time: 0.4962\n",
      "329/388, train_loss: 0.5158, step time: 0.4956\n",
      "330/388, train_loss: 0.8273, step time: 0.5001\n",
      "331/388, train_loss: 0.9035, step time: 0.4863\n",
      "332/388, train_loss: 0.5975, step time: 1.2223\n",
      "333/388, train_loss: 0.6841, step time: 0.5265\n",
      "334/388, train_loss: 0.5632, step time: 0.5129\n",
      "335/388, train_loss: 0.6109, step time: 0.4938\n",
      "336/388, train_loss: 0.4604, step time: 0.4938\n",
      "337/388, train_loss: 0.7932, step time: 0.4806\n",
      "338/388, train_loss: 0.6543, step time: 0.4766\n",
      "339/388, train_loss: 0.7671, step time: 1.1540\n",
      "340/388, train_loss: 0.6290, step time: 0.5225\n",
      "341/388, train_loss: 0.4741, step time: 0.4995\n",
      "342/388, train_loss: 0.5262, step time: 0.4874\n",
      "343/388, train_loss: 0.4278, step time: 0.4848\n",
      "344/388, train_loss: 0.6936, step time: 0.4870\n",
      "345/388, train_loss: 0.7773, step time: 0.4718\n",
      "346/388, train_loss: 0.6099, step time: 0.4710\n",
      "347/388, train_loss: 0.3066, step time: 0.4722\n",
      "348/388, train_loss: 0.7718, step time: 1.0040\n",
      "349/388, train_loss: 0.5073, step time: 0.5404\n",
      "350/388, train_loss: 0.6296, step time: 0.4999\n",
      "351/388, train_loss: 0.4073, step time: 0.4916\n",
      "352/388, train_loss: 0.6931, step time: 0.5151\n",
      "353/388, train_loss: 0.4527, step time: 0.4917\n",
      "354/388, train_loss: 0.4391, step time: 0.4943\n",
      "355/388, train_loss: 0.4738, step time: 0.4792\n",
      "356/388, train_loss: 0.3645, step time: 0.5183\n",
      "357/388, train_loss: 0.7184, step time: 0.5048\n",
      "358/388, train_loss: 0.2857, step time: 0.4976\n",
      "359/388, train_loss: 0.4508, step time: 1.0342\n",
      "360/388, train_loss: 0.9202, step time: 0.5452\n",
      "361/388, train_loss: 0.4472, step time: 0.5153\n",
      "362/388, train_loss: 0.4582, step time: 0.4928\n",
      "363/388, train_loss: 0.6374, step time: 0.4899\n",
      "364/388, train_loss: 0.5675, step time: 0.4905\n",
      "365/388, train_loss: 0.6416, step time: 0.4793\n",
      "366/388, train_loss: 0.4687, step time: 0.7622\n",
      "367/388, train_loss: 0.6361, step time: 0.5413\n",
      "368/388, train_loss: 0.8277, step time: 0.5057\n",
      "369/388, train_loss: 0.6452, step time: 0.4976\n",
      "370/388, train_loss: 0.4665, step time: 0.4819\n",
      "371/388, train_loss: 0.4793, step time: 0.4810\n",
      "372/388, train_loss: 0.6729, step time: 0.4804\n",
      "373/388, train_loss: 0.8915, step time: 0.4922\n",
      "374/388, train_loss: 0.8373, step time: 0.5211\n",
      "375/388, train_loss: 0.5425, step time: 0.5005\n",
      "376/388, train_loss: 0.3750, step time: 0.5017\n",
      "377/388, train_loss: 0.8773, step time: 0.4912\n",
      "378/388, train_loss: 0.4749, step time: 0.5110\n",
      "379/388, train_loss: 0.5641, step time: 0.4861\n",
      "380/388, train_loss: 0.5358, step time: 0.4945\n",
      "381/388, train_loss: 0.5084, step time: 0.4890\n",
      "382/388, train_loss: 0.6554, step time: 0.4930\n",
      "383/388, train_loss: 0.5526, step time: 0.5236\n",
      "384/388, train_loss: 0.5500, step time: 0.5085\n",
      "385/388, train_loss: 0.5541, step time: 0.4898\n",
      "386/388, train_loss: 0.9087, step time: 0.4742\n",
      "387/388, train_loss: 0.5948, step time: 0.4821\n",
      "388/388, train_loss: 0.3832, step time: 0.4776\n",
      "epoch 4 average loss: 0.6612\n",
      "saved new best metric model\n",
      "current epoch: 4 current mean dice: 0.6109 tc: 0.6650 wt: 0.8018 et: 0.3660\n",
      "best mean dice: 0.6109 at epoch: 4\n",
      "time consuming of epoch 4 is: 303.7213\n",
      "----------\n",
      "epoch 5/300\n",
      "1/388, train_loss: 0.4607, step time: 0.4782\n",
      "2/388, train_loss: 0.3844, step time: 0.4838\n",
      "3/388, train_loss: 0.3053, step time: 0.4834\n",
      "4/388, train_loss: 0.8801, step time: 0.5774\n",
      "5/388, train_loss: 0.4473, step time: 0.5053\n",
      "6/388, train_loss: 0.4379, step time: 0.4939\n",
      "7/388, train_loss: 0.6369, step time: 0.5519\n",
      "8/388, train_loss: 0.6862, step time: 0.5079\n",
      "9/388, train_loss: 0.6961, step time: 0.6464\n",
      "10/388, train_loss: 0.5132, step time: 0.5764\n",
      "11/388, train_loss: 0.5722, step time: 0.5759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/388, train_loss: 0.5124, step time: 0.6556\n",
      "13/388, train_loss: 0.7426, step time: 0.5621\n",
      "14/388, train_loss: 0.3781, step time: 0.5176\n",
      "15/388, train_loss: 0.5728, step time: 0.5254\n",
      "16/388, train_loss: 0.5231, step time: 0.5210\n",
      "17/388, train_loss: 0.6895, step time: 0.5030\n",
      "18/388, train_loss: 0.7416, step time: 0.5091\n",
      "19/388, train_loss: 0.4928, step time: 0.5510\n",
      "20/388, train_loss: 0.5138, step time: 0.5179\n",
      "21/388, train_loss: 0.5434, step time: 0.5137\n",
      "22/388, train_loss: 0.4883, step time: 0.4985\n",
      "23/388, train_loss: 0.5279, step time: 0.9236\n",
      "24/388, train_loss: 0.6186, step time: 0.5562\n",
      "25/388, train_loss: 0.4399, step time: 0.5255\n",
      "26/388, train_loss: 0.4740, step time: 0.5035\n",
      "27/388, train_loss: 0.5457, step time: 0.5358\n",
      "28/388, train_loss: 0.3447, step time: 0.5161\n",
      "29/388, train_loss: 0.6313, step time: 0.5196\n",
      "30/388, train_loss: 0.6502, step time: 0.4876\n",
      "31/388, train_loss: 0.5144, step time: 1.0685\n",
      "32/388, train_loss: 0.4734, step time: 0.5307\n",
      "33/388, train_loss: 0.9548, step time: 0.5104\n",
      "34/388, train_loss: 0.2512, step time: 0.5377\n",
      "35/388, train_loss: 0.3874, step time: 0.6637\n",
      "36/388, train_loss: 0.6759, step time: 0.5407\n",
      "37/388, train_loss: 0.6964, step time: 0.5015\n",
      "38/388, train_loss: 0.4057, step time: 0.4862\n",
      "39/388, train_loss: 0.7269, step time: 0.4994\n",
      "40/388, train_loss: 0.6913, step time: 0.5606\n",
      "41/388, train_loss: 0.6645, step time: 0.5371\n",
      "42/388, train_loss: 0.5776, step time: 0.5114\n",
      "43/388, train_loss: 0.6405, step time: 0.4962\n",
      "44/388, train_loss: 0.6754, step time: 0.5206\n",
      "45/388, train_loss: 0.3738, step time: 0.5075\n",
      "46/388, train_loss: 0.4904, step time: 0.5227\n",
      "47/388, train_loss: 0.4549, step time: 0.5285\n",
      "48/388, train_loss: 0.6738, step time: 0.5345\n",
      "49/388, train_loss: 0.2811, step time: 0.5230\n",
      "50/388, train_loss: 0.5747, step time: 0.5058\n",
      "51/388, train_loss: 0.8351, step time: 0.5254\n",
      "52/388, train_loss: 0.3642, step time: 0.5607\n",
      "53/388, train_loss: 0.5056, step time: 0.5294\n",
      "54/388, train_loss: 0.6859, step time: 0.5091\n",
      "55/388, train_loss: 0.5609, step time: 0.4937\n",
      "56/388, train_loss: 0.9344, step time: 1.0324\n",
      "57/388, train_loss: 0.4409, step time: 0.5544\n",
      "58/388, train_loss: 0.5563, step time: 0.5215\n",
      "59/388, train_loss: 0.4808, step time: 0.4956\n",
      "60/388, train_loss: 0.7636, step time: 0.4950\n",
      "61/388, train_loss: 0.5007, step time: 0.4953\n",
      "62/388, train_loss: 0.5629, step time: 0.5008\n",
      "63/388, train_loss: 0.5903, step time: 0.5286\n",
      "64/388, train_loss: 0.4732, step time: 0.5209\n",
      "65/388, train_loss: 0.7109, step time: 0.5034\n",
      "66/388, train_loss: 0.6690, step time: 0.5004\n",
      "67/388, train_loss: 0.2912, step time: 0.4940\n",
      "68/388, train_loss: 0.4394, step time: 0.5607\n",
      "69/388, train_loss: 0.4217, step time: 0.5476\n",
      "70/388, train_loss: 0.4338, step time: 0.5036\n",
      "71/388, train_loss: 0.6032, step time: 0.5007\n",
      "72/388, train_loss: 0.5187, step time: 0.4887\n",
      "73/388, train_loss: 0.7372, step time: 0.5482\n",
      "74/388, train_loss: 0.5032, step time: 0.5143\n",
      "75/388, train_loss: 0.5554, step time: 0.5343\n",
      "76/388, train_loss: 0.5690, step time: 0.5200\n",
      "77/388, train_loss: 0.7201, step time: 0.5035\n",
      "78/388, train_loss: 0.5150, step time: 0.5290\n",
      "79/388, train_loss: 0.7135, step time: 0.5072\n",
      "80/388, train_loss: 0.5588, step time: 0.5066\n",
      "81/388, train_loss: 0.5569, step time: 0.5091\n",
      "82/388, train_loss: 0.3664, step time: 0.5024\n",
      "83/388, train_loss: 0.6795, step time: 1.1697\n",
      "84/388, train_loss: 0.3608, step time: 0.5346\n",
      "85/388, train_loss: 0.4047, step time: 0.4980\n",
      "86/388, train_loss: 0.6410, step time: 0.5375\n",
      "87/388, train_loss: 0.4464, step time: 0.5206\n",
      "88/388, train_loss: 0.6506, step time: 0.5041\n",
      "89/388, train_loss: 0.6050, step time: 0.4821\n",
      "90/388, train_loss: 0.5844, step time: 1.0499\n",
      "91/388, train_loss: 0.3690, step time: 0.5393\n",
      "92/388, train_loss: 0.4417, step time: 0.5291\n",
      "93/388, train_loss: 0.3001, step time: 0.6077\n",
      "94/388, train_loss: 0.4013, step time: 0.5408\n",
      "95/388, train_loss: 0.5548, step time: 0.5048\n",
      "96/388, train_loss: 0.3978, step time: 0.4936\n",
      "97/388, train_loss: 0.8180, step time: 0.4953\n",
      "98/388, train_loss: 0.5539, step time: 0.5408\n",
      "99/388, train_loss: 0.4579, step time: 0.5221\n",
      "100/388, train_loss: 0.6148, step time: 0.5088\n",
      "101/388, train_loss: 0.4982, step time: 0.4977\n",
      "102/388, train_loss: 0.3664, step time: 0.4952\n",
      "103/388, train_loss: 0.5245, step time: 1.0786\n",
      "104/388, train_loss: 0.4948, step time: 0.5438\n",
      "105/388, train_loss: 0.7928, step time: 0.5209\n",
      "106/388, train_loss: 0.5133, step time: 0.5120\n",
      "107/388, train_loss: 0.4304, step time: 0.5551\n",
      "108/388, train_loss: 0.4351, step time: 0.6661\n",
      "109/388, train_loss: 0.5664, step time: 0.5348\n",
      "110/388, train_loss: 0.3738, step time: 0.5030\n",
      "111/388, train_loss: 0.6198, step time: 0.5364\n",
      "112/388, train_loss: 0.5887, step time: 0.5315\n",
      "113/388, train_loss: 0.5114, step time: 0.5069\n",
      "114/388, train_loss: 0.7857, step time: 0.5045\n",
      "115/388, train_loss: 0.3506, step time: 0.4876\n",
      "116/388, train_loss: 0.8421, step time: 0.7267\n",
      "117/388, train_loss: 0.5733, step time: 0.5509\n",
      "118/388, train_loss: 0.4754, step time: 0.5207\n",
      "119/388, train_loss: 0.5043, step time: 0.5018\n",
      "120/388, train_loss: 0.6569, step time: 0.5286\n",
      "121/388, train_loss: 0.7767, step time: 0.5111\n",
      "122/388, train_loss: 0.8926, step time: 0.5063\n",
      "123/388, train_loss: 0.8893, step time: 0.4899\n",
      "124/388, train_loss: 0.5008, step time: 0.4973\n",
      "125/388, train_loss: 0.5121, step time: 0.4836\n",
      "126/388, train_loss: 0.7381, step time: 1.0287\n",
      "127/388, train_loss: 0.5693, step time: 0.5385\n",
      "128/388, train_loss: 0.5413, step time: 0.5152\n",
      "129/388, train_loss: 0.3930, step time: 0.4910\n",
      "130/388, train_loss: 0.4964, step time: 0.5099\n",
      "131/388, train_loss: 0.8232, step time: 0.5829\n",
      "132/388, train_loss: 0.4915, step time: 0.5794\n",
      "133/388, train_loss: 0.2271, step time: 0.6086\n",
      "134/388, train_loss: 0.4436, step time: 0.5575\n",
      "135/388, train_loss: 0.5878, step time: 0.5337\n",
      "136/388, train_loss: 0.6640, step time: 0.5072\n",
      "137/388, train_loss: 0.5366, step time: 0.5372\n",
      "138/388, train_loss: 0.8279, step time: 0.5115\n",
      "139/388, train_loss: 0.6423, step time: 0.5062\n",
      "140/388, train_loss: 0.3405, step time: 0.5216\n",
      "141/388, train_loss: 0.5684, step time: 0.5047\n",
      "142/388, train_loss: 0.8254, step time: 0.4980\n",
      "143/388, train_loss: 0.4008, step time: 0.5250\n",
      "144/388, train_loss: 0.4802, step time: 0.5031\n",
      "145/388, train_loss: 0.5297, step time: 0.5242\n",
      "146/388, train_loss: 0.3778, step time: 0.4994\n",
      "147/388, train_loss: 0.5461, step time: 0.5115\n",
      "148/388, train_loss: 0.5922, step time: 0.5011\n",
      "149/388, train_loss: 0.8329, step time: 0.5212\n",
      "150/388, train_loss: 0.4173, step time: 0.4927\n",
      "151/388, train_loss: 0.5474, step time: 1.0972\n",
      "152/388, train_loss: 0.6678, step time: 0.5611\n",
      "153/388, train_loss: 0.7100, step time: 0.5365\n",
      "154/388, train_loss: 0.6816, step time: 0.5105\n",
      "155/388, train_loss: 0.4933, step time: 0.4985\n",
      "156/388, train_loss: 0.4563, step time: 0.4841\n",
      "157/388, train_loss: 0.7321, step time: 1.0566\n",
      "158/388, train_loss: 0.2692, step time: 0.5254\n",
      "159/388, train_loss: 0.3675, step time: 0.4945\n",
      "160/388, train_loss: 0.5475, step time: 0.5205\n",
      "161/388, train_loss: 0.2201, step time: 0.5180\n",
      "162/388, train_loss: 0.4843, step time: 0.5725\n",
      "163/388, train_loss: 0.4497, step time: 0.5322\n",
      "164/388, train_loss: 0.7833, step time: 0.5170\n",
      "165/388, train_loss: 0.5005, step time: 0.4936\n",
      "166/388, train_loss: 0.3139, step time: 1.0849\n",
      "167/388, train_loss: 0.3249, step time: 0.5264\n",
      "168/388, train_loss: 0.3956, step time: 0.5088\n",
      "169/388, train_loss: 0.3936, step time: 0.4971\n",
      "170/388, train_loss: 0.6983, step time: 0.4944\n",
      "171/388, train_loss: 0.6800, step time: 0.4801\n",
      "172/388, train_loss: 0.3880, step time: 0.4949\n",
      "173/388, train_loss: 0.3008, step time: 0.4998\n",
      "174/388, train_loss: 0.3093, step time: 0.4934\n",
      "175/388, train_loss: 0.3598, step time: 0.4781\n",
      "176/388, train_loss: 0.3730, step time: 0.6678\n",
      "177/388, train_loss: 0.4353, step time: 0.5772\n",
      "178/388, train_loss: 0.2877, step time: 0.5957\n",
      "179/388, train_loss: 0.4256, step time: 0.5405\n",
      "180/388, train_loss: 0.9097, step time: 0.5333\n",
      "181/388, train_loss: 0.4107, step time: 0.5357\n",
      "182/388, train_loss: 0.1785, step time: 0.5121\n",
      "183/388, train_loss: 0.8193, step time: 0.5003\n",
      "184/388, train_loss: 0.3927, step time: 0.4968\n",
      "185/388, train_loss: 0.7536, step time: 0.4831\n",
      "186/388, train_loss: 0.4398, step time: 0.5621\n",
      "187/388, train_loss: 0.7777, step time: 0.5336\n",
      "188/388, train_loss: 0.5319, step time: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/388, train_loss: 0.5247, step time: 0.4950\n",
      "190/388, train_loss: 0.5045, step time: 0.5148\n",
      "191/388, train_loss: 0.3842, step time: 0.5014\n",
      "192/388, train_loss: 0.5462, step time: 1.0344\n",
      "193/388, train_loss: 0.4745, step time: 0.5482\n",
      "194/388, train_loss: 0.4112, step time: 0.5124\n",
      "195/388, train_loss: 0.5285, step time: 0.4891\n",
      "196/388, train_loss: 0.2183, step time: 1.1567\n",
      "197/388, train_loss: 0.3310, step time: 0.5499\n",
      "198/388, train_loss: 0.3772, step time: 0.5022\n",
      "199/388, train_loss: 0.8505, step time: 0.4915\n",
      "200/388, train_loss: 0.6905, step time: 0.4826\n",
      "201/388, train_loss: 0.6252, step time: 0.5039\n",
      "202/388, train_loss: 0.3789, step time: 0.4932\n",
      "203/388, train_loss: 0.4587, step time: 0.5299\n",
      "204/388, train_loss: 0.4168, step time: 0.5143\n",
      "205/388, train_loss: 0.4139, step time: 0.4924\n",
      "206/388, train_loss: 0.4805, step time: 0.5018\n",
      "207/388, train_loss: 0.3379, step time: 0.5123\n",
      "208/388, train_loss: 0.4845, step time: 0.4869\n",
      "209/388, train_loss: 0.5294, step time: 0.4881\n",
      "210/388, train_loss: 0.6908, step time: 1.1484\n",
      "211/388, train_loss: 0.4461, step time: 0.5378\n",
      "212/388, train_loss: 0.3820, step time: 0.5059\n",
      "213/388, train_loss: 0.3681, step time: 0.4857\n",
      "214/388, train_loss: 0.6213, step time: 0.4961\n",
      "215/388, train_loss: 0.6577, step time: 0.5186\n",
      "216/388, train_loss: 0.3770, step time: 0.4977\n",
      "217/388, train_loss: 0.7554, step time: 1.1239\n",
      "218/388, train_loss: 0.8110, step time: 0.5403\n",
      "219/388, train_loss: 0.8903, step time: 0.5178\n",
      "220/388, train_loss: 0.8897, step time: 0.4938\n",
      "221/388, train_loss: 0.5146, step time: 0.4922\n",
      "222/388, train_loss: 0.3448, step time: 0.5067\n",
      "223/388, train_loss: 0.5002, step time: 0.5292\n",
      "224/388, train_loss: 0.4398, step time: 0.4995\n",
      "225/388, train_loss: 0.5527, step time: 0.4959\n",
      "226/388, train_loss: 0.3931, step time: 0.4886\n",
      "227/388, train_loss: 0.2742, step time: 0.4806\n",
      "228/388, train_loss: 0.7086, step time: 0.7259\n",
      "229/388, train_loss: 0.7847, step time: 0.5506\n",
      "230/388, train_loss: 0.5743, step time: 0.5213\n",
      "231/388, train_loss: 0.4688, step time: 0.4994\n",
      "232/388, train_loss: 0.7591, step time: 0.4953\n",
      "233/388, train_loss: 0.4520, step time: 1.1104\n",
      "234/388, train_loss: 0.3350, step time: 0.5296\n",
      "235/388, train_loss: 0.2944, step time: 0.5057\n",
      "236/388, train_loss: 0.4722, step time: 0.4892\n",
      "237/388, train_loss: 0.6356, step time: 0.4844\n",
      "238/388, train_loss: 0.3752, step time: 0.4932\n",
      "239/388, train_loss: 0.4293, step time: 1.0023\n",
      "240/388, train_loss: 0.3452, step time: 0.5364\n",
      "241/388, train_loss: 0.8839, step time: 0.5124\n",
      "242/388, train_loss: 0.4449, step time: 0.4885\n",
      "243/388, train_loss: 0.7640, step time: 0.5017\n",
      "244/388, train_loss: 0.3572, step time: 0.4854\n",
      "245/388, train_loss: 0.7435, step time: 0.5200\n",
      "246/388, train_loss: 0.3940, step time: 0.5005\n",
      "247/388, train_loss: 0.6436, step time: 0.4960\n",
      "248/388, train_loss: 0.1976, step time: 0.4857\n",
      "249/388, train_loss: 0.4115, step time: 0.4911\n",
      "250/388, train_loss: 0.4027, step time: 0.4926\n",
      "251/388, train_loss: 0.5349, step time: 0.4998\n",
      "252/388, train_loss: 0.4661, step time: 0.5132\n",
      "253/388, train_loss: 0.3447, step time: 0.4950\n",
      "254/388, train_loss: 0.3750, step time: 0.4944\n",
      "255/388, train_loss: 0.4393, step time: 0.5119\n",
      "256/388, train_loss: 0.7901, step time: 0.5026\n",
      "257/388, train_loss: 0.3697, step time: 0.4852\n",
      "258/388, train_loss: 0.3505, step time: 0.4917\n",
      "259/388, train_loss: 0.4940, step time: 0.5236\n",
      "260/388, train_loss: 0.5821, step time: 0.5248\n",
      "261/388, train_loss: 0.4746, step time: 0.9102\n",
      "262/388, train_loss: 0.5117, step time: 0.5233\n",
      "263/388, train_loss: 0.3424, step time: 0.5187\n",
      "264/388, train_loss: 0.5485, step time: 0.5128\n",
      "265/388, train_loss: 0.4718, step time: 0.4956\n",
      "266/388, train_loss: 0.8336, step time: 0.5046\n",
      "267/388, train_loss: 0.4352, step time: 0.5335\n",
      "268/388, train_loss: 0.2288, step time: 0.5987\n",
      "269/388, train_loss: 0.5038, step time: 0.5395\n",
      "270/388, train_loss: 0.4598, step time: 0.5116\n",
      "271/388, train_loss: 0.4337, step time: 0.4956\n",
      "272/388, train_loss: 0.2207, step time: 0.7401\n",
      "273/388, train_loss: 0.6864, step time: 0.5473\n",
      "274/388, train_loss: 0.5386, step time: 0.5084\n",
      "275/388, train_loss: 0.5924, step time: 0.5035\n",
      "276/388, train_loss: 0.4119, step time: 0.5172\n",
      "277/388, train_loss: 0.5266, step time: 0.5101\n",
      "278/388, train_loss: 0.3492, step time: 0.4894\n",
      "279/388, train_loss: 0.3613, step time: 0.4948\n",
      "280/388, train_loss: 0.3730, step time: 0.4773\n",
      "281/388, train_loss: 0.2953, step time: 0.5718\n",
      "282/388, train_loss: 0.3915, step time: 0.5072\n",
      "283/388, train_loss: 0.2734, step time: 0.5640\n",
      "284/388, train_loss: 0.3327, step time: 0.5259\n",
      "285/388, train_loss: 0.3798, step time: 0.5014\n",
      "286/388, train_loss: 0.9382, step time: 0.5004\n",
      "287/388, train_loss: 0.3669, step time: 0.4808\n",
      "288/388, train_loss: 0.4877, step time: 0.5263\n",
      "289/388, train_loss: 0.6759, step time: 0.5234\n",
      "290/388, train_loss: 0.6185, step time: 0.4985\n",
      "291/388, train_loss: 0.7858, step time: 0.4877\n",
      "292/388, train_loss: 0.4070, step time: 0.4955\n",
      "293/388, train_loss: 0.6424, step time: 0.4982\n",
      "294/388, train_loss: 0.3786, step time: 0.4972\n",
      "295/388, train_loss: 0.4525, step time: 0.4885\n",
      "296/388, train_loss: 0.4202, step time: 0.6689\n",
      "297/388, train_loss: 0.6278, step time: 0.5591\n",
      "298/388, train_loss: 0.6768, step time: 0.5305\n",
      "299/388, train_loss: 0.5259, step time: 0.5164\n",
      "300/388, train_loss: 0.3214, step time: 0.5115\n",
      "301/388, train_loss: 0.5350, step time: 0.4989\n",
      "302/388, train_loss: 0.3381, step time: 0.4937\n",
      "303/388, train_loss: 0.5781, step time: 1.0618\n",
      "304/388, train_loss: 0.3498, step time: 0.5430\n",
      "305/388, train_loss: 0.4609, step time: 0.5192\n",
      "306/388, train_loss: 0.3498, step time: 0.4944\n",
      "307/388, train_loss: 0.6083, step time: 0.4988\n",
      "308/388, train_loss: 0.2756, step time: 0.4831\n",
      "309/388, train_loss: 0.4578, step time: 0.5027\n",
      "310/388, train_loss: 0.6727, step time: 0.4936\n",
      "311/388, train_loss: 0.6318, step time: 0.4966\n",
      "312/388, train_loss: 0.4373, step time: 0.5021\n",
      "313/388, train_loss: 0.5127, step time: 0.5449\n",
      "314/388, train_loss: 0.2625, step time: 0.5264\n",
      "315/388, train_loss: 0.3749, step time: 0.5024\n",
      "316/388, train_loss: 0.3381, step time: 0.4920\n",
      "317/388, train_loss: 0.3910, step time: 0.4846\n",
      "318/388, train_loss: 0.6579, step time: 0.4895\n",
      "319/388, train_loss: 0.6948, step time: 1.1439\n",
      "320/388, train_loss: 0.4772, step time: 0.5358\n",
      "321/388, train_loss: 0.3029, step time: 0.5004\n",
      "322/388, train_loss: 0.4469, step time: 0.4907\n",
      "323/388, train_loss: 0.3530, step time: 0.4923\n",
      "324/388, train_loss: 0.4754, step time: 0.4944\n",
      "325/388, train_loss: 0.6721, step time: 0.4875\n",
      "326/388, train_loss: 0.3250, step time: 0.4750\n",
      "327/388, train_loss: 0.2906, step time: 1.0293\n",
      "328/388, train_loss: 0.4148, step time: 0.5297\n",
      "329/388, train_loss: 0.2485, step time: 0.5071\n",
      "330/388, train_loss: 0.6107, step time: 0.4902\n",
      "331/388, train_loss: 0.4254, step time: 0.4936\n",
      "332/388, train_loss: 0.6333, step time: 0.4811\n",
      "333/388, train_loss: 0.8894, step time: 0.4993\n",
      "334/388, train_loss: 0.4709, step time: 0.5463\n",
      "335/388, train_loss: 0.5639, step time: 0.5187\n",
      "336/388, train_loss: 0.8661, step time: 0.5016\n",
      "337/388, train_loss: 0.3828, step time: 0.4948\n",
      "338/388, train_loss: 0.3424, step time: 0.4892\n",
      "339/388, train_loss: 0.5242, step time: 0.4950\n",
      "340/388, train_loss: 0.3998, step time: 1.1421\n",
      "341/388, train_loss: 0.6912, step time: 0.5283\n",
      "342/388, train_loss: 0.4142, step time: 0.5000\n",
      "343/388, train_loss: 0.4608, step time: 0.4831\n",
      "344/388, train_loss: 0.3950, step time: 0.4888\n",
      "345/388, train_loss: 0.4229, step time: 0.4886\n",
      "346/388, train_loss: 0.3899, step time: 0.4847\n",
      "347/388, train_loss: 0.2222, step time: 0.5110\n",
      "348/388, train_loss: 0.6331, step time: 0.4885\n",
      "349/388, train_loss: 0.5259, step time: 0.4848\n",
      "350/388, train_loss: 0.2285, step time: 0.5099\n",
      "351/388, train_loss: 0.5666, step time: 0.4948\n",
      "352/388, train_loss: 0.5063, step time: 0.4937\n",
      "353/388, train_loss: 0.1748, step time: 0.4759\n",
      "354/388, train_loss: 0.3612, step time: 0.4858\n",
      "355/388, train_loss: 0.2647, step time: 1.1042\n",
      "356/388, train_loss: 0.2093, step time: 0.5329\n",
      "357/388, train_loss: 0.6423, step time: 0.5055\n",
      "358/388, train_loss: 0.5896, step time: 0.4871\n",
      "359/388, train_loss: 0.6003, step time: 0.4818\n",
      "360/388, train_loss: 0.4578, step time: 0.5083\n",
      "361/388, train_loss: 0.3972, step time: 0.4997\n",
      "362/388, train_loss: 0.3035, step time: 0.5216\n",
      "363/388, train_loss: 0.3277, step time: 0.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/388, train_loss: 0.1679, step time: 0.4980\n",
      "365/388, train_loss: 0.9272, step time: 0.4840\n",
      "366/388, train_loss: 0.7830, step time: 0.4888\n",
      "367/388, train_loss: 0.4192, step time: 1.0860\n",
      "368/388, train_loss: 0.9032, step time: 0.5329\n",
      "369/388, train_loss: 0.6294, step time: 0.5004\n",
      "370/388, train_loss: 0.6462, step time: 0.4812\n",
      "371/388, train_loss: 0.4810, step time: 0.4768\n",
      "372/388, train_loss: 0.3724, step time: 0.4811\n",
      "373/388, train_loss: 0.5853, step time: 0.4963\n",
      "374/388, train_loss: 0.7309, step time: 0.4952\n",
      "375/388, train_loss: 0.3474, step time: 0.8592\n",
      "376/388, train_loss: 0.5613, step time: 0.5534\n",
      "377/388, train_loss: 0.4984, step time: 0.5155\n",
      "378/388, train_loss: 0.5729, step time: 0.4862\n",
      "379/388, train_loss: 0.5651, step time: 0.4921\n",
      "380/388, train_loss: 0.8115, step time: 0.4866\n",
      "381/388, train_loss: 0.4984, step time: 0.4939\n",
      "382/388, train_loss: 0.2945, step time: 0.5217\n",
      "383/388, train_loss: 0.3215, step time: 0.4966\n",
      "384/388, train_loss: 0.2966, step time: 0.4840\n",
      "385/388, train_loss: 0.6165, step time: 0.4719\n",
      "386/388, train_loss: 0.4647, step time: 1.0614\n",
      "387/388, train_loss: 0.3800, step time: 0.5230\n",
      "388/388, train_loss: 0.6309, step time: 0.5043\n",
      "epoch 5 average loss: 0.5164\n",
      "current epoch: 5 current mean dice: 0.5961 tc: 0.6239 wt: 0.8156 et: 0.3489\n",
      "best mean dice: 0.6109 at epoch: 4\n",
      "time consuming of epoch 5 is: 302.1614\n",
      "----------\n",
      "epoch 6/300\n",
      "1/388, train_loss: 0.5234, step time: 0.4692\n",
      "2/388, train_loss: 0.5517, step time: 0.4855\n",
      "3/388, train_loss: 0.5307, step time: 1.1613\n",
      "4/388, train_loss: 0.5419, step time: 0.5287\n",
      "5/388, train_loss: 0.7610, step time: 0.4971\n",
      "6/388, train_loss: 0.7425, step time: 0.5169\n",
      "7/388, train_loss: 0.3140, step time: 0.5930\n",
      "8/388, train_loss: 0.3683, step time: 0.5196\n",
      "9/388, train_loss: 0.6732, step time: 0.5056\n",
      "10/388, train_loss: 0.3426, step time: 0.4878\n",
      "11/388, train_loss: 0.5465, step time: 0.5032\n",
      "12/388, train_loss: 0.6676, step time: 0.5954\n",
      "13/388, train_loss: 0.1988, step time: 0.5515\n",
      "14/388, train_loss: 0.4370, step time: 0.5157\n",
      "15/388, train_loss: 0.3644, step time: 0.4960\n",
      "16/388, train_loss: 0.8783, step time: 0.4971\n",
      "17/388, train_loss: 0.6426, step time: 0.5536\n",
      "18/388, train_loss: 0.8361, step time: 0.5259\n",
      "19/388, train_loss: 0.2776, step time: 0.5161\n",
      "20/388, train_loss: 0.8929, step time: 0.5111\n",
      "21/388, train_loss: 0.2671, step time: 1.1511\n",
      "22/388, train_loss: 0.2896, step time: 0.5361\n",
      "23/388, train_loss: 0.3228, step time: 0.5005\n",
      "24/388, train_loss: 0.3615, step time: 0.4899\n",
      "25/388, train_loss: 0.5150, step time: 1.0079\n",
      "26/388, train_loss: 0.5310, step time: 0.5449\n",
      "27/388, train_loss: 0.4043, step time: 0.5025\n",
      "28/388, train_loss: 0.2209, step time: 0.4958\n",
      "29/388, train_loss: 0.3024, step time: 0.4861\n",
      "30/388, train_loss: 0.4928, step time: 1.1423\n",
      "31/388, train_loss: 0.5180, step time: 0.5417\n",
      "32/388, train_loss: 0.4445, step time: 0.5187\n",
      "33/388, train_loss: 0.4500, step time: 0.4968\n",
      "34/388, train_loss: 0.3990, step time: 0.4928\n",
      "35/388, train_loss: 0.3874, step time: 0.4883\n",
      "36/388, train_loss: 0.4038, step time: 0.4801\n",
      "37/388, train_loss: 0.3708, step time: 1.0473\n",
      "38/388, train_loss: 0.7890, step time: 0.5382\n",
      "39/388, train_loss: 0.4737, step time: 0.5063\n",
      "40/388, train_loss: 0.3077, step time: 0.4948\n",
      "41/388, train_loss: 0.3399, step time: 0.4805\n",
      "42/388, train_loss: 0.3425, step time: 0.4840\n",
      "43/388, train_loss: 0.3621, step time: 0.5020\n",
      "44/388, train_loss: 0.6961, step time: 0.4908\n",
      "45/388, train_loss: 0.4261, step time: 0.5079\n",
      "46/388, train_loss: 0.2432, step time: 0.5025\n",
      "47/388, train_loss: 0.4390, step time: 0.4870\n",
      "48/388, train_loss: 0.3740, step time: 0.4873\n",
      "49/388, train_loss: 0.3248, step time: 0.4833\n",
      "50/388, train_loss: 0.3226, step time: 0.4845\n",
      "51/388, train_loss: 0.3216, step time: 0.4938\n",
      "52/388, train_loss: 0.3818, step time: 0.4970\n",
      "53/388, train_loss: 0.3665, step time: 0.4981\n",
      "54/388, train_loss: 0.4467, step time: 0.5065\n",
      "55/388, train_loss: 0.1886, step time: 0.5385\n",
      "56/388, train_loss: 0.3881, step time: 0.5130\n",
      "57/388, train_loss: 0.1614, step time: 0.4842\n",
      "58/388, train_loss: 0.4026, step time: 0.4824\n",
      "59/388, train_loss: 0.3906, step time: 0.5831\n",
      "60/388, train_loss: 0.4402, step time: 0.5559\n",
      "61/388, train_loss: 0.2734, step time: 0.5178\n",
      "62/388, train_loss: 0.2809, step time: 0.5355\n",
      "63/388, train_loss: 0.3345, step time: 0.5826\n",
      "64/388, train_loss: 0.3889, step time: 0.5373\n",
      "65/388, train_loss: 0.4138, step time: 0.5165\n",
      "66/388, train_loss: 0.3963, step time: 0.4906\n",
      "67/388, train_loss: 0.7821, step time: 0.5044\n",
      "68/388, train_loss: 0.3023, step time: 0.5619\n",
      "69/388, train_loss: 0.5366, step time: 0.5173\n",
      "70/388, train_loss: 0.3973, step time: 0.5006\n",
      "71/388, train_loss: 0.2589, step time: 0.4897\n",
      "72/388, train_loss: 0.4420, step time: 0.4927\n",
      "73/388, train_loss: 0.4520, step time: 0.9860\n",
      "74/388, train_loss: 0.4615, step time: 0.5344\n",
      "75/388, train_loss: 0.3869, step time: 0.5010\n",
      "76/388, train_loss: 0.3903, step time: 0.4917\n",
      "77/388, train_loss: 0.3877, step time: 0.4968\n",
      "78/388, train_loss: 0.4444, step time: 0.4804\n",
      "79/388, train_loss: 0.5596, step time: 0.4789\n",
      "80/388, train_loss: 0.3318, step time: 1.0138\n",
      "81/388, train_loss: 0.3919, step time: 0.5465\n",
      "82/388, train_loss: 0.3085, step time: 0.5150\n",
      "83/388, train_loss: 0.3302, step time: 0.5017\n",
      "84/388, train_loss: 0.3360, step time: 0.4886\n",
      "85/388, train_loss: 0.2311, step time: 0.4942\n",
      "86/388, train_loss: 0.8339, step time: 0.4819\n",
      "87/388, train_loss: 0.7645, step time: 0.4835\n",
      "88/388, train_loss: 0.5664, step time: 0.8862\n",
      "89/388, train_loss: 0.3025, step time: 0.5526\n",
      "90/388, train_loss: 0.3463, step time: 0.5173\n",
      "91/388, train_loss: 0.4375, step time: 0.4974\n",
      "92/388, train_loss: 0.7551, step time: 0.4873\n",
      "93/388, train_loss: 0.2911, step time: 0.9232\n",
      "94/388, train_loss: 0.3762, step time: 0.5605\n",
      "95/388, train_loss: 0.5696, step time: 0.5278\n",
      "96/388, train_loss: 0.3620, step time: 0.5046\n",
      "97/388, train_loss: 0.6266, step time: 0.5027\n",
      "98/388, train_loss: 0.2758, step time: 0.4981\n",
      "99/388, train_loss: 0.4011, step time: 0.4899\n",
      "100/388, train_loss: 0.4698, step time: 0.5116\n",
      "101/388, train_loss: 0.5391, step time: 0.5052\n",
      "102/388, train_loss: 0.2873, step time: 0.4915\n",
      "103/388, train_loss: 0.3950, step time: 0.9705\n",
      "104/388, train_loss: 0.2633, step time: 0.5422\n",
      "105/388, train_loss: 0.6724, step time: 0.5128\n",
      "106/388, train_loss: 0.7683, step time: 0.4978\n",
      "107/388, train_loss: 0.3302, step time: 0.4882\n",
      "108/388, train_loss: 0.6333, step time: 1.0809\n",
      "109/388, train_loss: 0.3975, step time: 0.5416\n",
      "110/388, train_loss: 0.7410, step time: 0.5128\n",
      "111/388, train_loss: 0.4576, step time: 0.4956\n",
      "112/388, train_loss: 0.3362, step time: 0.4836\n",
      "113/388, train_loss: 0.6858, step time: 0.4938\n",
      "114/388, train_loss: 0.3372, step time: 0.4860\n",
      "115/388, train_loss: 0.7318, step time: 1.0125\n",
      "116/388, train_loss: 0.6155, step time: 0.5333\n",
      "117/388, train_loss: 0.6363, step time: 0.5086\n",
      "118/388, train_loss: 0.6206, step time: 0.4900\n",
      "119/388, train_loss: 0.3021, step time: 0.5345\n",
      "120/388, train_loss: 0.3537, step time: 0.5153\n",
      "121/388, train_loss: 0.3791, step time: 0.4969\n",
      "122/388, train_loss: 0.4709, step time: 0.4928\n",
      "123/388, train_loss: 0.3462, step time: 0.4894\n",
      "124/388, train_loss: 0.7570, step time: 0.5131\n",
      "125/388, train_loss: 0.5213, step time: 0.4846\n",
      "126/388, train_loss: 0.6752, step time: 0.9279\n",
      "127/388, train_loss: 0.3754, step time: 0.5345\n",
      "128/388, train_loss: 0.6370, step time: 0.5029\n",
      "129/388, train_loss: 0.2807, step time: 0.4939\n",
      "130/388, train_loss: 0.4429, step time: 0.4895\n",
      "131/388, train_loss: 0.5402, step time: 0.4992\n",
      "132/388, train_loss: 0.2358, step time: 0.4993\n",
      "133/388, train_loss: 0.4244, step time: 0.7653\n",
      "134/388, train_loss: 0.2507, step time: 0.5373\n",
      "135/388, train_loss: 0.7277, step time: 0.5014\n",
      "136/388, train_loss: 0.3120, step time: 0.4942\n",
      "137/388, train_loss: 0.3926, step time: 0.4927\n",
      "138/388, train_loss: 0.5268, step time: 0.5037\n",
      "139/388, train_loss: 0.1565, step time: 0.5050\n",
      "140/388, train_loss: 0.4366, step time: 0.5284\n",
      "141/388, train_loss: 0.2418, step time: 0.5128\n",
      "142/388, train_loss: 0.5178, step time: 0.4993\n",
      "143/388, train_loss: 0.5195, step time: 0.4825\n",
      "144/388, train_loss: 0.3845, step time: 0.4967\n",
      "145/388, train_loss: 0.4724, step time: 0.5605\n",
      "146/388, train_loss: 0.4922, step time: 0.5374\n",
      "147/388, train_loss: 0.4339, step time: 0.5119\n",
      "148/388, train_loss: 0.5244, step time: 0.4967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/388, train_loss: 0.1679, step time: 0.4866\n",
      "150/388, train_loss: 0.2303, step time: 1.1471\n",
      "151/388, train_loss: 0.5284, step time: 0.5193\n",
      "152/388, train_loss: 0.3235, step time: 0.5057\n",
      "153/388, train_loss: 0.3813, step time: 0.4921\n",
      "154/388, train_loss: 0.2384, step time: 0.4793\n",
      "155/388, train_loss: 0.3732, step time: 1.1721\n",
      "156/388, train_loss: 0.3925, step time: 0.5383\n",
      "157/388, train_loss: 0.3879, step time: 0.5090\n",
      "158/388, train_loss: 0.3625, step time: 0.4905\n",
      "159/388, train_loss: 0.5527, step time: 0.4912\n",
      "160/388, train_loss: 0.6568, step time: 1.1882\n",
      "161/388, train_loss: 0.3302, step time: 0.5313\n",
      "162/388, train_loss: 0.1807, step time: 0.5032\n",
      "163/388, train_loss: 0.4805, step time: 0.4829\n",
      "164/388, train_loss: 0.3326, step time: 0.4927\n",
      "165/388, train_loss: 0.3340, step time: 0.4833\n",
      "166/388, train_loss: 0.8123, step time: 0.4789\n",
      "167/388, train_loss: 0.4880, step time: 0.4841\n",
      "168/388, train_loss: 0.5113, step time: 0.4745\n",
      "169/388, train_loss: 0.6836, step time: 1.0722\n",
      "170/388, train_loss: 0.3338, step time: 0.5286\n",
      "171/388, train_loss: 0.2640, step time: 0.5029\n",
      "172/388, train_loss: 0.1436, step time: 0.4935\n",
      "173/388, train_loss: 0.5828, step time: 0.4933\n",
      "174/388, train_loss: 0.2288, step time: 0.5130\n",
      "175/388, train_loss: 0.2781, step time: 0.4963\n",
      "176/388, train_loss: 0.4234, step time: 0.4866\n",
      "177/388, train_loss: 0.3179, step time: 0.4862\n",
      "178/388, train_loss: 0.3408, step time: 0.5070\n",
      "179/388, train_loss: 0.1766, step time: 0.4977\n",
      "180/388, train_loss: 0.3822, step time: 0.4955\n",
      "181/388, train_loss: 0.3900, step time: 0.4823\n",
      "182/388, train_loss: 0.5119, step time: 0.5249\n",
      "183/388, train_loss: 0.5055, step time: 0.5564\n",
      "184/388, train_loss: 0.2831, step time: 0.5417\n",
      "185/388, train_loss: 0.2506, step time: 0.5114\n",
      "186/388, train_loss: 0.4598, step time: 0.4962\n",
      "187/388, train_loss: 0.3379, step time: 0.4961\n",
      "188/388, train_loss: 0.3838, step time: 0.5083\n",
      "189/388, train_loss: 0.4944, step time: 0.5767\n",
      "190/388, train_loss: 0.4774, step time: 0.5281\n",
      "191/388, train_loss: 0.2165, step time: 0.5175\n",
      "192/388, train_loss: 0.3113, step time: 0.5052\n",
      "193/388, train_loss: 0.2809, step time: 0.4939\n",
      "194/388, train_loss: 0.4949, step time: 0.5517\n",
      "195/388, train_loss: 0.5214, step time: 0.5286\n",
      "196/388, train_loss: 0.4844, step time: 0.4972\n",
      "197/388, train_loss: 0.4201, step time: 0.4884\n",
      "198/388, train_loss: 0.6176, step time: 0.4957\n",
      "199/388, train_loss: 0.2377, step time: 0.7426\n",
      "200/388, train_loss: 0.7984, step time: 0.5622\n",
      "201/388, train_loss: 0.6681, step time: 0.5132\n",
      "202/388, train_loss: 0.3371, step time: 0.4990\n",
      "203/388, train_loss: 0.2522, step time: 0.4778\n",
      "204/388, train_loss: 0.3614, step time: 0.4893\n",
      "205/388, train_loss: 0.4496, step time: 0.4939\n",
      "206/388, train_loss: 0.3722, step time: 1.1203\n",
      "207/388, train_loss: 0.4705, step time: 0.5617\n",
      "208/388, train_loss: 0.4689, step time: 0.5123\n",
      "209/388, train_loss: 0.1421, step time: 0.4981\n",
      "210/388, train_loss: 0.1240, step time: 0.4955\n",
      "211/388, train_loss: 0.2934, step time: 0.4837\n",
      "212/388, train_loss: 0.3364, step time: 0.9473\n",
      "213/388, train_loss: 0.2626, step time: 0.5263\n",
      "214/388, train_loss: 0.4498, step time: 0.5052\n",
      "215/388, train_loss: 0.8767, step time: 0.4992\n",
      "216/388, train_loss: 0.6236, step time: 0.5263\n",
      "217/388, train_loss: 0.3730, step time: 0.5058\n",
      "218/388, train_loss: 0.8492, step time: 0.4900\n",
      "219/388, train_loss: 0.1814, step time: 0.5120\n",
      "220/388, train_loss: 0.4037, step time: 0.5014\n",
      "221/388, train_loss: 0.4265, step time: 0.5005\n",
      "222/388, train_loss: 0.4798, step time: 0.4927\n",
      "223/388, train_loss: 0.3190, step time: 0.5277\n",
      "224/388, train_loss: 0.3371, step time: 0.5207\n",
      "225/388, train_loss: 0.7172, step time: 0.5029\n",
      "226/388, train_loss: 0.4450, step time: 0.6216\n",
      "227/388, train_loss: 0.5928, step time: 0.5470\n",
      "228/388, train_loss: 0.3591, step time: 0.5164\n",
      "229/388, train_loss: 0.5748, step time: 0.5011\n",
      "230/388, train_loss: 0.3722, step time: 1.1215\n",
      "231/388, train_loss: 0.2748, step time: 0.5331\n",
      "232/388, train_loss: 0.3480, step time: 0.5058\n",
      "233/388, train_loss: 0.3195, step time: 0.4968\n",
      "234/388, train_loss: 0.2528, step time: 0.4877\n",
      "235/388, train_loss: 0.2806, step time: 0.5229\n",
      "236/388, train_loss: 0.3036, step time: 0.5187\n",
      "237/388, train_loss: 0.3299, step time: 0.5838\n",
      "238/388, train_loss: 0.3628, step time: 0.5422\n",
      "239/388, train_loss: 0.5530, step time: 0.5080\n",
      "240/388, train_loss: 0.3397, step time: 0.4822\n",
      "241/388, train_loss: 0.4341, step time: 0.4789\n",
      "242/388, train_loss: 0.4388, step time: 0.5043\n",
      "243/388, train_loss: 0.3823, step time: 0.4958\n",
      "244/388, train_loss: 0.3786, step time: 0.5565\n",
      "245/388, train_loss: 0.3075, step time: 0.5325\n",
      "246/388, train_loss: 0.3874, step time: 0.5008\n",
      "247/388, train_loss: 0.2432, step time: 0.4930\n",
      "248/388, train_loss: 0.3633, step time: 0.4935\n",
      "249/388, train_loss: 0.2892, step time: 0.4874\n",
      "250/388, train_loss: 0.5546, step time: 0.4906\n",
      "251/388, train_loss: 0.2987, step time: 1.0318\n",
      "252/388, train_loss: 0.3624, step time: 0.5572\n",
      "253/388, train_loss: 0.2846, step time: 0.5137\n",
      "254/388, train_loss: 0.4152, step time: 0.5078\n",
      "255/388, train_loss: 0.4305, step time: 0.5180\n",
      "256/388, train_loss: 0.3101, step time: 0.5015\n",
      "257/388, train_loss: 0.5262, step time: 0.4982\n",
      "258/388, train_loss: 0.6523, step time: 0.5059\n",
      "259/388, train_loss: 0.3720, step time: 0.5545\n",
      "260/388, train_loss: 0.2914, step time: 0.5278\n",
      "261/388, train_loss: 0.3000, step time: 0.5166\n",
      "262/388, train_loss: 0.4207, step time: 0.5080\n",
      "263/388, train_loss: 0.3727, step time: 0.4966\n",
      "264/388, train_loss: 0.3027, step time: 0.4897\n",
      "265/388, train_loss: 0.3527, step time: 0.4982\n",
      "266/388, train_loss: 0.2617, step time: 0.9514\n",
      "267/388, train_loss: 0.3200, step time: 0.5293\n",
      "268/388, train_loss: 0.6693, step time: 0.5039\n",
      "269/388, train_loss: 0.3128, step time: 0.4926\n",
      "270/388, train_loss: 0.4758, step time: 0.5018\n",
      "271/388, train_loss: 0.6003, step time: 0.5570\n",
      "272/388, train_loss: 0.3568, step time: 0.5342\n",
      "273/388, train_loss: 0.6350, step time: 0.5074\n",
      "274/388, train_loss: 0.3766, step time: 0.4935\n",
      "275/388, train_loss: 0.2448, step time: 0.4973\n",
      "276/388, train_loss: 0.5536, step time: 0.4782\n",
      "277/388, train_loss: 0.3310, step time: 0.5416\n",
      "278/388, train_loss: 0.4455, step time: 0.5265\n",
      "279/388, train_loss: 0.3810, step time: 0.5192\n",
      "280/388, train_loss: 0.4083, step time: 0.4981\n",
      "281/388, train_loss: 0.4317, step time: 0.4958\n",
      "282/388, train_loss: 0.3073, step time: 0.5083\n",
      "283/388, train_loss: 0.2843, step time: 0.5594\n",
      "284/388, train_loss: 0.7202, step time: 0.5235\n",
      "285/388, train_loss: 0.4752, step time: 0.5043\n",
      "286/388, train_loss: 0.6038, step time: 0.5013\n",
      "287/388, train_loss: 0.5069, step time: 0.5241\n",
      "288/388, train_loss: 0.3647, step time: 0.5875\n",
      "289/388, train_loss: 0.4635, step time: 0.5549\n",
      "290/388, train_loss: 0.7848, step time: 0.5168\n",
      "291/388, train_loss: 0.4960, step time: 0.5153\n",
      "292/388, train_loss: 0.3285, step time: 0.5826\n",
      "293/388, train_loss: 0.3202, step time: 0.5494\n",
      "294/388, train_loss: 0.3509, step time: 0.5114\n",
      "295/388, train_loss: 0.5713, step time: 0.5056\n",
      "296/388, train_loss: 0.2096, step time: 0.5989\n",
      "297/388, train_loss: 0.7056, step time: 0.5711\n",
      "298/388, train_loss: 0.3623, step time: 0.5433\n",
      "299/388, train_loss: 0.3377, step time: 0.5077\n",
      "300/388, train_loss: 0.5084, step time: 0.5328\n",
      "301/388, train_loss: 0.4847, step time: 0.6919\n",
      "302/388, train_loss: 0.3505, step time: 0.5157\n",
      "303/388, train_loss: 0.3178, step time: 0.4974\n",
      "304/388, train_loss: 0.3385, step time: 0.4939\n",
      "305/388, train_loss: 0.3653, step time: 1.0169\n",
      "306/388, train_loss: 0.3227, step time: 0.5390\n",
      "307/388, train_loss: 0.3571, step time: 0.5093\n",
      "308/388, train_loss: 0.3353, step time: 0.4943\n",
      "309/388, train_loss: 0.3791, step time: 0.5003\n",
      "310/388, train_loss: 0.4530, step time: 0.5490\n",
      "311/388, train_loss: 0.3871, step time: 0.6957\n",
      "312/388, train_loss: 0.8537, step time: 0.5515\n",
      "313/388, train_loss: 0.2708, step time: 0.5066\n",
      "314/388, train_loss: 0.6822, step time: 0.5028\n",
      "315/388, train_loss: 0.2174, step time: 0.4821\n",
      "316/388, train_loss: 0.3225, step time: 0.4863\n",
      "317/388, train_loss: 0.3280, step time: 0.4852\n",
      "318/388, train_loss: 0.4632, step time: 0.4859\n",
      "319/388, train_loss: 0.2787, step time: 0.5004\n",
      "320/388, train_loss: 0.3599, step time: 0.4832\n",
      "321/388, train_loss: 0.5522, step time: 0.4897\n",
      "322/388, train_loss: 0.1479, step time: 1.2065\n",
      "323/388, train_loss: 0.4084, step time: 0.5387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/388, train_loss: 0.2223, step time: 0.5065\n",
      "325/388, train_loss: 0.3750, step time: 0.5002\n",
      "326/388, train_loss: 0.1690, step time: 0.4903\n",
      "327/388, train_loss: 0.5482, step time: 0.5511\n",
      "328/388, train_loss: 0.2982, step time: 0.5481\n",
      "329/388, train_loss: 0.3934, step time: 0.5095\n",
      "330/388, train_loss: 0.3739, step time: 0.4984\n",
      "331/388, train_loss: 0.4237, step time: 0.4820\n",
      "332/388, train_loss: 0.2928, step time: 0.5070\n",
      "333/388, train_loss: 0.4546, step time: 0.5009\n",
      "334/388, train_loss: 0.3191, step time: 0.7162\n",
      "335/388, train_loss: 0.2636, step time: 0.5708\n",
      "336/388, train_loss: 0.5564, step time: 0.5228\n",
      "337/388, train_loss: 0.3896, step time: 0.4974\n",
      "338/388, train_loss: 0.2597, step time: 0.5073\n",
      "339/388, train_loss: 0.3724, step time: 0.5427\n",
      "340/388, train_loss: 0.4303, step time: 0.5234\n",
      "341/388, train_loss: 0.4801, step time: 0.5057\n",
      "342/388, train_loss: 0.2909, step time: 0.5083\n",
      "343/388, train_loss: 0.2483, step time: 0.5052\n",
      "344/388, train_loss: 0.4774, step time: 0.4934\n",
      "345/388, train_loss: 0.7831, step time: 0.4807\n",
      "346/388, train_loss: 0.5238, step time: 0.5221\n",
      "347/388, train_loss: 0.5308, step time: 0.5137\n",
      "348/388, train_loss: 0.3257, step time: 0.5007\n",
      "349/388, train_loss: 0.3372, step time: 0.4793\n",
      "350/388, train_loss: 0.7974, step time: 0.5298\n",
      "351/388, train_loss: 0.3722, step time: 0.4910\n",
      "352/388, train_loss: 0.3939, step time: 0.5229\n",
      "353/388, train_loss: 0.2529, step time: 0.6219\n",
      "354/388, train_loss: 0.3552, step time: 0.5649\n",
      "355/388, train_loss: 0.3259, step time: 0.5352\n",
      "356/388, train_loss: 0.4352, step time: 0.5243\n",
      "357/388, train_loss: 0.4645, step time: 0.5095\n",
      "358/388, train_loss: 0.4422, step time: 0.4985\n",
      "359/388, train_loss: 0.5465, step time: 0.4865\n",
      "360/388, train_loss: 0.7046, step time: 1.0316\n",
      "361/388, train_loss: 0.7229, step time: 0.5433\n",
      "362/388, train_loss: 0.3168, step time: 0.5196\n",
      "363/388, train_loss: 0.3099, step time: 0.5082\n",
      "364/388, train_loss: 0.6858, step time: 0.4907\n",
      "365/388, train_loss: 0.3344, step time: 0.5048\n",
      "366/388, train_loss: 0.2785, step time: 0.5669\n",
      "367/388, train_loss: 0.3272, step time: 0.5278\n",
      "368/388, train_loss: 0.3597, step time: 0.5098\n",
      "369/388, train_loss: 0.2906, step time: 0.5006\n",
      "370/388, train_loss: 0.4410, step time: 0.4895\n",
      "371/388, train_loss: 0.7126, step time: 0.4931\n",
      "372/388, train_loss: 0.2365, step time: 1.1645\n",
      "373/388, train_loss: 0.5128, step time: 0.5400\n",
      "374/388, train_loss: 0.2615, step time: 0.5016\n",
      "375/388, train_loss: 0.4671, step time: 0.5068\n",
      "376/388, train_loss: 0.3598, step time: 0.4872\n",
      "377/388, train_loss: 0.1995, step time: 0.4916\n",
      "378/388, train_loss: 0.5736, step time: 1.1181\n",
      "379/388, train_loss: 0.3844, step time: 0.5156\n",
      "380/388, train_loss: 0.4062, step time: 0.5038\n",
      "381/388, train_loss: 0.5395, step time: 0.4817\n",
      "382/388, train_loss: 0.2106, step time: 0.5041\n",
      "383/388, train_loss: 0.3188, step time: 0.4915\n",
      "384/388, train_loss: 0.2336, step time: 0.4728\n",
      "385/388, train_loss: 0.4191, step time: 0.4763\n",
      "386/388, train_loss: 0.3258, step time: 0.5074\n",
      "387/388, train_loss: 0.4607, step time: 0.4789\n",
      "388/388, train_loss: 0.1878, step time: 0.5031\n",
      "epoch 6 average loss: 0.4214\n",
      "saved new best metric model\n",
      "current epoch: 6 current mean dice: 0.6241 tc: 0.6863 wt: 0.7966 et: 0.3894\n",
      "best mean dice: 0.6241 at epoch: 6\n",
      "time consuming of epoch 6 is: 302.3362\n",
      "----------\n",
      "epoch 7/300\n",
      "1/388, train_loss: 0.4757, step time: 0.4696\n",
      "2/388, train_loss: 0.3290, step time: 0.4843\n",
      "3/388, train_loss: 0.4042, step time: 1.0103\n",
      "4/388, train_loss: 0.4269, step time: 0.5409\n",
      "5/388, train_loss: 0.3718, step time: 0.5099\n",
      "6/388, train_loss: 0.3130, step time: 0.5053\n",
      "7/388, train_loss: 0.2347, step time: 0.4944\n",
      "8/388, train_loss: 0.5691, step time: 0.4854\n",
      "9/388, train_loss: 0.2649, step time: 0.5057\n",
      "10/388, train_loss: 0.2380, step time: 1.0558\n",
      "11/388, train_loss: 0.3090, step time: 0.5483\n",
      "12/388, train_loss: 0.4348, step time: 0.5143\n",
      "13/388, train_loss: 0.3393, step time: 0.4916\n",
      "14/388, train_loss: 0.4785, step time: 0.5247\n",
      "15/388, train_loss: 0.5908, step time: 0.6439\n",
      "16/388, train_loss: 0.2863, step time: 0.6025\n",
      "17/388, train_loss: 0.3904, step time: 0.5426\n",
      "18/388, train_loss: 0.4039, step time: 0.5052\n",
      "19/388, train_loss: 0.2021, step time: 0.4975\n",
      "20/388, train_loss: 0.3588, step time: 0.4845\n",
      "21/388, train_loss: 0.2916, step time: 0.5250\n",
      "22/388, train_loss: 0.6062, step time: 0.4995\n",
      "23/388, train_loss: 0.2137, step time: 1.0515\n",
      "24/388, train_loss: 0.2176, step time: 0.5378\n",
      "25/388, train_loss: 0.6115, step time: 0.5206\n",
      "26/388, train_loss: 0.3349, step time: 0.4986\n",
      "27/388, train_loss: 0.4169, step time: 0.5032\n",
      "28/388, train_loss: 0.3057, step time: 0.5072\n",
      "29/388, train_loss: 0.2182, step time: 0.5705\n",
      "30/388, train_loss: 0.2034, step time: 0.5725\n",
      "31/388, train_loss: 0.6253, step time: 0.5367\n",
      "32/388, train_loss: 0.3314, step time: 0.5235\n",
      "33/388, train_loss: 0.2193, step time: 0.5234\n",
      "34/388, train_loss: 0.4001, step time: 0.5395\n",
      "35/388, train_loss: 0.3382, step time: 0.6065\n",
      "36/388, train_loss: 0.1834, step time: 0.5411\n",
      "37/388, train_loss: 0.3372, step time: 0.5164\n",
      "38/388, train_loss: 0.4886, step time: 0.5070\n",
      "39/388, train_loss: 0.5398, step time: 0.4846\n",
      "40/388, train_loss: 0.7734, step time: 1.0144\n",
      "41/388, train_loss: 0.5677, step time: 0.5598\n",
      "42/388, train_loss: 0.2996, step time: 0.5345\n",
      "43/388, train_loss: 0.3440, step time: 0.5017\n",
      "44/388, train_loss: 0.3599, step time: 0.4898\n",
      "45/388, train_loss: 0.2978, step time: 0.5059\n",
      "46/388, train_loss: 0.4766, step time: 0.5120\n",
      "47/388, train_loss: 0.2498, step time: 0.5406\n",
      "48/388, train_loss: 0.2679, step time: 0.5163\n",
      "49/388, train_loss: 0.6061, step time: 0.4945\n",
      "50/388, train_loss: 0.3289, step time: 0.4957\n",
      "51/388, train_loss: 0.3035, step time: 0.4798\n",
      "52/388, train_loss: 0.2411, step time: 0.4845\n",
      "53/388, train_loss: 0.3345, step time: 0.4875\n",
      "54/388, train_loss: 0.5623, step time: 0.4953\n",
      "55/388, train_loss: 0.3804, step time: 0.4933\n",
      "56/388, train_loss: 0.3380, step time: 0.9896\n",
      "57/388, train_loss: 0.2751, step time: 0.5541\n",
      "58/388, train_loss: 0.3710, step time: 0.5184\n",
      "59/388, train_loss: 0.4740, step time: 0.4945\n",
      "60/388, train_loss: 0.2612, step time: 0.4951\n",
      "61/388, train_loss: 0.4554, step time: 0.4976\n",
      "62/388, train_loss: 0.3707, step time: 0.4796\n",
      "63/388, train_loss: 0.2373, step time: 0.4905\n",
      "64/388, train_loss: 0.4261, step time: 0.5369\n",
      "65/388, train_loss: 0.3957, step time: 0.5125\n",
      "66/388, train_loss: 0.3179, step time: 0.4878\n",
      "67/388, train_loss: 0.1306, step time: 0.5119\n",
      "68/388, train_loss: 0.4233, step time: 0.4897\n",
      "69/388, train_loss: 0.3714, step time: 0.4930\n",
      "70/388, train_loss: 0.4197, step time: 0.4898\n",
      "71/388, train_loss: 0.3540, step time: 0.4782\n",
      "72/388, train_loss: 0.2521, step time: 0.5129\n",
      "73/388, train_loss: 0.2906, step time: 0.5014\n",
      "74/388, train_loss: 0.4036, step time: 0.5672\n",
      "75/388, train_loss: 0.3179, step time: 0.5431\n",
      "76/388, train_loss: 0.3214, step time: 0.5064\n",
      "77/388, train_loss: 0.3117, step time: 0.4985\n",
      "78/388, train_loss: 0.2766, step time: 0.4878\n",
      "79/388, train_loss: 0.4380, step time: 0.4897\n",
      "80/388, train_loss: 0.2992, step time: 0.4985\n",
      "81/388, train_loss: 0.6978, step time: 0.5601\n",
      "82/388, train_loss: 0.6855, step time: 0.5331\n",
      "83/388, train_loss: 0.5479, step time: 0.5094\n",
      "84/388, train_loss: 0.3741, step time: 0.4894\n",
      "85/388, train_loss: 0.1433, step time: 0.4960\n",
      "86/388, train_loss: 0.3250, step time: 0.4955\n",
      "87/388, train_loss: 0.1560, step time: 0.5126\n",
      "88/388, train_loss: 0.2205, step time: 0.6532\n",
      "89/388, train_loss: 0.2706, step time: 0.5530\n",
      "90/388, train_loss: 0.2461, step time: 0.5293\n",
      "91/388, train_loss: 0.6087, step time: 0.5140\n",
      "92/388, train_loss: 0.3409, step time: 0.4874\n",
      "93/388, train_loss: 0.4121, step time: 1.1078\n",
      "94/388, train_loss: 0.4710, step time: 0.5447\n",
      "95/388, train_loss: 0.2592, step time: 0.5071\n",
      "96/388, train_loss: 0.3346, step time: 0.4984\n",
      "97/388, train_loss: 0.3557, step time: 0.4833\n",
      "98/388, train_loss: 0.3906, step time: 0.4867\n",
      "99/388, train_loss: 0.4340, step time: 0.4904\n",
      "100/388, train_loss: 0.3829, step time: 0.5384\n",
      "101/388, train_loss: 0.3121, step time: 0.5291\n",
      "102/388, train_loss: 0.4881, step time: 0.5115\n",
      "103/388, train_loss: 0.3584, step time: 0.4883\n",
      "104/388, train_loss: 0.1844, step time: 0.4949\n",
      "105/388, train_loss: 0.4920, step time: 0.5022\n",
      "106/388, train_loss: 0.3168, step time: 1.0999\n",
      "107/388, train_loss: 0.3295, step time: 0.5344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/388, train_loss: 0.3704, step time: 0.5074\n",
      "109/388, train_loss: 0.3169, step time: 0.4990\n",
      "110/388, train_loss: 0.1790, step time: 0.4936\n",
      "111/388, train_loss: 0.2475, step time: 0.4826\n",
      "112/388, train_loss: 0.3137, step time: 0.4848\n",
      "113/388, train_loss: 0.2426, step time: 0.4806\n",
      "114/388, train_loss: 0.7683, step time: 0.8174\n",
      "115/388, train_loss: 0.1941, step time: 0.5641\n",
      "116/388, train_loss: 0.2857, step time: 0.5315\n",
      "117/388, train_loss: 0.3813, step time: 0.5048\n",
      "118/388, train_loss: 0.2772, step time: 0.5040\n",
      "119/388, train_loss: 0.2917, step time: 0.4938\n",
      "120/388, train_loss: 0.2456, step time: 0.4963\n",
      "121/388, train_loss: 0.8327, step time: 0.4916\n",
      "122/388, train_loss: 0.2329, step time: 0.5249\n",
      "123/388, train_loss: 0.4722, step time: 0.5032\n",
      "124/388, train_loss: 0.4389, step time: 0.5005\n",
      "125/388, train_loss: 0.5619, step time: 0.4848\n",
      "126/388, train_loss: 0.2432, step time: 0.4994\n",
      "127/388, train_loss: 0.3157, step time: 0.4826\n",
      "128/388, train_loss: 0.2556, step time: 0.4785\n",
      "129/388, train_loss: 0.5991, step time: 0.4900\n",
      "130/388, train_loss: 0.2061, step time: 0.5017\n",
      "131/388, train_loss: 0.1308, step time: 0.5023\n",
      "132/388, train_loss: 0.2730, step time: 0.4820\n",
      "133/388, train_loss: 0.3133, step time: 0.4883\n",
      "134/388, train_loss: 0.1253, step time: 0.4994\n",
      "135/388, train_loss: 0.3916, step time: 0.4992\n",
      "136/388, train_loss: 0.3430, step time: 0.5915\n",
      "137/388, train_loss: 0.2389, step time: 0.5387\n",
      "138/388, train_loss: 0.3531, step time: 0.5093\n",
      "139/388, train_loss: 0.5948, step time: 0.4980\n",
      "140/388, train_loss: 0.3556, step time: 1.0633\n",
      "141/388, train_loss: 0.6175, step time: 0.5478\n",
      "142/388, train_loss: 0.6823, step time: 0.5199\n",
      "143/388, train_loss: 0.3644, step time: 0.5013\n",
      "144/388, train_loss: 0.2410, step time: 0.4919\n",
      "145/388, train_loss: 0.3174, step time: 0.4949\n",
      "146/388, train_loss: 0.1484, step time: 0.4887\n",
      "147/388, train_loss: 0.7686, step time: 0.4989\n",
      "148/388, train_loss: 0.3624, step time: 0.4908\n",
      "149/388, train_loss: 0.7695, step time: 0.4751\n",
      "150/388, train_loss: 0.4407, step time: 0.4848\n",
      "151/388, train_loss: 0.3486, step time: 1.0177\n",
      "152/388, train_loss: 0.5508, step time: 0.5442\n",
      "153/388, train_loss: 0.2537, step time: 0.5143\n",
      "154/388, train_loss: 0.3099, step time: 0.4947\n",
      "155/388, train_loss: 0.9487, step time: 0.4907\n",
      "156/388, train_loss: 0.5239, step time: 0.4878\n",
      "157/388, train_loss: 0.4387, step time: 0.4901\n",
      "158/388, train_loss: 0.2396, step time: 0.9669\n",
      "159/388, train_loss: 0.8658, step time: 0.5427\n",
      "160/388, train_loss: 0.3327, step time: 0.4997\n",
      "161/388, train_loss: 0.6803, step time: 0.4914\n",
      "162/388, train_loss: 0.3839, step time: 0.4930\n",
      "163/388, train_loss: 0.3919, step time: 0.4781\n",
      "164/388, train_loss: 0.2867, step time: 0.4908\n",
      "165/388, train_loss: 0.4933, step time: 0.5133\n",
      "166/388, train_loss: 0.6796, step time: 0.5162\n",
      "167/388, train_loss: 0.3808, step time: 0.5050\n",
      "168/388, train_loss: 0.3278, step time: 0.5080\n",
      "169/388, train_loss: 0.3555, step time: 0.5382\n",
      "170/388, train_loss: 0.8488, step time: 0.5072\n",
      "171/388, train_loss: 0.2522, step time: 0.4995\n",
      "172/388, train_loss: 0.2862, step time: 0.4914\n",
      "173/388, train_loss: 0.3485, step time: 0.5048\n",
      "174/388, train_loss: 0.2130, step time: 0.4900\n",
      "175/388, train_loss: 0.6251, step time: 0.4943\n",
      "176/388, train_loss: 0.3016, step time: 0.4826\n",
      "177/388, train_loss: 0.5589, step time: 1.2226\n",
      "178/388, train_loss: 0.8462, step time: 0.5241\n",
      "179/388, train_loss: 0.2667, step time: 0.5002\n",
      "180/388, train_loss: 0.1652, step time: 0.4851\n",
      "181/388, train_loss: 0.3512, step time: 0.4920\n",
      "182/388, train_loss: 0.3466, step time: 0.4778\n",
      "183/388, train_loss: 0.2778, step time: 0.4739\n",
      "184/388, train_loss: 0.3768, step time: 0.9631\n",
      "185/388, train_loss: 0.6881, step time: 0.5567\n",
      "186/388, train_loss: 0.3742, step time: 0.5140\n",
      "187/388, train_loss: 0.2551, step time: 0.4922\n",
      "188/388, train_loss: 0.4446, step time: 0.5017\n",
      "189/388, train_loss: 0.8681, step time: 0.4809\n",
      "190/388, train_loss: 0.4930, step time: 0.4827\n",
      "191/388, train_loss: 0.4059, step time: 0.4881\n",
      "192/388, train_loss: 0.2997, step time: 0.4747\n",
      "193/388, train_loss: 0.4537, step time: 0.4776\n",
      "194/388, train_loss: 0.4420, step time: 0.8782\n",
      "195/388, train_loss: 0.5568, step time: 0.5405\n",
      "196/388, train_loss: 0.2672, step time: 0.5122\n",
      "197/388, train_loss: 0.3451, step time: 0.4930\n",
      "198/388, train_loss: 0.2173, step time: 0.4981\n",
      "199/388, train_loss: 0.3742, step time: 0.4791\n",
      "200/388, train_loss: 0.3394, step time: 0.4762\n",
      "201/388, train_loss: 0.6581, step time: 0.4849\n",
      "202/388, train_loss: 0.2879, step time: 1.0473\n",
      "203/388, train_loss: 0.2899, step time: 0.5392\n",
      "204/388, train_loss: 0.3438, step time: 0.5097\n",
      "205/388, train_loss: 0.6079, step time: 0.4962\n",
      "206/388, train_loss: 0.4122, step time: 0.4980\n",
      "207/388, train_loss: 0.7799, step time: 0.4837\n",
      "208/388, train_loss: 0.3505, step time: 0.4923\n",
      "209/388, train_loss: 0.4214, step time: 0.4781\n",
      "210/388, train_loss: 0.2880, step time: 1.0294\n",
      "211/388, train_loss: 0.2818, step time: 0.5389\n",
      "212/388, train_loss: 0.4257, step time: 0.5054\n",
      "213/388, train_loss: 0.5356, step time: 0.4776\n",
      "214/388, train_loss: 0.3701, step time: 0.4853\n",
      "215/388, train_loss: 0.4011, step time: 0.4918\n",
      "216/388, train_loss: 0.3035, step time: 0.4799\n",
      "217/388, train_loss: 0.2360, step time: 0.4952\n",
      "218/388, train_loss: 0.4061, step time: 0.4800\n",
      "219/388, train_loss: 0.4929, step time: 0.9221\n",
      "220/388, train_loss: 0.3689, step time: 0.5432\n",
      "221/388, train_loss: 0.5160, step time: 0.5114\n",
      "222/388, train_loss: 0.5318, step time: 0.4979\n",
      "223/388, train_loss: 0.3532, step time: 0.4845\n",
      "224/388, train_loss: 0.3433, step time: 0.5135\n",
      "225/388, train_loss: 0.7612, step time: 0.6518\n",
      "226/388, train_loss: 0.2826, step time: 0.5383\n",
      "227/388, train_loss: 0.5812, step time: 0.5014\n",
      "228/388, train_loss: 0.4021, step time: 0.5036\n",
      "229/388, train_loss: 0.3032, step time: 0.4892\n",
      "230/388, train_loss: 0.2692, step time: 0.4914\n",
      "231/388, train_loss: 0.3557, step time: 0.4878\n",
      "232/388, train_loss: 0.4113, step time: 0.4801\n",
      "233/388, train_loss: 0.3850, step time: 0.7660\n",
      "234/388, train_loss: 0.2897, step time: 0.5399\n",
      "235/388, train_loss: 0.2306, step time: 0.5212\n",
      "236/388, train_loss: 0.3633, step time: 0.5365\n",
      "237/388, train_loss: 0.4799, step time: 0.6261\n",
      "238/388, train_loss: 0.1989, step time: 0.5329\n",
      "239/388, train_loss: 0.5036, step time: 0.5017\n",
      "240/388, train_loss: 0.4363, step time: 0.4971\n",
      "241/388, train_loss: 0.7594, step time: 0.4923\n",
      "242/388, train_loss: 0.2517, step time: 0.4822\n",
      "243/388, train_loss: 0.4813, step time: 0.4866\n",
      "244/388, train_loss: 0.5017, step time: 0.4866\n",
      "245/388, train_loss: 0.2673, step time: 0.4970\n",
      "246/388, train_loss: 0.1846, step time: 0.5924\n",
      "247/388, train_loss: 0.2388, step time: 0.5340\n",
      "248/388, train_loss: 0.2136, step time: 0.5153\n",
      "249/388, train_loss: 0.1482, step time: 0.4986\n",
      "250/388, train_loss: 0.2835, step time: 0.5007\n",
      "251/388, train_loss: 0.2815, step time: 0.4941\n",
      "252/388, train_loss: 0.2992, step time: 0.5510\n",
      "253/388, train_loss: 0.4131, step time: 0.5161\n",
      "254/388, train_loss: 0.5568, step time: 0.5085\n",
      "255/388, train_loss: 0.3806, step time: 0.4855\n",
      "256/388, train_loss: 0.5587, step time: 0.5053\n",
      "257/388, train_loss: 0.5793, step time: 0.5016\n",
      "258/388, train_loss: 0.2700, step time: 0.4819\n",
      "259/388, train_loss: 0.2838, step time: 1.0179\n",
      "260/388, train_loss: 0.2968, step time: 0.5421\n",
      "261/388, train_loss: 0.5358, step time: 0.5250\n",
      "262/388, train_loss: 0.2432, step time: 0.4935\n",
      "263/388, train_loss: 0.2288, step time: 0.4959\n",
      "264/388, train_loss: 0.2744, step time: 0.4890\n",
      "265/388, train_loss: 0.5002, step time: 0.4911\n",
      "266/388, train_loss: 0.2901, step time: 0.4917\n",
      "267/388, train_loss: 0.2905, step time: 1.1765\n",
      "268/388, train_loss: 0.2879, step time: 0.5213\n",
      "269/388, train_loss: 0.4851, step time: 0.5031\n",
      "270/388, train_loss: 0.1952, step time: 0.4993\n",
      "271/388, train_loss: 0.2601, step time: 0.4901\n",
      "272/388, train_loss: 0.1739, step time: 0.4981\n",
      "273/388, train_loss: 0.4081, step time: 0.5071\n",
      "274/388, train_loss: 0.2628, step time: 0.5067\n",
      "275/388, train_loss: 0.3502, step time: 0.4901\n",
      "276/388, train_loss: 0.1977, step time: 0.5056\n",
      "277/388, train_loss: 0.2498, step time: 0.4885\n",
      "278/388, train_loss: 0.3161, step time: 0.4764\n",
      "279/388, train_loss: 0.3615, step time: 0.4857\n",
      "280/388, train_loss: 0.2634, step time: 0.4988\n",
      "281/388, train_loss: 0.4076, step time: 0.5558\n",
      "282/388, train_loss: 0.3019, step time: 0.5172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/388, train_loss: 0.2590, step time: 0.4979\n",
      "284/388, train_loss: 0.4256, step time: 0.4957\n",
      "285/388, train_loss: 0.3825, step time: 1.1477\n",
      "286/388, train_loss: 0.2914, step time: 0.5425\n",
      "287/388, train_loss: 0.2780, step time: 0.5104\n",
      "288/388, train_loss: 0.1957, step time: 0.4980\n",
      "289/388, train_loss: 0.3896, step time: 0.4901\n",
      "290/388, train_loss: 0.5968, step time: 0.4897\n",
      "291/388, train_loss: 0.7216, step time: 0.4906\n",
      "292/388, train_loss: 0.5163, step time: 1.1494\n",
      "293/388, train_loss: 0.2678, step time: 0.5292\n",
      "294/388, train_loss: 0.6283, step time: 0.5147\n",
      "295/388, train_loss: 0.2071, step time: 0.4912\n",
      "296/388, train_loss: 0.4704, step time: 0.4850\n",
      "297/388, train_loss: 0.6420, step time: 0.4893\n",
      "298/388, train_loss: 0.1747, step time: 0.7303\n",
      "299/388, train_loss: 0.2859, step time: 0.5507\n",
      "300/388, train_loss: 0.4435, step time: 0.5093\n",
      "301/388, train_loss: 0.3913, step time: 0.4966\n",
      "302/388, train_loss: 0.4910, step time: 0.4888\n",
      "303/388, train_loss: 0.4841, step time: 0.4857\n",
      "304/388, train_loss: 0.3588, step time: 0.4925\n",
      "305/388, train_loss: 0.3671, step time: 0.4829\n",
      "306/388, train_loss: 0.2888, step time: 1.1361\n",
      "307/388, train_loss: 0.2687, step time: 0.5381\n",
      "308/388, train_loss: 0.2799, step time: 0.4974\n",
      "309/388, train_loss: 0.1982, step time: 0.4977\n",
      "310/388, train_loss: 0.4878, step time: 0.4830\n",
      "311/388, train_loss: 0.3864, step time: 0.4746\n",
      "312/388, train_loss: 0.2087, step time: 0.4801\n",
      "313/388, train_loss: 0.1391, step time: 0.4763\n",
      "314/388, train_loss: 0.3839, step time: 0.4882\n",
      "315/388, train_loss: 0.4593, step time: 0.4941\n",
      "316/388, train_loss: 0.2911, step time: 0.5085\n",
      "317/388, train_loss: 0.5367, step time: 0.4997\n",
      "318/388, train_loss: 0.2376, step time: 0.4881\n",
      "319/388, train_loss: 0.3581, step time: 0.5208\n",
      "320/388, train_loss: 0.5209, step time: 0.4940\n",
      "321/388, train_loss: 0.3084, step time: 0.4978\n",
      "322/388, train_loss: 0.4902, step time: 0.5643\n",
      "323/388, train_loss: 0.4150, step time: 0.6121\n",
      "324/388, train_loss: 0.1961, step time: 0.5611\n",
      "325/388, train_loss: 0.7895, step time: 0.5235\n",
      "326/388, train_loss: 0.1729, step time: 0.5067\n",
      "327/388, train_loss: 0.3139, step time: 0.4933\n",
      "328/388, train_loss: 0.3725, step time: 0.5740\n",
      "329/388, train_loss: 0.2412, step time: 0.5495\n",
      "330/388, train_loss: 0.3454, step time: 0.5203\n",
      "331/388, train_loss: 0.3805, step time: 0.4965\n",
      "332/388, train_loss: 0.4175, step time: 0.4888\n",
      "333/388, train_loss: 0.1610, step time: 1.1152\n",
      "334/388, train_loss: 0.2396, step time: 0.5284\n",
      "335/388, train_loss: 0.6075, step time: 0.5060\n",
      "336/388, train_loss: 0.3120, step time: 0.4928\n",
      "337/388, train_loss: 0.2631, step time: 0.4854\n",
      "338/388, train_loss: 0.3663, step time: 0.5178\n",
      "339/388, train_loss: 0.2910, step time: 0.5056\n",
      "340/388, train_loss: 0.1553, step time: 0.5091\n",
      "341/388, train_loss: 0.3398, step time: 0.5082\n",
      "342/388, train_loss: 0.4928, step time: 0.5429\n",
      "343/388, train_loss: 0.2597, step time: 0.6579\n",
      "344/388, train_loss: 0.3522, step time: 0.5464\n",
      "345/388, train_loss: 0.2875, step time: 0.5192\n",
      "346/388, train_loss: 0.3207, step time: 0.4891\n",
      "347/388, train_loss: 0.3438, step time: 0.4933\n",
      "348/388, train_loss: 0.3385, step time: 0.4782\n",
      "349/388, train_loss: 0.4287, step time: 0.6857\n",
      "350/388, train_loss: 0.3477, step time: 0.5375\n",
      "351/388, train_loss: 0.2692, step time: 0.5137\n",
      "352/388, train_loss: 0.5263, step time: 0.5088\n",
      "353/388, train_loss: 0.3104, step time: 0.4954\n",
      "354/388, train_loss: 0.3176, step time: 0.4831\n",
      "355/388, train_loss: 0.3000, step time: 0.4771\n",
      "356/388, train_loss: 0.2613, step time: 0.8548\n",
      "357/388, train_loss: 0.1315, step time: 0.5462\n",
      "358/388, train_loss: 0.4791, step time: 0.5317\n",
      "359/388, train_loss: 0.3823, step time: 0.6012\n",
      "360/388, train_loss: 0.3227, step time: 0.5301\n",
      "361/388, train_loss: 0.6094, step time: 0.5042\n",
      "362/388, train_loss: 0.7743, step time: 0.4995\n",
      "363/388, train_loss: 0.4051, step time: 0.4806\n",
      "364/388, train_loss: 0.2637, step time: 0.4922\n",
      "365/388, train_loss: 0.3107, step time: 0.5362\n",
      "366/388, train_loss: 0.2866, step time: 0.5174\n",
      "367/388, train_loss: 0.2111, step time: 0.4963\n",
      "368/388, train_loss: 0.2862, step time: 0.4999\n",
      "369/388, train_loss: 0.1436, step time: 0.4804\n",
      "370/388, train_loss: 0.6331, step time: 0.4857\n",
      "371/388, train_loss: 0.3400, step time: 0.4973\n",
      "372/388, train_loss: 0.4429, step time: 1.0117\n",
      "373/388, train_loss: 0.3177, step time: 0.5547\n",
      "374/388, train_loss: 0.3672, step time: 0.5239\n",
      "375/388, train_loss: 0.2814, step time: 0.4933\n",
      "376/388, train_loss: 0.1779, step time: 0.4772\n",
      "377/388, train_loss: 0.6760, step time: 0.5223\n",
      "378/388, train_loss: 0.2460, step time: 0.5037\n",
      "379/388, train_loss: 0.2127, step time: 0.4995\n",
      "380/388, train_loss: 0.6796, step time: 0.4927\n",
      "381/388, train_loss: 0.2906, step time: 0.5310\n",
      "382/388, train_loss: 0.2559, step time: 0.5776\n",
      "383/388, train_loss: 0.5820, step time: 0.5326\n",
      "384/388, train_loss: 0.4569, step time: 0.5102\n",
      "385/388, train_loss: 0.6292, step time: 0.5022\n",
      "386/388, train_loss: 0.4167, step time: 0.4997\n",
      "387/388, train_loss: 0.1799, step time: 0.5173\n",
      "388/388, train_loss: 0.5872, step time: 0.4841\n",
      "epoch 7 average loss: 0.3778\n",
      "saved new best metric model\n",
      "current epoch: 7 current mean dice: 0.6471 tc: 0.6765 wt: 0.8466 et: 0.4181\n",
      "best mean dice: 0.6471 at epoch: 7\n",
      "time consuming of epoch 7 is: 302.2756\n",
      "----------\n",
      "epoch 8/300\n",
      "1/388, train_loss: 0.2851, step time: 0.4673\n",
      "2/388, train_loss: 0.6531, step time: 0.4790\n",
      "3/388, train_loss: 0.2569, step time: 1.1697\n",
      "4/388, train_loss: 0.4197, step time: 0.5561\n",
      "5/388, train_loss: 0.2427, step time: 0.5116\n",
      "6/388, train_loss: 0.4723, step time: 0.5086\n",
      "7/388, train_loss: 0.2670, step time: 0.4888\n",
      "8/388, train_loss: 0.3137, step time: 0.4927\n",
      "9/388, train_loss: 0.3174, step time: 0.6507\n",
      "10/388, train_loss: 0.1416, step time: 0.5674\n",
      "11/388, train_loss: 0.3658, step time: 0.5499\n",
      "12/388, train_loss: 0.2391, step time: 0.5199\n",
      "13/388, train_loss: 0.3426, step time: 0.5366\n",
      "14/388, train_loss: 0.2104, step time: 0.6452\n",
      "15/388, train_loss: 0.2437, step time: 0.5556\n",
      "16/388, train_loss: 0.2436, step time: 0.5417\n",
      "17/388, train_loss: 0.2811, step time: 0.5027\n",
      "18/388, train_loss: 0.2309, step time: 0.5170\n",
      "19/388, train_loss: 0.2446, step time: 0.5520\n",
      "20/388, train_loss: 0.3691, step time: 0.5133\n",
      "21/388, train_loss: 0.5644, step time: 0.5005\n",
      "22/388, train_loss: 0.2569, step time: 1.1610\n",
      "23/388, train_loss: 0.3185, step time: 0.5362\n",
      "24/388, train_loss: 0.3600, step time: 0.5116\n",
      "25/388, train_loss: 0.3666, step time: 0.4862\n",
      "26/388, train_loss: 0.3121, step time: 0.4835\n",
      "27/388, train_loss: 0.2693, step time: 0.5071\n",
      "28/388, train_loss: 0.2063, step time: 0.4980\n",
      "29/388, train_loss: 0.2863, step time: 0.4991\n",
      "30/388, train_loss: 0.2261, step time: 0.4806\n",
      "31/388, train_loss: 0.2073, step time: 0.4956\n",
      "32/388, train_loss: 0.7905, step time: 0.4800\n",
      "33/388, train_loss: 0.4117, step time: 1.1005\n",
      "34/388, train_loss: 0.6025, step time: 0.5270\n",
      "35/388, train_loss: 0.5378, step time: 0.5060\n",
      "36/388, train_loss: 0.3471, step time: 0.4949\n",
      "37/388, train_loss: 0.1271, step time: 0.5238\n",
      "38/388, train_loss: 0.2653, step time: 0.5058\n",
      "39/388, train_loss: 0.3973, step time: 0.4871\n",
      "40/388, train_loss: 0.2971, step time: 0.4850\n",
      "41/388, train_loss: 0.2572, step time: 0.5081\n",
      "42/388, train_loss: 0.1676, step time: 0.4888\n",
      "43/388, train_loss: 0.2685, step time: 0.5271\n",
      "44/388, train_loss: 0.3887, step time: 0.5268\n",
      "45/388, train_loss: 0.3404, step time: 0.5237\n",
      "46/388, train_loss: 0.4961, step time: 0.5186\n",
      "47/388, train_loss: 0.1694, step time: 0.5245\n",
      "48/388, train_loss: 0.3562, step time: 0.5038\n",
      "49/388, train_loss: 0.3106, step time: 0.5511\n",
      "50/388, train_loss: 0.3712, step time: 0.5207\n",
      "51/388, train_loss: 0.2420, step time: 0.5073\n",
      "52/388, train_loss: 0.6785, step time: 0.4882\n",
      "53/388, train_loss: 0.7190, step time: 0.5346\n",
      "54/388, train_loss: 0.3859, step time: 0.6218\n",
      "55/388, train_loss: 0.2440, step time: 0.5426\n",
      "56/388, train_loss: 0.2613, step time: 0.5161\n",
      "57/388, train_loss: 0.3202, step time: 0.5029\n",
      "58/388, train_loss: 0.3281, step time: 0.5140\n",
      "59/388, train_loss: 0.3339, step time: 0.5354\n",
      "60/388, train_loss: 0.2122, step time: 0.5141\n",
      "61/388, train_loss: 0.3319, step time: 0.4932\n",
      "62/388, train_loss: 0.1949, step time: 0.5409\n",
      "63/388, train_loss: 0.3576, step time: 0.5211\n",
      "64/388, train_loss: 0.3132, step time: 0.5060\n",
      "65/388, train_loss: 0.2489, step time: 0.4889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/388, train_loss: 0.1311, step time: 0.4949\n",
      "67/388, train_loss: 0.2379, step time: 0.4813\n",
      "68/388, train_loss: 0.3724, step time: 0.4925\n",
      "69/388, train_loss: 0.3018, step time: 1.1691\n",
      "70/388, train_loss: 0.2974, step time: 0.5477\n",
      "71/388, train_loss: 0.2769, step time: 0.5190\n",
      "72/388, train_loss: 0.2309, step time: 0.4966\n",
      "73/388, train_loss: 0.2562, step time: 0.4979\n",
      "74/388, train_loss: 0.1964, step time: 0.4821\n",
      "75/388, train_loss: 0.3202, step time: 0.4907\n",
      "76/388, train_loss: 0.3837, step time: 0.4794\n",
      "77/388, train_loss: 0.6889, step time: 1.1236\n",
      "78/388, train_loss: 0.2640, step time: 0.5254\n",
      "79/388, train_loss: 0.5904, step time: 0.5070\n",
      "80/388, train_loss: 0.2701, step time: 0.4913\n",
      "81/388, train_loss: 0.2960, step time: 0.4939\n",
      "82/388, train_loss: 0.2769, step time: 0.4788\n",
      "83/388, train_loss: 0.5853, step time: 0.4854\n",
      "84/388, train_loss: 0.6653, step time: 0.5074\n",
      "85/388, train_loss: 0.3598, step time: 0.4879\n",
      "86/388, train_loss: 0.3424, step time: 0.4923\n",
      "87/388, train_loss: 0.2942, step time: 0.4908\n",
      "88/388, train_loss: 0.2566, step time: 0.4843\n",
      "89/388, train_loss: 0.3060, step time: 0.5001\n",
      "90/388, train_loss: 0.2533, step time: 0.4908\n",
      "91/388, train_loss: 0.3271, step time: 0.9284\n",
      "92/388, train_loss: 0.4104, step time: 0.5485\n",
      "93/388, train_loss: 0.6445, step time: 0.5060\n",
      "94/388, train_loss: 0.4531, step time: 0.4966\n",
      "95/388, train_loss: 0.2925, step time: 0.4955\n",
      "96/388, train_loss: 0.4687, step time: 0.4883\n",
      "97/388, train_loss: 0.6987, step time: 0.4824\n",
      "98/388, train_loss: 0.2781, step time: 1.1289\n",
      "99/388, train_loss: 0.4184, step time: 0.5317\n",
      "100/388, train_loss: 0.5486, step time: 0.5006\n",
      "101/388, train_loss: 0.3631, step time: 0.4897\n",
      "102/388, train_loss: 0.3339, step time: 0.4913\n",
      "103/388, train_loss: 0.5973, step time: 0.4792\n",
      "104/388, train_loss: 0.3060, step time: 0.4816\n",
      "105/388, train_loss: 0.4797, step time: 0.4878\n",
      "106/388, train_loss: 0.2733, step time: 0.4850\n",
      "107/388, train_loss: 0.3339, step time: 0.4766\n",
      "108/388, train_loss: 0.6510, step time: 0.4894\n",
      "109/388, train_loss: 0.2680, step time: 0.5066\n",
      "110/388, train_loss: 0.1274, step time: 0.5163\n",
      "111/388, train_loss: 0.6413, step time: 0.5076\n",
      "112/388, train_loss: 0.3233, step time: 0.4918\n",
      "113/388, train_loss: 0.7838, step time: 0.5309\n",
      "114/388, train_loss: 0.3122, step time: 0.5293\n",
      "115/388, train_loss: 0.2979, step time: 0.5085\n",
      "116/388, train_loss: 0.3234, step time: 0.5128\n",
      "117/388, train_loss: 0.5371, step time: 0.5125\n",
      "118/388, train_loss: 0.2958, step time: 0.4971\n",
      "119/388, train_loss: 0.3358, step time: 0.8499\n",
      "120/388, train_loss: 0.4564, step time: 0.5709\n",
      "121/388, train_loss: 0.6039, step time: 0.5177\n",
      "122/388, train_loss: 0.3843, step time: 0.5029\n",
      "123/388, train_loss: 0.2657, step time: 0.4849\n",
      "124/388, train_loss: 0.2641, step time: 0.4837\n",
      "125/388, train_loss: 0.2849, step time: 0.4812\n",
      "126/388, train_loss: 0.2626, step time: 0.4895\n",
      "127/388, train_loss: 0.2950, step time: 0.4882\n",
      "128/388, train_loss: 0.3117, step time: 0.4918\n",
      "129/388, train_loss: 0.4967, step time: 0.4912\n",
      "130/388, train_loss: 0.3087, step time: 1.1162\n",
      "131/388, train_loss: 0.4045, step time: 0.5438\n",
      "132/388, train_loss: 0.6163, step time: 0.5041\n",
      "133/388, train_loss: 0.2520, step time: 0.4933\n",
      "134/388, train_loss: 0.4830, step time: 0.4978\n",
      "135/388, train_loss: 0.4763, step time: 0.4797\n",
      "136/388, train_loss: 0.2336, step time: 0.6087\n",
      "137/388, train_loss: 0.6039, step time: 0.5297\n",
      "138/388, train_loss: 0.2791, step time: 0.5024\n",
      "139/388, train_loss: 0.1882, step time: 0.4969\n",
      "140/388, train_loss: 0.7513, step time: 0.5006\n",
      "141/388, train_loss: 0.4768, step time: 0.5014\n",
      "142/388, train_loss: 0.5396, step time: 0.4888\n",
      "143/388, train_loss: 0.1936, step time: 0.5177\n",
      "144/388, train_loss: 0.7826, step time: 0.6172\n",
      "145/388, train_loss: 0.4093, step time: 0.5342\n",
      "146/388, train_loss: 0.2819, step time: 0.4987\n",
      "147/388, train_loss: 0.3653, step time: 0.4978\n",
      "148/388, train_loss: 0.3654, step time: 0.4959\n",
      "149/388, train_loss: 0.2861, step time: 0.4753\n",
      "150/388, train_loss: 0.3737, step time: 0.7299\n",
      "151/388, train_loss: 0.7390, step time: 0.5349\n",
      "152/388, train_loss: 0.3967, step time: 0.5146\n",
      "153/388, train_loss: 0.5094, step time: 0.5041\n",
      "154/388, train_loss: 0.4815, step time: 0.4856\n",
      "155/388, train_loss: 0.2709, step time: 0.4856\n",
      "156/388, train_loss: 0.3038, step time: 0.4753\n",
      "157/388, train_loss: 0.4223, step time: 0.4847\n",
      "158/388, train_loss: 0.5520, step time: 0.5337\n",
      "159/388, train_loss: 0.2440, step time: 0.4969\n",
      "160/388, train_loss: 0.3122, step time: 0.4919\n",
      "161/388, train_loss: 0.7906, step time: 0.4816\n",
      "162/388, train_loss: 0.1486, step time: 0.9197\n",
      "163/388, train_loss: 0.1869, step time: 0.5340\n",
      "164/388, train_loss: 0.3515, step time: 0.4952\n",
      "165/388, train_loss: 0.5060, step time: 0.4862\n",
      "166/388, train_loss: 0.2945, step time: 0.4912\n",
      "167/388, train_loss: 0.1179, step time: 0.4755\n",
      "168/388, train_loss: 0.1913, step time: 0.4781\n",
      "169/388, train_loss: 0.1562, step time: 0.4732\n",
      "170/388, train_loss: 0.5417, step time: 0.4813\n",
      "171/388, train_loss: 0.2004, step time: 0.4959\n",
      "172/388, train_loss: 0.5876, step time: 0.5011\n",
      "173/388, train_loss: 0.2042, step time: 0.4938\n",
      "174/388, train_loss: 0.2659, step time: 1.0666\n",
      "175/388, train_loss: 0.1340, step time: 0.5415\n",
      "176/388, train_loss: 0.5453, step time: 0.5132\n",
      "177/388, train_loss: 0.2737, step time: 0.4857\n",
      "178/388, train_loss: 0.3296, step time: 0.4941\n",
      "179/388, train_loss: 0.3416, step time: 0.4802\n",
      "180/388, train_loss: 0.2802, step time: 0.4872\n",
      "181/388, train_loss: 0.7820, step time: 0.4951\n",
      "182/388, train_loss: 0.4951, step time: 0.4822\n",
      "183/388, train_loss: 0.2407, step time: 0.5060\n",
      "184/388, train_loss: 0.2111, step time: 0.4907\n",
      "185/388, train_loss: 0.4030, step time: 0.4926\n",
      "186/388, train_loss: 0.2063, step time: 0.4895\n",
      "187/388, train_loss: 0.2876, step time: 0.4730\n",
      "188/388, train_loss: 0.2673, step time: 1.0463\n",
      "189/388, train_loss: 0.1681, step time: 0.5370\n",
      "190/388, train_loss: 0.1923, step time: 0.5030\n",
      "191/388, train_loss: 0.5209, step time: 0.4830\n",
      "192/388, train_loss: 0.1136, step time: 0.4795\n",
      "193/388, train_loss: 0.2666, step time: 0.4832\n",
      "194/388, train_loss: 0.3641, step time: 0.4729\n",
      "195/388, train_loss: 0.2273, step time: 0.5063\n",
      "196/388, train_loss: 0.1303, step time: 0.4999\n",
      "197/388, train_loss: 0.2204, step time: 0.5127\n",
      "198/388, train_loss: 0.4579, step time: 0.5084\n",
      "199/388, train_loss: 0.3186, step time: 0.4948\n",
      "200/388, train_loss: 0.6587, step time: 0.5124\n",
      "201/388, train_loss: 0.4240, step time: 0.5172\n",
      "202/388, train_loss: 0.3540, step time: 0.5489\n",
      "203/388, train_loss: 0.1828, step time: 0.5164\n",
      "204/388, train_loss: 0.2205, step time: 0.5024\n",
      "205/388, train_loss: 0.3568, step time: 0.4896\n",
      "206/388, train_loss: 0.4289, step time: 0.4919\n",
      "207/388, train_loss: 0.2172, step time: 1.1309\n",
      "208/388, train_loss: 0.2420, step time: 0.5286\n",
      "209/388, train_loss: 0.4345, step time: 0.5050\n",
      "210/388, train_loss: 0.3276, step time: 0.4976\n",
      "211/388, train_loss: 0.4859, step time: 0.4976\n",
      "212/388, train_loss: 0.4030, step time: 0.4850\n",
      "213/388, train_loss: 0.3343, step time: 0.4894\n",
      "214/388, train_loss: 0.2729, step time: 0.4771\n",
      "215/388, train_loss: 0.1999, step time: 0.4849\n",
      "216/388, train_loss: 0.3391, step time: 0.6181\n",
      "217/388, train_loss: 0.3692, step time: 0.5457\n",
      "218/388, train_loss: 0.4213, step time: 0.5190\n",
      "219/388, train_loss: 0.3563, step time: 0.4998\n",
      "220/388, train_loss: 0.3563, step time: 0.4966\n",
      "221/388, train_loss: 0.2963, step time: 0.5050\n",
      "222/388, train_loss: 0.4162, step time: 0.4971\n",
      "223/388, train_loss: 0.1542, step time: 0.4836\n",
      "224/388, train_loss: 0.2216, step time: 0.4803\n",
      "225/388, train_loss: 0.5116, step time: 0.4883\n",
      "226/388, train_loss: 0.4365, step time: 0.5000\n",
      "227/388, train_loss: 0.3619, step time: 0.4831\n",
      "228/388, train_loss: 0.2735, step time: 0.5151\n",
      "229/388, train_loss: 0.1353, step time: 0.5464\n",
      "230/388, train_loss: 0.4858, step time: 0.5263\n",
      "231/388, train_loss: 0.1916, step time: 0.5081\n",
      "232/388, train_loss: 0.3295, step time: 0.4911\n",
      "233/388, train_loss: 0.2495, step time: 0.4938\n",
      "234/388, train_loss: 0.1180, step time: 0.4899\n",
      "235/388, train_loss: 0.2584, step time: 0.4796\n",
      "236/388, train_loss: 0.2682, step time: 0.6262\n",
      "237/388, train_loss: 0.2401, step time: 0.5504\n",
      "238/388, train_loss: 0.3472, step time: 0.5110\n",
      "239/388, train_loss: 0.2439, step time: 0.5000\n",
      "240/388, train_loss: 0.3308, step time: 0.5284\n",
      "241/388, train_loss: 0.2457, step time: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/388, train_loss: 0.2736, step time: 0.5080\n",
      "243/388, train_loss: 0.2139, step time: 0.4994\n",
      "244/388, train_loss: 0.2040, step time: 0.4870\n",
      "245/388, train_loss: 0.2821, step time: 0.5238\n",
      "246/388, train_loss: 0.2163, step time: 0.5099\n",
      "247/388, train_loss: 0.2080, step time: 0.5045\n",
      "248/388, train_loss: 0.2223, step time: 0.4923\n",
      "249/388, train_loss: 0.2094, step time: 0.5158\n",
      "250/388, train_loss: 0.6888, step time: 0.4968\n",
      "251/388, train_loss: 0.1700, step time: 0.4972\n",
      "252/388, train_loss: 0.3538, step time: 0.4897\n",
      "253/388, train_loss: 0.1642, step time: 1.1379\n",
      "254/388, train_loss: 0.2077, step time: 0.5413\n",
      "255/388, train_loss: 0.3870, step time: 0.5148\n",
      "256/388, train_loss: 0.1808, step time: 0.4925\n",
      "257/388, train_loss: 0.1503, step time: 0.6244\n",
      "258/388, train_loss: 0.2899, step time: 0.5493\n",
      "259/388, train_loss: 0.1692, step time: 0.4977\n",
      "260/388, train_loss: 0.5753, step time: 0.4892\n",
      "261/388, train_loss: 0.4616, step time: 0.4906\n",
      "262/388, train_loss: 0.5027, step time: 0.4816\n",
      "263/388, train_loss: 0.3695, step time: 0.4791\n",
      "264/388, train_loss: 0.3388, step time: 0.4881\n",
      "265/388, train_loss: 0.3146, step time: 0.6959\n",
      "266/388, train_loss: 0.3487, step time: 0.5393\n",
      "267/388, train_loss: 0.2992, step time: 0.5034\n",
      "268/388, train_loss: 0.3365, step time: 0.4891\n",
      "269/388, train_loss: 0.4120, step time: 0.4870\n",
      "270/388, train_loss: 0.1741, step time: 0.4986\n",
      "271/388, train_loss: 0.2840, step time: 0.4825\n",
      "272/388, train_loss: 0.2233, step time: 0.4807\n",
      "273/388, train_loss: 0.5224, step time: 0.4875\n",
      "274/388, train_loss: 0.2165, step time: 0.4966\n",
      "275/388, train_loss: 0.1873, step time: 0.4822\n",
      "276/388, train_loss: 0.2117, step time: 1.1440\n",
      "277/388, train_loss: 0.2559, step time: 0.5373\n",
      "278/388, train_loss: 0.4581, step time: 0.5047\n",
      "279/388, train_loss: 0.2048, step time: 0.4929\n",
      "280/388, train_loss: 0.5954, step time: 0.4832\n",
      "281/388, train_loss: 0.3624, step time: 0.4781\n",
      "282/388, train_loss: 0.5577, step time: 0.4811\n",
      "283/388, train_loss: 0.3263, step time: 0.4859\n",
      "284/388, train_loss: 0.3886, step time: 0.4964\n",
      "285/388, train_loss: 0.3123, step time: 0.4795\n",
      "286/388, train_loss: 0.4809, step time: 0.4908\n",
      "287/388, train_loss: 0.4590, step time: 0.4968\n",
      "288/388, train_loss: 0.2400, step time: 0.5190\n",
      "289/388, train_loss: 0.2124, step time: 0.4999\n",
      "290/388, train_loss: 0.1614, step time: 0.4946\n",
      "291/388, train_loss: 0.2753, step time: 0.4907\n",
      "292/388, train_loss: 0.2931, step time: 0.5002\n",
      "293/388, train_loss: 0.3734, step time: 0.4917\n",
      "294/388, train_loss: 0.4430, step time: 0.5508\n",
      "295/388, train_loss: 0.2487, step time: 0.5260\n",
      "296/388, train_loss: 0.2530, step time: 0.4959\n",
      "297/388, train_loss: 0.2574, step time: 0.4969\n",
      "298/388, train_loss: 0.2449, step time: 0.5132\n",
      "299/388, train_loss: 0.2109, step time: 0.5076\n",
      "300/388, train_loss: 0.3283, step time: 0.4991\n",
      "301/388, train_loss: 0.3030, step time: 0.4817\n",
      "302/388, train_loss: 0.3176, step time: 0.4916\n",
      "303/388, train_loss: 0.7730, step time: 0.6502\n",
      "304/388, train_loss: 0.2942, step time: 0.5763\n",
      "305/388, train_loss: 0.4331, step time: 0.5359\n",
      "306/388, train_loss: 0.8074, step time: 0.5061\n",
      "307/388, train_loss: 0.0849, step time: 0.4890\n",
      "308/388, train_loss: 0.3033, step time: 0.4959\n",
      "309/388, train_loss: 0.3705, step time: 1.1578\n",
      "310/388, train_loss: 0.3588, step time: 0.5268\n",
      "311/388, train_loss: 0.4082, step time: 0.5075\n",
      "312/388, train_loss: 0.3250, step time: 0.4860\n",
      "313/388, train_loss: 0.1827, step time: 0.4962\n",
      "314/388, train_loss: 0.7452, step time: 0.4811\n",
      "315/388, train_loss: 0.3610, step time: 0.4854\n",
      "316/388, train_loss: 0.3981, step time: 0.4975\n",
      "317/388, train_loss: 0.3605, step time: 0.4929\n",
      "318/388, train_loss: 0.2797, step time: 0.4991\n",
      "319/388, train_loss: 0.2571, step time: 0.4835\n",
      "320/388, train_loss: 0.2662, step time: 1.1571\n",
      "321/388, train_loss: 0.4614, step time: 0.5348\n",
      "322/388, train_loss: 0.2959, step time: 0.5020\n",
      "323/388, train_loss: 0.4854, step time: 0.4859\n",
      "324/388, train_loss: 0.3604, step time: 0.4849\n",
      "325/388, train_loss: 0.2168, step time: 0.4947\n",
      "326/388, train_loss: 0.2025, step time: 0.5002\n",
      "327/388, train_loss: 0.4599, step time: 0.4979\n",
      "328/388, train_loss: 0.5851, step time: 0.4844\n",
      "329/388, train_loss: 0.2147, step time: 0.8437\n",
      "330/388, train_loss: 0.1651, step time: 0.5366\n",
      "331/388, train_loss: 0.4216, step time: 0.5166\n",
      "332/388, train_loss: 0.2544, step time: 0.4889\n",
      "333/388, train_loss: 0.2186, step time: 0.4924\n",
      "334/388, train_loss: 0.2837, step time: 0.4934\n",
      "335/388, train_loss: 0.7841, step time: 0.4771\n",
      "336/388, train_loss: 0.2222, step time: 0.5027\n",
      "337/388, train_loss: 0.2446, step time: 0.4846\n",
      "338/388, train_loss: 0.2139, step time: 0.5041\n",
      "339/388, train_loss: 0.1107, step time: 0.5051\n",
      "340/388, train_loss: 0.1840, step time: 0.5022\n",
      "341/388, train_loss: 0.4226, step time: 0.4893\n",
      "342/388, train_loss: 0.2976, step time: 0.5269\n",
      "343/388, train_loss: 0.4009, step time: 0.5041\n",
      "344/388, train_loss: 0.2176, step time: 0.5023\n",
      "345/388, train_loss: 0.2657, step time: 0.4847\n",
      "346/388, train_loss: 0.3733, step time: 0.4785\n",
      "347/388, train_loss: 0.1512, step time: 1.1115\n",
      "348/388, train_loss: 0.2032, step time: 0.5128\n",
      "349/388, train_loss: 0.2862, step time: 0.4901\n",
      "350/388, train_loss: 0.2607, step time: 0.5133\n",
      "351/388, train_loss: 0.2998, step time: 0.5024\n",
      "352/388, train_loss: 0.1380, step time: 0.4927\n",
      "353/388, train_loss: 0.6366, step time: 0.4920\n",
      "354/388, train_loss: 0.2871, step time: 0.4924\n",
      "355/388, train_loss: 0.2837, step time: 0.4970\n",
      "356/388, train_loss: 0.2839, step time: 0.4840\n",
      "357/388, train_loss: 0.5125, step time: 0.4847\n",
      "358/388, train_loss: 0.2013, step time: 0.4749\n",
      "359/388, train_loss: 0.2588, step time: 0.4748\n",
      "360/388, train_loss: 0.3169, step time: 0.4814\n",
      "361/388, train_loss: 0.6918, step time: 1.0539\n",
      "362/388, train_loss: 0.3507, step time: 0.5427\n",
      "363/388, train_loss: 0.2792, step time: 0.5152\n",
      "364/388, train_loss: 0.2971, step time: 0.4893\n",
      "365/388, train_loss: 0.3672, step time: 0.4957\n",
      "366/388, train_loss: 0.5177, step time: 0.4830\n",
      "367/388, train_loss: 0.6056, step time: 0.4878\n",
      "368/388, train_loss: 0.4858, step time: 0.4798\n",
      "369/388, train_loss: 0.2369, step time: 0.4954\n",
      "370/388, train_loss: 0.3033, step time: 0.4910\n",
      "371/388, train_loss: 0.2967, step time: 0.4856\n",
      "372/388, train_loss: 0.3071, step time: 0.5400\n",
      "373/388, train_loss: 0.2130, step time: 0.5150\n",
      "374/388, train_loss: 0.2441, step time: 0.4923\n",
      "375/388, train_loss: 0.2372, step time: 0.8488\n",
      "376/388, train_loss: 0.2241, step time: 0.5628\n",
      "377/388, train_loss: 0.2934, step time: 0.5213\n",
      "378/388, train_loss: 0.6796, step time: 0.5375\n",
      "379/388, train_loss: 0.1213, step time: 0.5149\n",
      "380/388, train_loss: 0.1259, step time: 0.5101\n",
      "381/388, train_loss: 0.4986, step time: 0.4914\n",
      "382/388, train_loss: 0.3781, step time: 0.4939\n",
      "383/388, train_loss: 0.3424, step time: 0.4918\n",
      "384/388, train_loss: 0.2971, step time: 0.4861\n",
      "385/388, train_loss: 0.6856, step time: 0.4935\n",
      "386/388, train_loss: 0.3253, step time: 0.4832\n",
      "387/388, train_loss: 0.4337, step time: 0.4937\n",
      "388/388, train_loss: 0.1933, step time: 1.2254\n",
      "epoch 8 average loss: 0.3427\n",
      "saved new best metric model\n",
      "current epoch: 8 current mean dice: 0.6708 tc: 0.7136 wt: 0.8622 et: 0.4367\n",
      "best mean dice: 0.6708 at epoch: 8\n",
      "time consuming of epoch 8 is: 298.7481\n",
      "----------\n",
      "epoch 9/300\n",
      "1/388, train_loss: 0.3089, step time: 0.4827\n",
      "2/388, train_loss: 0.2919, step time: 0.4832\n",
      "3/388, train_loss: 0.6337, step time: 0.4776\n",
      "4/388, train_loss: 0.2088, step time: 0.9102\n",
      "5/388, train_loss: 0.1411, step time: 0.5565\n",
      "6/388, train_loss: 0.3559, step time: 0.5235\n",
      "7/388, train_loss: 0.2014, step time: 0.5023\n",
      "8/388, train_loss: 0.4603, step time: 0.4895\n",
      "9/388, train_loss: 0.2674, step time: 0.5121\n",
      "10/388, train_loss: 0.3604, step time: 0.5248\n",
      "11/388, train_loss: 0.3163, step time: 0.5514\n",
      "12/388, train_loss: 0.2106, step time: 0.5403\n",
      "13/388, train_loss: 0.1704, step time: 0.5298\n",
      "14/388, train_loss: 0.5208, step time: 0.7577\n",
      "15/388, train_loss: 0.4723, step time: 0.6306\n",
      "16/388, train_loss: 0.2979, step time: 0.5772\n",
      "17/388, train_loss: 0.3373, step time: 0.5326\n",
      "18/388, train_loss: 0.3684, step time: 0.5088\n",
      "19/388, train_loss: 0.4365, step time: 0.4965\n",
      "20/388, train_loss: 0.2228, step time: 0.7850\n",
      "21/388, train_loss: 0.3093, step time: 0.5331\n",
      "22/388, train_loss: 0.1346, step time: 0.5289\n",
      "23/388, train_loss: 0.2633, step time: 0.6690\n",
      "24/388, train_loss: 0.3975, step time: 0.5554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/388, train_loss: 0.3046, step time: 0.5217\n",
      "26/388, train_loss: 0.1622, step time: 0.4986\n",
      "27/388, train_loss: 0.2298, step time: 0.9913\n",
      "28/388, train_loss: 0.2769, step time: 0.5517\n",
      "29/388, train_loss: 0.4890, step time: 0.5318\n",
      "30/388, train_loss: 0.2538, step time: 0.4996\n",
      "31/388, train_loss: 0.6151, step time: 0.4993\n",
      "32/388, train_loss: 0.4937, step time: 0.4832\n",
      "33/388, train_loss: 0.2184, step time: 0.5015\n",
      "34/388, train_loss: 0.3508, step time: 0.5402\n",
      "35/388, train_loss: 0.1508, step time: 0.5827\n",
      "36/388, train_loss: 0.4999, step time: 0.5287\n",
      "37/388, train_loss: 0.2835, step time: 0.5028\n",
      "38/388, train_loss: 0.6799, step time: 0.5685\n",
      "39/388, train_loss: 0.2521, step time: 0.6227\n",
      "40/388, train_loss: 0.3670, step time: 0.5394\n",
      "41/388, train_loss: 0.5204, step time: 0.5081\n",
      "42/388, train_loss: 0.3247, step time: 0.5768\n",
      "43/388, train_loss: 0.2455, step time: 0.5759\n",
      "44/388, train_loss: 0.3054, step time: 0.5319\n",
      "45/388, train_loss: 0.2129, step time: 0.5117\n",
      "46/388, train_loss: 0.4656, step time: 0.5251\n",
      "47/388, train_loss: 0.0915, step time: 0.5335\n",
      "48/388, train_loss: 0.2398, step time: 0.6079\n",
      "49/388, train_loss: 0.4440, step time: 0.5671\n",
      "50/388, train_loss: 0.1966, step time: 0.5282\n",
      "51/388, train_loss: 0.1296, step time: 0.5114\n",
      "52/388, train_loss: 0.6442, step time: 0.4919\n",
      "53/388, train_loss: 0.5214, step time: 0.4840\n",
      "54/388, train_loss: 0.7505, step time: 0.4916\n",
      "55/388, train_loss: 0.3452, step time: 0.4919\n",
      "56/388, train_loss: 0.7307, step time: 0.9231\n",
      "57/388, train_loss: 0.4136, step time: 0.5346\n",
      "58/388, train_loss: 0.2530, step time: 0.5001\n",
      "59/388, train_loss: 0.4839, step time: 0.4925\n",
      "60/388, train_loss: 0.4738, step time: 0.4889\n",
      "61/388, train_loss: 0.1533, step time: 0.6259\n",
      "62/388, train_loss: 0.2366, step time: 0.5496\n",
      "63/388, train_loss: 0.2018, step time: 0.5185\n",
      "64/388, train_loss: 0.1785, step time: 0.5011\n",
      "65/388, train_loss: 0.1274, step time: 0.5076\n",
      "66/388, train_loss: 0.4767, step time: 0.4928\n",
      "67/388, train_loss: 0.1796, step time: 0.5155\n",
      "68/388, train_loss: 0.5496, step time: 0.5051\n",
      "69/388, train_loss: 0.2679, step time: 0.5147\n",
      "70/388, train_loss: 0.2808, step time: 0.4971\n",
      "71/388, train_loss: 0.2628, step time: 0.4899\n",
      "72/388, train_loss: 0.2601, step time: 0.4926\n",
      "73/388, train_loss: 0.5357, step time: 0.4968\n",
      "74/388, train_loss: 0.3427, step time: 0.9847\n",
      "75/388, train_loss: 0.2188, step time: 0.5566\n",
      "76/388, train_loss: 0.2280, step time: 0.5219\n",
      "77/388, train_loss: 0.2666, step time: 0.5072\n",
      "78/388, train_loss: 0.2041, step time: 0.5017\n",
      "79/388, train_loss: 0.1859, step time: 0.5021\n",
      "80/388, train_loss: 0.1293, step time: 0.5174\n",
      "81/388, train_loss: 0.2972, step time: 0.7373\n",
      "82/388, train_loss: 0.2349, step time: 0.5685\n",
      "83/388, train_loss: 0.0929, step time: 0.5289\n",
      "84/388, train_loss: 0.1818, step time: 0.5011\n",
      "85/388, train_loss: 0.0886, step time: 0.4981\n",
      "86/388, train_loss: 0.5043, step time: 0.5426\n",
      "87/388, train_loss: 0.1759, step time: 0.7027\n",
      "88/388, train_loss: 0.2438, step time: 0.5513\n",
      "89/388, train_loss: 0.3881, step time: 0.5231\n",
      "90/388, train_loss: 0.1829, step time: 0.5367\n",
      "91/388, train_loss: 0.2800, step time: 0.5207\n",
      "92/388, train_loss: 0.3599, step time: 0.5181\n",
      "93/388, train_loss: 0.1690, step time: 0.5039\n",
      "94/388, train_loss: 0.2449, step time: 0.5438\n",
      "95/388, train_loss: 0.2545, step time: 0.5240\n",
      "96/388, train_loss: 0.6674, step time: 0.5538\n",
      "97/388, train_loss: 0.2969, step time: 0.5189\n",
      "98/388, train_loss: 0.4472, step time: 0.5094\n",
      "99/388, train_loss: 0.3813, step time: 0.4939\n",
      "100/388, train_loss: 0.1974, step time: 0.4932\n",
      "101/388, train_loss: 0.4375, step time: 1.1368\n",
      "102/388, train_loss: 0.3273, step time: 0.5389\n",
      "103/388, train_loss: 0.1898, step time: 0.5153\n",
      "104/388, train_loss: 0.1266, step time: 0.4999\n",
      "105/388, train_loss: 0.1987, step time: 0.5333\n",
      "106/388, train_loss: 0.2331, step time: 0.5112\n",
      "107/388, train_loss: 0.3866, step time: 0.4972\n",
      "108/388, train_loss: 0.1969, step time: 0.4932\n",
      "109/388, train_loss: 0.2428, step time: 0.4905\n",
      "110/388, train_loss: 0.7008, step time: 0.4804\n",
      "111/388, train_loss: 0.2521, step time: 0.8933\n",
      "112/388, train_loss: 0.1656, step time: 0.5486\n",
      "113/388, train_loss: 0.3278, step time: 0.5256\n",
      "114/388, train_loss: 0.2251, step time: 0.5231\n",
      "115/388, train_loss: 0.0808, step time: 0.5170\n",
      "116/388, train_loss: 0.2357, step time: 0.6017\n",
      "117/388, train_loss: 0.0980, step time: 0.5364\n",
      "118/388, train_loss: 0.1926, step time: 0.5105\n",
      "119/388, train_loss: 0.6752, step time: 0.5027\n",
      "120/388, train_loss: 0.0978, step time: 0.4918\n",
      "121/388, train_loss: 0.2732, step time: 0.4969\n",
      "122/388, train_loss: 0.1095, step time: 1.1631\n",
      "123/388, train_loss: 0.2491, step time: 0.5404\n",
      "124/388, train_loss: 0.1980, step time: 0.5078\n",
      "125/388, train_loss: 0.1317, step time: 0.5027\n",
      "126/388, train_loss: 0.2049, step time: 0.4898\n",
      "127/388, train_loss: 0.6515, step time: 0.4900\n",
      "128/388, train_loss: 0.2522, step time: 0.4842\n",
      "129/388, train_loss: 0.3162, step time: 0.4872\n",
      "130/388, train_loss: 0.1195, step time: 1.0534\n",
      "131/388, train_loss: 0.3050, step time: 0.5378\n",
      "132/388, train_loss: 0.2761, step time: 0.4951\n",
      "133/388, train_loss: 0.3168, step time: 0.4937\n",
      "134/388, train_loss: 0.2171, step time: 0.4862\n",
      "135/388, train_loss: 0.5363, step time: 0.4765\n",
      "136/388, train_loss: 0.3001, step time: 0.4911\n",
      "137/388, train_loss: 0.1991, step time: 0.4877\n",
      "138/388, train_loss: 0.3021, step time: 0.4926\n",
      "139/388, train_loss: 0.2586, step time: 0.4818\n",
      "140/388, train_loss: 0.2697, step time: 0.5320\n",
      "141/388, train_loss: 0.4512, step time: 0.4946\n",
      "142/388, train_loss: 0.2032, step time: 0.4906\n",
      "143/388, train_loss: 0.2549, step time: 0.4865\n",
      "144/388, train_loss: 0.1962, step time: 0.7305\n",
      "145/388, train_loss: 0.5220, step time: 0.5753\n",
      "146/388, train_loss: 0.3809, step time: 0.5282\n",
      "147/388, train_loss: 0.4370, step time: 0.5107\n",
      "148/388, train_loss: 0.3911, step time: 0.4888\n",
      "149/388, train_loss: 0.5535, step time: 0.4979\n",
      "150/388, train_loss: 0.2182, step time: 0.5423\n",
      "151/388, train_loss: 0.2882, step time: 0.5227\n",
      "152/388, train_loss: 0.1733, step time: 0.4938\n",
      "153/388, train_loss: 0.4183, step time: 0.4789\n",
      "154/388, train_loss: 0.2493, step time: 1.0321\n",
      "155/388, train_loss: 0.2797, step time: 0.5622\n",
      "156/388, train_loss: 0.2216, step time: 0.5193\n",
      "157/388, train_loss: 0.2767, step time: 0.4964\n",
      "158/388, train_loss: 0.2333, step time: 0.4951\n",
      "159/388, train_loss: 0.5408, step time: 0.4852\n",
      "160/388, train_loss: 0.3966, step time: 0.5006\n",
      "161/388, train_loss: 0.2972, step time: 0.5455\n",
      "162/388, train_loss: 0.2082, step time: 0.5100\n",
      "163/388, train_loss: 0.3780, step time: 0.5059\n",
      "164/388, train_loss: 0.1875, step time: 0.5248\n",
      "165/388, train_loss: 0.2833, step time: 0.5135\n",
      "166/388, train_loss: 0.1717, step time: 0.4918\n",
      "167/388, train_loss: 0.1411, step time: 0.5003\n",
      "168/388, train_loss: 0.1226, step time: 0.4941\n",
      "169/388, train_loss: 0.1708, step time: 0.4945\n",
      "170/388, train_loss: 0.1196, step time: 0.4844\n",
      "171/388, train_loss: 0.3039, step time: 0.5174\n",
      "172/388, train_loss: 0.1437, step time: 0.4970\n",
      "173/388, train_loss: 0.2294, step time: 0.9873\n",
      "174/388, train_loss: 0.3517, step time: 0.5362\n",
      "175/388, train_loss: 0.1164, step time: 0.5135\n",
      "176/388, train_loss: 0.6978, step time: 0.4942\n",
      "177/388, train_loss: 0.5817, step time: 1.0926\n",
      "178/388, train_loss: 0.2260, step time: 0.5294\n",
      "179/388, train_loss: 0.2839, step time: 0.5117\n",
      "180/388, train_loss: 0.5279, step time: 0.4923\n",
      "181/388, train_loss: 0.3554, step time: 0.5120\n",
      "182/388, train_loss: 0.2848, step time: 0.5499\n",
      "183/388, train_loss: 0.2606, step time: 0.5227\n",
      "184/388, train_loss: 0.2734, step time: 0.5054\n",
      "185/388, train_loss: 0.1599, step time: 0.4943\n",
      "186/388, train_loss: 0.2138, step time: 0.4967\n",
      "187/388, train_loss: 0.1828, step time: 0.4787\n",
      "188/388, train_loss: 0.3061, step time: 0.4896\n",
      "189/388, train_loss: 0.1835, step time: 0.4897\n",
      "190/388, train_loss: 0.2286, step time: 1.0077\n",
      "191/388, train_loss: 0.3418, step time: 0.5371\n",
      "192/388, train_loss: 0.2124, step time: 0.4980\n",
      "193/388, train_loss: 0.3739, step time: 0.4932\n",
      "194/388, train_loss: 0.5313, step time: 0.4813\n",
      "195/388, train_loss: 0.3188, step time: 0.5066\n",
      "196/388, train_loss: 0.2144, step time: 0.4957\n",
      "197/388, train_loss: 0.9727, step time: 0.5074\n",
      "198/388, train_loss: 0.2774, step time: 0.5125\n",
      "199/388, train_loss: 0.2613, step time: 0.5003\n",
      "200/388, train_loss: 0.3246, step time: 0.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/388, train_loss: 0.3046, step time: 0.4878\n",
      "202/388, train_loss: 0.2621, step time: 1.1342\n",
      "203/388, train_loss: 0.3735, step time: 0.5312\n",
      "204/388, train_loss: 0.2603, step time: 0.5110\n",
      "205/388, train_loss: 0.2301, step time: 0.5022\n",
      "206/388, train_loss: 0.3035, step time: 0.4959\n",
      "207/388, train_loss: 0.2213, step time: 0.4795\n",
      "208/388, train_loss: 0.2195, step time: 0.4855\n",
      "209/388, train_loss: 0.2780, step time: 0.4798\n",
      "210/388, train_loss: 0.1578, step time: 0.4816\n",
      "211/388, train_loss: 0.3275, step time: 0.4913\n",
      "212/388, train_loss: 0.2811, step time: 0.4946\n",
      "213/388, train_loss: 0.2290, step time: 0.5571\n",
      "214/388, train_loss: 0.2592, step time: 0.5316\n",
      "215/388, train_loss: 0.1905, step time: 0.4928\n",
      "216/388, train_loss: 0.5256, step time: 0.4937\n",
      "217/388, train_loss: 0.1840, step time: 0.4795\n",
      "218/388, train_loss: 0.2464, step time: 0.4871\n",
      "219/388, train_loss: 0.2205, step time: 0.5945\n",
      "220/388, train_loss: 0.3410, step time: 0.5622\n",
      "221/388, train_loss: 0.4940, step time: 0.5247\n",
      "222/388, train_loss: 0.2160, step time: 0.5081\n",
      "223/388, train_loss: 0.0984, step time: 0.5024\n",
      "224/388, train_loss: 0.2712, step time: 0.4850\n",
      "225/388, train_loss: 0.3349, step time: 0.4915\n",
      "226/388, train_loss: 0.6294, step time: 0.4992\n",
      "227/388, train_loss: 0.3386, step time: 0.5114\n",
      "228/388, train_loss: 0.4166, step time: 0.5094\n",
      "229/388, train_loss: 0.2031, step time: 0.4971\n",
      "230/388, train_loss: 0.3644, step time: 0.5054\n",
      "231/388, train_loss: 0.2519, step time: 0.5402\n",
      "232/388, train_loss: 0.1612, step time: 0.5189\n",
      "233/388, train_loss: 0.2341, step time: 0.4973\n",
      "234/388, train_loss: 0.3129, step time: 0.5023\n",
      "235/388, train_loss: 0.4704, step time: 0.4867\n",
      "236/388, train_loss: 0.2198, step time: 1.1039\n",
      "237/388, train_loss: 0.2994, step time: 0.5297\n",
      "238/388, train_loss: 0.1835, step time: 0.5010\n",
      "239/388, train_loss: 0.3020, step time: 0.4945\n",
      "240/388, train_loss: 0.3491, step time: 0.4759\n",
      "241/388, train_loss: 0.4617, step time: 0.4731\n",
      "242/388, train_loss: 0.3273, step time: 1.1147\n",
      "243/388, train_loss: 0.1522, step time: 0.5357\n",
      "244/388, train_loss: 0.2692, step time: 0.5019\n",
      "245/388, train_loss: 0.3679, step time: 0.4972\n",
      "246/388, train_loss: 0.2951, step time: 0.4864\n",
      "247/388, train_loss: 0.2267, step time: 0.4987\n",
      "248/388, train_loss: 0.2006, step time: 0.4887\n",
      "249/388, train_loss: 0.3628, step time: 0.5078\n",
      "250/388, train_loss: 0.2402, step time: 0.5088\n",
      "251/388, train_loss: 0.6523, step time: 0.4883\n",
      "252/388, train_loss: 0.1781, step time: 0.4941\n",
      "253/388, train_loss: 0.2071, step time: 0.4801\n",
      "254/388, train_loss: 0.3084, step time: 0.9761\n",
      "255/388, train_loss: 0.3188, step time: 0.5409\n",
      "256/388, train_loss: 0.3828, step time: 0.5071\n",
      "257/388, train_loss: 0.1726, step time: 0.4955\n",
      "258/388, train_loss: 0.2776, step time: 0.4957\n",
      "259/388, train_loss: 0.5935, step time: 0.4815\n",
      "260/388, train_loss: 0.1722, step time: 0.5015\n",
      "261/388, train_loss: 0.3371, step time: 0.4902\n",
      "262/388, train_loss: 0.2819, step time: 0.4856\n",
      "263/388, train_loss: 0.2310, step time: 0.5014\n",
      "264/388, train_loss: 0.2632, step time: 0.4922\n",
      "265/388, train_loss: 0.2017, step time: 0.4854\n",
      "266/388, train_loss: 0.2157, step time: 0.4924\n",
      "267/388, train_loss: 0.4721, step time: 0.4819\n",
      "268/388, train_loss: 0.7084, step time: 1.1005\n",
      "269/388, train_loss: 0.1967, step time: 0.5504\n",
      "270/388, train_loss: 0.1194, step time: 0.5159\n",
      "271/388, train_loss: 0.3343, step time: 0.5042\n",
      "272/388, train_loss: 0.1412, step time: 0.4887\n",
      "273/388, train_loss: 0.1313, step time: 0.4895\n",
      "274/388, train_loss: 0.1839, step time: 0.4914\n",
      "275/388, train_loss: 0.4105, step time: 0.4783\n",
      "276/388, train_loss: 0.3513, step time: 0.9887\n",
      "277/388, train_loss: 0.2522, step time: 0.5396\n",
      "278/388, train_loss: 0.5805, step time: 0.5123\n",
      "279/388, train_loss: 0.4312, step time: 0.4903\n",
      "280/388, train_loss: 0.2286, step time: 0.4882\n",
      "281/388, train_loss: 0.2475, step time: 0.4938\n",
      "282/388, train_loss: 0.2832, step time: 1.0628\n",
      "283/388, train_loss: 0.2298, step time: 0.5423\n",
      "284/388, train_loss: 0.1950, step time: 0.5193\n",
      "285/388, train_loss: 0.3910, step time: 0.5005\n",
      "286/388, train_loss: 0.2433, step time: 0.4943\n",
      "287/388, train_loss: 0.1745, step time: 0.4785\n",
      "288/388, train_loss: 0.3629, step time: 0.4788\n",
      "289/388, train_loss: 0.2467, step time: 0.4742\n",
      "290/388, train_loss: 0.3749, step time: 0.4818\n",
      "291/388, train_loss: 0.1210, step time: 0.4762\n",
      "292/388, train_loss: 0.2029, step time: 0.4778\n",
      "293/388, train_loss: 0.1358, step time: 0.4781\n",
      "294/388, train_loss: 0.1054, step time: 0.4707\n",
      "295/388, train_loss: 0.3449, step time: 0.7223\n",
      "296/388, train_loss: 0.2529, step time: 0.5498\n",
      "297/388, train_loss: 0.3359, step time: 0.5274\n",
      "298/388, train_loss: 0.1892, step time: 0.5062\n",
      "299/388, train_loss: 0.1874, step time: 0.4969\n",
      "300/388, train_loss: 0.7050, step time: 0.4852\n",
      "301/388, train_loss: 0.2074, step time: 0.4923\n",
      "302/388, train_loss: 0.2636, step time: 0.4933\n",
      "303/388, train_loss: 0.0992, step time: 0.4973\n",
      "304/388, train_loss: 0.2948, step time: 0.4958\n",
      "305/388, train_loss: 0.1683, step time: 0.5486\n",
      "306/388, train_loss: 0.2435, step time: 0.5314\n",
      "307/388, train_loss: 0.2438, step time: 0.5004\n",
      "308/388, train_loss: 0.4959, step time: 0.5038\n",
      "309/388, train_loss: 0.3552, step time: 0.4896\n",
      "310/388, train_loss: 0.3670, step time: 0.4909\n",
      "311/388, train_loss: 0.3414, step time: 1.0906\n",
      "312/388, train_loss: 0.3802, step time: 0.5412\n",
      "313/388, train_loss: 0.1278, step time: 0.5151\n",
      "314/388, train_loss: 0.1011, step time: 0.4884\n",
      "315/388, train_loss: 0.2658, step time: 0.4884\n",
      "316/388, train_loss: 0.3772, step time: 0.5157\n",
      "317/388, train_loss: 0.6943, step time: 0.4951\n",
      "318/388, train_loss: 0.2286, step time: 0.4921\n",
      "319/388, train_loss: 0.6638, step time: 0.4841\n",
      "320/388, train_loss: 0.2212, step time: 0.8158\n",
      "321/388, train_loss: 0.3431, step time: 0.5408\n",
      "322/388, train_loss: 0.3846, step time: 0.5130\n",
      "323/388, train_loss: 0.1506, step time: 0.4899\n",
      "324/388, train_loss: 0.3530, step time: 0.4943\n",
      "325/388, train_loss: 0.2108, step time: 0.4909\n",
      "326/388, train_loss: 0.4146, step time: 0.4798\n",
      "327/388, train_loss: 0.2101, step time: 0.8190\n",
      "328/388, train_loss: 0.2478, step time: 0.5429\n",
      "329/388, train_loss: 0.3907, step time: 0.5106\n",
      "330/388, train_loss: 0.2429, step time: 0.4942\n",
      "331/388, train_loss: 0.4022, step time: 0.5031\n",
      "332/388, train_loss: 0.2151, step time: 0.4840\n",
      "333/388, train_loss: 0.4238, step time: 0.4824\n",
      "334/388, train_loss: 0.1481, step time: 0.4938\n",
      "335/388, train_loss: 0.2840, step time: 0.4809\n",
      "336/388, train_loss: 0.1742, step time: 1.0717\n",
      "337/388, train_loss: 0.3796, step time: 0.5293\n",
      "338/388, train_loss: 0.1631, step time: 0.4950\n",
      "339/388, train_loss: 0.3029, step time: 0.5116\n",
      "340/388, train_loss: 0.3897, step time: 0.5116\n",
      "341/388, train_loss: 0.4798, step time: 0.4960\n",
      "342/388, train_loss: 0.3816, step time: 0.4829\n",
      "343/388, train_loss: 0.4036, step time: 0.4846\n",
      "344/388, train_loss: 0.2827, step time: 0.5787\n",
      "345/388, train_loss: 0.3333, step time: 0.5236\n",
      "346/388, train_loss: 0.1448, step time: 0.5075\n",
      "347/388, train_loss: 0.1161, step time: 0.4867\n",
      "348/388, train_loss: 0.3912, step time: 0.4943\n",
      "349/388, train_loss: 0.2060, step time: 0.5006\n",
      "350/388, train_loss: 0.3214, step time: 0.4820\n",
      "351/388, train_loss: 0.3111, step time: 0.4933\n",
      "352/388, train_loss: 0.2989, step time: 0.4773\n",
      "353/388, train_loss: 0.1883, step time: 0.4732\n",
      "354/388, train_loss: 0.1505, step time: 0.4747\n",
      "355/388, train_loss: 0.4150, step time: 1.0433\n",
      "356/388, train_loss: 0.3503, step time: 0.5473\n",
      "357/388, train_loss: 0.3408, step time: 0.5091\n",
      "358/388, train_loss: 0.4626, step time: 0.4965\n",
      "359/388, train_loss: 0.2222, step time: 0.5275\n",
      "360/388, train_loss: 0.6301, step time: 0.4999\n",
      "361/388, train_loss: 0.1823, step time: 0.4995\n",
      "362/388, train_loss: 0.3457, step time: 0.4866\n",
      "363/388, train_loss: 0.1782, step time: 1.0590\n",
      "364/388, train_loss: 0.2159, step time: 0.5240\n",
      "365/388, train_loss: 0.1361, step time: 0.5042\n",
      "366/388, train_loss: 0.1334, step time: 0.4941\n",
      "367/388, train_loss: 0.1936, step time: 0.4835\n",
      "368/388, train_loss: 0.3103, step time: 0.4906\n",
      "369/388, train_loss: 0.3075, step time: 0.4865\n",
      "370/388, train_loss: 0.1514, step time: 0.4771\n",
      "371/388, train_loss: 0.8157, step time: 0.6136\n",
      "372/388, train_loss: 0.2950, step time: 0.5269\n",
      "373/388, train_loss: 0.1293, step time: 0.5028\n",
      "374/388, train_loss: 0.2401, step time: 0.4841\n",
      "375/388, train_loss: 0.1541, step time: 0.4877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/388, train_loss: 0.1274, step time: 0.4902\n",
      "377/388, train_loss: 0.2116, step time: 0.4794\n",
      "378/388, train_loss: 0.2906, step time: 0.4894\n",
      "379/388, train_loss: 0.1987, step time: 0.4811\n",
      "380/388, train_loss: 0.4402, step time: 0.5017\n",
      "381/388, train_loss: 0.2553, step time: 0.4944\n",
      "382/388, train_loss: 0.2004, step time: 0.4808\n",
      "383/388, train_loss: 0.1739, step time: 0.4975\n",
      "384/388, train_loss: 0.3048, step time: 0.4782\n",
      "385/388, train_loss: 0.2449, step time: 0.5487\n",
      "386/388, train_loss: 0.3106, step time: 0.5255\n",
      "387/388, train_loss: 0.2411, step time: 0.4927\n",
      "388/388, train_loss: 0.3361, step time: 0.4908\n",
      "epoch 9 average loss: 0.2993\n",
      "current epoch: 9 current mean dice: 0.6630 tc: 0.7339 wt: 0.8520 et: 0.4031\n",
      "best mean dice: 0.6708 at epoch: 8\n",
      "time consuming of epoch 9 is: 300.4522\n",
      "----------\n",
      "epoch 10/300\n",
      "1/388, train_loss: 0.6214, step time: 0.4788\n",
      "2/388, train_loss: 0.4422, step time: 0.4958\n",
      "3/388, train_loss: 0.1457, step time: 1.1119\n",
      "4/388, train_loss: 0.2380, step time: 0.5566\n",
      "5/388, train_loss: 0.2281, step time: 0.5069\n",
      "6/388, train_loss: 0.1825, step time: 0.4918\n",
      "7/388, train_loss: 0.2016, step time: 0.5477\n",
      "8/388, train_loss: 0.2344, step time: 0.5804\n",
      "9/388, train_loss: 0.3379, step time: 0.5323\n",
      "10/388, train_loss: 0.1864, step time: 0.5154\n",
      "11/388, train_loss: 0.2646, step time: 0.4910\n",
      "12/388, train_loss: 0.2665, step time: 0.6407\n",
      "13/388, train_loss: 0.2551, step time: 0.5432\n",
      "14/388, train_loss: 0.1442, step time: 0.5228\n",
      "15/388, train_loss: 0.4582, step time: 0.5350\n",
      "16/388, train_loss: 0.2649, step time: 0.5193\n",
      "17/388, train_loss: 0.4597, step time: 0.5059\n",
      "18/388, train_loss: 0.1779, step time: 0.5832\n",
      "19/388, train_loss: 0.3889, step time: 0.5589\n",
      "20/388, train_loss: 0.3605, step time: 0.5188\n",
      "21/388, train_loss: 0.4894, step time: 0.5078\n",
      "22/388, train_loss: 0.2939, step time: 0.5835\n",
      "23/388, train_loss: 0.4931, step time: 0.5341\n",
      "24/388, train_loss: 0.2459, step time: 0.4953\n",
      "25/388, train_loss: 0.6726, step time: 0.4988\n",
      "26/388, train_loss: 0.1351, step time: 0.4961\n",
      "27/388, train_loss: 0.3246, step time: 0.8496\n",
      "28/388, train_loss: 0.4142, step time: 0.5753\n",
      "29/388, train_loss: 0.1709, step time: 0.5458\n",
      "30/388, train_loss: 0.4078, step time: 0.5148\n",
      "31/388, train_loss: 0.2843, step time: 0.4933\n",
      "32/388, train_loss: 0.3496, step time: 0.4823\n",
      "33/388, train_loss: 0.3406, step time: 0.5122\n",
      "34/388, train_loss: 0.4008, step time: 0.5049\n",
      "35/388, train_loss: 0.1709, step time: 0.5154\n",
      "36/388, train_loss: 0.4771, step time: 0.4999\n",
      "37/388, train_loss: 0.2036, step time: 0.4939\n",
      "38/388, train_loss: 0.2861, step time: 0.4988\n",
      "39/388, train_loss: 0.3483, step time: 1.1467\n",
      "40/388, train_loss: 0.2941, step time: 0.5460\n",
      "41/388, train_loss: 0.1906, step time: 0.5093\n",
      "42/388, train_loss: 0.3988, step time: 0.5038\n",
      "43/388, train_loss: 0.5986, step time: 0.5001\n",
      "44/388, train_loss: 0.1590, step time: 0.4998\n",
      "45/388, train_loss: 0.3989, step time: 1.1464\n",
      "46/388, train_loss: 0.2622, step time: 0.5387\n",
      "47/388, train_loss: 0.4432, step time: 0.5115\n",
      "48/388, train_loss: 0.5973, step time: 0.4894\n",
      "49/388, train_loss: 0.2386, step time: 0.4961\n",
      "50/388, train_loss: 0.2992, step time: 0.4839\n",
      "51/388, train_loss: 0.2915, step time: 0.5056\n",
      "52/388, train_loss: 0.2672, step time: 0.4838\n",
      "53/388, train_loss: 0.3242, step time: 0.4855\n",
      "54/388, train_loss: 0.7409, step time: 0.4789\n",
      "55/388, train_loss: 0.2830, step time: 0.7837\n",
      "56/388, train_loss: 0.2319, step time: 0.5561\n",
      "57/388, train_loss: 0.3062, step time: 0.5067\n",
      "58/388, train_loss: 0.1601, step time: 0.4851\n",
      "59/388, train_loss: 0.2169, step time: 0.5231\n",
      "60/388, train_loss: 0.4951, step time: 0.4987\n",
      "61/388, train_loss: 0.3127, step time: 0.4934\n",
      "62/388, train_loss: 0.4451, step time: 1.0539\n",
      "63/388, train_loss: 0.3270, step time: 0.5343\n",
      "64/388, train_loss: 0.2384, step time: 0.5162\n",
      "65/388, train_loss: 0.3572, step time: 0.4927\n",
      "66/388, train_loss: 0.2007, step time: 0.4967\n",
      "67/388, train_loss: 0.1870, step time: 0.4820\n",
      "68/388, train_loss: 0.1637, step time: 0.4728\n",
      "69/388, train_loss: 0.2069, step time: 0.4779\n",
      "70/388, train_loss: 0.4541, step time: 0.4835\n",
      "71/388, train_loss: 0.2751, step time: 0.5419\n",
      "72/388, train_loss: 0.2624, step time: 0.5197\n",
      "73/388, train_loss: 0.2335, step time: 0.5029\n",
      "74/388, train_loss: 0.1787, step time: 0.4840\n",
      "75/388, train_loss: 0.1975, step time: 0.5381\n",
      "76/388, train_loss: 0.7713, step time: 0.5375\n",
      "77/388, train_loss: 0.3021, step time: 0.5069\n",
      "78/388, train_loss: 0.2978, step time: 0.4886\n",
      "79/388, train_loss: 0.2027, step time: 0.5098\n",
      "80/388, train_loss: 0.2080, step time: 0.5238\n",
      "81/388, train_loss: 0.2405, step time: 0.5030\n",
      "82/388, train_loss: 0.4395, step time: 1.0185\n",
      "83/388, train_loss: 0.3184, step time: 0.5443\n",
      "84/388, train_loss: 0.2419, step time: 0.5228\n",
      "85/388, train_loss: 0.2808, step time: 0.5074\n",
      "86/388, train_loss: 0.3614, step time: 0.4940\n",
      "87/388, train_loss: 0.2177, step time: 0.5157\n",
      "88/388, train_loss: 0.2280, step time: 0.5257\n",
      "89/388, train_loss: 0.1074, step time: 0.4930\n",
      "90/388, train_loss: 0.6838, step time: 0.4854\n",
      "91/388, train_loss: 0.1189, step time: 0.4836\n",
      "92/388, train_loss: 0.1054, step time: 1.1081\n",
      "93/388, train_loss: 0.2915, step time: 0.5289\n",
      "94/388, train_loss: 0.1543, step time: 0.5124\n",
      "95/388, train_loss: 0.4214, step time: 0.4916\n",
      "96/388, train_loss: 0.2535, step time: 1.1639\n",
      "97/388, train_loss: 0.2685, step time: 0.5452\n",
      "98/388, train_loss: 0.1935, step time: 0.5162\n",
      "99/388, train_loss: 0.3273, step time: 0.4939\n",
      "100/388, train_loss: 0.4419, step time: 0.5008\n",
      "101/388, train_loss: 0.4635, step time: 0.4869\n",
      "102/388, train_loss: 0.1512, step time: 0.4835\n",
      "103/388, train_loss: 0.2326, step time: 0.4964\n",
      "104/388, train_loss: 0.2163, step time: 0.4920\n",
      "105/388, train_loss: 0.1998, step time: 0.5258\n",
      "106/388, train_loss: 0.1619, step time: 0.5042\n",
      "107/388, train_loss: 0.2133, step time: 0.5535\n",
      "108/388, train_loss: 0.4558, step time: 0.6345\n",
      "109/388, train_loss: 0.2028, step time: 0.5434\n",
      "110/388, train_loss: 0.6997, step time: 0.5281\n",
      "111/388, train_loss: 0.2187, step time: 0.5053\n",
      "112/388, train_loss: 0.4099, step time: 0.4976\n",
      "113/388, train_loss: 0.2149, step time: 0.5061\n",
      "114/388, train_loss: 0.2334, step time: 0.4891\n",
      "115/388, train_loss: 0.1292, step time: 0.5059\n",
      "116/388, train_loss: 0.0628, step time: 0.5028\n",
      "117/388, train_loss: 0.4426, step time: 0.4977\n",
      "118/388, train_loss: 0.2348, step time: 0.5407\n",
      "119/388, train_loss: 0.2864, step time: 0.5285\n",
      "120/388, train_loss: 0.7093, step time: 0.5992\n",
      "121/388, train_loss: 0.3657, step time: 0.5535\n",
      "122/388, train_loss: 0.1276, step time: 0.5303\n",
      "123/388, train_loss: 0.4588, step time: 0.5783\n",
      "124/388, train_loss: 0.1431, step time: 0.5505\n",
      "125/388, train_loss: 0.3639, step time: 0.5155\n",
      "126/388, train_loss: 0.6432, step time: 0.5087\n",
      "127/388, train_loss: 0.1312, step time: 0.4946\n",
      "128/388, train_loss: 0.1439, step time: 0.4937\n",
      "129/388, train_loss: 0.6801, step time: 0.5141\n",
      "130/388, train_loss: 0.3997, step time: 0.5985\n",
      "131/388, train_loss: 0.2894, step time: 0.5418\n",
      "132/388, train_loss: 0.2519, step time: 0.5141\n",
      "133/388, train_loss: 0.2416, step time: 0.4866\n",
      "134/388, train_loss: 0.1988, step time: 1.1004\n",
      "135/388, train_loss: 0.2095, step time: 0.5361\n",
      "136/388, train_loss: 0.0644, step time: 0.4988\n",
      "137/388, train_loss: 0.2100, step time: 0.4975\n",
      "138/388, train_loss: 0.1592, step time: 0.4858\n",
      "139/388, train_loss: 0.1026, step time: 0.4966\n",
      "140/388, train_loss: 0.4251, step time: 1.0656\n",
      "141/388, train_loss: 0.2137, step time: 0.5409\n",
      "142/388, train_loss: 0.1313, step time: 0.5079\n",
      "143/388, train_loss: 0.1970, step time: 0.4974\n",
      "144/388, train_loss: 0.3941, step time: 0.4889\n",
      "145/388, train_loss: 0.1930, step time: 0.4956\n",
      "146/388, train_loss: 0.2644, step time: 0.4942\n",
      "147/388, train_loss: 0.1553, step time: 1.2060\n",
      "148/388, train_loss: 0.1598, step time: 0.5259\n",
      "149/388, train_loss: 0.2938, step time: 0.5105\n",
      "150/388, train_loss: 0.7519, step time: 0.4963\n",
      "151/388, train_loss: 0.3575, step time: 0.4958\n",
      "152/388, train_loss: 0.2106, step time: 0.4797\n",
      "153/388, train_loss: 0.2745, step time: 0.4871\n",
      "154/388, train_loss: 0.1617, step time: 0.4938\n",
      "155/388, train_loss: 0.3062, step time: 0.4821\n",
      "156/388, train_loss: 0.4708, step time: 0.4898\n",
      "157/388, train_loss: 0.3838, step time: 0.4757\n",
      "158/388, train_loss: 0.4679, step time: 0.4821\n",
      "159/388, train_loss: 0.2733, step time: 0.5022\n",
      "160/388, train_loss: 0.1419, step time: 1.1638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/388, train_loss: 0.1691, step time: 0.5359\n",
      "162/388, train_loss: 0.2134, step time: 0.5057\n",
      "163/388, train_loss: 0.5337, step time: 0.4952\n",
      "164/388, train_loss: 0.1733, step time: 0.4885\n",
      "165/388, train_loss: 0.0978, step time: 0.5453\n",
      "166/388, train_loss: 0.1691, step time: 0.5311\n",
      "167/388, train_loss: 0.2414, step time: 0.5028\n",
      "168/388, train_loss: 0.4251, step time: 0.5081\n",
      "169/388, train_loss: 0.3685, step time: 0.4885\n",
      "170/388, train_loss: 0.2065, step time: 0.4848\n",
      "171/388, train_loss: 0.1970, step time: 0.5120\n",
      "172/388, train_loss: 0.3783, step time: 0.4962\n",
      "173/388, train_loss: 0.1584, step time: 0.4944\n",
      "174/388, train_loss: 0.1051, step time: 0.4813\n",
      "175/388, train_loss: 0.1193, step time: 0.5216\n",
      "176/388, train_loss: 0.3167, step time: 0.4980\n",
      "177/388, train_loss: 0.2751, step time: 0.5025\n",
      "178/388, train_loss: 0.3564, step time: 0.5278\n",
      "179/388, train_loss: 0.4749, step time: 0.5131\n",
      "180/388, train_loss: 0.2356, step time: 0.4990\n",
      "181/388, train_loss: 0.1704, step time: 0.4906\n",
      "182/388, train_loss: 0.3060, step time: 0.4790\n",
      "183/388, train_loss: 0.1910, step time: 0.5033\n",
      "184/388, train_loss: 0.1576, step time: 0.8741\n",
      "185/388, train_loss: 0.1522, step time: 0.5549\n",
      "186/388, train_loss: 0.2800, step time: 0.5198\n",
      "187/388, train_loss: 0.7311, step time: 0.4965\n",
      "188/388, train_loss: 0.2762, step time: 0.4864\n",
      "189/388, train_loss: 0.1599, step time: 0.5018\n",
      "190/388, train_loss: 0.3030, step time: 0.5000\n",
      "191/388, train_loss: 0.3098, step time: 0.5047\n",
      "192/388, train_loss: 0.3425, step time: 0.8938\n",
      "193/388, train_loss: 0.1413, step time: 0.5529\n",
      "194/388, train_loss: 0.4011, step time: 0.5252\n",
      "195/388, train_loss: 0.2926, step time: 0.5032\n",
      "196/388, train_loss: 0.2294, step time: 0.5125\n",
      "197/388, train_loss: 0.0989, step time: 0.4930\n",
      "198/388, train_loss: 0.3334, step time: 0.4929\n",
      "199/388, train_loss: 0.3258, step time: 0.4928\n",
      "200/388, train_loss: 0.3699, step time: 0.5002\n",
      "201/388, train_loss: 0.5953, step time: 0.4939\n",
      "202/388, train_loss: 0.2279, step time: 0.9998\n",
      "203/388, train_loss: 0.1969, step time: 0.5572\n",
      "204/388, train_loss: 0.1878, step time: 0.5284\n",
      "205/388, train_loss: 0.2371, step time: 0.4985\n",
      "206/388, train_loss: 0.3342, step time: 0.4937\n",
      "207/388, train_loss: 0.2594, step time: 1.0999\n",
      "208/388, train_loss: 0.2844, step time: 0.5331\n",
      "209/388, train_loss: 0.1759, step time: 0.5103\n",
      "210/388, train_loss: 0.1447, step time: 0.4918\n",
      "211/388, train_loss: 0.2961, step time: 0.5057\n",
      "212/388, train_loss: 0.4157, step time: 0.5476\n",
      "213/388, train_loss: 0.4964, step time: 0.5450\n",
      "214/388, train_loss: 0.5828, step time: 0.5268\n",
      "215/388, train_loss: 0.1416, step time: 0.5653\n",
      "216/388, train_loss: 0.3121, step time: 0.5326\n",
      "217/388, train_loss: 0.2766, step time: 0.5097\n",
      "218/388, train_loss: 0.1204, step time: 0.5083\n",
      "219/388, train_loss: 0.1018, step time: 0.5568\n",
      "220/388, train_loss: 0.7544, step time: 0.5422\n",
      "221/388, train_loss: 0.2616, step time: 0.5231\n",
      "222/388, train_loss: 0.5537, step time: 0.5011\n",
      "223/388, train_loss: 0.1249, step time: 1.2256\n",
      "224/388, train_loss: 0.1129, step time: 0.5299\n",
      "225/388, train_loss: 0.1851, step time: 0.4979\n",
      "226/388, train_loss: 0.2391, step time: 0.4950\n",
      "227/388, train_loss: 0.1003, step time: 0.4811\n",
      "228/388, train_loss: 0.3957, step time: 0.4828\n",
      "229/388, train_loss: 0.3282, step time: 0.4884\n",
      "230/388, train_loss: 0.3073, step time: 0.5051\n",
      "231/388, train_loss: 0.5098, step time: 0.5086\n",
      "232/388, train_loss: 0.1214, step time: 0.5100\n",
      "233/388, train_loss: 0.1491, step time: 0.5711\n",
      "234/388, train_loss: 0.2229, step time: 0.5385\n",
      "235/388, train_loss: 0.3374, step time: 0.5146\n",
      "236/388, train_loss: 0.5800, step time: 0.5009\n",
      "237/388, train_loss: 0.2632, step time: 1.0216\n",
      "238/388, train_loss: 0.4517, step time: 0.5356\n",
      "239/388, train_loss: 0.2182, step time: 0.5151\n",
      "240/388, train_loss: 0.1141, step time: 0.4958\n",
      "241/388, train_loss: 0.2534, step time: 0.4933\n",
      "242/388, train_loss: 0.2618, step time: 0.4828\n",
      "243/388, train_loss: 0.3096, step time: 0.4957\n",
      "244/388, train_loss: 0.2714, step time: 0.5056\n",
      "245/388, train_loss: 0.1996, step time: 0.5026\n",
      "246/388, train_loss: 0.3445, step time: 0.4854\n",
      "247/388, train_loss: 0.3074, step time: 1.1030\n",
      "248/388, train_loss: 0.5121, step time: 0.5449\n",
      "249/388, train_loss: 0.2636, step time: 0.5114\n",
      "250/388, train_loss: 0.4154, step time: 0.5064\n",
      "251/388, train_loss: 0.2705, step time: 0.5003\n",
      "252/388, train_loss: 0.1969, step time: 0.5285\n",
      "253/388, train_loss: 0.1836, step time: 0.5033\n",
      "254/388, train_loss: 0.1707, step time: 0.5467\n",
      "255/388, train_loss: 0.3205, step time: 0.5278\n",
      "256/388, train_loss: 0.2485, step time: 0.5000\n",
      "257/388, train_loss: 0.3583, step time: 0.4976\n",
      "258/388, train_loss: 0.5166, step time: 0.5002\n",
      "259/388, train_loss: 0.2542, step time: 0.5687\n",
      "260/388, train_loss: 0.2621, step time: 0.5386\n",
      "261/388, train_loss: 0.1633, step time: 0.5024\n",
      "262/388, train_loss: 0.1293, step time: 0.4940\n",
      "263/388, train_loss: 0.3320, step time: 0.4993\n",
      "264/388, train_loss: 0.3562, step time: 0.5003\n",
      "265/388, train_loss: 0.5083, step time: 0.4987\n",
      "266/388, train_loss: 0.2152, step time: 0.5025\n",
      "267/388, train_loss: 0.2270, step time: 0.5269\n",
      "268/388, train_loss: 0.4355, step time: 0.5124\n",
      "269/388, train_loss: 0.1618, step time: 0.4990\n",
      "270/388, train_loss: 0.1156, step time: 0.5354\n",
      "271/388, train_loss: 0.0835, step time: 0.6086\n",
      "272/388, train_loss: 0.1555, step time: 0.5217\n",
      "273/388, train_loss: 0.2007, step time: 0.4960\n",
      "274/388, train_loss: 0.6180, step time: 0.4975\n",
      "275/388, train_loss: 0.1853, step time: 0.4787\n",
      "276/388, train_loss: 0.1415, step time: 0.5289\n",
      "277/388, train_loss: 0.2229, step time: 0.5343\n",
      "278/388, train_loss: 0.1293, step time: 0.5081\n",
      "279/388, train_loss: 0.2805, step time: 0.4912\n",
      "280/388, train_loss: 0.1986, step time: 0.5121\n",
      "281/388, train_loss: 0.1354, step time: 0.4961\n",
      "282/388, train_loss: 0.2801, step time: 0.4986\n",
      "283/388, train_loss: 0.1498, step time: 1.1504\n",
      "284/388, train_loss: 0.4734, step time: 0.5280\n",
      "285/388, train_loss: 0.4962, step time: 0.5037\n",
      "286/388, train_loss: 0.1339, step time: 0.4785\n",
      "287/388, train_loss: 0.1433, step time: 1.0899\n",
      "288/388, train_loss: 0.5423, step time: 0.5203\n",
      "289/388, train_loss: 0.5124, step time: 0.5035\n",
      "290/388, train_loss: 0.1554, step time: 0.5102\n",
      "291/388, train_loss: 0.3103, step time: 0.5005\n",
      "292/388, train_loss: 0.2529, step time: 0.4911\n",
      "293/388, train_loss: 0.1592, step time: 0.4938\n",
      "294/388, train_loss: 0.7282, step time: 0.4837\n",
      "295/388, train_loss: 0.2142, step time: 0.7374\n",
      "296/388, train_loss: 0.3721, step time: 0.5480\n",
      "297/388, train_loss: 0.2452, step time: 0.5120\n",
      "298/388, train_loss: 0.1811, step time: 0.5014\n",
      "299/388, train_loss: 0.3549, step time: 0.4876\n",
      "300/388, train_loss: 0.4104, step time: 0.4979\n",
      "301/388, train_loss: 0.2600, step time: 0.5048\n",
      "302/388, train_loss: 0.2119, step time: 0.5866\n",
      "303/388, train_loss: 0.5919, step time: 0.5568\n",
      "304/388, train_loss: 0.2056, step time: 0.5162\n",
      "305/388, train_loss: 0.2593, step time: 0.5008\n",
      "306/388, train_loss: 0.3550, step time: 0.5150\n",
      "307/388, train_loss: 0.4859, step time: 0.5200\n",
      "308/388, train_loss: 0.2600, step time: 0.5679\n",
      "309/388, train_loss: 0.2313, step time: 0.5431\n",
      "310/388, train_loss: 0.5200, step time: 0.5141\n",
      "311/388, train_loss: 0.1168, step time: 0.4977\n",
      "312/388, train_loss: 0.2120, step time: 0.4946\n",
      "313/388, train_loss: 0.2506, step time: 0.4872\n",
      "314/388, train_loss: 0.2855, step time: 0.5112\n",
      "315/388, train_loss: 0.4642, step time: 0.4908\n",
      "316/388, train_loss: 0.2014, step time: 0.4794\n",
      "317/388, train_loss: 0.3058, step time: 0.4790\n",
      "318/388, train_loss: 0.2680, step time: 0.5601\n",
      "319/388, train_loss: 0.4238, step time: 0.5103\n",
      "320/388, train_loss: 0.1726, step time: 0.5487\n",
      "321/388, train_loss: 0.4261, step time: 0.5180\n",
      "322/388, train_loss: 0.1562, step time: 0.5079\n",
      "323/388, train_loss: 0.1510, step time: 0.4963\n",
      "324/388, train_loss: 0.1022, step time: 1.1344\n",
      "325/388, train_loss: 0.6516, step time: 0.5446\n",
      "326/388, train_loss: 0.2733, step time: 0.5180\n",
      "327/388, train_loss: 0.2035, step time: 0.4946\n",
      "328/388, train_loss: 0.1923, step time: 0.4856\n",
      "329/388, train_loss: 0.1907, step time: 0.5121\n",
      "330/388, train_loss: 0.5458, step time: 0.4929\n",
      "331/388, train_loss: 0.1784, step time: 0.4881\n",
      "332/388, train_loss: 0.1812, step time: 1.1027\n",
      "333/388, train_loss: 0.2730, step time: 0.5347\n",
      "334/388, train_loss: 0.1345, step time: 0.5226\n",
      "335/388, train_loss: 0.2799, step time: 0.4916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/388, train_loss: 0.1597, step time: 0.4967\n",
      "337/388, train_loss: 0.4002, step time: 0.4796\n",
      "338/388, train_loss: 0.2957, step time: 0.4937\n",
      "339/388, train_loss: 0.3820, step time: 0.5630\n",
      "340/388, train_loss: 0.2713, step time: 0.5396\n",
      "341/388, train_loss: 0.2668, step time: 0.5030\n",
      "342/388, train_loss: 0.2734, step time: 0.5002\n",
      "343/388, train_loss: 0.1362, step time: 0.4895\n",
      "344/388, train_loss: 0.2153, step time: 0.4950\n",
      "345/388, train_loss: 0.1614, step time: 1.1166\n",
      "346/388, train_loss: 0.1985, step time: 0.5262\n",
      "347/388, train_loss: 0.3208, step time: 0.4999\n",
      "348/388, train_loss: 0.1904, step time: 0.4870\n",
      "349/388, train_loss: 0.1529, step time: 0.5043\n",
      "350/388, train_loss: 0.5924, step time: 0.6187\n",
      "351/388, train_loss: 0.1407, step time: 0.5482\n",
      "352/388, train_loss: 0.2623, step time: 0.5175\n",
      "353/388, train_loss: 0.2285, step time: 0.5065\n",
      "354/388, train_loss: 0.2342, step time: 0.4898\n",
      "355/388, train_loss: 0.4360, step time: 0.5038\n",
      "356/388, train_loss: 0.1667, step time: 0.5713\n",
      "357/388, train_loss: 0.5457, step time: 0.5348\n",
      "358/388, train_loss: 0.1745, step time: 0.5167\n",
      "359/388, train_loss: 0.1224, step time: 0.5030\n",
      "360/388, train_loss: 0.2163, step time: 0.4957\n",
      "361/388, train_loss: 0.1657, step time: 0.4812\n",
      "362/388, train_loss: 0.5255, step time: 0.5024\n",
      "363/388, train_loss: 0.2430, step time: 0.4965\n",
      "364/388, train_loss: 0.1248, step time: 0.4996\n",
      "365/388, train_loss: 0.1815, step time: 0.4838\n",
      "366/388, train_loss: 0.1886, step time: 0.6708\n",
      "367/388, train_loss: 0.2302, step time: 0.5642\n",
      "368/388, train_loss: 0.5677, step time: 0.5309\n",
      "369/388, train_loss: 0.1498, step time: 0.5002\n",
      "370/388, train_loss: 0.3021, step time: 0.4955\n",
      "371/388, train_loss: 0.3080, step time: 0.4826\n",
      "372/388, train_loss: 0.6457, step time: 1.0169\n",
      "373/388, train_loss: 0.1480, step time: 0.5287\n",
      "374/388, train_loss: 0.3098, step time: 0.5000\n",
      "375/388, train_loss: 0.1072, step time: 0.4943\n",
      "376/388, train_loss: 0.1281, step time: 0.4837\n",
      "377/388, train_loss: 0.1062, step time: 0.5413\n",
      "378/388, train_loss: 0.1323, step time: 0.5232\n",
      "379/388, train_loss: 0.2182, step time: 0.4983\n",
      "380/388, train_loss: 0.2604, step time: 0.4885\n",
      "381/388, train_loss: 0.5891, step time: 0.4829\n",
      "382/388, train_loss: 0.1724, step time: 0.4879\n",
      "383/388, train_loss: 0.1979, step time: 0.5059\n",
      "384/388, train_loss: 0.2001, step time: 0.4981\n",
      "385/388, train_loss: 0.1623, step time: 0.4787\n",
      "386/388, train_loss: 0.1276, step time: 0.4940\n",
      "387/388, train_loss: 0.2670, step time: 0.5040\n",
      "388/388, train_loss: 0.5385, step time: 0.5540\n",
      "epoch 10 average loss: 0.2882\n",
      "saved new best metric model\n",
      "current epoch: 10 current mean dice: 0.6952 tc: 0.7339 wt: 0.8500 et: 0.5016\n",
      "best mean dice: 0.6952 at epoch: 10\n",
      "time consuming of epoch 10 is: 302.7421\n",
      "----------\n",
      "epoch 11/300\n",
      "1/388, train_loss: 0.2963, step time: 0.4824\n",
      "2/388, train_loss: 0.1835, step time: 0.4693\n",
      "3/388, train_loss: 0.5142, step time: 0.5197\n",
      "4/388, train_loss: 0.2295, step time: 0.5274\n",
      "5/388, train_loss: 0.1727, step time: 0.5001\n",
      "6/388, train_loss: 0.2326, step time: 0.5068\n",
      "7/388, train_loss: 0.2682, step time: 0.5075\n",
      "8/388, train_loss: 0.3907, step time: 1.1900\n",
      "9/388, train_loss: 0.1882, step time: 0.5354\n",
      "10/388, train_loss: 0.1442, step time: 0.5081\n",
      "11/388, train_loss: 0.3573, step time: 0.4897\n",
      "12/388, train_loss: 0.2611, step time: 0.4924\n",
      "13/388, train_loss: 0.1649, step time: 1.0726\n",
      "14/388, train_loss: 0.2790, step time: 0.5352\n",
      "15/388, train_loss: 0.1888, step time: 0.5161\n",
      "16/388, train_loss: 0.1189, step time: 0.4929\n",
      "17/388, train_loss: 0.2840, step time: 0.4925\n",
      "18/388, train_loss: 0.1260, step time: 0.5086\n",
      "19/388, train_loss: 0.1902, step time: 0.4970\n",
      "20/388, train_loss: 0.2571, step time: 0.4859\n",
      "21/388, train_loss: 0.4007, step time: 0.4976\n",
      "22/388, train_loss: 0.2262, step time: 0.4845\n",
      "23/388, train_loss: 0.1877, step time: 1.1430\n",
      "24/388, train_loss: 0.1368, step time: 0.5420\n",
      "25/388, train_loss: 0.1350, step time: 0.5176\n",
      "26/388, train_loss: 0.3625, step time: 0.4994\n",
      "27/388, train_loss: 0.7265, step time: 0.4936\n",
      "28/388, train_loss: 0.3400, step time: 0.4905\n",
      "29/388, train_loss: 0.4757, step time: 0.4819\n",
      "30/388, train_loss: 0.6103, step time: 0.7033\n",
      "31/388, train_loss: 0.3064, step time: 0.5461\n",
      "32/388, train_loss: 0.1647, step time: 0.5142\n",
      "33/388, train_loss: 0.1925, step time: 0.5097\n",
      "34/388, train_loss: 0.4677, step time: 0.5010\n",
      "35/388, train_loss: 0.2166, step time: 1.0837\n",
      "36/388, train_loss: 0.2525, step time: 0.5383\n",
      "37/388, train_loss: 0.2436, step time: 0.5043\n",
      "38/388, train_loss: 0.1688, step time: 0.4836\n",
      "39/388, train_loss: 0.3574, step time: 0.4815\n",
      "40/388, train_loss: 0.2061, step time: 0.4877\n",
      "41/388, train_loss: 0.3375, step time: 0.4756\n",
      "42/388, train_loss: 0.2219, step time: 0.4787\n",
      "43/388, train_loss: 0.1581, step time: 0.8179\n",
      "44/388, train_loss: 0.7032, step time: 0.5606\n",
      "45/388, train_loss: 0.4089, step time: 0.5408\n",
      "46/388, train_loss: 0.2547, step time: 0.5097\n",
      "47/388, train_loss: 0.1972, step time: 0.5042\n",
      "48/388, train_loss: 0.2073, step time: 0.4987\n",
      "49/388, train_loss: 0.2875, step time: 0.4857\n",
      "50/388, train_loss: 0.2797, step time: 0.5007\n",
      "51/388, train_loss: 0.4308, step time: 0.7395\n",
      "52/388, train_loss: 0.0733, step time: 0.5421\n",
      "53/388, train_loss: 0.2133, step time: 0.5082\n",
      "54/388, train_loss: 0.3799, step time: 0.5028\n",
      "55/388, train_loss: 0.2138, step time: 0.4884\n",
      "56/388, train_loss: 0.1864, step time: 0.4894\n",
      "57/388, train_loss: 0.2973, step time: 0.4877\n",
      "58/388, train_loss: 0.3300, step time: 0.4720\n",
      "59/388, train_loss: 0.2671, step time: 0.4869\n",
      "60/388, train_loss: 0.1499, step time: 0.5112\n",
      "61/388, train_loss: 0.1661, step time: 0.5359\n",
      "62/388, train_loss: 0.2345, step time: 0.5154\n",
      "63/388, train_loss: 0.1319, step time: 0.5037\n",
      "64/388, train_loss: 0.1804, step time: 0.4864\n",
      "65/388, train_loss: 0.1159, step time: 0.4919\n",
      "66/388, train_loss: 0.2582, step time: 0.4757\n",
      "67/388, train_loss: 0.4201, step time: 0.4822\n",
      "68/388, train_loss: 0.1774, step time: 0.4811\n",
      "69/388, train_loss: 0.2292, step time: 1.0390\n",
      "70/388, train_loss: 0.2237, step time: 0.5384\n",
      "71/388, train_loss: 0.4061, step time: 0.5143\n",
      "72/388, train_loss: 0.1993, step time: 0.4984\n",
      "73/388, train_loss: 0.3033, step time: 0.4934\n",
      "74/388, train_loss: 0.2874, step time: 0.5167\n",
      "75/388, train_loss: 0.2623, step time: 0.4964\n",
      "76/388, train_loss: 0.2931, step time: 0.4956\n",
      "77/388, train_loss: 0.5474, step time: 0.4821\n",
      "78/388, train_loss: 0.2524, step time: 0.4950\n",
      "79/388, train_loss: 0.2247, step time: 0.9033\n",
      "80/388, train_loss: 0.4629, step time: 0.5365\n",
      "81/388, train_loss: 0.1140, step time: 0.5165\n",
      "82/388, train_loss: 0.2099, step time: 0.4887\n",
      "83/388, train_loss: 0.1908, step time: 0.4862\n",
      "84/388, train_loss: 0.1635, step time: 0.4824\n",
      "85/388, train_loss: 0.1981, step time: 0.5104\n",
      "86/388, train_loss: 0.2746, step time: 0.4979\n",
      "87/388, train_loss: 0.3454, step time: 0.4831\n",
      "88/388, train_loss: 0.1038, step time: 0.4893\n",
      "89/388, train_loss: 0.2070, step time: 1.0295\n",
      "90/388, train_loss: 0.2487, step time: 0.5397\n",
      "91/388, train_loss: 0.0914, step time: 0.5148\n",
      "92/388, train_loss: 0.2672, step time: 0.4953\n",
      "93/388, train_loss: 0.2509, step time: 0.4875\n",
      "94/388, train_loss: 0.4563, step time: 0.4895\n",
      "95/388, train_loss: 0.1704, step time: 0.4786\n",
      "96/388, train_loss: 0.4360, step time: 0.4916\n",
      "97/388, train_loss: 0.4109, step time: 0.5300\n",
      "98/388, train_loss: 0.2689, step time: 0.4982\n",
      "99/388, train_loss: 0.4078, step time: 0.4993\n",
      "100/388, train_loss: 0.0937, step time: 0.4880\n",
      "101/388, train_loss: 0.4268, step time: 0.4792\n",
      "102/388, train_loss: 0.4121, step time: 0.8970\n",
      "103/388, train_loss: 0.1216, step time: 0.5360\n",
      "104/388, train_loss: 0.3212, step time: 0.5014\n",
      "105/388, train_loss: 0.2670, step time: 0.4972\n",
      "106/388, train_loss: 0.5476, step time: 0.4874\n",
      "107/388, train_loss: 0.4405, step time: 0.5096\n",
      "108/388, train_loss: 0.4324, step time: 0.5069\n",
      "109/388, train_loss: 0.1143, step time: 0.5006\n",
      "110/388, train_loss: 0.1644, step time: 0.4837\n",
      "111/388, train_loss: 0.3283, step time: 0.4882\n",
      "112/388, train_loss: 0.3083, step time: 0.5130\n",
      "113/388, train_loss: 0.1603, step time: 0.5225\n",
      "114/388, train_loss: 0.2102, step time: 0.5656\n",
      "115/388, train_loss: 0.2221, step time: 0.5285\n",
      "116/388, train_loss: 0.3575, step time: 0.5095\n",
      "117/388, train_loss: 0.2890, step time: 0.4999\n",
      "118/388, train_loss: 0.5262, step time: 0.5513\n",
      "119/388, train_loss: 0.1453, step time: 0.5465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/388, train_loss: 0.1473, step time: 0.5185\n",
      "121/388, train_loss: 0.3914, step time: 0.4945\n",
      "122/388, train_loss: 0.2841, step time: 0.4957\n",
      "123/388, train_loss: 0.3650, step time: 0.4789\n",
      "124/388, train_loss: 0.2445, step time: 0.8928\n",
      "125/388, train_loss: 0.1447, step time: 0.5363\n",
      "126/388, train_loss: 0.2910, step time: 0.5114\n",
      "127/388, train_loss: 0.1500, step time: 0.5044\n",
      "128/388, train_loss: 0.6357, step time: 0.4916\n",
      "129/388, train_loss: 0.1884, step time: 0.4857\n",
      "130/388, train_loss: 0.3896, step time: 0.4945\n",
      "131/388, train_loss: 0.3048, step time: 0.4789\n",
      "132/388, train_loss: 0.4715, step time: 0.4851\n",
      "133/388, train_loss: 0.3693, step time: 1.1217\n",
      "134/388, train_loss: 0.2257, step time: 0.5368\n",
      "135/388, train_loss: 0.4844, step time: 0.5032\n",
      "136/388, train_loss: 0.4907, step time: 0.4903\n",
      "137/388, train_loss: 0.2234, step time: 0.4889\n",
      "138/388, train_loss: 0.2529, step time: 0.4988\n",
      "139/388, train_loss: 0.2729, step time: 0.4998\n",
      "140/388, train_loss: 0.2982, step time: 0.5189\n",
      "141/388, train_loss: 0.2501, step time: 0.5065\n",
      "142/388, train_loss: 0.2091, step time: 0.4889\n",
      "143/388, train_loss: 0.1604, step time: 1.0141\n",
      "144/388, train_loss: 0.1590, step time: 0.5222\n",
      "145/388, train_loss: 0.2420, step time: 0.5020\n",
      "146/388, train_loss: 0.1425, step time: 0.4915\n",
      "147/388, train_loss: 0.5846, step time: 0.4804\n",
      "148/388, train_loss: 0.2555, step time: 0.4754\n",
      "149/388, train_loss: 0.2895, step time: 1.0002\n",
      "150/388, train_loss: 0.1269, step time: 0.5287\n",
      "151/388, train_loss: 0.3522, step time: 0.5054\n",
      "152/388, train_loss: 0.1130, step time: 0.4925\n",
      "153/388, train_loss: 0.3328, step time: 0.4796\n",
      "154/388, train_loss: 0.3011, step time: 0.4856\n",
      "155/388, train_loss: 0.2888, step time: 0.4855\n",
      "156/388, train_loss: 0.1181, step time: 0.4761\n",
      "157/388, train_loss: 0.6097, step time: 0.4839\n",
      "158/388, train_loss: 0.3986, step time: 0.4877\n",
      "159/388, train_loss: 0.1965, step time: 0.4825\n",
      "160/388, train_loss: 0.1518, step time: 0.4865\n",
      "161/388, train_loss: 0.2456, step time: 0.4793\n",
      "162/388, train_loss: 0.1824, step time: 0.4799\n",
      "163/388, train_loss: 0.3064, step time: 0.9893\n",
      "164/388, train_loss: 0.2299, step time: 0.5255\n",
      "165/388, train_loss: 0.6850, step time: 0.5129\n",
      "166/388, train_loss: 0.1802, step time: 0.4864\n",
      "167/388, train_loss: 0.1769, step time: 0.4955\n",
      "168/388, train_loss: 0.1678, step time: 0.5141\n",
      "169/388, train_loss: 0.2534, step time: 0.4915\n",
      "170/388, train_loss: 0.2063, step time: 0.4869\n",
      "171/388, train_loss: 0.3718, step time: 0.4920\n",
      "172/388, train_loss: 0.0519, step time: 1.1502\n",
      "173/388, train_loss: 0.5456, step time: 0.5200\n",
      "174/388, train_loss: 0.2218, step time: 0.4952\n",
      "175/388, train_loss: 0.1744, step time: 0.4884\n",
      "176/388, train_loss: 0.4242, step time: 0.4923\n",
      "177/388, train_loss: 0.2849, step time: 0.4759\n",
      "178/388, train_loss: 0.2351, step time: 0.4770\n",
      "179/388, train_loss: 0.2380, step time: 0.5085\n",
      "180/388, train_loss: 0.1971, step time: 0.4959\n",
      "181/388, train_loss: 0.0888, step time: 0.5147\n",
      "182/388, train_loss: 0.1402, step time: 0.5052\n",
      "183/388, train_loss: 0.6866, step time: 0.4970\n",
      "184/388, train_loss: 0.2580, step time: 1.0491\n",
      "185/388, train_loss: 0.1394, step time: 0.5442\n",
      "186/388, train_loss: 0.2671, step time: 0.5107\n",
      "187/388, train_loss: 0.2223, step time: 0.4911\n",
      "188/388, train_loss: 0.0789, step time: 0.4877\n",
      "189/388, train_loss: 0.1563, step time: 0.4893\n",
      "190/388, train_loss: 0.3497, step time: 0.4826\n",
      "191/388, train_loss: 0.4478, step time: 0.4897\n",
      "192/388, train_loss: 0.2976, step time: 0.4782\n",
      "193/388, train_loss: 0.2695, step time: 0.4822\n",
      "194/388, train_loss: 0.1187, step time: 0.4775\n",
      "195/388, train_loss: 0.3486, step time: 0.4831\n",
      "196/388, train_loss: 0.2874, step time: 0.5048\n",
      "197/388, train_loss: 0.1972, step time: 0.7159\n",
      "198/388, train_loss: 0.1474, step time: 0.5603\n",
      "199/388, train_loss: 0.0924, step time: 0.5347\n",
      "200/388, train_loss: 0.1549, step time: 0.5144\n",
      "201/388, train_loss: 0.1393, step time: 0.5050\n",
      "202/388, train_loss: 0.1375, step time: 0.5154\n",
      "203/388, train_loss: 0.2549, step time: 0.5076\n",
      "204/388, train_loss: 0.2355, step time: 0.5337\n",
      "205/388, train_loss: 0.2987, step time: 0.5048\n",
      "206/388, train_loss: 0.2074, step time: 0.4978\n",
      "207/388, train_loss: 0.3439, step time: 0.4901\n",
      "208/388, train_loss: 0.5475, step time: 1.0605\n",
      "209/388, train_loss: 0.2125, step time: 0.5322\n",
      "210/388, train_loss: 0.0869, step time: 0.4929\n",
      "211/388, train_loss: 0.1460, step time: 0.4866\n",
      "212/388, train_loss: 0.2277, step time: 0.4945\n",
      "213/388, train_loss: 0.0622, step time: 0.4825\n",
      "214/388, train_loss: 0.2458, step time: 0.4853\n",
      "215/388, train_loss: 0.2691, step time: 0.5831\n",
      "216/388, train_loss: 0.1185, step time: 0.5307\n",
      "217/388, train_loss: 0.5937, step time: 0.5143\n",
      "218/388, train_loss: 0.1017, step time: 0.4945\n",
      "219/388, train_loss: 0.2008, step time: 0.5035\n",
      "220/388, train_loss: 0.2095, step time: 0.5285\n",
      "221/388, train_loss: 0.0893, step time: 0.5295\n",
      "222/388, train_loss: 0.4738, step time: 0.5060\n",
      "223/388, train_loss: 0.2995, step time: 0.5306\n",
      "224/388, train_loss: 0.6219, step time: 0.5115\n",
      "225/388, train_loss: 0.1713, step time: 0.4931\n",
      "226/388, train_loss: 0.0980, step time: 0.4802\n",
      "227/388, train_loss: 0.3117, step time: 0.5060\n",
      "228/388, train_loss: 0.1614, step time: 0.5085\n",
      "229/388, train_loss: 0.4936, step time: 0.8343\n",
      "230/388, train_loss: 0.2652, step time: 0.5420\n",
      "231/388, train_loss: 0.5485, step time: 0.5036\n",
      "232/388, train_loss: 0.1674, step time: 0.4926\n",
      "233/388, train_loss: 0.1963, step time: 0.4865\n",
      "234/388, train_loss: 0.1847, step time: 0.4929\n",
      "235/388, train_loss: 0.1128, step time: 0.4831\n",
      "236/388, train_loss: 0.2276, step time: 0.5440\n",
      "237/388, train_loss: 0.1963, step time: 0.5379\n",
      "238/388, train_loss: 0.3970, step time: 0.5100\n",
      "239/388, train_loss: 0.4304, step time: 0.5071\n",
      "240/388, train_loss: 0.5153, step time: 0.5415\n",
      "241/388, train_loss: 0.4038, step time: 0.5134\n",
      "242/388, train_loss: 0.3289, step time: 0.4950\n",
      "243/388, train_loss: 0.2054, step time: 0.5171\n",
      "244/388, train_loss: 0.1649, step time: 0.5068\n",
      "245/388, train_loss: 0.3405, step time: 0.4976\n",
      "246/388, train_loss: 0.2827, step time: 0.4871\n",
      "247/388, train_loss: 0.4687, step time: 0.5065\n",
      "248/388, train_loss: 0.6767, step time: 0.4991\n",
      "249/388, train_loss: 0.1098, step time: 0.4968\n",
      "250/388, train_loss: 0.3206, step time: 0.4772\n",
      "251/388, train_loss: 0.3070, step time: 0.6701\n",
      "252/388, train_loss: 0.1766, step time: 0.5691\n",
      "253/388, train_loss: 0.3263, step time: 0.5350\n",
      "254/388, train_loss: 0.1715, step time: 0.5034\n",
      "255/388, train_loss: 0.4074, step time: 0.5011\n",
      "256/388, train_loss: 0.2751, step time: 0.4924\n",
      "257/388, train_loss: 0.2594, step time: 0.5157\n",
      "258/388, train_loss: 0.2919, step time: 0.6155\n",
      "259/388, train_loss: 0.7283, step time: 0.5327\n",
      "260/388, train_loss: 0.4210, step time: 0.5118\n",
      "261/388, train_loss: 0.3714, step time: 0.5085\n",
      "262/388, train_loss: 0.2521, step time: 0.4875\n",
      "263/388, train_loss: 0.1562, step time: 0.5218\n",
      "264/388, train_loss: 0.2304, step time: 0.5092\n",
      "265/388, train_loss: 0.6085, step time: 0.5006\n",
      "266/388, train_loss: 0.3262, step time: 0.4933\n",
      "267/388, train_loss: 0.4810, step time: 0.4779\n",
      "268/388, train_loss: 0.2910, step time: 0.5129\n",
      "269/388, train_loss: 0.2967, step time: 0.4912\n",
      "270/388, train_loss: 0.2902, step time: 0.5198\n",
      "271/388, train_loss: 0.3230, step time: 0.5070\n",
      "272/388, train_loss: 0.4660, step time: 0.5493\n",
      "273/388, train_loss: 0.2334, step time: 0.5110\n",
      "274/388, train_loss: 0.2142, step time: 0.4974\n",
      "275/388, train_loss: 0.1549, step time: 0.9175\n",
      "276/388, train_loss: 0.5378, step time: 0.5527\n",
      "277/388, train_loss: 0.1681, step time: 0.5264\n",
      "278/388, train_loss: 0.1758, step time: 0.5017\n",
      "279/388, train_loss: 0.2174, step time: 0.5034\n",
      "280/388, train_loss: 0.2329, step time: 0.4899\n",
      "281/388, train_loss: 0.3228, step time: 0.4971\n",
      "282/388, train_loss: 0.1907, step time: 1.2026\n",
      "283/388, train_loss: 0.1439, step time: 0.5418\n",
      "284/388, train_loss: 0.3002, step time: 0.5026\n",
      "285/388, train_loss: 0.2268, step time: 0.4946\n",
      "286/388, train_loss: 0.0854, step time: 0.4894\n",
      "287/388, train_loss: 0.2004, step time: 0.4820\n",
      "288/388, train_loss: 0.3643, step time: 0.4908\n",
      "289/388, train_loss: 0.4346, step time: 0.4811\n",
      "290/388, train_loss: 0.1766, step time: 0.5893\n",
      "291/388, train_loss: 0.3984, step time: 0.5533\n",
      "292/388, train_loss: 0.1143, step time: 0.5136\n",
      "293/388, train_loss: 0.1541, step time: 0.4944\n",
      "294/388, train_loss: 0.2216, step time: 0.5034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/388, train_loss: 0.1955, step time: 0.4916\n",
      "296/388, train_loss: 0.1595, step time: 0.4782\n",
      "297/388, train_loss: 0.3743, step time: 0.4785\n",
      "298/388, train_loss: 0.0885, step time: 0.5061\n",
      "299/388, train_loss: 0.3550, step time: 0.5194\n",
      "300/388, train_loss: 0.4822, step time: 0.5317\n",
      "301/388, train_loss: 0.3876, step time: 0.5408\n",
      "302/388, train_loss: 0.0868, step time: 0.5232\n",
      "303/388, train_loss: 0.3607, step time: 0.5670\n",
      "304/388, train_loss: 0.2011, step time: 0.5257\n",
      "305/388, train_loss: 0.1714, step time: 0.5033\n",
      "306/388, train_loss: 0.1756, step time: 0.4981\n",
      "307/388, train_loss: 0.5180, step time: 0.4851\n",
      "308/388, train_loss: 0.3034, step time: 0.4918\n",
      "309/388, train_loss: 0.2794, step time: 0.4861\n",
      "310/388, train_loss: 0.2534, step time: 0.4853\n",
      "311/388, train_loss: 0.3683, step time: 1.1918\n",
      "312/388, train_loss: 0.1640, step time: 0.5426\n",
      "313/388, train_loss: 0.1715, step time: 0.5187\n",
      "314/388, train_loss: 0.1016, step time: 0.4985\n",
      "315/388, train_loss: 0.5008, step time: 0.4960\n",
      "316/388, train_loss: 0.1594, step time: 0.4894\n",
      "317/388, train_loss: 0.1652, step time: 1.1703\n",
      "318/388, train_loss: 0.4377, step time: 0.5417\n",
      "319/388, train_loss: 0.5644, step time: 0.5175\n",
      "320/388, train_loss: 0.6518, step time: 0.4926\n",
      "321/388, train_loss: 0.7409, step time: 0.4989\n",
      "322/388, train_loss: 0.1496, step time: 0.4807\n",
      "323/388, train_loss: 0.1771, step time: 0.4858\n",
      "324/388, train_loss: 0.2163, step time: 0.5636\n",
      "325/388, train_loss: 0.2783, step time: 0.5270\n",
      "326/388, train_loss: 0.1485, step time: 0.4950\n",
      "327/388, train_loss: 0.2152, step time: 1.0555\n",
      "328/388, train_loss: 0.2169, step time: 0.5414\n",
      "329/388, train_loss: 0.6670, step time: 0.5152\n",
      "330/388, train_loss: 0.2776, step time: 0.4983\n",
      "331/388, train_loss: 0.1246, step time: 0.5128\n",
      "332/388, train_loss: 0.1792, step time: 0.5522\n",
      "333/388, train_loss: 0.1337, step time: 0.5256\n",
      "334/388, train_loss: 0.2384, step time: 0.5093\n",
      "335/388, train_loss: 0.1877, step time: 0.4916\n",
      "336/388, train_loss: 0.1444, step time: 0.5148\n",
      "337/388, train_loss: 0.0907, step time: 0.5146\n",
      "338/388, train_loss: 0.2208, step time: 0.5072\n",
      "339/388, train_loss: 0.0550, step time: 0.4852\n",
      "340/388, train_loss: 0.1309, step time: 0.5122\n",
      "341/388, train_loss: 0.5331, step time: 0.5071\n",
      "342/388, train_loss: 0.1998, step time: 0.5073\n",
      "343/388, train_loss: 0.2600, step time: 0.4944\n",
      "344/388, train_loss: 0.2839, step time: 0.5205\n",
      "345/388, train_loss: 0.2254, step time: 0.4915\n",
      "346/388, train_loss: 0.3895, step time: 0.4906\n",
      "347/388, train_loss: 0.2748, step time: 1.1191\n",
      "348/388, train_loss: 0.1169, step time: 0.5244\n",
      "349/388, train_loss: 0.3097, step time: 0.5094\n",
      "350/388, train_loss: 0.3243, step time: 0.4915\n",
      "351/388, train_loss: 0.2496, step time: 0.4887\n",
      "352/388, train_loss: 0.2198, step time: 0.5032\n",
      "353/388, train_loss: 0.2438, step time: 0.5278\n",
      "354/388, train_loss: 0.1413, step time: 0.6212\n",
      "355/388, train_loss: 0.1401, step time: 0.5666\n",
      "356/388, train_loss: 0.1208, step time: 0.5224\n",
      "357/388, train_loss: 0.6921, step time: 0.5013\n",
      "358/388, train_loss: 0.5162, step time: 0.4941\n",
      "359/388, train_loss: 0.2438, step time: 0.4998\n",
      "360/388, train_loss: 0.2288, step time: 0.4962\n",
      "361/388, train_loss: 0.2628, step time: 0.4899\n",
      "362/388, train_loss: 0.4404, step time: 0.4843\n",
      "363/388, train_loss: 0.2260, step time: 0.5644\n",
      "364/388, train_loss: 0.3073, step time: 0.5486\n",
      "365/388, train_loss: 0.3472, step time: 0.5713\n",
      "366/388, train_loss: 0.2008, step time: 0.7031\n",
      "367/388, train_loss: 0.1576, step time: 0.5465\n",
      "368/388, train_loss: 0.1961, step time: 0.5252\n",
      "369/388, train_loss: 0.4756, step time: 0.6003\n",
      "370/388, train_loss: 0.1508, step time: 0.5547\n",
      "371/388, train_loss: 0.2016, step time: 0.5258\n",
      "372/388, train_loss: 0.1729, step time: 0.5057\n",
      "373/388, train_loss: 0.3340, step time: 0.4959\n",
      "374/388, train_loss: 0.1677, step time: 0.4953\n",
      "375/388, train_loss: 0.1501, step time: 0.4813\n",
      "376/388, train_loss: 0.3709, step time: 0.4754\n",
      "377/388, train_loss: 0.2295, step time: 0.4837\n",
      "378/388, train_loss: 0.1261, step time: 0.4846\n",
      "379/388, train_loss: 0.0835, step time: 0.4903\n",
      "380/388, train_loss: 0.4874, step time: 0.5018\n",
      "381/388, train_loss: 0.1402, step time: 0.9399\n",
      "382/388, train_loss: 0.1225, step time: 0.5323\n",
      "383/388, train_loss: 0.7739, step time: 0.5150\n",
      "384/388, train_loss: 0.1947, step time: 0.4905\n",
      "385/388, train_loss: 0.1991, step time: 0.4840\n",
      "386/388, train_loss: 0.2385, step time: 0.4688\n",
      "387/388, train_loss: 0.1558, step time: 0.5445\n",
      "388/388, train_loss: 0.2842, step time: 0.5096\n",
      "epoch 11 average loss: 0.2746\n",
      "saved new best metric model\n",
      "current epoch: 11 current mean dice: 0.7004 tc: 0.7512 wt: 0.8492 et: 0.5006\n",
      "best mean dice: 0.7004 at epoch: 11\n",
      "time consuming of epoch 11 is: 301.4682\n",
      "----------\n",
      "epoch 12/300\n",
      "1/388, train_loss: 0.0875, step time: 0.4897\n",
      "2/388, train_loss: 0.2214, step time: 0.4974\n",
      "3/388, train_loss: 0.3739, step time: 0.4848\n",
      "4/388, train_loss: 0.3038, step time: 0.5597\n",
      "5/388, train_loss: 0.1687, step time: 0.5016\n",
      "6/388, train_loss: 0.1451, step time: 0.5239\n",
      "7/388, train_loss: 0.0913, step time: 0.5105\n",
      "8/388, train_loss: 0.7811, step time: 0.5187\n",
      "9/388, train_loss: 0.3025, step time: 0.9691\n",
      "10/388, train_loss: 0.1732, step time: 0.5797\n",
      "11/388, train_loss: 0.1251, step time: 0.5321\n",
      "12/388, train_loss: 0.0822, step time: 0.5073\n",
      "13/388, train_loss: 0.1861, step time: 0.5272\n",
      "14/388, train_loss: 0.2104, step time: 0.5027\n",
      "15/388, train_loss: 0.3434, step time: 0.4968\n",
      "16/388, train_loss: 0.2572, step time: 0.5934\n",
      "17/388, train_loss: 0.2859, step time: 0.5667\n",
      "18/388, train_loss: 0.3014, step time: 0.6097\n",
      "19/388, train_loss: 0.1837, step time: 0.6880\n",
      "20/388, train_loss: 0.1110, step time: 0.5728\n",
      "21/388, train_loss: 0.6969, step time: 0.5216\n",
      "22/388, train_loss: 0.1126, step time: 0.6145\n",
      "23/388, train_loss: 0.4220, step time: 0.5403\n",
      "24/388, train_loss: 0.1156, step time: 0.5153\n",
      "25/388, train_loss: 0.2412, step time: 0.5908\n",
      "26/388, train_loss: 0.2105, step time: 0.5439\n",
      "27/388, train_loss: 0.1618, step time: 0.5279\n",
      "28/388, train_loss: 0.0946, step time: 0.5183\n",
      "29/388, train_loss: 0.0751, step time: 0.5066\n",
      "30/388, train_loss: 0.5288, step time: 0.4921\n",
      "31/388, train_loss: 0.2714, step time: 0.5135\n",
      "32/388, train_loss: 0.1231, step time: 0.5641\n",
      "33/388, train_loss: 0.1377, step time: 0.5589\n",
      "34/388, train_loss: 0.1288, step time: 0.5423\n",
      "35/388, train_loss: 0.2682, step time: 0.5021\n",
      "36/388, train_loss: 0.1104, step time: 0.4892\n",
      "37/388, train_loss: 0.3707, step time: 0.5896\n",
      "38/388, train_loss: 0.2498, step time: 0.5794\n",
      "39/388, train_loss: 0.1243, step time: 0.5510\n",
      "40/388, train_loss: 0.1811, step time: 0.5218\n",
      "41/388, train_loss: 0.2432, step time: 0.5015\n",
      "42/388, train_loss: 0.1911, step time: 0.5262\n",
      "43/388, train_loss: 0.2980, step time: 0.5227\n",
      "44/388, train_loss: 0.1675, step time: 0.5568\n",
      "45/388, train_loss: 0.4063, step time: 0.5486\n",
      "46/388, train_loss: 0.1247, step time: 0.5164\n",
      "47/388, train_loss: 0.2512, step time: 0.5039\n",
      "48/388, train_loss: 0.6784, step time: 0.5917\n",
      "49/388, train_loss: 0.0521, step time: 0.5394\n",
      "50/388, train_loss: 0.4682, step time: 0.5017\n",
      "51/388, train_loss: 0.1213, step time: 0.4912\n",
      "52/388, train_loss: 0.1988, step time: 0.4868\n",
      "53/388, train_loss: 0.3473, step time: 1.1613\n",
      "54/388, train_loss: 0.1778, step time: 0.5457\n",
      "55/388, train_loss: 0.1297, step time: 0.5098\n",
      "56/388, train_loss: 0.4592, step time: 0.4906\n",
      "57/388, train_loss: 0.3321, step time: 0.4989\n",
      "58/388, train_loss: 0.3000, step time: 0.4994\n",
      "59/388, train_loss: 0.1644, step time: 0.5039\n",
      "60/388, train_loss: 0.3360, step time: 0.4836\n",
      "61/388, train_loss: 0.3913, step time: 0.4991\n",
      "62/388, train_loss: 0.8143, step time: 1.2307\n",
      "63/388, train_loss: 0.3905, step time: 0.5507\n",
      "64/388, train_loss: 0.2792, step time: 0.5273\n",
      "65/388, train_loss: 0.5729, step time: 0.5230\n",
      "66/388, train_loss: 0.2592, step time: 0.5254\n",
      "67/388, train_loss: 0.2067, step time: 0.4945\n",
      "68/388, train_loss: 0.1591, step time: 0.5072\n",
      "69/388, train_loss: 0.1004, step time: 0.4932\n",
      "70/388, train_loss: 0.4509, step time: 0.5309\n",
      "71/388, train_loss: 0.1756, step time: 0.5200\n",
      "72/388, train_loss: 0.3686, step time: 0.5072\n",
      "73/388, train_loss: 0.3052, step time: 0.5361\n",
      "74/388, train_loss: 0.5912, step time: 0.5166\n",
      "75/388, train_loss: 0.1083, step time: 0.5162\n",
      "76/388, train_loss: 0.3010, step time: 0.5145\n",
      "77/388, train_loss: 0.3338, step time: 0.4974\n",
      "78/388, train_loss: 0.1320, step time: 0.4936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/388, train_loss: 0.1044, step time: 0.4938\n",
      "80/388, train_loss: 0.4156, step time: 0.4840\n",
      "81/388, train_loss: 0.4849, step time: 0.9607\n",
      "82/388, train_loss: 0.1712, step time: 0.5711\n",
      "83/388, train_loss: 0.2956, step time: 0.5277\n",
      "84/388, train_loss: 0.3149, step time: 0.5254\n",
      "85/388, train_loss: 0.3515, step time: 0.5046\n",
      "86/388, train_loss: 0.4373, step time: 0.5073\n",
      "87/388, train_loss: 0.6011, step time: 0.5857\n",
      "88/388, train_loss: 0.1854, step time: 0.5417\n",
      "89/388, train_loss: 0.0862, step time: 0.5160\n",
      "90/388, train_loss: 0.4225, step time: 0.5070\n",
      "91/388, train_loss: 0.4356, step time: 0.4985\n",
      "92/388, train_loss: 0.1921, step time: 0.5161\n",
      "93/388, train_loss: 0.1472, step time: 0.5011\n",
      "94/388, train_loss: 0.3867, step time: 0.4958\n",
      "95/388, train_loss: 0.3084, step time: 0.4907\n",
      "96/388, train_loss: 0.2271, step time: 0.4777\n",
      "97/388, train_loss: 0.7423, step time: 0.5017\n",
      "98/388, train_loss: 0.2485, step time: 0.4834\n",
      "99/388, train_loss: 0.1462, step time: 0.4998\n",
      "100/388, train_loss: 0.6902, step time: 0.4869\n",
      "101/388, train_loss: 0.1633, step time: 0.5611\n",
      "102/388, train_loss: 0.3969, step time: 0.5728\n",
      "103/388, train_loss: 0.3958, step time: 0.5277\n",
      "104/388, train_loss: 0.3437, step time: 0.5069\n",
      "105/388, train_loss: 0.2320, step time: 0.4950\n",
      "106/388, train_loss: 0.1322, step time: 0.4968\n",
      "107/388, train_loss: 0.3390, step time: 1.1453\n",
      "108/388, train_loss: 0.1404, step time: 0.5340\n",
      "109/388, train_loss: 0.2861, step time: 0.5041\n",
      "110/388, train_loss: 0.2318, step time: 0.4819\n",
      "111/388, train_loss: 0.2628, step time: 0.4822\n",
      "112/388, train_loss: 0.1788, step time: 0.4821\n",
      "113/388, train_loss: 0.2727, step time: 0.4825\n",
      "114/388, train_loss: 0.1780, step time: 0.5207\n",
      "115/388, train_loss: 0.5145, step time: 0.5599\n",
      "116/388, train_loss: 0.1388, step time: 0.6405\n",
      "117/388, train_loss: 0.1535, step time: 0.5403\n",
      "118/388, train_loss: 0.1361, step time: 0.5175\n",
      "119/388, train_loss: 0.2822, step time: 0.4957\n",
      "120/388, train_loss: 0.1347, step time: 0.4898\n",
      "121/388, train_loss: 0.1466, step time: 0.4908\n",
      "122/388, train_loss: 0.1516, step time: 0.8987\n",
      "123/388, train_loss: 0.6985, step time: 0.5646\n",
      "124/388, train_loss: 0.1617, step time: 0.5249\n",
      "125/388, train_loss: 0.1921, step time: 0.5010\n",
      "126/388, train_loss: 0.1349, step time: 0.4953\n",
      "127/388, train_loss: 0.2979, step time: 0.4842\n",
      "128/388, train_loss: 0.4030, step time: 0.4992\n",
      "129/388, train_loss: 0.5630, step time: 0.5013\n",
      "130/388, train_loss: 0.2130, step time: 0.5014\n",
      "131/388, train_loss: 0.1914, step time: 0.4955\n",
      "132/388, train_loss: 0.1497, step time: 0.5003\n",
      "133/388, train_loss: 0.1505, step time: 0.9236\n",
      "134/388, train_loss: 0.2421, step time: 0.5416\n",
      "135/388, train_loss: 0.2483, step time: 0.5170\n",
      "136/388, train_loss: 0.3098, step time: 0.4921\n",
      "137/388, train_loss: 0.2559, step time: 0.4957\n",
      "138/388, train_loss: 0.2865, step time: 0.4801\n",
      "139/388, train_loss: 0.2559, step time: 0.4819\n",
      "140/388, train_loss: 0.2072, step time: 0.4869\n",
      "141/388, train_loss: 0.2200, step time: 0.4831\n",
      "142/388, train_loss: 0.1889, step time: 0.6057\n",
      "143/388, train_loss: 0.2445, step time: 0.5394\n",
      "144/388, train_loss: 0.2138, step time: 0.5221\n",
      "145/388, train_loss: 0.4156, step time: 0.5089\n",
      "146/388, train_loss: 0.1448, step time: 0.4908\n",
      "147/388, train_loss: 0.2198, step time: 0.4979\n",
      "148/388, train_loss: 0.3386, step time: 0.4797\n",
      "149/388, train_loss: 0.1551, step time: 0.5118\n",
      "150/388, train_loss: 0.3659, step time: 0.4945\n",
      "151/388, train_loss: 0.1367, step time: 0.5254\n",
      "152/388, train_loss: 0.1762, step time: 0.5030\n",
      "153/388, train_loss: 0.1349, step time: 0.5016\n",
      "154/388, train_loss: 0.4046, step time: 0.4868\n",
      "155/388, train_loss: 0.2527, step time: 0.5069\n",
      "156/388, train_loss: 0.2625, step time: 0.4908\n",
      "157/388, train_loss: 0.2445, step time: 0.4839\n",
      "158/388, train_loss: 0.1445, step time: 0.4914\n",
      "159/388, train_loss: 0.1123, step time: 0.4852\n",
      "160/388, train_loss: 0.1866, step time: 1.1837\n",
      "161/388, train_loss: 0.2127, step time: 0.5401\n",
      "162/388, train_loss: 0.6069, step time: 0.5180\n",
      "163/388, train_loss: 0.3066, step time: 0.4945\n",
      "164/388, train_loss: 0.1803, step time: 0.4972\n",
      "165/388, train_loss: 0.5303, step time: 0.4836\n",
      "166/388, train_loss: 0.0978, step time: 0.4911\n",
      "167/388, train_loss: 0.1605, step time: 0.4889\n",
      "168/388, train_loss: 0.2894, step time: 0.4883\n",
      "169/388, train_loss: 0.4622, step time: 0.5087\n",
      "170/388, train_loss: 0.5409, step time: 0.4853\n",
      "171/388, train_loss: 0.1849, step time: 0.4955\n",
      "172/388, train_loss: 0.2014, step time: 0.5638\n",
      "173/388, train_loss: 0.2477, step time: 0.5374\n",
      "174/388, train_loss: 0.1901, step time: 0.5207\n",
      "175/388, train_loss: 0.2085, step time: 0.5065\n",
      "176/388, train_loss: 0.6988, step time: 0.5007\n",
      "177/388, train_loss: 0.2960, step time: 0.4840\n",
      "178/388, train_loss: 0.1874, step time: 1.1500\n",
      "179/388, train_loss: 0.3566, step time: 0.5316\n",
      "180/388, train_loss: 0.3077, step time: 0.4980\n",
      "181/388, train_loss: 0.1176, step time: 0.4912\n",
      "182/388, train_loss: 0.1232, step time: 0.4801\n",
      "183/388, train_loss: 0.3945, step time: 0.4885\n",
      "184/388, train_loss: 0.0786, step time: 0.4786\n",
      "185/388, train_loss: 0.9297, step time: 0.5025\n",
      "186/388, train_loss: 0.1195, step time: 0.4952\n",
      "187/388, train_loss: 0.1597, step time: 0.5218\n",
      "188/388, train_loss: 0.3185, step time: 0.5083\n",
      "189/388, train_loss: 0.2905, step time: 0.5003\n",
      "190/388, train_loss: 0.1638, step time: 0.5023\n",
      "191/388, train_loss: 0.1908, step time: 0.5021\n",
      "192/388, train_loss: 0.1544, step time: 0.4905\n",
      "193/388, train_loss: 0.2133, step time: 0.4923\n",
      "194/388, train_loss: 0.1576, step time: 0.4755\n",
      "195/388, train_loss: 0.6477, step time: 0.5002\n",
      "196/388, train_loss: 0.2066, step time: 0.5293\n",
      "197/388, train_loss: 0.1837, step time: 0.5186\n",
      "198/388, train_loss: 0.3300, step time: 0.5017\n",
      "199/388, train_loss: 0.6834, step time: 0.5031\n",
      "200/388, train_loss: 0.2703, step time: 0.4863\n",
      "201/388, train_loss: 0.1823, step time: 0.5152\n",
      "202/388, train_loss: 0.5481, step time: 0.4897\n",
      "203/388, train_loss: 0.3933, step time: 0.5537\n",
      "204/388, train_loss: 0.0863, step time: 0.5322\n",
      "205/388, train_loss: 0.1024, step time: 0.5008\n",
      "206/388, train_loss: 0.1455, step time: 0.5385\n",
      "207/388, train_loss: 0.1608, step time: 0.5277\n",
      "208/388, train_loss: 0.0917, step time: 0.5375\n",
      "209/388, train_loss: 0.5735, step time: 0.5259\n",
      "210/388, train_loss: 0.1035, step time: 0.5011\n",
      "211/388, train_loss: 0.0986, step time: 0.5091\n",
      "212/388, train_loss: 0.2359, step time: 0.4947\n",
      "213/388, train_loss: 0.2530, step time: 0.4958\n",
      "214/388, train_loss: 0.1348, step time: 0.4763\n",
      "215/388, train_loss: 0.2753, step time: 0.4795\n",
      "216/388, train_loss: 0.1832, step time: 0.4801\n",
      "217/388, train_loss: 0.4926, step time: 0.5214\n",
      "218/388, train_loss: 0.2404, step time: 0.5247\n",
      "219/388, train_loss: 0.1979, step time: 0.4938\n",
      "220/388, train_loss: 0.3060, step time: 0.5686\n",
      "221/388, train_loss: 0.2805, step time: 0.5365\n",
      "222/388, train_loss: 0.4093, step time: 0.5014\n",
      "223/388, train_loss: 0.6826, step time: 0.5180\n",
      "224/388, train_loss: 0.3698, step time: 0.6755\n",
      "225/388, train_loss: 0.3633, step time: 0.5569\n",
      "226/388, train_loss: 0.2454, step time: 0.5218\n",
      "227/388, train_loss: 0.1882, step time: 0.5017\n",
      "228/388, train_loss: 0.2516, step time: 0.4849\n",
      "229/388, train_loss: 0.3256, step time: 0.4934\n",
      "230/388, train_loss: 0.4850, step time: 0.4878\n",
      "231/388, train_loss: 0.2244, step time: 1.0878\n",
      "232/388, train_loss: 0.2895, step time: 0.5246\n",
      "233/388, train_loss: 0.2856, step time: 0.5036\n",
      "234/388, train_loss: 0.1230, step time: 0.4950\n",
      "235/388, train_loss: 0.3452, step time: 0.5002\n",
      "236/388, train_loss: 0.1667, step time: 0.4802\n",
      "237/388, train_loss: 0.5058, step time: 0.4871\n",
      "238/388, train_loss: 0.1926, step time: 0.4878\n",
      "239/388, train_loss: 0.2073, step time: 0.4786\n",
      "240/388, train_loss: 0.3315, step time: 0.5272\n",
      "241/388, train_loss: 0.2951, step time: 0.5074\n",
      "242/388, train_loss: 0.0924, step time: 0.4986\n",
      "243/388, train_loss: 0.1016, step time: 0.5265\n",
      "244/388, train_loss: 0.2472, step time: 0.5088\n",
      "245/388, train_loss: 0.1777, step time: 0.4926\n",
      "246/388, train_loss: 0.2154, step time: 0.4921\n",
      "247/388, train_loss: 0.1198, step time: 0.4922\n",
      "248/388, train_loss: 0.2210, step time: 0.4998\n",
      "249/388, train_loss: 0.4460, step time: 0.4869\n",
      "250/388, train_loss: 0.0485, step time: 0.4996\n",
      "251/388, train_loss: 0.1058, step time: 0.4903\n",
      "252/388, train_loss: 0.2701, step time: 0.4923\n",
      "253/388, train_loss: 0.4090, step time: 0.4789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/388, train_loss: 0.2500, step time: 0.4904\n",
      "255/388, train_loss: 0.1595, step time: 0.4998\n",
      "256/388, train_loss: 0.1816, step time: 0.4967\n",
      "257/388, train_loss: 0.1695, step time: 0.4950\n",
      "258/388, train_loss: 0.2085, step time: 0.5149\n",
      "259/388, train_loss: 0.1204, step time: 0.5351\n",
      "260/388, train_loss: 0.1637, step time: 0.5324\n",
      "261/388, train_loss: 0.4154, step time: 0.5490\n",
      "262/388, train_loss: 0.2122, step time: 0.5220\n",
      "263/388, train_loss: 0.1571, step time: 0.4995\n",
      "264/388, train_loss: 0.2055, step time: 0.5018\n",
      "265/388, train_loss: 0.3093, step time: 0.4848\n",
      "266/388, train_loss: 0.5355, step time: 1.0611\n",
      "267/388, train_loss: 0.2194, step time: 0.5256\n",
      "268/388, train_loss: 0.1763, step time: 0.5055\n",
      "269/388, train_loss: 0.5753, step time: 0.4872\n",
      "270/388, train_loss: 0.2066, step time: 0.4816\n",
      "271/388, train_loss: 0.5853, step time: 0.4861\n",
      "272/388, train_loss: 0.5126, step time: 0.5039\n",
      "273/388, train_loss: 0.2409, step time: 0.4936\n",
      "274/388, train_loss: 0.2533, step time: 0.4847\n",
      "275/388, train_loss: 0.2323, step time: 0.4939\n",
      "276/388, train_loss: 0.1433, step time: 0.4815\n",
      "277/388, train_loss: 0.3240, step time: 0.4959\n",
      "278/388, train_loss: 0.3069, step time: 0.5102\n",
      "279/388, train_loss: 0.1863, step time: 0.5057\n",
      "280/388, train_loss: 0.3342, step time: 0.9908\n",
      "281/388, train_loss: 0.2150, step time: 0.5341\n",
      "282/388, train_loss: 0.3831, step time: 0.5109\n",
      "283/388, train_loss: 0.5473, step time: 0.4942\n",
      "284/388, train_loss: 0.1814, step time: 0.4947\n",
      "285/388, train_loss: 0.1720, step time: 0.4823\n",
      "286/388, train_loss: 0.1856, step time: 0.4867\n",
      "287/388, train_loss: 0.3190, step time: 0.4832\n",
      "288/388, train_loss: 0.5647, step time: 0.4903\n",
      "289/388, train_loss: 0.2895, step time: 0.9446\n",
      "290/388, train_loss: 0.2152, step time: 0.5364\n",
      "291/388, train_loss: 0.2476, step time: 0.5116\n",
      "292/388, train_loss: 0.2725, step time: 0.4945\n",
      "293/388, train_loss: 0.2471, step time: 0.4977\n",
      "294/388, train_loss: 0.4516, step time: 0.4853\n",
      "295/388, train_loss: 0.2206, step time: 0.5232\n",
      "296/388, train_loss: 0.0704, step time: 0.4925\n",
      "297/388, train_loss: 0.7224, step time: 0.4934\n",
      "298/388, train_loss: 0.4283, step time: 0.4964\n",
      "299/388, train_loss: 0.1275, step time: 1.1247\n",
      "300/388, train_loss: 0.2849, step time: 0.5367\n",
      "301/388, train_loss: 0.1484, step time: 0.4995\n",
      "302/388, train_loss: 0.1530, step time: 0.4931\n",
      "303/388, train_loss: 0.0883, step time: 0.4910\n",
      "304/388, train_loss: 0.3080, step time: 0.4831\n",
      "305/388, train_loss: 0.1345, step time: 0.4857\n",
      "306/388, train_loss: 0.0989, step time: 0.4747\n",
      "307/388, train_loss: 0.2856, step time: 0.4853\n",
      "308/388, train_loss: 0.1965, step time: 0.4737\n",
      "309/388, train_loss: 0.3396, step time: 0.8904\n",
      "310/388, train_loss: 0.3200, step time: 0.5434\n",
      "311/388, train_loss: 0.2519, step time: 0.5003\n",
      "312/388, train_loss: 0.2287, step time: 0.4942\n",
      "313/388, train_loss: 0.3030, step time: 0.4938\n",
      "314/388, train_loss: 0.0795, step time: 0.4865\n",
      "315/388, train_loss: 0.1644, step time: 0.4904\n",
      "316/388, train_loss: 0.5119, step time: 0.4766\n",
      "317/388, train_loss: 0.1573, step time: 0.4757\n",
      "318/388, train_loss: 0.2303, step time: 1.1351\n",
      "319/388, train_loss: 0.1949, step time: 0.5401\n",
      "320/388, train_loss: 0.4423, step time: 0.5085\n",
      "321/388, train_loss: 0.1697, step time: 0.4969\n",
      "322/388, train_loss: 0.1371, step time: 0.4928\n",
      "323/388, train_loss: 0.1824, step time: 0.4768\n",
      "324/388, train_loss: 0.1836, step time: 0.4852\n",
      "325/388, train_loss: 0.2409, step time: 0.4970\n",
      "326/388, train_loss: 0.2135, step time: 0.4849\n",
      "327/388, train_loss: 0.1737, step time: 0.4966\n",
      "328/388, train_loss: 0.3056, step time: 0.4969\n",
      "329/388, train_loss: 0.2208, step time: 1.1109\n",
      "330/388, train_loss: 0.3285, step time: 0.5264\n",
      "331/388, train_loss: 0.2883, step time: 0.5124\n",
      "332/388, train_loss: 0.3094, step time: 0.4928\n",
      "333/388, train_loss: 0.1039, step time: 0.5001\n",
      "334/388, train_loss: 0.6927, step time: 0.4807\n",
      "335/388, train_loss: 0.1812, step time: 0.4825\n",
      "336/388, train_loss: 0.2550, step time: 0.5368\n",
      "337/388, train_loss: 0.2611, step time: 0.5235\n",
      "338/388, train_loss: 0.1637, step time: 0.5024\n",
      "339/388, train_loss: 0.3433, step time: 0.4982\n",
      "340/388, train_loss: 0.1818, step time: 0.4859\n",
      "341/388, train_loss: 0.1270, step time: 0.6920\n",
      "342/388, train_loss: 0.3351, step time: 0.5363\n",
      "343/388, train_loss: 0.2407, step time: 0.4995\n",
      "344/388, train_loss: 0.1920, step time: 0.5039\n",
      "345/388, train_loss: 0.1861, step time: 0.4974\n",
      "346/388, train_loss: 0.4050, step time: 0.4851\n",
      "347/388, train_loss: 0.1816, step time: 0.4909\n",
      "348/388, train_loss: 0.4017, step time: 0.4862\n",
      "349/388, train_loss: 0.2955, step time: 0.4912\n",
      "350/388, train_loss: 0.0931, step time: 0.4902\n",
      "351/388, train_loss: 0.0770, step time: 0.5041\n",
      "352/388, train_loss: 0.2044, step time: 0.5013\n",
      "353/388, train_loss: 0.1470, step time: 0.4882\n",
      "354/388, train_loss: 0.1384, step time: 0.4851\n",
      "355/388, train_loss: 0.3327, step time: 0.4812\n",
      "356/388, train_loss: 0.2169, step time: 0.4760\n",
      "357/388, train_loss: 0.2922, step time: 0.4947\n",
      "358/388, train_loss: 0.1796, step time: 0.5084\n",
      "359/388, train_loss: 0.0992, step time: 0.5043\n",
      "360/388, train_loss: 0.2418, step time: 0.4998\n",
      "361/388, train_loss: 0.2508, step time: 0.4820\n",
      "362/388, train_loss: 0.1419, step time: 0.4797\n",
      "363/388, train_loss: 0.2320, step time: 1.2393\n",
      "364/388, train_loss: 0.4037, step time: 0.5276\n",
      "365/388, train_loss: 0.3352, step time: 0.5038\n",
      "366/388, train_loss: 0.3053, step time: 0.4964\n",
      "367/388, train_loss: 0.1688, step time: 0.4843\n",
      "368/388, train_loss: 0.2295, step time: 0.4934\n",
      "369/388, train_loss: 0.0929, step time: 0.5329\n",
      "370/388, train_loss: 0.2775, step time: 0.5106\n",
      "371/388, train_loss: 0.6031, step time: 0.4932\n",
      "372/388, train_loss: 0.2823, step time: 0.4967\n",
      "373/388, train_loss: 0.1909, step time: 0.4826\n",
      "374/388, train_loss: 0.1099, step time: 0.4830\n",
      "375/388, train_loss: 0.2369, step time: 0.5039\n",
      "376/388, train_loss: 0.5946, step time: 0.4891\n",
      "377/388, train_loss: 0.1690, step time: 0.4781\n",
      "378/388, train_loss: 0.2649, step time: 0.4958\n",
      "379/388, train_loss: 0.1907, step time: 0.4964\n",
      "380/388, train_loss: 0.3260, step time: 0.4814\n",
      "381/388, train_loss: 0.1835, step time: 0.5133\n",
      "382/388, train_loss: 0.1854, step time: 0.5752\n",
      "383/388, train_loss: 0.2245, step time: 0.5796\n",
      "384/388, train_loss: 0.1725, step time: 0.5249\n",
      "385/388, train_loss: 0.1566, step time: 0.5043\n",
      "386/388, train_loss: 0.2353, step time: 0.4878\n",
      "387/388, train_loss: 0.1947, step time: 0.5006\n",
      "388/388, train_loss: 0.3953, step time: 0.4974\n",
      "epoch 12 average loss: 0.2664\n",
      "current epoch: 12 current mean dice: 0.6627 tc: 0.6948 wt: 0.8414 et: 0.4519\n",
      "best mean dice: 0.7004 at epoch: 11\n",
      "time consuming of epoch 12 is: 298.0366\n",
      "----------\n",
      "epoch 13/300\n",
      "1/388, train_loss: 0.4191, step time: 0.4703\n",
      "2/388, train_loss: 0.2752, step time: 0.4919\n",
      "3/388, train_loss: 0.1419, step time: 0.9470\n",
      "4/388, train_loss: 0.3377, step time: 0.5794\n",
      "5/388, train_loss: 0.1902, step time: 0.5513\n",
      "6/388, train_loss: 0.1491, step time: 0.5142\n",
      "7/388, train_loss: 0.5412, step time: 0.5001\n",
      "8/388, train_loss: 0.0511, step time: 0.4927\n",
      "9/388, train_loss: 0.1045, step time: 0.5408\n",
      "10/388, train_loss: 0.3758, step time: 0.5354\n",
      "11/388, train_loss: 0.2619, step time: 0.5433\n",
      "12/388, train_loss: 0.1342, step time: 0.4987\n",
      "13/388, train_loss: 0.3051, step time: 0.5880\n",
      "14/388, train_loss: 0.3781, step time: 0.5711\n",
      "15/388, train_loss: 0.2075, step time: 0.5223\n",
      "16/388, train_loss: 0.3209, step time: 0.4856\n",
      "17/388, train_loss: 0.1069, step time: 1.0846\n",
      "18/388, train_loss: 0.4583, step time: 0.5602\n",
      "19/388, train_loss: 0.4392, step time: 0.5228\n",
      "20/388, train_loss: 0.2296, step time: 0.4952\n",
      "21/388, train_loss: 0.2439, step time: 0.5494\n",
      "22/388, train_loss: 0.1711, step time: 0.6747\n",
      "23/388, train_loss: 0.1418, step time: 0.5505\n",
      "24/388, train_loss: 0.1479, step time: 0.5373\n",
      "25/388, train_loss: 0.3074, step time: 0.5907\n",
      "26/388, train_loss: 0.2776, step time: 0.5420\n",
      "27/388, train_loss: 0.2008, step time: 0.5236\n",
      "28/388, train_loss: 0.2059, step time: 0.5010\n",
      "29/388, train_loss: 0.1832, step time: 0.5145\n",
      "30/388, train_loss: 0.1212, step time: 0.6122\n",
      "31/388, train_loss: 0.2372, step time: 0.5786\n",
      "32/388, train_loss: 0.1311, step time: 0.5504\n",
      "33/388, train_loss: 0.2625, step time: 0.5095\n",
      "34/388, train_loss: 0.3482, step time: 0.5021\n",
      "35/388, train_loss: 0.1648, step time: 0.5351\n",
      "36/388, train_loss: 0.1337, step time: 0.5239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/388, train_loss: 0.1700, step time: 0.4986\n",
      "38/388, train_loss: 0.0434, step time: 0.5830\n",
      "39/388, train_loss: 0.1071, step time: 0.5711\n",
      "40/388, train_loss: 0.2736, step time: 0.5410\n",
      "41/388, train_loss: 0.1304, step time: 0.5116\n",
      "42/388, train_loss: 0.1804, step time: 0.5105\n",
      "43/388, train_loss: 0.0922, step time: 0.5780\n",
      "44/388, train_loss: 0.2313, step time: 0.6082\n",
      "45/388, train_loss: 0.5095, step time: 0.5435\n",
      "46/388, train_loss: 0.1562, step time: 0.5117\n",
      "47/388, train_loss: 0.3059, step time: 0.5043\n",
      "48/388, train_loss: 0.1280, step time: 0.4847\n",
      "49/388, train_loss: 0.1078, step time: 0.4842\n",
      "50/388, train_loss: 0.1791, step time: 0.5053\n",
      "51/388, train_loss: 0.1954, step time: 0.5190\n",
      "52/388, train_loss: 0.2299, step time: 0.5891\n",
      "53/388, train_loss: 0.2471, step time: 0.5413\n",
      "54/388, train_loss: 0.1055, step time: 0.5001\n",
      "55/388, train_loss: 0.3308, step time: 0.5867\n",
      "56/388, train_loss: 0.1576, step time: 0.5817\n",
      "57/388, train_loss: 0.1251, step time: 0.5414\n",
      "58/388, train_loss: 0.1223, step time: 0.5030\n",
      "59/388, train_loss: 0.3725, step time: 0.4952\n",
      "60/388, train_loss: 0.4861, step time: 0.5095\n",
      "61/388, train_loss: 0.4246, step time: 0.6215\n",
      "62/388, train_loss: 0.2920, step time: 0.5650\n",
      "63/388, train_loss: 0.2184, step time: 0.5085\n",
      "64/388, train_loss: 0.3974, step time: 0.4934\n",
      "65/388, train_loss: 0.2539, step time: 1.0491\n",
      "66/388, train_loss: 0.2161, step time: 0.5380\n",
      "67/388, train_loss: 0.3384, step time: 0.5186\n",
      "68/388, train_loss: 0.5448, step time: 0.5736\n",
      "69/388, train_loss: 0.1576, step time: 0.5295\n",
      "70/388, train_loss: 0.2156, step time: 0.5134\n",
      "71/388, train_loss: 0.3531, step time: 0.4911\n",
      "72/388, train_loss: 0.2284, step time: 0.4987\n",
      "73/388, train_loss: 0.2731, step time: 0.5146\n",
      "74/388, train_loss: 0.1279, step time: 0.5646\n",
      "75/388, train_loss: 0.2767, step time: 0.7128\n",
      "76/388, train_loss: 0.1654, step time: 0.5567\n",
      "77/388, train_loss: 0.3160, step time: 0.5266\n",
      "78/388, train_loss: 0.2552, step time: 0.5092\n",
      "79/388, train_loss: 0.1547, step time: 0.4893\n",
      "80/388, train_loss: 0.5278, step time: 0.4851\n",
      "81/388, train_loss: 0.6079, step time: 0.9764\n",
      "82/388, train_loss: 0.1868, step time: 0.5382\n",
      "83/388, train_loss: 0.2393, step time: 0.5149\n",
      "84/388, train_loss: 0.2149, step time: 0.4925\n",
      "85/388, train_loss: 0.0790, step time: 0.5180\n",
      "86/388, train_loss: 0.2382, step time: 0.4996\n",
      "87/388, train_loss: 0.3821, step time: 0.5049\n",
      "88/388, train_loss: 0.1489, step time: 0.4946\n",
      "89/388, train_loss: 0.4360, step time: 0.5022\n",
      "90/388, train_loss: 0.1993, step time: 0.4965\n",
      "91/388, train_loss: 0.1987, step time: 0.4972\n",
      "92/388, train_loss: 0.0947, step time: 0.5183\n",
      "93/388, train_loss: 0.6007, step time: 0.4957\n",
      "94/388, train_loss: 0.2474, step time: 0.5020\n",
      "95/388, train_loss: 0.1348, step time: 0.5277\n",
      "96/388, train_loss: 0.5626, step time: 0.5114\n",
      "97/388, train_loss: 0.1942, step time: 0.5225\n",
      "98/388, train_loss: 0.1838, step time: 0.4939\n",
      "99/388, train_loss: 0.2987, step time: 0.5167\n",
      "100/388, train_loss: 0.2379, step time: 0.6056\n",
      "101/388, train_loss: 0.3482, step time: 0.5795\n",
      "102/388, train_loss: 0.1447, step time: 0.5351\n",
      "103/388, train_loss: 0.1189, step time: 0.5212\n",
      "104/388, train_loss: 0.3072, step time: 0.5263\n",
      "105/388, train_loss: 0.4365, step time: 0.5657\n",
      "106/388, train_loss: 0.3634, step time: 0.5726\n",
      "107/388, train_loss: 0.2770, step time: 0.5240\n",
      "108/388, train_loss: 0.2957, step time: 0.5038\n",
      "109/388, train_loss: 0.1588, step time: 0.4986\n",
      "110/388, train_loss: 0.1833, step time: 0.5650\n",
      "111/388, train_loss: 0.3040, step time: 0.5460\n",
      "112/388, train_loss: 0.2837, step time: 0.5100\n",
      "113/388, train_loss: 0.2985, step time: 0.4916\n",
      "114/388, train_loss: 0.4575, step time: 1.1089\n",
      "115/388, train_loss: 0.1956, step time: 0.5288\n",
      "116/388, train_loss: 0.5049, step time: 0.5000\n",
      "117/388, train_loss: 0.2373, step time: 0.8708\n",
      "118/388, train_loss: 0.0923, step time: 0.5399\n",
      "119/388, train_loss: 0.2613, step time: 0.5045\n",
      "120/388, train_loss: 0.6752, step time: 0.4948\n",
      "121/388, train_loss: 0.1093, step time: 0.4845\n",
      "122/388, train_loss: 0.1518, step time: 0.4919\n",
      "123/388, train_loss: 0.2245, step time: 0.4860\n",
      "124/388, train_loss: 0.2182, step time: 0.5019\n",
      "125/388, train_loss: 0.3473, step time: 0.4848\n",
      "126/388, train_loss: 0.3866, step time: 0.4790\n",
      "127/388, train_loss: 0.1131, step time: 0.5080\n",
      "128/388, train_loss: 0.2254, step time: 0.5119\n",
      "129/388, train_loss: 0.5383, step time: 0.6247\n",
      "130/388, train_loss: 0.1157, step time: 0.5371\n",
      "131/388, train_loss: 0.4772, step time: 0.5070\n",
      "132/388, train_loss: 0.2264, step time: 0.4904\n",
      "133/388, train_loss: 0.1404, step time: 0.5356\n",
      "134/388, train_loss: 0.3064, step time: 0.5272\n",
      "135/388, train_loss: 0.1555, step time: 0.5078\n",
      "136/388, train_loss: 0.1925, step time: 0.5049\n",
      "137/388, train_loss: 0.1546, step time: 0.5530\n",
      "138/388, train_loss: 0.2299, step time: 0.5350\n",
      "139/388, train_loss: 0.1415, step time: 0.5045\n",
      "140/388, train_loss: 0.5285, step time: 0.5024\n",
      "141/388, train_loss: 0.1543, step time: 0.5012\n",
      "142/388, train_loss: 0.3462, step time: 0.4948\n",
      "143/388, train_loss: 0.3742, step time: 0.5546\n",
      "144/388, train_loss: 0.1671, step time: 0.5322\n",
      "145/388, train_loss: 0.1810, step time: 0.5074\n",
      "146/388, train_loss: 0.1949, step time: 0.4988\n",
      "147/388, train_loss: 0.1799, step time: 1.1708\n",
      "148/388, train_loss: 0.2368, step time: 0.5275\n",
      "149/388, train_loss: 0.1666, step time: 0.5076\n",
      "150/388, train_loss: 0.1615, step time: 0.4925\n",
      "151/388, train_loss: 0.2244, step time: 0.4947\n",
      "152/388, train_loss: 0.1330, step time: 0.4807\n",
      "153/388, train_loss: 0.3099, step time: 0.4953\n",
      "154/388, train_loss: 0.4780, step time: 0.5192\n",
      "155/388, train_loss: 0.2056, step time: 0.5093\n",
      "156/388, train_loss: 0.1625, step time: 0.5019\n",
      "157/388, train_loss: 0.1934, step time: 0.4817\n",
      "158/388, train_loss: 0.2166, step time: 0.4948\n",
      "159/388, train_loss: 0.2040, step time: 0.9440\n",
      "160/388, train_loss: 0.3472, step time: 0.5420\n",
      "161/388, train_loss: 0.2529, step time: 0.5159\n",
      "162/388, train_loss: 0.1454, step time: 0.4871\n",
      "163/388, train_loss: 0.2654, step time: 0.4899\n",
      "164/388, train_loss: 0.2368, step time: 0.4921\n",
      "165/388, train_loss: 0.5118, step time: 0.4883\n",
      "166/388, train_loss: 0.0817, step time: 0.4977\n",
      "167/388, train_loss: 0.1454, step time: 0.4842\n",
      "168/388, train_loss: 0.1451, step time: 0.4911\n",
      "169/388, train_loss: 0.3771, step time: 0.4942\n",
      "170/388, train_loss: 0.5030, step time: 1.1057\n",
      "171/388, train_loss: 0.1024, step time: 0.5337\n",
      "172/388, train_loss: 0.3776, step time: 0.5195\n",
      "173/388, train_loss: 0.1354, step time: 0.4909\n",
      "174/388, train_loss: 0.1544, step time: 0.4883\n",
      "175/388, train_loss: 0.3536, step time: 0.5021\n",
      "176/388, train_loss: 0.0753, step time: 0.5884\n",
      "177/388, train_loss: 0.7324, step time: 0.5412\n",
      "178/388, train_loss: 0.1517, step time: 0.5163\n",
      "179/388, train_loss: 0.1006, step time: 0.5050\n",
      "180/388, train_loss: 0.1497, step time: 0.5061\n",
      "181/388, train_loss: 0.1702, step time: 0.4834\n",
      "182/388, train_loss: 0.1668, step time: 0.4995\n",
      "183/388, train_loss: 0.1520, step time: 0.4877\n",
      "184/388, train_loss: 0.1987, step time: 0.4986\n",
      "185/388, train_loss: 0.3420, step time: 0.5205\n",
      "186/388, train_loss: 0.0843, step time: 0.5072\n",
      "187/388, train_loss: 0.0900, step time: 0.5186\n",
      "188/388, train_loss: 0.4886, step time: 0.5027\n",
      "189/388, train_loss: 0.1647, step time: 0.4874\n",
      "190/388, train_loss: 0.1150, step time: 0.5057\n",
      "191/388, train_loss: 0.6915, step time: 0.5007\n",
      "192/388, train_loss: 0.1001, step time: 0.5607\n",
      "193/388, train_loss: 0.1548, step time: 0.5463\n",
      "194/388, train_loss: 0.1809, step time: 0.5137\n",
      "195/388, train_loss: 0.1182, step time: 0.4993\n",
      "196/388, train_loss: 0.1580, step time: 0.4990\n",
      "197/388, train_loss: 0.4213, step time: 1.1849\n",
      "198/388, train_loss: 0.3174, step time: 0.5330\n",
      "199/388, train_loss: 0.1816, step time: 0.5093\n",
      "200/388, train_loss: 0.2549, step time: 0.4911\n",
      "201/388, train_loss: 0.2792, step time: 0.4862\n",
      "202/388, train_loss: 0.1740, step time: 0.4864\n",
      "203/388, train_loss: 0.1566, step time: 0.4746\n",
      "204/388, train_loss: 0.1433, step time: 0.4768\n",
      "205/388, train_loss: 0.1715, step time: 0.4711\n",
      "206/388, train_loss: 0.1175, step time: 1.0424\n",
      "207/388, train_loss: 0.2280, step time: 0.5259\n",
      "208/388, train_loss: 0.2934, step time: 0.5081\n",
      "209/388, train_loss: 0.1771, step time: 0.4916\n",
      "210/388, train_loss: 0.4120, step time: 0.4910\n",
      "211/388, train_loss: 0.1549, step time: 0.4987\n",
      "212/388, train_loss: 0.3196, step time: 0.4836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/388, train_loss: 0.1949, step time: 0.5144\n",
      "214/388, train_loss: 0.1532, step time: 0.5038\n",
      "215/388, train_loss: 0.2269, step time: 0.5021\n",
      "216/388, train_loss: 0.2387, step time: 0.4913\n",
      "217/388, train_loss: 0.1026, step time: 0.5309\n",
      "218/388, train_loss: 0.2144, step time: 0.5199\n",
      "219/388, train_loss: 0.3314, step time: 0.5142\n",
      "220/388, train_loss: 0.4143, step time: 0.4949\n",
      "221/388, train_loss: 0.5363, step time: 0.5102\n",
      "222/388, train_loss: 0.5419, step time: 0.4950\n",
      "223/388, train_loss: 0.1030, step time: 0.4951\n",
      "224/388, train_loss: 0.2163, step time: 0.4795\n",
      "225/388, train_loss: 0.6655, step time: 0.6653\n",
      "226/388, train_loss: 0.1843, step time: 0.5502\n",
      "227/388, train_loss: 0.6913, step time: 0.5368\n",
      "228/388, train_loss: 0.2135, step time: 0.5077\n",
      "229/388, train_loss: 0.2448, step time: 0.4970\n",
      "230/388, train_loss: 0.1434, step time: 0.5110\n",
      "231/388, train_loss: 0.1272, step time: 0.4955\n",
      "232/388, train_loss: 0.3194, step time: 0.4812\n",
      "233/388, train_loss: 0.1502, step time: 0.5161\n",
      "234/388, train_loss: 0.1318, step time: 0.5019\n",
      "235/388, train_loss: 0.1044, step time: 0.5383\n",
      "236/388, train_loss: 0.2012, step time: 0.5262\n",
      "237/388, train_loss: 0.1677, step time: 0.4996\n",
      "238/388, train_loss: 0.6482, step time: 0.4958\n",
      "239/388, train_loss: 0.2005, step time: 0.7345\n",
      "240/388, train_loss: 0.3309, step time: 0.5407\n",
      "241/388, train_loss: 0.1868, step time: 0.5137\n",
      "242/388, train_loss: 0.1327, step time: 0.5011\n",
      "243/388, train_loss: 0.3404, step time: 0.5091\n",
      "244/388, train_loss: 0.0745, step time: 0.4973\n",
      "245/388, train_loss: 0.1215, step time: 0.4924\n",
      "246/388, train_loss: 0.3526, step time: 0.5145\n",
      "247/388, train_loss: 0.2797, step time: 0.4840\n",
      "248/388, train_loss: 0.1634, step time: 0.4811\n",
      "249/388, train_loss: 0.5634, step time: 0.4971\n",
      "250/388, train_loss: 0.1962, step time: 0.4999\n",
      "251/388, train_loss: 0.2340, step time: 0.4875\n",
      "252/388, train_loss: 0.0634, step time: 0.4919\n",
      "253/388, train_loss: 0.2840, step time: 0.9030\n",
      "254/388, train_loss: 0.1466, step time: 0.5494\n",
      "255/388, train_loss: 0.1310, step time: 0.5229\n",
      "256/388, train_loss: 0.1901, step time: 0.5008\n",
      "257/388, train_loss: 0.1990, step time: 0.4841\n",
      "258/388, train_loss: 0.2087, step time: 0.4943\n",
      "259/388, train_loss: 0.1106, step time: 0.4848\n",
      "260/388, train_loss: 0.1410, step time: 0.7383\n",
      "261/388, train_loss: 0.2055, step time: 0.5770\n",
      "262/388, train_loss: 0.3061, step time: 0.5239\n",
      "263/388, train_loss: 0.4830, step time: 0.5165\n",
      "264/388, train_loss: 0.2578, step time: 0.4932\n",
      "265/388, train_loss: 0.1500, step time: 0.5060\n",
      "266/388, train_loss: 0.3728, step time: 0.4942\n",
      "267/388, train_loss: 0.1829, step time: 0.4824\n",
      "268/388, train_loss: 0.3272, step time: 0.4940\n",
      "269/388, train_loss: 0.2441, step time: 0.4832\n",
      "270/388, train_loss: 0.2072, step time: 0.4891\n",
      "271/388, train_loss: 0.1173, step time: 1.1535\n",
      "272/388, train_loss: 0.6156, step time: 0.5237\n",
      "273/388, train_loss: 0.7757, step time: 0.5028\n",
      "274/388, train_loss: 0.3388, step time: 0.4893\n",
      "275/388, train_loss: 0.3596, step time: 0.4957\n",
      "276/388, train_loss: 0.1427, step time: 0.4804\n",
      "277/388, train_loss: 0.2649, step time: 0.4792\n",
      "278/388, train_loss: 0.1621, step time: 1.1423\n",
      "279/388, train_loss: 0.2153, step time: 0.5163\n",
      "280/388, train_loss: 0.2148, step time: 0.5029\n",
      "281/388, train_loss: 0.3653, step time: 0.4847\n",
      "282/388, train_loss: 0.0920, step time: 0.5026\n",
      "283/388, train_loss: 0.1933, step time: 0.4863\n",
      "284/388, train_loss: 0.2248, step time: 0.4739\n",
      "285/388, train_loss: 0.1313, step time: 0.9125\n",
      "286/388, train_loss: 0.4235, step time: 0.5543\n",
      "287/388, train_loss: 0.2809, step time: 0.5184\n",
      "288/388, train_loss: 0.2592, step time: 0.4997\n",
      "289/388, train_loss: 0.1948, step time: 0.4896\n",
      "290/388, train_loss: 0.7763, step time: 0.4890\n",
      "291/388, train_loss: 0.2871, step time: 0.4787\n",
      "292/388, train_loss: 0.1050, step time: 0.4776\n",
      "293/388, train_loss: 0.1393, step time: 0.4798\n",
      "294/388, train_loss: 0.4792, step time: 0.8787\n",
      "295/388, train_loss: 0.2130, step time: 0.5375\n",
      "296/388, train_loss: 0.1360, step time: 0.5136\n",
      "297/388, train_loss: 0.2739, step time: 0.4931\n",
      "298/388, train_loss: 0.1829, step time: 0.4822\n",
      "299/388, train_loss: 0.2791, step time: 0.4886\n",
      "300/388, train_loss: 0.2034, step time: 0.4743\n",
      "301/388, train_loss: 0.3202, step time: 0.4888\n",
      "302/388, train_loss: 0.2308, step time: 0.7300\n",
      "303/388, train_loss: 0.2205, step time: 0.5364\n",
      "304/388, train_loss: 0.1364, step time: 0.5077\n",
      "305/388, train_loss: 0.4353, step time: 0.4971\n",
      "306/388, train_loss: 0.3218, step time: 0.4882\n",
      "307/388, train_loss: 0.1744, step time: 0.4905\n",
      "308/388, train_loss: 0.1415, step time: 0.4776\n",
      "309/388, train_loss: 0.2611, step time: 0.4773\n",
      "310/388, train_loss: 0.6358, step time: 0.4801\n",
      "311/388, train_loss: 0.2545, step time: 0.9237\n",
      "312/388, train_loss: 0.1665, step time: 0.5248\n",
      "313/388, train_loss: 0.3951, step time: 0.5082\n",
      "314/388, train_loss: 0.3550, step time: 0.4858\n",
      "315/388, train_loss: 0.2467, step time: 0.4799\n",
      "316/388, train_loss: 0.1961, step time: 0.4856\n",
      "317/388, train_loss: 0.1543, step time: 0.7406\n",
      "318/388, train_loss: 0.1299, step time: 0.5412\n",
      "319/388, train_loss: 0.4196, step time: 0.5090\n",
      "320/388, train_loss: 0.3072, step time: 0.4942\n",
      "321/388, train_loss: 0.1195, step time: 0.4964\n",
      "322/388, train_loss: 0.3542, step time: 0.4782\n",
      "323/388, train_loss: 0.1961, step time: 0.4701\n",
      "324/388, train_loss: 0.2931, step time: 0.7129\n",
      "325/388, train_loss: 0.2762, step time: 0.5411\n",
      "326/388, train_loss: 0.1270, step time: 0.5156\n",
      "327/388, train_loss: 0.2117, step time: 0.5014\n",
      "328/388, train_loss: 0.3160, step time: 0.4878\n",
      "329/388, train_loss: 0.2641, step time: 0.4853\n",
      "330/388, train_loss: 0.1745, step time: 0.5004\n",
      "331/388, train_loss: 0.4271, step time: 0.4798\n",
      "332/388, train_loss: 0.2838, step time: 0.4837\n",
      "333/388, train_loss: 0.2080, step time: 0.4805\n",
      "334/388, train_loss: 0.1467, step time: 1.2315\n",
      "335/388, train_loss: 0.1694, step time: 0.5153\n",
      "336/388, train_loss: 0.1615, step time: 0.4953\n",
      "337/388, train_loss: 0.1918, step time: 0.4840\n",
      "338/388, train_loss: 0.3835, step time: 0.4922\n",
      "339/388, train_loss: 0.1993, step time: 0.4746\n",
      "340/388, train_loss: 0.1265, step time: 0.4722\n",
      "341/388, train_loss: 0.4259, step time: 0.4830\n",
      "342/388, train_loss: 0.0811, step time: 0.4744\n",
      "343/388, train_loss: 0.1493, step time: 0.4775\n",
      "344/388, train_loss: 0.0939, step time: 0.4754\n",
      "345/388, train_loss: 0.4034, step time: 1.0097\n",
      "346/388, train_loss: 0.4802, step time: 0.5333\n",
      "347/388, train_loss: 0.3167, step time: 0.5016\n",
      "348/388, train_loss: 0.1067, step time: 0.4916\n",
      "349/388, train_loss: 0.2151, step time: 0.4832\n",
      "350/388, train_loss: 0.3568, step time: 0.4925\n",
      "351/388, train_loss: 0.2622, step time: 0.4787\n",
      "352/388, train_loss: 0.4508, step time: 0.4727\n",
      "353/388, train_loss: 0.2463, step time: 0.4994\n",
      "354/388, train_loss: 0.0934, step time: 0.5452\n",
      "355/388, train_loss: 0.4928, step time: 0.5335\n",
      "356/388, train_loss: 0.1803, step time: 0.5051\n",
      "357/388, train_loss: 0.1477, step time: 0.4898\n",
      "358/388, train_loss: 0.2043, step time: 0.4892\n",
      "359/388, train_loss: 0.2441, step time: 0.4825\n",
      "360/388, train_loss: 0.1291, step time: 0.4929\n",
      "361/388, train_loss: 0.6062, step time: 0.4759\n",
      "362/388, train_loss: 0.0833, step time: 0.9607\n",
      "363/388, train_loss: 0.2888, step time: 0.5403\n",
      "364/388, train_loss: 0.5345, step time: 0.5074\n",
      "365/388, train_loss: 0.6885, step time: 0.4885\n",
      "366/388, train_loss: 0.2950, step time: 0.4945\n",
      "367/388, train_loss: 0.2168, step time: 0.4942\n",
      "368/388, train_loss: 0.1576, step time: 0.4819\n",
      "369/388, train_loss: 0.3866, step time: 0.4903\n",
      "370/388, train_loss: 0.2433, step time: 0.4901\n",
      "371/388, train_loss: 0.4225, step time: 0.4772\n",
      "372/388, train_loss: 0.4736, step time: 0.4853\n",
      "373/388, train_loss: 0.3164, step time: 0.7568\n",
      "374/388, train_loss: 0.3903, step time: 0.5386\n",
      "375/388, train_loss: 0.1182, step time: 0.5174\n",
      "376/388, train_loss: 0.3725, step time: 0.4920\n",
      "377/388, train_loss: 0.3234, step time: 0.4939\n",
      "378/388, train_loss: 0.1446, step time: 0.4784\n",
      "379/388, train_loss: 0.2302, step time: 0.9710\n",
      "380/388, train_loss: 0.5613, step time: 0.5498\n",
      "381/388, train_loss: 0.2561, step time: 0.5074\n",
      "382/388, train_loss: 0.1950, step time: 0.5004\n",
      "383/388, train_loss: 0.1668, step time: 0.4823\n",
      "384/388, train_loss: 0.3325, step time: 0.4751\n",
      "385/388, train_loss: 0.2162, step time: 0.4920\n",
      "386/388, train_loss: 0.6383, step time: 0.4783\n",
      "387/388, train_loss: 0.1465, step time: 0.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388/388, train_loss: 0.1479, step time: 0.4726\n",
      "epoch 13 average loss: 0.2575\n",
      "saved new best metric model\n",
      "current epoch: 13 current mean dice: 0.7174 tc: 0.7734 wt: 0.8618 et: 0.5172\n",
      "best mean dice: 0.7174 at epoch: 13\n",
      "time consuming of epoch 13 is: 298.6160\n",
      "----------\n",
      "epoch 14/300\n",
      "1/388, train_loss: 0.1646, step time: 0.4694\n",
      "2/388, train_loss: 0.0828, step time: 0.4836\n",
      "3/388, train_loss: 0.0949, step time: 0.4733\n",
      "4/388, train_loss: 0.1728, step time: 1.0567\n",
      "5/388, train_loss: 0.2127, step time: 0.5416\n",
      "6/388, train_loss: 0.1033, step time: 0.5317\n",
      "7/388, train_loss: 0.2113, step time: 0.5119\n",
      "8/388, train_loss: 0.3239, step time: 0.5510\n",
      "9/388, train_loss: 0.1963, step time: 0.5055\n",
      "10/388, train_loss: 0.1899, step time: 0.5367\n",
      "11/388, train_loss: 0.3159, step time: 0.6364\n",
      "12/388, train_loss: 0.2275, step time: 0.5631\n",
      "13/388, train_loss: 0.1674, step time: 0.5340\n",
      "14/388, train_loss: 0.2721, step time: 0.5031\n",
      "15/388, train_loss: 0.2176, step time: 0.5005\n",
      "16/388, train_loss: 0.1968, step time: 0.5583\n",
      "17/388, train_loss: 0.2193, step time: 0.5137\n",
      "18/388, train_loss: 0.1300, step time: 0.4914\n",
      "19/388, train_loss: 0.1558, step time: 0.7465\n",
      "20/388, train_loss: 0.1696, step time: 0.6019\n",
      "21/388, train_loss: 0.6310, step time: 0.5354\n",
      "22/388, train_loss: 0.3006, step time: 0.5226\n",
      "23/388, train_loss: 0.1154, step time: 0.5269\n",
      "24/388, train_loss: 0.1667, step time: 0.5998\n",
      "25/388, train_loss: 0.1735, step time: 0.5377\n",
      "26/388, train_loss: 0.2172, step time: 0.5000\n",
      "27/388, train_loss: 0.1531, step time: 0.4895\n",
      "28/388, train_loss: 0.5182, step time: 1.0202\n",
      "29/388, train_loss: 0.1951, step time: 0.5479\n",
      "30/388, train_loss: 0.2808, step time: 0.5206\n",
      "31/388, train_loss: 0.3188, step time: 0.4920\n",
      "32/388, train_loss: 0.2150, step time: 0.4925\n",
      "33/388, train_loss: 0.2986, step time: 0.4891\n",
      "34/388, train_loss: 0.3397, step time: 0.4987\n",
      "35/388, train_loss: 0.2021, step time: 0.4920\n",
      "36/388, train_loss: 0.1850, step time: 0.4905\n",
      "37/388, train_loss: 0.2598, step time: 0.6609\n",
      "38/388, train_loss: 0.2746, step time: 0.5552\n",
      "39/388, train_loss: 0.1550, step time: 0.5333\n",
      "40/388, train_loss: 0.3678, step time: 0.5139\n",
      "41/388, train_loss: 0.3431, step time: 0.5495\n",
      "42/388, train_loss: 0.1748, step time: 0.5078\n",
      "43/388, train_loss: 0.1557, step time: 0.4872\n",
      "44/388, train_loss: 0.3120, step time: 0.7006\n",
      "45/388, train_loss: 0.4202, step time: 0.5551\n",
      "46/388, train_loss: 0.1532, step time: 0.5257\n",
      "47/388, train_loss: 0.1171, step time: 0.5110\n",
      "48/388, train_loss: 0.4693, step time: 0.4943\n",
      "49/388, train_loss: 0.5415, step time: 0.4973\n",
      "50/388, train_loss: 0.3717, step time: 0.5640\n",
      "51/388, train_loss: 0.2527, step time: 0.5295\n",
      "52/388, train_loss: 0.2352, step time: 0.5022\n",
      "53/388, train_loss: 0.1395, step time: 0.4873\n",
      "54/388, train_loss: 0.2914, step time: 0.4969\n",
      "55/388, train_loss: 0.3511, step time: 0.5181\n",
      "56/388, train_loss: 0.0916, step time: 0.4901\n",
      "57/388, train_loss: 0.1144, step time: 0.5053\n",
      "58/388, train_loss: 0.3735, step time: 0.4870\n",
      "59/388, train_loss: 0.1972, step time: 0.5157\n",
      "60/388, train_loss: 0.3041, step time: 0.5199\n",
      "61/388, train_loss: 0.0823, step time: 0.5343\n",
      "62/388, train_loss: 0.3938, step time: 0.5648\n",
      "63/388, train_loss: 0.1499, step time: 0.5312\n",
      "64/388, train_loss: 0.8048, step time: 0.4897\n",
      "65/388, train_loss: 0.2813, step time: 0.4848\n",
      "66/388, train_loss: 0.2594, step time: 0.5007\n",
      "67/388, train_loss: 0.3840, step time: 0.9627\n",
      "68/388, train_loss: 0.6501, step time: 0.5523\n",
      "69/388, train_loss: 0.1494, step time: 0.5032\n",
      "70/388, train_loss: 0.2807, step time: 0.4965\n",
      "71/388, train_loss: 0.2084, step time: 0.4956\n",
      "72/388, train_loss: 0.1978, step time: 0.4836\n",
      "73/388, train_loss: 0.2034, step time: 0.4876\n",
      "74/388, train_loss: 0.3068, step time: 0.4792\n",
      "75/388, train_loss: 0.1523, step time: 1.0863\n",
      "76/388, train_loss: 0.2858, step time: 0.5443\n",
      "77/388, train_loss: 0.2535, step time: 0.5095\n",
      "78/388, train_loss: 0.3474, step time: 0.4881\n",
      "79/388, train_loss: 0.4032, step time: 0.5214\n",
      "80/388, train_loss: 0.1233, step time: 0.4968\n",
      "81/388, train_loss: 0.3230, step time: 0.5038\n",
      "82/388, train_loss: 0.2330, step time: 0.4922\n",
      "83/388, train_loss: 0.2171, step time: 0.5180\n",
      "84/388, train_loss: 0.2560, step time: 0.5120\n",
      "85/388, train_loss: 0.7355, step time: 0.4947\n",
      "86/388, train_loss: 0.1585, step time: 0.4823\n",
      "87/388, train_loss: 0.1321, step time: 0.5181\n",
      "88/388, train_loss: 0.4721, step time: 0.4856\n",
      "89/388, train_loss: 0.1873, step time: 0.5003\n",
      "90/388, train_loss: 0.2082, step time: 0.4913\n",
      "91/388, train_loss: 0.1785, step time: 0.4888\n",
      "92/388, train_loss: 0.1741, step time: 0.4978\n",
      "93/388, train_loss: 0.3243, step time: 1.1040\n",
      "94/388, train_loss: 0.2735, step time: 0.5433\n",
      "95/388, train_loss: 0.1119, step time: 0.4988\n",
      "96/388, train_loss: 0.1191, step time: 0.4936\n",
      "97/388, train_loss: 0.0638, step time: 0.4803\n",
      "98/388, train_loss: 0.3849, step time: 0.4864\n",
      "99/388, train_loss: 0.2695, step time: 1.0005\n",
      "100/388, train_loss: 0.0426, step time: 0.5368\n",
      "101/388, train_loss: 0.2350, step time: 0.5143\n",
      "102/388, train_loss: 0.2676, step time: 0.4909\n",
      "103/388, train_loss: 0.2190, step time: 0.4972\n",
      "104/388, train_loss: 0.1601, step time: 0.4795\n",
      "105/388, train_loss: 0.3957, step time: 0.4811\n",
      "106/388, train_loss: 0.2081, step time: 0.4827\n",
      "107/388, train_loss: 0.1244, step time: 1.0110\n",
      "108/388, train_loss: 0.2088, step time: 0.5414\n",
      "109/388, train_loss: 0.1843, step time: 0.5042\n",
      "110/388, train_loss: 0.3950, step time: 0.4976\n",
      "111/388, train_loss: 0.2569, step time: 0.5316\n",
      "112/388, train_loss: 0.3635, step time: 0.5082\n",
      "113/388, train_loss: 0.1578, step time: 0.6085\n",
      "114/388, train_loss: 0.1741, step time: 0.5456\n",
      "115/388, train_loss: 0.1064, step time: 0.5133\n",
      "116/388, train_loss: 0.1874, step time: 0.5024\n",
      "117/388, train_loss: 0.1081, step time: 0.4836\n",
      "118/388, train_loss: 0.0856, step time: 0.4857\n",
      "119/388, train_loss: 0.1274, step time: 0.4876\n",
      "120/388, train_loss: 0.4169, step time: 0.4784\n",
      "121/388, train_loss: 0.2184, step time: 0.4877\n",
      "122/388, train_loss: 0.3528, step time: 0.5280\n",
      "123/388, train_loss: 0.1704, step time: 0.5005\n",
      "124/388, train_loss: 0.7475, step time: 0.4909\n",
      "125/388, train_loss: 0.3645, step time: 0.8191\n",
      "126/388, train_loss: 0.3693, step time: 0.5586\n",
      "127/388, train_loss: 0.1525, step time: 0.5216\n",
      "128/388, train_loss: 0.1278, step time: 0.5001\n",
      "129/388, train_loss: 0.1653, step time: 0.5245\n",
      "130/388, train_loss: 0.5457, step time: 0.4959\n",
      "131/388, train_loss: 0.4822, step time: 0.5082\n",
      "132/388, train_loss: 0.2308, step time: 0.5016\n",
      "133/388, train_loss: 0.2651, step time: 0.5328\n",
      "134/388, train_loss: 0.2143, step time: 0.5111\n",
      "135/388, train_loss: 0.1613, step time: 0.4973\n",
      "136/388, train_loss: 0.2655, step time: 0.4935\n",
      "137/388, train_loss: 0.2345, step time: 0.4817\n",
      "138/388, train_loss: 0.4430, step time: 0.6577\n",
      "139/388, train_loss: 0.1928, step time: 0.5455\n",
      "140/388, train_loss: 0.1817, step time: 0.5263\n",
      "141/388, train_loss: 0.1465, step time: 0.5035\n",
      "142/388, train_loss: 0.3424, step time: 0.4924\n",
      "143/388, train_loss: 0.1853, step time: 0.5040\n",
      "144/388, train_loss: 0.0560, step time: 0.5035\n",
      "145/388, train_loss: 0.2511, step time: 0.4837\n",
      "146/388, train_loss: 0.3547, step time: 0.9969\n",
      "147/388, train_loss: 0.2330, step time: 0.5307\n",
      "148/388, train_loss: 0.2699, step time: 0.5128\n",
      "149/388, train_loss: 0.1548, step time: 0.4835\n",
      "150/388, train_loss: 0.3489, step time: 0.4814\n",
      "151/388, train_loss: 0.1509, step time: 0.5297\n",
      "152/388, train_loss: 0.1495, step time: 0.5093\n",
      "153/388, train_loss: 0.1570, step time: 0.4978\n",
      "154/388, train_loss: 0.1492, step time: 0.4949\n",
      "155/388, train_loss: 0.6356, step time: 0.4960\n",
      "156/388, train_loss: 0.4049, step time: 0.4869\n",
      "157/388, train_loss: 0.2620, step time: 0.4926\n",
      "158/388, train_loss: 0.2080, step time: 0.7759\n",
      "159/388, train_loss: 0.1171, step time: 0.5549\n",
      "160/388, train_loss: 0.3091, step time: 0.5412\n",
      "161/388, train_loss: 0.1602, step time: 0.5212\n",
      "162/388, train_loss: 0.2847, step time: 0.4981\n",
      "163/388, train_loss: 0.5394, step time: 0.4920\n",
      "164/388, train_loss: 0.0875, step time: 0.9956\n",
      "165/388, train_loss: 0.1108, step time: 0.5320\n",
      "166/388, train_loss: 0.3303, step time: 0.5025\n",
      "167/388, train_loss: 0.1064, step time: 0.4877\n",
      "168/388, train_loss: 0.4193, step time: 0.4926\n",
      "169/388, train_loss: 0.1001, step time: 0.5170\n",
      "170/388, train_loss: 0.2429, step time: 0.4960\n",
      "171/388, train_loss: 0.1643, step time: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/388, train_loss: 0.4476, step time: 0.5076\n",
      "173/388, train_loss: 0.4499, step time: 0.4869\n",
      "174/388, train_loss: 0.1346, step time: 1.0678\n",
      "175/388, train_loss: 0.1089, step time: 0.5484\n",
      "176/388, train_loss: 0.4124, step time: 0.5188\n",
      "177/388, train_loss: 0.3974, step time: 0.5296\n",
      "178/388, train_loss: 0.1109, step time: 0.5217\n",
      "179/388, train_loss: 0.1259, step time: 0.5028\n",
      "180/388, train_loss: 0.1559, step time: 0.4879\n",
      "181/388, train_loss: 0.1151, step time: 0.4945\n",
      "182/388, train_loss: 0.4046, step time: 0.4761\n",
      "183/388, train_loss: 0.0953, step time: 0.4911\n",
      "184/388, train_loss: 0.6555, step time: 0.5307\n",
      "185/388, train_loss: 0.2636, step time: 0.5209\n",
      "186/388, train_loss: 0.1403, step time: 0.5026\n",
      "187/388, train_loss: 0.2486, step time: 0.4950\n",
      "188/388, train_loss: 0.2277, step time: 0.7683\n",
      "189/388, train_loss: 0.1549, step time: 0.5449\n",
      "190/388, train_loss: 0.2020, step time: 0.5069\n",
      "191/388, train_loss: 0.4915, step time: 0.4819\n",
      "192/388, train_loss: 0.2140, step time: 0.6811\n",
      "193/388, train_loss: 0.1821, step time: 0.5403\n",
      "194/388, train_loss: 0.7341, step time: 0.5207\n",
      "195/388, train_loss: 0.3921, step time: 0.4944\n",
      "196/388, train_loss: 0.1470, step time: 0.4933\n",
      "197/388, train_loss: 0.1056, step time: 1.1468\n",
      "198/388, train_loss: 0.3032, step time: 0.5271\n",
      "199/388, train_loss: 0.1107, step time: 0.5130\n",
      "200/388, train_loss: 0.3759, step time: 0.4952\n",
      "201/388, train_loss: 0.1755, step time: 0.4968\n",
      "202/388, train_loss: 0.0927, step time: 0.4817\n",
      "203/388, train_loss: 0.2063, step time: 0.6081\n",
      "204/388, train_loss: 0.3759, step time: 0.5524\n",
      "205/388, train_loss: 0.0881, step time: 0.5218\n",
      "206/388, train_loss: 0.3596, step time: 0.5044\n",
      "207/388, train_loss: 0.0728, step time: 0.4940\n",
      "208/388, train_loss: 0.1142, step time: 0.4945\n",
      "209/388, train_loss: 0.5441, step time: 0.4818\n",
      "210/388, train_loss: 0.3304, step time: 0.4902\n",
      "211/388, train_loss: 0.2427, step time: 0.4983\n",
      "212/388, train_loss: 0.2396, step time: 0.5218\n",
      "213/388, train_loss: 0.2295, step time: 0.6091\n",
      "214/388, train_loss: 0.1572, step time: 0.5701\n",
      "215/388, train_loss: 0.1591, step time: 0.5380\n",
      "216/388, train_loss: 0.1212, step time: 0.5088\n",
      "217/388, train_loss: 0.4906, step time: 0.5743\n",
      "218/388, train_loss: 0.1754, step time: 0.5242\n",
      "219/388, train_loss: 0.1641, step time: 0.5133\n",
      "220/388, train_loss: 0.1042, step time: 0.5345\n",
      "221/388, train_loss: 0.3686, step time: 0.6693\n",
      "222/388, train_loss: 0.0950, step time: 0.5546\n",
      "223/388, train_loss: 0.1345, step time: 0.5119\n",
      "224/388, train_loss: 0.1686, step time: 0.5017\n",
      "225/388, train_loss: 0.2588, step time: 0.7998\n",
      "226/388, train_loss: 0.2634, step time: 0.5416\n",
      "227/388, train_loss: 0.2457, step time: 0.5169\n",
      "228/388, train_loss: 0.3515, step time: 0.5057\n",
      "229/388, train_loss: 0.2581, step time: 0.5009\n",
      "230/388, train_loss: 0.3097, step time: 0.4885\n",
      "231/388, train_loss: 0.2549, step time: 0.5014\n",
      "232/388, train_loss: 0.4352, step time: 0.5014\n",
      "233/388, train_loss: 0.2241, step time: 0.5273\n",
      "234/388, train_loss: 0.2117, step time: 0.4977\n",
      "235/388, train_loss: 0.2899, step time: 1.0174\n",
      "236/388, train_loss: 0.2077, step time: 0.5404\n",
      "237/388, train_loss: 0.1740, step time: 0.5149\n",
      "238/388, train_loss: 0.1502, step time: 0.5022\n",
      "239/388, train_loss: 0.3770, step time: 0.4907\n",
      "240/388, train_loss: 0.2881, step time: 0.4936\n",
      "241/388, train_loss: 0.2560, step time: 0.5214\n",
      "242/388, train_loss: 0.2360, step time: 0.5302\n",
      "243/388, train_loss: 0.3270, step time: 0.5270\n",
      "244/388, train_loss: 0.3273, step time: 0.5565\n",
      "245/388, train_loss: 0.0796, step time: 0.5377\n",
      "246/388, train_loss: 0.1508, step time: 0.5067\n",
      "247/388, train_loss: 0.2636, step time: 0.4936\n",
      "248/388, train_loss: 0.2123, step time: 0.5028\n",
      "249/388, train_loss: 0.3550, step time: 0.4872\n",
      "250/388, train_loss: 0.2388, step time: 0.5017\n",
      "251/388, train_loss: 0.1746, step time: 0.5195\n",
      "252/388, train_loss: 0.2209, step time: 0.4986\n",
      "253/388, train_loss: 0.1547, step time: 0.4938\n",
      "254/388, train_loss: 0.3680, step time: 0.5518\n",
      "255/388, train_loss: 0.2140, step time: 0.5403\n",
      "256/388, train_loss: 0.2564, step time: 0.5179\n",
      "257/388, train_loss: 0.1071, step time: 0.5228\n",
      "258/388, train_loss: 0.1861, step time: 0.5142\n",
      "259/388, train_loss: 0.1804, step time: 0.4989\n",
      "260/388, train_loss: 0.3556, step time: 0.5164\n",
      "261/388, train_loss: 0.4612, step time: 0.5062\n",
      "262/388, train_loss: 0.1193, step time: 0.4839\n",
      "263/388, train_loss: 0.1982, step time: 0.5016\n",
      "264/388, train_loss: 0.3318, step time: 0.7956\n",
      "265/388, train_loss: 0.7491, step time: 0.5590\n",
      "266/388, train_loss: 0.0955, step time: 0.5160\n",
      "267/388, train_loss: 0.2262, step time: 0.5000\n",
      "268/388, train_loss: 0.6391, step time: 0.4840\n",
      "269/388, train_loss: 0.2190, step time: 1.0188\n",
      "270/388, train_loss: 0.3548, step time: 0.5573\n",
      "271/388, train_loss: 0.1918, step time: 0.5215\n",
      "272/388, train_loss: 0.4363, step time: 0.5523\n",
      "273/388, train_loss: 0.1468, step time: 0.5201\n",
      "274/388, train_loss: 0.1020, step time: 0.5057\n",
      "275/388, train_loss: 0.0928, step time: 0.4985\n",
      "276/388, train_loss: 0.1260, step time: 0.4923\n",
      "277/388, train_loss: 0.0926, step time: 0.4939\n",
      "278/388, train_loss: 0.5027, step time: 0.4883\n",
      "279/388, train_loss: 0.1926, step time: 0.4941\n",
      "280/388, train_loss: 0.1876, step time: 1.1316\n",
      "281/388, train_loss: 0.0382, step time: 0.5435\n",
      "282/388, train_loss: 0.1737, step time: 0.5194\n",
      "283/388, train_loss: 0.1178, step time: 0.5075\n",
      "284/388, train_loss: 0.1696, step time: 0.4950\n",
      "285/388, train_loss: 0.1692, step time: 0.4994\n",
      "286/388, train_loss: 0.3834, step time: 0.4841\n",
      "287/388, train_loss: 0.1567, step time: 0.4979\n",
      "288/388, train_loss: 0.1447, step time: 0.5143\n",
      "289/388, train_loss: 0.1686, step time: 0.5857\n",
      "290/388, train_loss: 0.3091, step time: 0.5477\n",
      "291/388, train_loss: 0.1297, step time: 0.5174\n",
      "292/388, train_loss: 0.2319, step time: 0.4979\n",
      "293/388, train_loss: 0.1653, step time: 0.5195\n",
      "294/388, train_loss: 0.1078, step time: 0.6037\n",
      "295/388, train_loss: 0.2204, step time: 0.5587\n",
      "296/388, train_loss: 0.2783, step time: 0.5345\n",
      "297/388, train_loss: 0.2619, step time: 0.5152\n",
      "298/388, train_loss: 0.0938, step time: 0.5568\n",
      "299/388, train_loss: 0.2785, step time: 0.5381\n",
      "300/388, train_loss: 0.0789, step time: 0.5139\n",
      "301/388, train_loss: 0.2125, step time: 0.5084\n",
      "302/388, train_loss: 0.2681, step time: 0.4876\n",
      "303/388, train_loss: 0.4015, step time: 0.4927\n",
      "304/388, train_loss: 0.3479, step time: 0.8046\n",
      "305/388, train_loss: 0.1206, step time: 0.5389\n",
      "306/388, train_loss: 0.2617, step time: 0.5154\n",
      "307/388, train_loss: 0.2063, step time: 0.5051\n",
      "308/388, train_loss: 0.1667, step time: 0.4890\n",
      "309/388, train_loss: 0.5654, step time: 0.4883\n",
      "310/388, train_loss: 0.2008, step time: 0.8960\n",
      "311/388, train_loss: 0.4524, step time: 0.5455\n",
      "312/388, train_loss: 0.5716, step time: 0.5066\n",
      "313/388, train_loss: 0.3356, step time: 0.4979\n",
      "314/388, train_loss: 0.3953, step time: 0.8263\n",
      "315/388, train_loss: 0.5390, step time: 0.5412\n",
      "316/388, train_loss: 0.5811, step time: 0.5124\n",
      "317/388, train_loss: 0.1341, step time: 0.4999\n",
      "318/388, train_loss: 0.1541, step time: 0.4857\n",
      "319/388, train_loss: 0.1107, step time: 0.4940\n",
      "320/388, train_loss: 0.2235, step time: 0.4908\n",
      "321/388, train_loss: 0.5676, step time: 0.4829\n",
      "322/388, train_loss: 0.1165, step time: 1.1153\n",
      "323/388, train_loss: 0.0800, step time: 0.5316\n",
      "324/388, train_loss: 0.2089, step time: 0.5055\n",
      "325/388, train_loss: 0.1474, step time: 0.4974\n",
      "326/388, train_loss: 0.6845, step time: 0.4980\n",
      "327/388, train_loss: 0.2083, step time: 0.4974\n",
      "328/388, train_loss: 0.1427, step time: 0.4802\n",
      "329/388, train_loss: 0.1854, step time: 0.4792\n",
      "330/388, train_loss: 0.1475, step time: 0.4830\n",
      "331/388, train_loss: 0.1996, step time: 0.4754\n",
      "332/388, train_loss: 0.3398, step time: 0.7782\n",
      "333/388, train_loss: 0.1571, step time: 0.5552\n",
      "334/388, train_loss: 0.1094, step time: 0.5170\n",
      "335/388, train_loss: 0.1015, step time: 0.5052\n",
      "336/388, train_loss: 0.2567, step time: 0.5016\n",
      "337/388, train_loss: 0.3044, step time: 0.5095\n",
      "338/388, train_loss: 0.3281, step time: 0.5001\n",
      "339/388, train_loss: 0.1367, step time: 0.4838\n",
      "340/388, train_loss: 0.2313, step time: 0.4940\n",
      "341/388, train_loss: 0.0795, step time: 0.4922\n",
      "342/388, train_loss: 0.1444, step time: 0.5073\n",
      "343/388, train_loss: 0.1748, step time: 0.5031\n",
      "344/388, train_loss: 0.1818, step time: 0.4893\n",
      "345/388, train_loss: 0.1905, step time: 0.4787\n",
      "346/388, train_loss: 0.0900, step time: 1.1194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/388, train_loss: 0.3610, step time: 0.5290\n",
      "348/388, train_loss: 0.1023, step time: 0.4936\n",
      "349/388, train_loss: 0.2514, step time: 0.4960\n",
      "350/388, train_loss: 0.3094, step time: 0.5131\n",
      "351/388, train_loss: 0.3359, step time: 0.5010\n",
      "352/388, train_loss: 0.2828, step time: 0.4819\n",
      "353/388, train_loss: 0.1672, step time: 0.4762\n",
      "354/388, train_loss: 0.1308, step time: 0.4769\n",
      "355/388, train_loss: 0.1436, step time: 0.4930\n",
      "356/388, train_loss: 0.3169, step time: 0.4959\n",
      "357/388, train_loss: 0.5634, step time: 0.4834\n",
      "358/388, train_loss: 0.1635, step time: 0.5003\n",
      "359/388, train_loss: 0.1650, step time: 0.5061\n",
      "360/388, train_loss: 0.1046, step time: 0.5362\n",
      "361/388, train_loss: 0.5302, step time: 0.5201\n",
      "362/388, train_loss: 0.3172, step time: 0.5108\n",
      "363/388, train_loss: 0.2284, step time: 0.4986\n",
      "364/388, train_loss: 0.4200, step time: 0.4921\n",
      "365/388, train_loss: 0.0939, step time: 0.4978\n",
      "366/388, train_loss: 0.2129, step time: 0.5436\n",
      "367/388, train_loss: 0.3191, step time: 0.5314\n",
      "368/388, train_loss: 0.1331, step time: 0.6066\n",
      "369/388, train_loss: 0.2720, step time: 0.5421\n",
      "370/388, train_loss: 0.2099, step time: 0.5053\n",
      "371/388, train_loss: 0.2811, step time: 0.4921\n",
      "372/388, train_loss: 0.2860, step time: 0.5033\n",
      "373/388, train_loss: 0.2726, step time: 0.4939\n",
      "374/388, train_loss: 0.2396, step time: 0.4916\n",
      "375/388, train_loss: 0.3145, step time: 1.0895\n",
      "376/388, train_loss: 0.2863, step time: 0.5435\n",
      "377/388, train_loss: 0.1893, step time: 0.5154\n",
      "378/388, train_loss: 0.1789, step time: 0.4959\n",
      "379/388, train_loss: 0.1244, step time: 0.4961\n",
      "380/388, train_loss: 0.2801, step time: 0.4830\n",
      "381/388, train_loss: 0.3324, step time: 0.4895\n",
      "382/388, train_loss: 0.1086, step time: 0.4974\n",
      "383/388, train_loss: 0.2240, step time: 0.4760\n",
      "384/388, train_loss: 0.1923, step time: 0.4729\n",
      "385/388, train_loss: 0.1266, step time: 0.4819\n",
      "386/388, train_loss: 0.1897, step time: 0.4896\n",
      "387/388, train_loss: 0.2489, step time: 0.5118\n",
      "388/388, train_loss: 0.2059, step time: 0.4811\n",
      "epoch 14 average loss: 0.2482\n",
      "saved new best metric model\n",
      "current epoch: 14 current mean dice: 0.7270 tc: 0.7752 wt: 0.8763 et: 0.5296\n",
      "best mean dice: 0.7270 at epoch: 14\n",
      "time consuming of epoch 14 is: 300.4231\n",
      "----------\n",
      "epoch 15/300\n",
      "1/388, train_loss: 0.1757, step time: 0.4696\n",
      "2/388, train_loss: 0.0813, step time: 0.4769\n",
      "3/388, train_loss: 0.1182, step time: 0.4799\n",
      "4/388, train_loss: 0.2602, step time: 0.4993\n",
      "5/388, train_loss: 0.4410, step time: 0.5341\n",
      "6/388, train_loss: 0.7232, step time: 0.5075\n",
      "7/388, train_loss: 0.1394, step time: 0.5078\n",
      "8/388, train_loss: 0.1922, step time: 0.6017\n",
      "9/388, train_loss: 0.2192, step time: 0.5450\n",
      "10/388, train_loss: 0.1070, step time: 0.5822\n",
      "11/388, train_loss: 0.2452, step time: 0.5374\n",
      "12/388, train_loss: 0.3692, step time: 0.5309\n",
      "13/388, train_loss: 0.2327, step time: 0.5128\n",
      "14/388, train_loss: 0.3156, step time: 0.5012\n",
      "15/388, train_loss: 0.1667, step time: 0.5140\n",
      "16/388, train_loss: 0.4821, step time: 0.6519\n",
      "17/388, train_loss: 0.3558, step time: 0.5462\n",
      "18/388, train_loss: 0.1408, step time: 0.5066\n",
      "19/388, train_loss: 0.3801, step time: 0.4934\n",
      "20/388, train_loss: 0.1370, step time: 0.5530\n",
      "21/388, train_loss: 0.2127, step time: 0.6175\n",
      "22/388, train_loss: 0.1750, step time: 0.5609\n",
      "23/388, train_loss: 0.2373, step time: 0.5212\n",
      "24/388, train_loss: 0.4862, step time: 0.5212\n",
      "25/388, train_loss: 0.2550, step time: 0.4983\n",
      "26/388, train_loss: 0.3753, step time: 0.4951\n",
      "27/388, train_loss: 0.4023, step time: 0.4798\n",
      "28/388, train_loss: 0.1203, step time: 0.5352\n",
      "29/388, train_loss: 0.1623, step time: 0.4988\n",
      "30/388, train_loss: 0.3378, step time: 0.4939\n",
      "31/388, train_loss: 0.1675, step time: 0.5157\n",
      "32/388, train_loss: 0.3352, step time: 0.5978\n",
      "33/388, train_loss: 0.2053, step time: 0.5454\n",
      "34/388, train_loss: 0.2319, step time: 0.5233\n",
      "35/388, train_loss: 0.6506, step time: 0.5015\n",
      "36/388, train_loss: 0.1652, step time: 0.5544\n",
      "37/388, train_loss: 0.1408, step time: 0.5374\n",
      "38/388, train_loss: 0.4336, step time: 0.5202\n",
      "39/388, train_loss: 0.3561, step time: 0.5078\n",
      "40/388, train_loss: 0.2073, step time: 0.5083\n",
      "41/388, train_loss: 0.1959, step time: 0.5592\n",
      "42/388, train_loss: 0.1355, step time: 0.5254\n",
      "43/388, train_loss: 0.4143, step time: 0.5052\n",
      "44/388, train_loss: 0.1585, step time: 0.5164\n",
      "45/388, train_loss: 0.3342, step time: 0.4983\n",
      "46/388, train_loss: 0.4339, step time: 0.4988\n",
      "47/388, train_loss: 0.1413, step time: 0.4874\n",
      "48/388, train_loss: 0.3941, step time: 0.4974\n",
      "49/388, train_loss: 0.1944, step time: 0.4885\n",
      "50/388, train_loss: 0.3549, step time: 1.0505\n",
      "51/388, train_loss: 0.1617, step time: 0.5397\n",
      "52/388, train_loss: 0.0998, step time: 0.5181\n",
      "53/388, train_loss: 0.2645, step time: 0.4964\n",
      "54/388, train_loss: 0.3754, step time: 0.4999\n",
      "55/388, train_loss: 0.1900, step time: 0.4831\n",
      "56/388, train_loss: 0.1324, step time: 0.4792\n",
      "57/388, train_loss: 0.3695, step time: 0.9965\n",
      "58/388, train_loss: 0.4377, step time: 0.5682\n",
      "59/388, train_loss: 0.3280, step time: 0.5247\n",
      "60/388, train_loss: 0.3110, step time: 0.4979\n",
      "61/388, train_loss: 0.0859, step time: 0.5026\n",
      "62/388, train_loss: 0.1623, step time: 0.4856\n",
      "63/388, train_loss: 0.2817, step time: 0.4959\n",
      "64/388, train_loss: 0.2176, step time: 0.4860\n",
      "65/388, train_loss: 0.3637, step time: 1.0288\n",
      "66/388, train_loss: 0.5010, step time: 0.5450\n",
      "67/388, train_loss: 0.2856, step time: 0.5068\n",
      "68/388, train_loss: 0.3947, step time: 0.4933\n",
      "69/388, train_loss: 0.1821, step time: 0.4794\n",
      "70/388, train_loss: 0.1836, step time: 0.5144\n",
      "71/388, train_loss: 0.2194, step time: 0.5181\n",
      "72/388, train_loss: 0.2428, step time: 0.5038\n",
      "73/388, train_loss: 0.2619, step time: 0.5116\n",
      "74/388, train_loss: 0.4241, step time: 0.5030\n",
      "75/388, train_loss: 0.2116, step time: 0.4835\n",
      "76/388, train_loss: 0.1986, step time: 0.4896\n",
      "77/388, train_loss: 0.7359, step time: 1.0453\n",
      "78/388, train_loss: 0.0999, step time: 0.5342\n",
      "79/388, train_loss: 0.2213, step time: 0.5120\n",
      "80/388, train_loss: 0.1204, step time: 0.4943\n",
      "81/388, train_loss: 0.2082, step time: 0.4958\n",
      "82/388, train_loss: 0.7783, step time: 0.4804\n",
      "83/388, train_loss: 0.1335, step time: 1.1553\n",
      "84/388, train_loss: 0.2055, step time: 0.5167\n",
      "85/388, train_loss: 0.1427, step time: 0.4934\n",
      "86/388, train_loss: 0.1196, step time: 0.4908\n",
      "87/388, train_loss: 0.2147, step time: 0.4842\n",
      "88/388, train_loss: 0.4450, step time: 0.4809\n",
      "89/388, train_loss: 0.2018, step time: 0.4750\n",
      "90/388, train_loss: 0.1180, step time: 0.5829\n",
      "91/388, train_loss: 0.2445, step time: 0.5296\n",
      "92/388, train_loss: 0.1787, step time: 0.5105\n",
      "93/388, train_loss: 0.2184, step time: 0.4984\n",
      "94/388, train_loss: 0.1834, step time: 0.4800\n",
      "95/388, train_loss: 0.3864, step time: 0.4899\n",
      "96/388, train_loss: 0.1792, step time: 0.4716\n",
      "97/388, train_loss: 0.0898, step time: 0.4716\n",
      "98/388, train_loss: 0.1626, step time: 0.6189\n",
      "99/388, train_loss: 0.2135, step time: 0.5458\n",
      "100/388, train_loss: 0.1125, step time: 0.5217\n",
      "101/388, train_loss: 0.1569, step time: 0.5101\n",
      "102/388, train_loss: 0.1542, step time: 0.4898\n",
      "103/388, train_loss: 0.2502, step time: 0.5040\n",
      "104/388, train_loss: 0.1497, step time: 0.4951\n",
      "105/388, train_loss: 0.2576, step time: 0.4787\n",
      "106/388, train_loss: 0.1511, step time: 0.4880\n",
      "107/388, train_loss: 0.1744, step time: 0.4819\n",
      "108/388, train_loss: 0.3094, step time: 0.4887\n",
      "109/388, train_loss: 0.1997, step time: 0.4858\n",
      "110/388, train_loss: 0.2403, step time: 0.4875\n",
      "111/388, train_loss: 0.1536, step time: 0.4745\n",
      "112/388, train_loss: 0.4643, step time: 0.4759\n",
      "113/388, train_loss: 0.5452, step time: 0.4861\n",
      "114/388, train_loss: 0.1082, step time: 0.9639\n",
      "115/388, train_loss: 0.1145, step time: 0.5321\n",
      "116/388, train_loss: 0.0982, step time: 0.5059\n",
      "117/388, train_loss: 0.1289, step time: 0.4953\n",
      "118/388, train_loss: 0.3340, step time: 0.4870\n",
      "119/388, train_loss: 0.1731, step time: 0.4908\n",
      "120/388, train_loss: 0.6399, step time: 0.4744\n",
      "121/388, train_loss: 0.1042, step time: 0.7205\n",
      "122/388, train_loss: 0.1426, step time: 0.5369\n",
      "123/388, train_loss: 0.1211, step time: 0.5155\n",
      "124/388, train_loss: 0.2178, step time: 0.5020\n",
      "125/388, train_loss: 0.2592, step time: 0.5306\n",
      "126/388, train_loss: 0.1706, step time: 0.5152\n",
      "127/388, train_loss: 0.1330, step time: 0.4928\n",
      "128/388, train_loss: 0.1631, step time: 0.4978\n",
      "129/388, train_loss: 0.1095, step time: 1.1256\n",
      "130/388, train_loss: 0.6183, step time: 0.5217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/388, train_loss: 0.1584, step time: 0.4965\n",
      "132/388, train_loss: 0.1360, step time: 0.4808\n",
      "133/388, train_loss: 0.1340, step time: 0.4950\n",
      "134/388, train_loss: 0.3989, step time: 0.4834\n",
      "135/388, train_loss: 0.1304, step time: 0.4805\n",
      "136/388, train_loss: 0.0499, step time: 0.4786\n",
      "137/388, train_loss: 0.2335, step time: 0.4892\n",
      "138/388, train_loss: 0.2615, step time: 0.5129\n",
      "139/388, train_loss: 0.4350, step time: 0.4963\n",
      "140/388, train_loss: 0.1512, step time: 0.4968\n",
      "141/388, train_loss: 0.1053, step time: 0.4827\n",
      "142/388, train_loss: 0.2405, step time: 0.4776\n",
      "143/388, train_loss: 0.2392, step time: 0.4816\n",
      "144/388, train_loss: 0.3302, step time: 0.4716\n",
      "145/388, train_loss: 0.2671, step time: 0.4907\n",
      "146/388, train_loss: 0.1889, step time: 0.4965\n",
      "147/388, train_loss: 0.2423, step time: 1.0353\n",
      "148/388, train_loss: 0.2656, step time: 0.5449\n",
      "149/388, train_loss: 0.3714, step time: 0.5205\n",
      "150/388, train_loss: 0.3041, step time: 0.4962\n",
      "151/388, train_loss: 0.1018, step time: 0.4913\n",
      "152/388, train_loss: 0.1582, step time: 0.4887\n",
      "153/388, train_loss: 0.1936, step time: 0.4767\n",
      "154/388, train_loss: 0.4423, step time: 0.4820\n",
      "155/388, train_loss: 0.0659, step time: 0.4878\n",
      "156/388, train_loss: 0.2654, step time: 1.0987\n",
      "157/388, train_loss: 0.1629, step time: 0.5383\n",
      "158/388, train_loss: 0.3296, step time: 0.5169\n",
      "159/388, train_loss: 0.2159, step time: 0.4972\n",
      "160/388, train_loss: 0.6469, step time: 0.5020\n",
      "161/388, train_loss: 0.1165, step time: 0.4820\n",
      "162/388, train_loss: 0.1795, step time: 0.4817\n",
      "163/388, train_loss: 0.1511, step time: 0.4847\n",
      "164/388, train_loss: 0.3040, step time: 0.4793\n",
      "165/388, train_loss: 0.2367, step time: 0.4810\n",
      "166/388, train_loss: 0.2491, step time: 0.4758\n",
      "167/388, train_loss: 0.3293, step time: 0.9643\n",
      "168/388, train_loss: 0.1149, step time: 0.5346\n",
      "169/388, train_loss: 0.2225, step time: 0.5027\n",
      "170/388, train_loss: 0.2625, step time: 0.5111\n",
      "171/388, train_loss: 0.1432, step time: 0.4901\n",
      "172/388, train_loss: 0.0901, step time: 0.4970\n",
      "173/388, train_loss: 0.1314, step time: 0.4889\n",
      "174/388, train_loss: 0.1243, step time: 0.5535\n",
      "175/388, train_loss: 0.1148, step time: 0.7360\n",
      "176/388, train_loss: 0.2666, step time: 0.5461\n",
      "177/388, train_loss: 0.2327, step time: 0.5141\n",
      "178/388, train_loss: 0.1399, step time: 0.5027\n",
      "179/388, train_loss: 0.2177, step time: 0.4929\n",
      "180/388, train_loss: 0.4558, step time: 0.4824\n",
      "181/388, train_loss: 0.3807, step time: 0.4830\n",
      "182/388, train_loss: 0.1917, step time: 1.1477\n",
      "183/388, train_loss: 0.3034, step time: 0.5466\n",
      "184/388, train_loss: 0.2072, step time: 0.5060\n",
      "185/388, train_loss: 0.2003, step time: 0.4899\n",
      "186/388, train_loss: 0.3078, step time: 0.4964\n",
      "187/388, train_loss: 0.6011, step time: 0.5017\n",
      "188/388, train_loss: 0.2446, step time: 0.4907\n",
      "189/388, train_loss: 0.4936, step time: 0.4916\n",
      "190/388, train_loss: 0.0951, step time: 0.4762\n",
      "191/388, train_loss: 0.5893, step time: 0.4748\n",
      "192/388, train_loss: 0.2233, step time: 0.4768\n",
      "193/388, train_loss: 0.2176, step time: 0.6806\n",
      "194/388, train_loss: 0.1516, step time: 0.5387\n",
      "195/388, train_loss: 0.1542, step time: 0.5196\n",
      "196/388, train_loss: 0.2096, step time: 0.5116\n",
      "197/388, train_loss: 0.0955, step time: 0.4923\n",
      "198/388, train_loss: 0.3452, step time: 0.4848\n",
      "199/388, train_loss: 0.3459, step time: 1.1443\n",
      "200/388, train_loss: 0.1739, step time: 0.5437\n",
      "201/388, train_loss: 0.1895, step time: 0.5021\n",
      "202/388, train_loss: 0.4879, step time: 0.4845\n",
      "203/388, train_loss: 0.2538, step time: 0.4824\n",
      "204/388, train_loss: 0.1949, step time: 0.4881\n",
      "205/388, train_loss: 0.2637, step time: 0.4768\n",
      "206/388, train_loss: 0.1462, step time: 0.4815\n",
      "207/388, train_loss: 0.3423, step time: 0.4861\n",
      "208/388, train_loss: 0.3220, step time: 1.2068\n",
      "209/388, train_loss: 0.2190, step time: 0.5441\n",
      "210/388, train_loss: 0.1319, step time: 0.5151\n",
      "211/388, train_loss: 0.2660, step time: 0.4970\n",
      "212/388, train_loss: 0.1626, step time: 0.4874\n",
      "213/388, train_loss: 0.1713, step time: 0.4910\n",
      "214/388, train_loss: 0.2286, step time: 0.4880\n",
      "215/388, train_loss: 0.1753, step time: 0.4877\n",
      "216/388, train_loss: 0.2056, step time: 0.4894\n",
      "217/388, train_loss: 0.1772, step time: 0.4815\n",
      "218/388, train_loss: 0.2595, step time: 0.4826\n",
      "219/388, train_loss: 0.1329, step time: 0.4852\n",
      "220/388, train_loss: 0.1379, step time: 0.4983\n",
      "221/388, train_loss: 0.1660, step time: 0.4871\n",
      "222/388, train_loss: 0.1220, step time: 0.4926\n",
      "223/388, train_loss: 0.1500, step time: 0.5155\n",
      "224/388, train_loss: 0.0786, step time: 0.4975\n",
      "225/388, train_loss: 0.1886, step time: 0.4844\n",
      "226/388, train_loss: 0.2899, step time: 0.4913\n",
      "227/388, train_loss: 0.4291, step time: 0.4997\n",
      "228/388, train_loss: 0.2616, step time: 0.5081\n",
      "229/388, train_loss: 0.2939, step time: 0.5658\n",
      "230/388, train_loss: 0.1354, step time: 0.5845\n",
      "231/388, train_loss: 0.1504, step time: 0.5419\n",
      "232/388, train_loss: 0.0915, step time: 0.5149\n",
      "233/388, train_loss: 0.2298, step time: 0.4979\n",
      "234/388, train_loss: 0.1829, step time: 0.5020\n",
      "235/388, train_loss: 0.2196, step time: 0.4885\n",
      "236/388, train_loss: 0.3018, step time: 0.8194\n",
      "237/388, train_loss: 0.1237, step time: 0.5504\n",
      "238/388, train_loss: 0.4728, step time: 0.5273\n",
      "239/388, train_loss: 0.3689, step time: 0.5045\n",
      "240/388, train_loss: 0.4633, step time: 0.5040\n",
      "241/388, train_loss: 0.5457, step time: 0.5001\n",
      "242/388, train_loss: 0.4951, step time: 0.4819\n",
      "243/388, train_loss: 0.1647, step time: 0.4881\n",
      "244/388, train_loss: 0.2940, step time: 0.5373\n",
      "245/388, train_loss: 0.2430, step time: 0.5092\n",
      "246/388, train_loss: 0.2957, step time: 0.4975\n",
      "247/388, train_loss: 0.1062, step time: 0.4796\n",
      "248/388, train_loss: 0.1629, step time: 0.4992\n",
      "249/388, train_loss: 0.1435, step time: 0.4968\n",
      "250/388, train_loss: 0.2136, step time: 0.5011\n",
      "251/388, train_loss: 0.0839, step time: 0.5010\n",
      "252/388, train_loss: 0.3836, step time: 0.4830\n",
      "253/388, train_loss: 0.1523, step time: 1.0858\n",
      "254/388, train_loss: 0.1866, step time: 0.5331\n",
      "255/388, train_loss: 0.2730, step time: 0.5031\n",
      "256/388, train_loss: 0.0421, step time: 0.4952\n",
      "257/388, train_loss: 0.2588, step time: 0.4899\n",
      "258/388, train_loss: 0.2965, step time: 0.4949\n",
      "259/388, train_loss: 0.5359, step time: 0.5101\n",
      "260/388, train_loss: 0.7074, step time: 0.5642\n",
      "261/388, train_loss: 0.3476, step time: 0.5320\n",
      "262/388, train_loss: 0.1746, step time: 0.5063\n",
      "263/388, train_loss: 0.4006, step time: 0.4969\n",
      "264/388, train_loss: 0.0700, step time: 0.5146\n",
      "265/388, train_loss: 0.1575, step time: 0.5557\n",
      "266/388, train_loss: 0.1190, step time: 0.5336\n",
      "267/388, train_loss: 0.1460, step time: 0.5072\n",
      "268/388, train_loss: 0.2176, step time: 0.5259\n",
      "269/388, train_loss: 0.2385, step time: 0.5132\n",
      "270/388, train_loss: 0.1810, step time: 0.4964\n",
      "271/388, train_loss: 0.0974, step time: 0.4961\n",
      "272/388, train_loss: 0.3964, step time: 0.4791\n",
      "273/388, train_loss: 0.2114, step time: 0.5039\n",
      "274/388, train_loss: 0.1299, step time: 0.4963\n",
      "275/388, train_loss: 0.4003, step time: 1.0544\n",
      "276/388, train_loss: 0.1079, step time: 0.5441\n",
      "277/388, train_loss: 0.3555, step time: 0.5234\n",
      "278/388, train_loss: 0.2720, step time: 0.5059\n",
      "279/388, train_loss: 0.2529, step time: 0.4975\n",
      "280/388, train_loss: 0.1668, step time: 0.4950\n",
      "281/388, train_loss: 0.5627, step time: 0.4915\n",
      "282/388, train_loss: 0.1798, step time: 0.4982\n",
      "283/388, train_loss: 0.2087, step time: 0.4829\n",
      "284/388, train_loss: 0.2565, step time: 0.4784\n",
      "285/388, train_loss: 0.2753, step time: 0.4820\n",
      "286/388, train_loss: 0.2377, step time: 0.8015\n",
      "287/388, train_loss: 0.0971, step time: 0.5576\n",
      "288/388, train_loss: 0.3040, step time: 0.5207\n",
      "289/388, train_loss: 0.1245, step time: 0.4939\n",
      "290/388, train_loss: 0.0789, step time: 0.4862\n",
      "291/388, train_loss: 0.0850, step time: 0.5337\n",
      "292/388, train_loss: 0.1203, step time: 0.5041\n",
      "293/388, train_loss: 0.6536, step time: 0.5004\n",
      "294/388, train_loss: 0.3015, step time: 0.4831\n",
      "295/388, train_loss: 0.1432, step time: 0.5213\n",
      "296/388, train_loss: 0.1253, step time: 0.5213\n",
      "297/388, train_loss: 0.2115, step time: 0.5037\n",
      "298/388, train_loss: 0.2528, step time: 0.4946\n",
      "299/388, train_loss: 0.2847, step time: 1.0220\n",
      "300/388, train_loss: 0.1113, step time: 0.5378\n",
      "301/388, train_loss: 0.0706, step time: 0.5138\n",
      "302/388, train_loss: 0.3180, step time: 0.4919\n",
      "303/388, train_loss: 0.2835, step time: 0.4840\n",
      "304/388, train_loss: 0.3353, step time: 0.5053\n",
      "305/388, train_loss: 0.2552, step time: 0.5431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/388, train_loss: 0.1007, step time: 0.5306\n",
      "307/388, train_loss: 0.2124, step time: 0.5065\n",
      "308/388, train_loss: 0.2757, step time: 0.5086\n",
      "309/388, train_loss: 0.1904, step time: 0.6107\n",
      "310/388, train_loss: 0.1839, step time: 0.5465\n",
      "311/388, train_loss: 0.2280, step time: 0.5172\n",
      "312/388, train_loss: 0.3741, step time: 0.5058\n",
      "313/388, train_loss: 0.1257, step time: 0.4916\n",
      "314/388, train_loss: 0.5455, step time: 0.4894\n",
      "315/388, train_loss: 0.2211, step time: 0.4832\n",
      "316/388, train_loss: 0.1768, step time: 0.4842\n",
      "317/388, train_loss: 0.4071, step time: 1.1361\n",
      "318/388, train_loss: 0.1354, step time: 0.5361\n",
      "319/388, train_loss: 0.0968, step time: 0.5138\n",
      "320/388, train_loss: 0.1939, step time: 0.4988\n",
      "321/388, train_loss: 0.3030, step time: 0.4934\n",
      "322/388, train_loss: 0.3462, step time: 0.5024\n",
      "323/388, train_loss: 0.4661, step time: 0.5032\n",
      "324/388, train_loss: 0.3780, step time: 0.4868\n",
      "325/388, train_loss: 0.2744, step time: 0.5087\n",
      "326/388, train_loss: 0.2055, step time: 0.4991\n",
      "327/388, train_loss: 0.2356, step time: 0.4982\n",
      "328/388, train_loss: 0.3642, step time: 0.4892\n",
      "329/388, train_loss: 0.3662, step time: 0.5020\n",
      "330/388, train_loss: 0.1625, step time: 0.5789\n",
      "331/388, train_loss: 0.1697, step time: 0.5369\n",
      "332/388, train_loss: 0.1684, step time: 0.5129\n",
      "333/388, train_loss: 0.2325, step time: 0.4970\n",
      "334/388, train_loss: 0.5676, step time: 0.4965\n",
      "335/388, train_loss: 0.1954, step time: 1.1614\n",
      "336/388, train_loss: 0.1312, step time: 0.5372\n",
      "337/388, train_loss: 0.2206, step time: 0.5158\n",
      "338/388, train_loss: 0.1536, step time: 0.4995\n",
      "339/388, train_loss: 0.1636, step time: 0.4905\n",
      "340/388, train_loss: 0.0617, step time: 0.9992\n",
      "341/388, train_loss: 0.2281, step time: 0.5302\n",
      "342/388, train_loss: 0.3100, step time: 0.5027\n",
      "343/388, train_loss: 0.2693, step time: 0.4978\n",
      "344/388, train_loss: 0.1748, step time: 0.4879\n",
      "345/388, train_loss: 0.4739, step time: 0.4901\n",
      "346/388, train_loss: 0.2209, step time: 0.4842\n",
      "347/388, train_loss: 0.4898, step time: 0.4745\n",
      "348/388, train_loss: 0.1253, step time: 1.0300\n",
      "349/388, train_loss: 0.1787, step time: 0.5407\n",
      "350/388, train_loss: 0.1197, step time: 0.5130\n",
      "351/388, train_loss: 0.3339, step time: 0.4937\n",
      "352/388, train_loss: 0.4298, step time: 0.4921\n",
      "353/388, train_loss: 0.1221, step time: 0.4756\n",
      "354/388, train_loss: 0.0893, step time: 0.4734\n",
      "355/388, train_loss: 0.1801, step time: 0.9478\n",
      "356/388, train_loss: 0.1033, step time: 0.5391\n",
      "357/388, train_loss: 0.2564, step time: 0.5142\n",
      "358/388, train_loss: 0.0702, step time: 0.4929\n",
      "359/388, train_loss: 0.2670, step time: 0.4932\n",
      "360/388, train_loss: 0.2765, step time: 0.4908\n",
      "361/388, train_loss: 0.1189, step time: 0.4793\n",
      "362/388, train_loss: 0.3735, step time: 0.4908\n",
      "363/388, train_loss: 0.1135, step time: 1.1496\n",
      "364/388, train_loss: 0.2691, step time: 0.5377\n",
      "365/388, train_loss: 0.6251, step time: 0.5033\n",
      "366/388, train_loss: 0.1357, step time: 0.5005\n",
      "367/388, train_loss: 0.1287, step time: 0.4864\n",
      "368/388, train_loss: 0.2546, step time: 0.4843\n",
      "369/388, train_loss: 0.6313, step time: 1.1760\n",
      "370/388, train_loss: 0.2126, step time: 0.5328\n",
      "371/388, train_loss: 0.2054, step time: 0.5058\n",
      "372/388, train_loss: 0.3635, step time: 0.4878\n",
      "373/388, train_loss: 0.1164, step time: 0.5015\n",
      "374/388, train_loss: 0.1618, step time: 0.4813\n",
      "375/388, train_loss: 0.3117, step time: 0.4836\n",
      "376/388, train_loss: 0.6214, step time: 0.4877\n",
      "377/388, train_loss: 0.1368, step time: 0.4804\n",
      "378/388, train_loss: 0.5371, step time: 1.0446\n",
      "379/388, train_loss: 0.2708, step time: 0.5393\n",
      "380/388, train_loss: 0.0877, step time: 0.5025\n",
      "381/388, train_loss: 0.7303, step time: 0.4943\n",
      "382/388, train_loss: 0.1421, step time: 0.5047\n",
      "383/388, train_loss: 0.1049, step time: 0.5876\n",
      "384/388, train_loss: 0.0800, step time: 0.5254\n",
      "385/388, train_loss: 0.1401, step time: 0.5015\n",
      "386/388, train_loss: 0.2222, step time: 0.5485\n",
      "387/388, train_loss: 0.1110, step time: 0.5004\n",
      "388/388, train_loss: 0.4265, step time: 0.5030\n",
      "epoch 15 average loss: 0.2476\n",
      "current epoch: 15 current mean dice: 0.7107 tc: 0.7578 wt: 0.8830 et: 0.4913\n",
      "best mean dice: 0.7270 at epoch: 14\n",
      "time consuming of epoch 15 is: 301.3635\n",
      "----------\n",
      "epoch 16/300\n",
      "1/388, train_loss: 0.1281, step time: 0.4700\n",
      "2/388, train_loss: 0.2516, step time: 0.4765\n",
      "3/388, train_loss: 0.7144, step time: 0.5002\n",
      "4/388, train_loss: 0.0945, step time: 0.8513\n",
      "5/388, train_loss: 0.4780, step time: 0.5534\n",
      "6/388, train_loss: 0.4371, step time: 0.5226\n",
      "7/388, train_loss: 0.2656, step time: 0.5464\n",
      "8/388, train_loss: 0.1031, step time: 0.4999\n",
      "9/388, train_loss: 0.3953, step time: 0.5140\n",
      "10/388, train_loss: 0.1822, step time: 0.5095\n",
      "11/388, train_loss: 0.3570, step time: 0.5384\n",
      "12/388, train_loss: 0.1464, step time: 0.6445\n",
      "13/388, train_loss: 0.3784, step time: 0.5569\n",
      "14/388, train_loss: 0.4061, step time: 0.5341\n",
      "15/388, train_loss: 0.1698, step time: 0.5192\n",
      "16/388, train_loss: 0.0989, step time: 0.5160\n",
      "17/388, train_loss: 0.1923, step time: 0.5817\n",
      "18/388, train_loss: 0.4777, step time: 0.5773\n",
      "19/388, train_loss: 0.1957, step time: 0.5636\n",
      "20/388, train_loss: 0.4623, step time: 0.5336\n",
      "21/388, train_loss: 0.7027, step time: 0.4993\n",
      "22/388, train_loss: 0.2255, step time: 0.5009\n",
      "23/388, train_loss: 0.1043, step time: 0.4931\n",
      "24/388, train_loss: 0.4240, step time: 0.6599\n",
      "25/388, train_loss: 0.1046, step time: 0.5634\n",
      "26/388, train_loss: 0.6228, step time: 0.5360\n",
      "27/388, train_loss: 0.5967, step time: 0.5579\n",
      "28/388, train_loss: 0.2584, step time: 0.5355\n",
      "29/388, train_loss: 0.0844, step time: 0.5142\n",
      "30/388, train_loss: 0.2311, step time: 0.4863\n",
      "31/388, train_loss: 0.0617, step time: 0.4786\n",
      "32/388, train_loss: 0.3435, step time: 0.4854\n",
      "33/388, train_loss: 0.2444, step time: 0.5522\n",
      "34/388, train_loss: 0.0935, step time: 0.5413\n",
      "35/388, train_loss: 0.0934, step time: 0.5062\n",
      "36/388, train_loss: 0.1391, step time: 0.4911\n",
      "37/388, train_loss: 0.1614, step time: 0.5157\n",
      "38/388, train_loss: 0.1391, step time: 0.6179\n",
      "39/388, train_loss: 0.6538, step time: 0.5641\n",
      "40/388, train_loss: 0.1438, step time: 0.5462\n",
      "41/388, train_loss: 0.1430, step time: 0.5226\n",
      "42/388, train_loss: 0.1960, step time: 0.5252\n",
      "43/388, train_loss: 0.1669, step time: 0.5405\n",
      "44/388, train_loss: 0.2418, step time: 0.6191\n",
      "45/388, train_loss: 0.6045, step time: 0.5587\n",
      "46/388, train_loss: 0.2455, step time: 0.5310\n",
      "47/388, train_loss: 0.5975, step time: 0.5162\n",
      "48/388, train_loss: 0.3106, step time: 0.9503\n",
      "49/388, train_loss: 0.2930, step time: 0.5468\n",
      "50/388, train_loss: 0.2623, step time: 0.5058\n",
      "51/388, train_loss: 0.2780, step time: 0.5026\n",
      "52/388, train_loss: 0.0898, step time: 0.4870\n",
      "53/388, train_loss: 0.3525, step time: 0.4847\n",
      "54/388, train_loss: 0.1160, step time: 1.0993\n",
      "55/388, train_loss: 0.1644, step time: 0.5329\n",
      "56/388, train_loss: 0.1258, step time: 0.5084\n",
      "57/388, train_loss: 0.1800, step time: 0.4977\n",
      "58/388, train_loss: 0.1569, step time: 0.4892\n",
      "59/388, train_loss: 0.1644, step time: 0.4973\n",
      "60/388, train_loss: 0.2296, step time: 0.4847\n",
      "61/388, train_loss: 0.2810, step time: 0.4979\n",
      "62/388, train_loss: 0.0573, step time: 0.4946\n",
      "63/388, train_loss: 0.1963, step time: 0.4839\n",
      "64/388, train_loss: 0.1975, step time: 0.4901\n",
      "65/388, train_loss: 0.1919, step time: 0.5766\n",
      "66/388, train_loss: 0.1980, step time: 0.5460\n",
      "67/388, train_loss: 0.3673, step time: 0.5329\n",
      "68/388, train_loss: 0.4099, step time: 0.5256\n",
      "69/388, train_loss: 0.2712, step time: 0.5148\n",
      "70/388, train_loss: 0.2411, step time: 0.5050\n",
      "71/388, train_loss: 0.3539, step time: 0.5447\n",
      "72/388, train_loss: 0.3748, step time: 0.5147\n",
      "73/388, train_loss: 0.3704, step time: 0.5067\n",
      "74/388, train_loss: 0.1628, step time: 0.4866\n",
      "75/388, train_loss: 0.2415, step time: 0.4954\n",
      "76/388, train_loss: 0.2109, step time: 0.4856\n",
      "77/388, train_loss: 0.3043, step time: 0.4927\n",
      "78/388, train_loss: 0.1587, step time: 0.4929\n",
      "79/388, train_loss: 0.1681, step time: 1.0153\n",
      "80/388, train_loss: 0.3467, step time: 0.5335\n",
      "81/388, train_loss: 0.1116, step time: 0.5062\n",
      "82/388, train_loss: 0.2000, step time: 0.4947\n",
      "83/388, train_loss: 0.2073, step time: 0.4852\n",
      "84/388, train_loss: 0.1846, step time: 0.5224\n",
      "85/388, train_loss: 0.2710, step time: 0.5564\n",
      "86/388, train_loss: 0.5802, step time: 0.5355\n",
      "87/388, train_loss: 0.2527, step time: 0.5089\n",
      "88/388, train_loss: 0.4136, step time: 0.5022\n",
      "89/388, train_loss: 0.0991, step time: 0.5043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/388, train_loss: 0.1761, step time: 0.4871\n",
      "91/388, train_loss: 0.2934, step time: 0.4865\n",
      "92/388, train_loss: 0.6210, step time: 0.4754\n",
      "93/388, train_loss: 0.2033, step time: 0.4869\n",
      "94/388, train_loss: 0.1028, step time: 0.5176\n",
      "95/388, train_loss: 0.3752, step time: 0.5136\n",
      "96/388, train_loss: 0.1156, step time: 0.4939\n",
      "97/388, train_loss: 0.2455, step time: 0.4935\n",
      "98/388, train_loss: 0.1461, step time: 0.4786\n",
      "99/388, train_loss: 0.2932, step time: 0.9868\n",
      "100/388, train_loss: 0.3296, step time: 0.5330\n",
      "101/388, train_loss: 0.4986, step time: 0.5088\n",
      "102/388, train_loss: 0.2370, step time: 0.4971\n",
      "103/388, train_loss: 0.1784, step time: 0.4837\n",
      "104/388, train_loss: 0.3557, step time: 0.4831\n",
      "105/388, train_loss: 0.1956, step time: 0.4839\n",
      "106/388, train_loss: 0.0786, step time: 0.4714\n",
      "107/388, train_loss: 0.2226, step time: 0.4755\n",
      "108/388, train_loss: 0.0944, step time: 0.4773\n",
      "109/388, train_loss: 0.5941, step time: 0.5111\n",
      "110/388, train_loss: 0.0950, step time: 0.5000\n",
      "111/388, train_loss: 0.1717, step time: 0.4861\n",
      "112/388, train_loss: 0.2583, step time: 0.5101\n",
      "113/388, train_loss: 0.1197, step time: 0.4856\n",
      "114/388, train_loss: 0.3162, step time: 0.5139\n",
      "115/388, train_loss: 0.1110, step time: 0.4860\n",
      "116/388, train_loss: 0.2055, step time: 0.4847\n",
      "117/388, train_loss: 0.1351, step time: 0.4747\n",
      "118/388, train_loss: 0.1917, step time: 0.4965\n",
      "119/388, train_loss: 0.1055, step time: 0.5191\n",
      "120/388, train_loss: 0.2728, step time: 0.6313\n",
      "121/388, train_loss: 0.3116, step time: 0.5557\n",
      "122/388, train_loss: 0.1840, step time: 0.5335\n",
      "123/388, train_loss: 0.3193, step time: 0.5133\n",
      "124/388, train_loss: 0.2184, step time: 0.5066\n",
      "125/388, train_loss: 0.1128, step time: 0.4838\n",
      "126/388, train_loss: 0.0535, step time: 0.4844\n",
      "127/388, train_loss: 0.2333, step time: 0.5355\n",
      "128/388, train_loss: 0.3556, step time: 0.5014\n",
      "129/388, train_loss: 0.3179, step time: 0.4929\n",
      "130/388, train_loss: 0.2757, step time: 0.4891\n",
      "131/388, train_loss: 0.2891, step time: 0.5390\n",
      "132/388, train_loss: 0.1247, step time: 0.5877\n",
      "133/388, train_loss: 0.4660, step time: 0.5304\n",
      "134/388, train_loss: 0.2453, step time: 0.5038\n",
      "135/388, train_loss: 0.3633, step time: 0.4964\n",
      "136/388, train_loss: 0.3198, step time: 0.4806\n",
      "137/388, train_loss: 0.3157, step time: 0.5052\n",
      "138/388, train_loss: 0.2642, step time: 0.4948\n",
      "139/388, train_loss: 0.1763, step time: 0.4791\n",
      "140/388, train_loss: 0.1661, step time: 0.5990\n",
      "141/388, train_loss: 0.1501, step time: 0.5963\n",
      "142/388, train_loss: 0.2464, step time: 0.5216\n",
      "143/388, train_loss: 0.0750, step time: 0.4976\n",
      "144/388, train_loss: 0.1886, step time: 0.5058\n",
      "145/388, train_loss: 0.2099, step time: 0.6087\n",
      "146/388, train_loss: 0.3396, step time: 0.5610\n",
      "147/388, train_loss: 0.3621, step time: 0.5390\n",
      "148/388, train_loss: 0.2771, step time: 0.5140\n",
      "149/388, train_loss: 0.1321, step time: 0.5107\n",
      "150/388, train_loss: 0.1918, step time: 0.5385\n",
      "151/388, train_loss: 0.3271, step time: 0.5228\n",
      "152/388, train_loss: 0.2314, step time: 0.5105\n",
      "153/388, train_loss: 0.3745, step time: 0.4887\n",
      "154/388, train_loss: 0.3584, step time: 1.0460\n",
      "155/388, train_loss: 0.1405, step time: 0.5452\n",
      "156/388, train_loss: 0.2141, step time: 0.5215\n",
      "157/388, train_loss: 0.0823, step time: 0.5086\n",
      "158/388, train_loss: 0.2779, step time: 0.5878\n",
      "159/388, train_loss: 0.2015, step time: 0.5406\n",
      "160/388, train_loss: 0.1745, step time: 0.5132\n",
      "161/388, train_loss: 0.1352, step time: 0.4963\n",
      "162/388, train_loss: 0.2869, step time: 0.4987\n",
      "163/388, train_loss: 0.1496, step time: 0.4820\n",
      "164/388, train_loss: 0.2437, step time: 0.4823\n",
      "165/388, train_loss: 0.2007, step time: 0.4918\n",
      "166/388, train_loss: 0.1536, step time: 0.6970\n",
      "167/388, train_loss: 0.0844, step time: 0.5554\n",
      "168/388, train_loss: 0.2156, step time: 0.5241\n",
      "169/388, train_loss: 0.1505, step time: 0.5055\n",
      "170/388, train_loss: 0.1619, step time: 0.5071\n",
      "171/388, train_loss: 0.2040, step time: 0.4935\n",
      "172/388, train_loss: 0.1706, step time: 0.4954\n",
      "173/388, train_loss: 0.0950, step time: 0.4812\n",
      "174/388, train_loss: 0.2807, step time: 0.5027\n",
      "175/388, train_loss: 0.4811, step time: 0.5118\n",
      "176/388, train_loss: 0.1082, step time: 1.1225\n",
      "177/388, train_loss: 0.1506, step time: 0.5291\n",
      "178/388, train_loss: 0.2761, step time: 0.5073\n",
      "179/388, train_loss: 0.3186, step time: 0.4898\n",
      "180/388, train_loss: 0.2488, step time: 0.4970\n",
      "181/388, train_loss: 0.2037, step time: 0.5021\n",
      "182/388, train_loss: 0.0431, step time: 0.4838\n",
      "183/388, train_loss: 0.3757, step time: 0.4894\n",
      "184/388, train_loss: 0.0979, step time: 0.4970\n",
      "185/388, train_loss: 0.1012, step time: 0.4969\n",
      "186/388, train_loss: 0.1226, step time: 0.4816\n",
      "187/388, train_loss: 0.1460, step time: 0.4758\n",
      "188/388, train_loss: 0.2696, step time: 1.1672\n",
      "189/388, train_loss: 0.6676, step time: 0.5442\n",
      "190/388, train_loss: 0.1501, step time: 0.5119\n",
      "191/388, train_loss: 0.4727, step time: 0.5000\n",
      "192/388, train_loss: 0.1635, step time: 0.4907\n",
      "193/388, train_loss: 0.1745, step time: 0.4782\n",
      "194/388, train_loss: 0.1463, step time: 0.4819\n",
      "195/388, train_loss: 0.1393, step time: 0.4928\n",
      "196/388, train_loss: 0.4125, step time: 0.4904\n",
      "197/388, train_loss: 0.1366, step time: 0.4931\n",
      "198/388, train_loss: 0.1031, step time: 0.4753\n",
      "199/388, train_loss: 0.2539, step time: 0.4977\n",
      "200/388, train_loss: 0.2577, step time: 0.4832\n",
      "201/388, train_loss: 0.1834, step time: 0.4837\n",
      "202/388, train_loss: 0.4628, step time: 1.0035\n",
      "203/388, train_loss: 0.2786, step time: 0.5539\n",
      "204/388, train_loss: 0.2265, step time: 0.5272\n",
      "205/388, train_loss: 0.1766, step time: 0.5020\n",
      "206/388, train_loss: 0.0758, step time: 0.5508\n",
      "207/388, train_loss: 0.1377, step time: 0.5263\n",
      "208/388, train_loss: 0.5401, step time: 0.5035\n",
      "209/388, train_loss: 0.1061, step time: 0.5012\n",
      "210/388, train_loss: 0.1420, step time: 0.4817\n",
      "211/388, train_loss: 0.2017, step time: 0.4781\n",
      "212/388, train_loss: 0.3275, step time: 0.5090\n",
      "213/388, train_loss: 0.1671, step time: 0.4978\n",
      "214/388, train_loss: 0.0735, step time: 0.8697\n",
      "215/388, train_loss: 0.2360, step time: 0.5414\n",
      "216/388, train_loss: 0.1377, step time: 0.5083\n",
      "217/388, train_loss: 0.1532, step time: 0.4931\n",
      "218/388, train_loss: 0.1304, step time: 0.4952\n",
      "219/388, train_loss: 0.2289, step time: 0.4803\n",
      "220/388, train_loss: 0.2383, step time: 0.9938\n",
      "221/388, train_loss: 0.1548, step time: 0.5325\n",
      "222/388, train_loss: 0.3099, step time: 0.5220\n",
      "223/388, train_loss: 0.2806, step time: 0.5022\n",
      "224/388, train_loss: 0.2589, step time: 0.5010\n",
      "225/388, train_loss: 0.2306, step time: 0.4847\n",
      "226/388, train_loss: 0.1098, step time: 0.4863\n",
      "227/388, train_loss: 0.2286, step time: 0.4853\n",
      "228/388, train_loss: 0.1653, step time: 0.4817\n",
      "229/388, train_loss: 0.3208, step time: 1.1028\n",
      "230/388, train_loss: 0.1513, step time: 0.5426\n",
      "231/388, train_loss: 0.6597, step time: 0.5185\n",
      "232/388, train_loss: 0.3274, step time: 0.4858\n",
      "233/388, train_loss: 0.2215, step time: 0.4840\n",
      "234/388, train_loss: 0.1753, step time: 0.4888\n",
      "235/388, train_loss: 0.0948, step time: 0.4784\n",
      "236/388, train_loss: 0.1187, step time: 0.4858\n",
      "237/388, train_loss: 0.2471, step time: 0.5082\n",
      "238/388, train_loss: 0.1125, step time: 0.4997\n",
      "239/388, train_loss: 0.3550, step time: 0.4847\n",
      "240/388, train_loss: 0.3940, step time: 1.1070\n",
      "241/388, train_loss: 0.2099, step time: 0.5382\n",
      "242/388, train_loss: 0.0927, step time: 0.5113\n",
      "243/388, train_loss: 0.1394, step time: 0.4912\n",
      "244/388, train_loss: 0.1245, step time: 0.4955\n",
      "245/388, train_loss: 0.3058, step time: 0.4794\n",
      "246/388, train_loss: 0.2242, step time: 0.4881\n",
      "247/388, train_loss: 0.4591, step time: 0.4900\n",
      "248/388, train_loss: 0.1647, step time: 0.4806\n",
      "249/388, train_loss: 0.1253, step time: 0.4780\n",
      "250/388, train_loss: 0.1328, step time: 0.5064\n",
      "251/388, train_loss: 0.3773, step time: 0.4938\n",
      "252/388, train_loss: 0.2274, step time: 0.5448\n",
      "253/388, train_loss: 0.1375, step time: 0.5164\n",
      "254/388, train_loss: 0.1592, step time: 0.4871\n",
      "255/388, train_loss: 0.5196, step time: 0.9157\n",
      "256/388, train_loss: 0.1673, step time: 0.5539\n",
      "257/388, train_loss: 0.1943, step time: 0.5230\n",
      "258/388, train_loss: 0.3313, step time: 0.5193\n",
      "259/388, train_loss: 0.1892, step time: 0.5527\n",
      "260/388, train_loss: 0.1558, step time: 0.5218\n",
      "261/388, train_loss: 0.4183, step time: 0.5732\n",
      "262/388, train_loss: 0.1305, step time: 0.5262\n",
      "263/388, train_loss: 0.2335, step time: 0.5054\n",
      "264/388, train_loss: 0.1459, step time: 0.5104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/388, train_loss: 0.1632, step time: 0.4861\n",
      "266/388, train_loss: 0.1761, step time: 0.4859\n",
      "267/388, train_loss: 0.2342, step time: 1.0992\n",
      "268/388, train_loss: 0.0447, step time: 0.5485\n",
      "269/388, train_loss: 0.2209, step time: 0.5229\n",
      "270/388, train_loss: 0.1724, step time: 0.4989\n",
      "271/388, train_loss: 0.1490, step time: 0.4978\n",
      "272/388, train_loss: 0.4109, step time: 0.4804\n",
      "273/388, train_loss: 0.5963, step time: 0.4781\n",
      "274/388, train_loss: 0.2320, step time: 0.4891\n",
      "275/388, train_loss: 0.2128, step time: 0.4987\n",
      "276/388, train_loss: 0.6911, step time: 0.5217\n",
      "277/388, train_loss: 0.1408, step time: 0.6031\n",
      "278/388, train_loss: 0.3931, step time: 0.5521\n",
      "279/388, train_loss: 0.3395, step time: 0.5301\n",
      "280/388, train_loss: 0.2973, step time: 0.5116\n",
      "281/388, train_loss: 0.1133, step time: 0.5016\n",
      "282/388, train_loss: 0.3747, step time: 0.4980\n",
      "283/388, train_loss: 0.6074, step time: 0.4799\n",
      "284/388, train_loss: 0.4857, step time: 1.1774\n",
      "285/388, train_loss: 0.1208, step time: 0.5412\n",
      "286/388, train_loss: 0.4087, step time: 0.5147\n",
      "287/388, train_loss: 0.2908, step time: 0.4926\n",
      "288/388, train_loss: 0.6774, step time: 0.4974\n",
      "289/388, train_loss: 0.2201, step time: 0.4836\n",
      "290/388, train_loss: 0.3616, step time: 0.4871\n",
      "291/388, train_loss: 0.3697, step time: 0.7767\n",
      "292/388, train_loss: 0.3646, step time: 0.5455\n",
      "293/388, train_loss: 0.2609, step time: 0.5176\n",
      "294/388, train_loss: 0.2686, step time: 0.5069\n",
      "295/388, train_loss: 0.6760, step time: 0.4912\n",
      "296/388, train_loss: 0.1840, step time: 0.4973\n",
      "297/388, train_loss: 0.2096, step time: 0.4860\n",
      "298/388, train_loss: 0.1358, step time: 0.9865\n",
      "299/388, train_loss: 0.2922, step time: 0.5277\n",
      "300/388, train_loss: 0.1894, step time: 0.4960\n",
      "301/388, train_loss: 0.1816, step time: 0.4896\n",
      "302/388, train_loss: 0.0920, step time: 0.4867\n",
      "303/388, train_loss: 0.1194, step time: 0.5000\n",
      "304/388, train_loss: 0.1142, step time: 0.4906\n",
      "305/388, train_loss: 0.3115, step time: 0.4955\n",
      "306/388, train_loss: 0.1439, step time: 0.4870\n",
      "307/388, train_loss: 0.2209, step time: 0.4958\n",
      "308/388, train_loss: 0.1369, step time: 0.4870\n",
      "309/388, train_loss: 0.4047, step time: 1.1308\n",
      "310/388, train_loss: 0.4712, step time: 0.5382\n",
      "311/388, train_loss: 0.2060, step time: 0.5149\n",
      "312/388, train_loss: 0.1676, step time: 0.4917\n",
      "313/388, train_loss: 0.2874, step time: 0.4982\n",
      "314/388, train_loss: 0.1643, step time: 0.4957\n",
      "315/388, train_loss: 0.1140, step time: 0.4867\n",
      "316/388, train_loss: 0.2443, step time: 0.4946\n",
      "317/388, train_loss: 0.0866, step time: 0.4973\n",
      "318/388, train_loss: 0.0894, step time: 0.4926\n",
      "319/388, train_loss: 0.1577, step time: 0.4982\n",
      "320/388, train_loss: 0.3089, step time: 1.1961\n",
      "321/388, train_loss: 0.4578, step time: 0.5372\n",
      "322/388, train_loss: 0.1680, step time: 0.5015\n",
      "323/388, train_loss: 0.0808, step time: 0.4955\n",
      "324/388, train_loss: 0.1789, step time: 0.4821\n",
      "325/388, train_loss: 0.4186, step time: 0.4850\n",
      "326/388, train_loss: 0.3229, step time: 1.1454\n",
      "327/388, train_loss: 0.1360, step time: 0.5307\n",
      "328/388, train_loss: 0.1117, step time: 0.5087\n",
      "329/388, train_loss: 0.1108, step time: 0.4934\n",
      "330/388, train_loss: 0.1715, step time: 0.4842\n",
      "331/388, train_loss: 0.2908, step time: 0.5110\n",
      "332/388, train_loss: 0.1522, step time: 0.4985\n",
      "333/388, train_loss: 0.2218, step time: 0.4946\n",
      "334/388, train_loss: 0.2246, step time: 0.4816\n",
      "335/388, train_loss: 0.1209, step time: 0.4878\n",
      "336/388, train_loss: 0.1425, step time: 0.5227\n",
      "337/388, train_loss: 0.1160, step time: 0.5017\n",
      "338/388, train_loss: 0.1098, step time: 0.5010\n",
      "339/388, train_loss: 0.1993, step time: 0.5516\n",
      "340/388, train_loss: 0.0940, step time: 0.5263\n",
      "341/388, train_loss: 0.1340, step time: 0.6047\n",
      "342/388, train_loss: 0.0709, step time: 0.5431\n",
      "343/388, train_loss: 0.6626, step time: 0.5086\n",
      "344/388, train_loss: 0.3776, step time: 0.5446\n",
      "345/388, train_loss: 0.2071, step time: 0.5064\n",
      "346/388, train_loss: 0.1984, step time: 0.5016\n",
      "347/388, train_loss: 0.2187, step time: 0.4909\n",
      "348/388, train_loss: 0.0974, step time: 0.4896\n",
      "349/388, train_loss: 0.2819, step time: 0.4988\n",
      "350/388, train_loss: 0.1804, step time: 0.4789\n",
      "351/388, train_loss: 0.1644, step time: 0.5016\n",
      "352/388, train_loss: 0.2713, step time: 0.5312\n",
      "353/388, train_loss: 0.2390, step time: 0.5346\n",
      "354/388, train_loss: 0.2271, step time: 0.5164\n",
      "355/388, train_loss: 0.1125, step time: 0.5101\n",
      "356/388, train_loss: 0.5841, step time: 0.4966\n",
      "357/388, train_loss: 0.3524, step time: 0.5040\n",
      "358/388, train_loss: 0.1512, step time: 0.4957\n",
      "359/388, train_loss: 0.1024, step time: 0.4854\n",
      "360/388, train_loss: 0.3747, step time: 0.4833\n",
      "361/388, train_loss: 0.0625, step time: 0.7963\n",
      "362/388, train_loss: 0.1592, step time: 0.5924\n",
      "363/388, train_loss: 0.2554, step time: 0.5414\n",
      "364/388, train_loss: 0.1196, step time: 0.5136\n",
      "365/388, train_loss: 0.2894, step time: 0.4948\n",
      "366/388, train_loss: 0.1032, step time: 1.1485\n",
      "367/388, train_loss: 0.2162, step time: 0.5614\n",
      "368/388, train_loss: 0.2408, step time: 0.5165\n",
      "369/388, train_loss: 0.1734, step time: 0.4933\n",
      "370/388, train_loss: 0.3992, step time: 0.4955\n",
      "371/388, train_loss: 0.1324, step time: 0.4901\n",
      "372/388, train_loss: 0.3520, step time: 0.4784\n",
      "373/388, train_loss: 0.2748, step time: 0.5501\n",
      "374/388, train_loss: 0.2393, step time: 0.5388\n",
      "375/388, train_loss: 0.4734, step time: 0.5126\n",
      "376/388, train_loss: 0.1326, step time: 0.4991\n",
      "377/388, train_loss: 0.3863, step time: 0.4906\n",
      "378/388, train_loss: 0.2696, step time: 0.4985\n",
      "379/388, train_loss: 0.2365, step time: 0.5509\n",
      "380/388, train_loss: 0.3462, step time: 0.5205\n",
      "381/388, train_loss: 0.1827, step time: 0.4913\n",
      "382/388, train_loss: 0.1724, step time: 1.1351\n",
      "383/388, train_loss: 0.3298, step time: 0.5017\n",
      "384/388, train_loss: 0.5253, step time: 0.5039\n",
      "385/388, train_loss: 0.1952, step time: 0.5223\n",
      "386/388, train_loss: 0.2484, step time: 0.5259\n",
      "387/388, train_loss: 0.3172, step time: 0.5041\n",
      "388/388, train_loss: 0.3259, step time: 0.5846\n",
      "epoch 16 average loss: 0.2445\n",
      "current epoch: 16 current mean dice: 0.7127 tc: 0.7491 wt: 0.8829 et: 0.5061\n",
      "best mean dice: 0.7270 at epoch: 14\n",
      "time consuming of epoch 16 is: 301.9594\n",
      "----------\n",
      "epoch 17/300\n",
      "1/388, train_loss: 0.2116, step time: 0.4781\n",
      "2/388, train_loss: 0.3351, step time: 0.4951\n",
      "3/388, train_loss: 0.1720, step time: 1.1644\n",
      "4/388, train_loss: 0.1295, step time: 0.5378\n",
      "5/388, train_loss: 0.3324, step time: 0.5148\n",
      "6/388, train_loss: 0.3049, step time: 0.4970\n",
      "7/388, train_loss: 0.4125, step time: 0.9297\n",
      "8/388, train_loss: 0.1502, step time: 0.5715\n",
      "9/388, train_loss: 0.1429, step time: 0.5214\n",
      "10/388, train_loss: 0.2079, step time: 0.5043\n",
      "11/388, train_loss: 0.3490, step time: 0.4974\n",
      "12/388, train_loss: 0.0566, step time: 0.9168\n",
      "13/388, train_loss: 0.1215, step time: 0.5399\n",
      "14/388, train_loss: 0.0921, step time: 0.5327\n",
      "15/388, train_loss: 0.3942, step time: 0.5058\n",
      "16/388, train_loss: 0.1861, step time: 0.5471\n",
      "17/388, train_loss: 0.2001, step time: 0.6693\n",
      "18/388, train_loss: 0.2565, step time: 0.5352\n",
      "19/388, train_loss: 0.1727, step time: 0.4968\n",
      "20/388, train_loss: 0.1475, step time: 0.5533\n",
      "21/388, train_loss: 0.2772, step time: 0.5127\n",
      "22/388, train_loss: 0.3599, step time: 0.4976\n",
      "23/388, train_loss: 0.3441, step time: 0.5357\n",
      "24/388, train_loss: 0.1130, step time: 0.5335\n",
      "25/388, train_loss: 0.2213, step time: 0.5286\n",
      "26/388, train_loss: 0.2439, step time: 0.5179\n",
      "27/388, train_loss: 0.5992, step time: 0.5081\n",
      "28/388, train_loss: 0.2798, step time: 0.4949\n",
      "29/388, train_loss: 0.2542, step time: 0.5086\n",
      "30/388, train_loss: 0.1856, step time: 0.5577\n",
      "31/388, train_loss: 0.2237, step time: 0.5298\n",
      "32/388, train_loss: 0.5175, step time: 0.5079\n",
      "33/388, train_loss: 0.2394, step time: 1.1756\n",
      "34/388, train_loss: 0.4628, step time: 0.5451\n",
      "35/388, train_loss: 0.1065, step time: 0.5133\n",
      "36/388, train_loss: 0.3310, step time: 0.4919\n",
      "37/388, train_loss: 0.0811, step time: 0.5024\n",
      "38/388, train_loss: 0.3991, step time: 0.4923\n",
      "39/388, train_loss: 0.3090, step time: 1.0913\n",
      "40/388, train_loss: 0.1671, step time: 0.5325\n",
      "41/388, train_loss: 0.2041, step time: 0.4985\n",
      "42/388, train_loss: 0.3670, step time: 0.5102\n",
      "43/388, train_loss: 0.1862, step time: 0.4964\n",
      "44/388, train_loss: 0.2831, step time: 0.4993\n",
      "45/388, train_loss: 0.0977, step time: 0.8088\n",
      "46/388, train_loss: 0.1135, step time: 0.5583\n",
      "47/388, train_loss: 0.1465, step time: 0.5282\n",
      "48/388, train_loss: 0.2427, step time: 0.5059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/388, train_loss: 0.1978, step time: 0.4881\n",
      "50/388, train_loss: 0.1645, step time: 0.4728\n",
      "51/388, train_loss: 0.2129, step time: 1.0666\n",
      "52/388, train_loss: 0.1114, step time: 0.5323\n",
      "53/388, train_loss: 0.6730, step time: 0.5157\n",
      "54/388, train_loss: 0.1438, step time: 0.4987\n",
      "55/388, train_loss: 0.4309, step time: 0.4950\n",
      "56/388, train_loss: 0.2098, step time: 0.9680\n",
      "57/388, train_loss: 0.1210, step time: 0.5378\n",
      "58/388, train_loss: 0.6838, step time: 0.5124\n",
      "59/388, train_loss: 0.1706, step time: 0.4973\n",
      "60/388, train_loss: 0.1188, step time: 0.4826\n",
      "61/388, train_loss: 0.3334, step time: 0.4829\n",
      "62/388, train_loss: 0.1954, step time: 0.4835\n",
      "63/388, train_loss: 0.2697, step time: 0.4753\n",
      "64/388, train_loss: 0.3157, step time: 0.5162\n",
      "65/388, train_loss: 0.2143, step time: 0.5068\n",
      "66/388, train_loss: 0.0822, step time: 0.4881\n",
      "67/388, train_loss: 0.2888, step time: 0.4907\n",
      "68/388, train_loss: 0.1706, step time: 0.5067\n",
      "69/388, train_loss: 0.1425, step time: 0.5512\n",
      "70/388, train_loss: 0.1147, step time: 0.5124\n",
      "71/388, train_loss: 0.1595, step time: 0.4997\n",
      "72/388, train_loss: 0.1363, step time: 0.5110\n",
      "73/388, train_loss: 0.3032, step time: 0.5014\n",
      "74/388, train_loss: 0.1154, step time: 0.5003\n",
      "75/388, train_loss: 0.1169, step time: 0.4825\n",
      "76/388, train_loss: 0.1788, step time: 0.9919\n",
      "77/388, train_loss: 0.2396, step time: 0.5404\n",
      "78/388, train_loss: 0.2880, step time: 0.5264\n",
      "79/388, train_loss: 0.3333, step time: 0.4981\n",
      "80/388, train_loss: 0.2291, step time: 0.4930\n",
      "81/388, train_loss: 0.3749, step time: 0.5113\n",
      "82/388, train_loss: 0.4319, step time: 0.5160\n",
      "83/388, train_loss: 0.3883, step time: 0.5058\n",
      "84/388, train_loss: 0.1259, step time: 0.4931\n",
      "85/388, train_loss: 0.1276, step time: 0.4930\n",
      "86/388, train_loss: 0.1600, step time: 0.4815\n",
      "87/388, train_loss: 0.3643, step time: 0.4831\n",
      "88/388, train_loss: 0.2126, step time: 0.4959\n",
      "89/388, train_loss: 0.4162, step time: 0.4828\n",
      "90/388, train_loss: 0.1499, step time: 0.9908\n",
      "91/388, train_loss: 0.1485, step time: 0.5354\n",
      "92/388, train_loss: 0.3020, step time: 0.5080\n",
      "93/388, train_loss: 0.1628, step time: 0.5151\n",
      "94/388, train_loss: 0.4303, step time: 0.5999\n",
      "95/388, train_loss: 0.2337, step time: 0.5230\n",
      "96/388, train_loss: 0.1522, step time: 0.5125\n",
      "97/388, train_loss: 0.0898, step time: 0.4921\n",
      "98/388, train_loss: 0.2464, step time: 0.4950\n",
      "99/388, train_loss: 0.0803, step time: 1.1661\n",
      "100/388, train_loss: 0.1016, step time: 0.5240\n",
      "101/388, train_loss: 0.1160, step time: 0.4980\n",
      "102/388, train_loss: 0.2592, step time: 0.4847\n",
      "103/388, train_loss: 0.1155, step time: 0.4939\n",
      "104/388, train_loss: 0.5977, step time: 0.4811\n",
      "105/388, train_loss: 0.2625, step time: 0.4786\n",
      "106/388, train_loss: 0.1950, step time: 0.9680\n",
      "107/388, train_loss: 0.1407, step time: 0.5371\n",
      "108/388, train_loss: 0.1724, step time: 0.5081\n",
      "109/388, train_loss: 0.0862, step time: 0.5000\n",
      "110/388, train_loss: 0.2304, step time: 0.4827\n",
      "111/388, train_loss: 0.1079, step time: 0.4880\n",
      "112/388, train_loss: 0.2786, step time: 0.4834\n",
      "113/388, train_loss: 0.2839, step time: 0.5004\n",
      "114/388, train_loss: 0.1475, step time: 0.4803\n",
      "115/388, train_loss: 0.1808, step time: 0.5122\n",
      "116/388, train_loss: 0.2259, step time: 0.5055\n",
      "117/388, train_loss: 0.2548, step time: 0.5543\n",
      "118/388, train_loss: 0.2634, step time: 0.5123\n",
      "119/388, train_loss: 0.1020, step time: 0.5002\n",
      "120/388, train_loss: 0.2307, step time: 0.4968\n",
      "121/388, train_loss: 0.2770, step time: 0.4802\n",
      "122/388, train_loss: 0.1553, step time: 0.4855\n",
      "123/388, train_loss: 0.2431, step time: 0.4785\n",
      "124/388, train_loss: 0.3566, step time: 0.4774\n",
      "125/388, train_loss: 0.1925, step time: 0.4870\n",
      "126/388, train_loss: 0.1253, step time: 0.5488\n",
      "127/388, train_loss: 0.1307, step time: 0.5278\n",
      "128/388, train_loss: 0.1964, step time: 0.5030\n",
      "129/388, train_loss: 0.0403, step time: 0.4848\n",
      "130/388, train_loss: 0.2177, step time: 0.4861\n",
      "131/388, train_loss: 0.1988, step time: 0.4950\n",
      "132/388, train_loss: 0.3243, step time: 0.4810\n",
      "133/388, train_loss: 0.2460, step time: 0.7098\n",
      "134/388, train_loss: 0.2031, step time: 0.5559\n",
      "135/388, train_loss: 0.2826, step time: 0.5281\n",
      "136/388, train_loss: 0.6315, step time: 0.5049\n",
      "137/388, train_loss: 0.4331, step time: 0.4965\n",
      "138/388, train_loss: 0.1098, step time: 0.4906\n",
      "139/388, train_loss: 0.1312, step time: 0.4905\n",
      "140/388, train_loss: 0.1521, step time: 0.4848\n",
      "141/388, train_loss: 0.4526, step time: 0.4983\n",
      "142/388, train_loss: 0.1037, step time: 0.9526\n",
      "143/388, train_loss: 0.2336, step time: 0.5304\n",
      "144/388, train_loss: 0.0926, step time: 0.5029\n",
      "145/388, train_loss: 0.1251, step time: 0.4930\n",
      "146/388, train_loss: 0.0969, step time: 0.4979\n",
      "147/388, train_loss: 0.1774, step time: 0.4828\n",
      "148/388, train_loss: 0.1225, step time: 0.4910\n",
      "149/388, train_loss: 0.1398, step time: 0.4941\n",
      "150/388, train_loss: 0.1460, step time: 0.4842\n",
      "151/388, train_loss: 0.0427, step time: 0.6451\n",
      "152/388, train_loss: 0.1563, step time: 0.5357\n",
      "153/388, train_loss: 0.1735, step time: 0.5093\n",
      "154/388, train_loss: 0.2360, step time: 0.4932\n",
      "155/388, train_loss: 0.3217, step time: 0.5058\n",
      "156/388, train_loss: 0.1620, step time: 0.4886\n",
      "157/388, train_loss: 0.1786, step time: 0.4856\n",
      "158/388, train_loss: 0.3801, step time: 0.4845\n",
      "159/388, train_loss: 0.3222, step time: 0.4816\n",
      "160/388, train_loss: 0.2160, step time: 0.4823\n",
      "161/388, train_loss: 0.3647, step time: 0.5249\n",
      "162/388, train_loss: 0.1181, step time: 0.5222\n",
      "163/388, train_loss: 0.1841, step time: 0.5025\n",
      "164/388, train_loss: 0.2436, step time: 1.1440\n",
      "165/388, train_loss: 0.2708, step time: 0.5365\n",
      "166/388, train_loss: 0.2355, step time: 0.5140\n",
      "167/388, train_loss: 0.2055, step time: 0.5009\n",
      "168/388, train_loss: 0.2492, step time: 0.4944\n",
      "169/388, train_loss: 0.1429, step time: 0.4872\n",
      "170/388, train_loss: 0.3048, step time: 0.4918\n",
      "171/388, train_loss: 0.4241, step time: 1.0285\n",
      "172/388, train_loss: 0.1354, step time: 0.5239\n",
      "173/388, train_loss: 0.3938, step time: 0.5025\n",
      "174/388, train_loss: 0.1959, step time: 0.4949\n",
      "175/388, train_loss: 0.2650, step time: 0.4862\n",
      "176/388, train_loss: 0.2330, step time: 0.4894\n",
      "177/388, train_loss: 0.1452, step time: 0.4891\n",
      "178/388, train_loss: 0.2578, step time: 0.5039\n",
      "179/388, train_loss: 0.2048, step time: 0.5619\n",
      "180/388, train_loss: 0.2126, step time: 0.5284\n",
      "181/388, train_loss: 0.1339, step time: 0.4957\n",
      "182/388, train_loss: 0.1624, step time: 0.5003\n",
      "183/388, train_loss: 0.1242, step time: 0.4849\n",
      "184/388, train_loss: 0.1417, step time: 0.4840\n",
      "185/388, train_loss: 0.1616, step time: 0.4816\n",
      "186/388, train_loss: 0.2273, step time: 0.9158\n",
      "187/388, train_loss: 0.1413, step time: 0.5356\n",
      "188/388, train_loss: 0.1593, step time: 0.5173\n",
      "189/388, train_loss: 0.5469, step time: 0.5000\n",
      "190/388, train_loss: 0.3205, step time: 0.4819\n",
      "191/388, train_loss: 0.1017, step time: 0.4942\n",
      "192/388, train_loss: 0.2143, step time: 0.5004\n",
      "193/388, train_loss: 0.0914, step time: 0.4893\n",
      "194/388, train_loss: 0.1117, step time: 0.4889\n",
      "195/388, train_loss: 0.1931, step time: 0.4773\n",
      "196/388, train_loss: 0.2692, step time: 0.9702\n",
      "197/388, train_loss: 0.2496, step time: 0.5380\n",
      "198/388, train_loss: 0.1223, step time: 0.5042\n",
      "199/388, train_loss: 0.2853, step time: 0.4973\n",
      "200/388, train_loss: 0.5142, step time: 0.4820\n",
      "201/388, train_loss: 0.1328, step time: 0.4824\n",
      "202/388, train_loss: 0.1172, step time: 0.4796\n",
      "203/388, train_loss: 0.1073, step time: 0.4729\n",
      "204/388, train_loss: 0.1778, step time: 0.4858\n",
      "205/388, train_loss: 0.1010, step time: 0.4855\n",
      "206/388, train_loss: 0.2046, step time: 0.5166\n",
      "207/388, train_loss: 0.3365, step time: 0.5019\n",
      "208/388, train_loss: 0.1563, step time: 0.4861\n",
      "209/388, train_loss: 0.1301, step time: 1.1249\n",
      "210/388, train_loss: 0.0997, step time: 0.5340\n",
      "211/388, train_loss: 0.3122, step time: 0.5035\n",
      "212/388, train_loss: 0.1171, step time: 0.4942\n",
      "213/388, train_loss: 0.1966, step time: 0.4954\n",
      "214/388, train_loss: 0.1795, step time: 0.4853\n",
      "215/388, train_loss: 0.3166, step time: 0.4866\n",
      "216/388, train_loss: 0.1393, step time: 0.4959\n",
      "217/388, train_loss: 0.1418, step time: 0.4963\n",
      "218/388, train_loss: 0.1817, step time: 0.5710\n",
      "219/388, train_loss: 0.1402, step time: 0.5632\n",
      "220/388, train_loss: 0.2403, step time: 0.5124\n",
      "221/388, train_loss: 0.1260, step time: 0.5041\n",
      "222/388, train_loss: 0.1387, step time: 0.4991\n",
      "223/388, train_loss: 0.2439, step time: 0.5051\n",
      "224/388, train_loss: 0.1907, step time: 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/388, train_loss: 0.0523, step time: 0.4902\n",
      "226/388, train_loss: 0.0721, step time: 0.4790\n",
      "227/388, train_loss: 0.3538, step time: 0.4812\n",
      "228/388, train_loss: 0.1367, step time: 0.4797\n",
      "229/388, train_loss: 0.1745, step time: 0.4798\n",
      "230/388, train_loss: 0.5019, step time: 0.8306\n",
      "231/388, train_loss: 0.3386, step time: 0.5515\n",
      "232/388, train_loss: 0.0636, step time: 0.5089\n",
      "233/388, train_loss: 0.1786, step time: 0.4811\n",
      "234/388, train_loss: 0.2140, step time: 0.4839\n",
      "235/388, train_loss: 0.3634, step time: 0.4993\n",
      "236/388, train_loss: 0.3017, step time: 0.5164\n",
      "237/388, train_loss: 0.3669, step time: 0.4958\n",
      "238/388, train_loss: 0.1951, step time: 0.5018\n",
      "239/388, train_loss: 0.2007, step time: 0.4884\n",
      "240/388, train_loss: 0.0742, step time: 0.5107\n",
      "241/388, train_loss: 0.1356, step time: 0.4921\n",
      "242/388, train_loss: 0.3609, step time: 0.4952\n",
      "243/388, train_loss: 0.1796, step time: 0.4904\n",
      "244/388, train_loss: 0.2849, step time: 0.5047\n",
      "245/388, train_loss: 0.6380, step time: 0.4981\n",
      "246/388, train_loss: 0.1696, step time: 0.5037\n",
      "247/388, train_loss: 0.1618, step time: 0.5479\n",
      "248/388, train_loss: 0.1095, step time: 0.5159\n",
      "249/388, train_loss: 0.4413, step time: 0.4997\n",
      "250/388, train_loss: 0.1001, step time: 0.4969\n",
      "251/388, train_loss: 0.2999, step time: 0.4864\n",
      "252/388, train_loss: 0.1998, step time: 1.1278\n",
      "253/388, train_loss: 0.5665, step time: 0.5471\n",
      "254/388, train_loss: 0.2264, step time: 0.5118\n",
      "255/388, train_loss: 0.1581, step time: 0.4946\n",
      "256/388, train_loss: 0.4346, step time: 0.4873\n",
      "257/388, train_loss: 0.0995, step time: 0.4812\n",
      "258/388, train_loss: 0.2275, step time: 0.4851\n",
      "259/388, train_loss: 0.1185, step time: 0.4970\n",
      "260/388, train_loss: 0.7016, step time: 0.5012\n",
      "261/388, train_loss: 0.1871, step time: 0.5279\n",
      "262/388, train_loss: 0.2174, step time: 0.5063\n",
      "263/388, train_loss: 0.2737, step time: 0.4942\n",
      "264/388, train_loss: 0.0731, step time: 0.4909\n",
      "265/388, train_loss: 0.1380, step time: 0.4845\n",
      "266/388, train_loss: 0.1511, step time: 0.4941\n",
      "267/388, train_loss: 0.3140, step time: 0.4820\n",
      "268/388, train_loss: 0.1173, step time: 0.4896\n",
      "269/388, train_loss: 0.1136, step time: 0.7346\n",
      "270/388, train_loss: 0.2524, step time: 0.5661\n",
      "271/388, train_loss: 0.2643, step time: 0.5430\n",
      "272/388, train_loss: 0.2896, step time: 0.5083\n",
      "273/388, train_loss: 0.2278, step time: 0.5145\n",
      "274/388, train_loss: 0.1948, step time: 0.5016\n",
      "275/388, train_loss: 0.2872, step time: 0.4963\n",
      "276/388, train_loss: 0.4549, step time: 0.4802\n",
      "277/388, train_loss: 0.3297, step time: 0.5118\n",
      "278/388, train_loss: 0.4705, step time: 0.4943\n",
      "279/388, train_loss: 0.1131, step time: 0.4990\n",
      "280/388, train_loss: 0.1135, step time: 1.0875\n",
      "281/388, train_loss: 0.2824, step time: 0.5318\n",
      "282/388, train_loss: 0.1794, step time: 0.5053\n",
      "283/388, train_loss: 0.1551, step time: 0.4861\n",
      "284/388, train_loss: 0.2532, step time: 0.4804\n",
      "285/388, train_loss: 0.1150, step time: 0.5040\n",
      "286/388, train_loss: 0.2543, step time: 0.5474\n",
      "287/388, train_loss: 0.2378, step time: 0.5162\n",
      "288/388, train_loss: 0.0688, step time: 0.5100\n",
      "289/388, train_loss: 0.5923, step time: 0.4923\n",
      "290/388, train_loss: 0.1088, step time: 0.4939\n",
      "291/388, train_loss: 0.3246, step time: 0.4770\n",
      "292/388, train_loss: 0.5860, step time: 0.4780\n",
      "293/388, train_loss: 0.2413, step time: 1.0502\n",
      "294/388, train_loss: 0.2529, step time: 0.5285\n",
      "295/388, train_loss: 0.1676, step time: 0.5011\n",
      "296/388, train_loss: 0.1173, step time: 0.4870\n",
      "297/388, train_loss: 0.6913, step time: 0.5125\n",
      "298/388, train_loss: 0.6667, step time: 0.5055\n",
      "299/388, train_loss: 0.0705, step time: 0.4984\n",
      "300/388, train_loss: 0.1479, step time: 0.5098\n",
      "301/388, train_loss: 0.3351, step time: 0.5383\n",
      "302/388, train_loss: 0.1103, step time: 0.5115\n",
      "303/388, train_loss: 0.2437, step time: 0.5043\n",
      "304/388, train_loss: 0.3142, step time: 0.4952\n",
      "305/388, train_loss: 0.4063, step time: 0.4805\n",
      "306/388, train_loss: 0.0768, step time: 0.7747\n",
      "307/388, train_loss: 0.2402, step time: 0.5501\n",
      "308/388, train_loss: 0.3495, step time: 0.5308\n",
      "309/388, train_loss: 0.2472, step time: 0.5011\n",
      "310/388, train_loss: 0.1304, step time: 0.4819\n",
      "311/388, train_loss: 0.2220, step time: 0.4909\n",
      "312/388, train_loss: 0.2654, step time: 0.4805\n",
      "313/388, train_loss: 0.1539, step time: 0.4730\n",
      "314/388, train_loss: 0.4402, step time: 0.7621\n",
      "315/388, train_loss: 0.1755, step time: 0.5553\n",
      "316/388, train_loss: 0.2772, step time: 0.5362\n",
      "317/388, train_loss: 0.3000, step time: 0.5044\n",
      "318/388, train_loss: 0.4340, step time: 0.5088\n",
      "319/388, train_loss: 0.1596, step time: 0.4890\n",
      "320/388, train_loss: 0.2307, step time: 0.4867\n",
      "321/388, train_loss: 0.1265, step time: 0.5669\n",
      "322/388, train_loss: 0.2028, step time: 0.5534\n",
      "323/388, train_loss: 0.4283, step time: 0.5255\n",
      "324/388, train_loss: 0.0726, step time: 0.5052\n",
      "325/388, train_loss: 0.4796, step time: 0.4886\n",
      "326/388, train_loss: 0.2973, step time: 1.0683\n",
      "327/388, train_loss: 0.0704, step time: 0.5368\n",
      "328/388, train_loss: 0.3296, step time: 0.5086\n",
      "329/388, train_loss: 0.3344, step time: 0.4980\n",
      "330/388, train_loss: 0.1316, step time: 0.4869\n",
      "331/388, train_loss: 0.1654, step time: 0.4936\n",
      "332/388, train_loss: 0.1586, step time: 1.1931\n",
      "333/388, train_loss: 0.1152, step time: 0.5384\n",
      "334/388, train_loss: 0.3311, step time: 0.5060\n",
      "335/388, train_loss: 0.1137, step time: 0.4940\n",
      "336/388, train_loss: 0.3401, step time: 0.4861\n",
      "337/388, train_loss: 0.3247, step time: 0.4893\n",
      "338/388, train_loss: 0.2103, step time: 0.5010\n",
      "339/388, train_loss: 0.2865, step time: 0.4837\n",
      "340/388, train_loss: 0.2078, step time: 0.5145\n",
      "341/388, train_loss: 0.1037, step time: 0.4935\n",
      "342/388, train_loss: 0.0764, step time: 0.4848\n",
      "343/388, train_loss: 0.3646, step time: 1.2172\n",
      "344/388, train_loss: 0.1444, step time: 0.5311\n",
      "345/388, train_loss: 0.1941, step time: 0.5023\n",
      "346/388, train_loss: 0.2288, step time: 0.4927\n",
      "347/388, train_loss: 0.1116, step time: 0.4948\n",
      "348/388, train_loss: 0.4698, step time: 0.4810\n",
      "349/388, train_loss: 0.2382, step time: 0.4936\n",
      "350/388, train_loss: 0.1525, step time: 0.5215\n",
      "351/388, train_loss: 0.1369, step time: 0.5078\n",
      "352/388, train_loss: 0.3134, step time: 0.5039\n",
      "353/388, train_loss: 0.1273, step time: 0.5073\n",
      "354/388, train_loss: 0.5053, step time: 0.4879\n",
      "355/388, train_loss: 0.4168, step time: 0.5127\n",
      "356/388, train_loss: 0.2713, step time: 0.4949\n",
      "357/388, train_loss: 0.2875, step time: 0.5013\n",
      "358/388, train_loss: 0.0824, step time: 0.4925\n",
      "359/388, train_loss: 0.2418, step time: 0.4968\n",
      "360/388, train_loss: 0.0501, step time: 1.0322\n",
      "361/388, train_loss: 0.2201, step time: 0.5374\n",
      "362/388, train_loss: 0.1111, step time: 0.5125\n",
      "363/388, train_loss: 0.2803, step time: 0.4975\n",
      "364/388, train_loss: 0.1975, step time: 0.4862\n",
      "365/388, train_loss: 0.4967, step time: 1.1544\n",
      "366/388, train_loss: 0.2229, step time: 0.5253\n",
      "367/388, train_loss: 0.3188, step time: 0.4926\n",
      "368/388, train_loss: 0.1239, step time: 0.4944\n",
      "369/388, train_loss: 0.6374, step time: 0.4847\n",
      "370/388, train_loss: 0.4941, step time: 0.4809\n",
      "371/388, train_loss: 0.1079, step time: 0.4859\n",
      "372/388, train_loss: 0.1216, step time: 0.4854\n",
      "373/388, train_loss: 0.2123, step time: 0.4938\n",
      "374/388, train_loss: 0.1357, step time: 0.4770\n",
      "375/388, train_loss: 0.2816, step time: 0.4782\n",
      "376/388, train_loss: 0.1389, step time: 0.4712\n",
      "377/388, train_loss: 0.2839, step time: 1.0235\n",
      "378/388, train_loss: 0.4743, step time: 0.5375\n",
      "379/388, train_loss: 0.0858, step time: 0.5036\n",
      "380/388, train_loss: 0.1712, step time: 0.4945\n",
      "381/388, train_loss: 0.1373, step time: 0.4806\n",
      "382/388, train_loss: 0.1358, step time: 0.4906\n",
      "383/388, train_loss: 0.6903, step time: 0.4797\n",
      "384/388, train_loss: 0.2897, step time: 0.4873\n",
      "385/388, train_loss: 0.1794, step time: 0.4776\n",
      "386/388, train_loss: 0.2021, step time: 0.4722\n",
      "387/388, train_loss: 0.2121, step time: 0.4730\n",
      "388/388, train_loss: 0.0720, step time: 1.0128\n",
      "epoch 17 average loss: 0.2336\n",
      "current epoch: 17 current mean dice: 0.6990 tc: 0.7440 wt: 0.8490 et: 0.5039\n",
      "best mean dice: 0.7270 at epoch: 14\n",
      "time consuming of epoch 17 is: 302.6480\n",
      "----------\n",
      "epoch 18/300\n",
      "1/388, train_loss: 0.2441, step time: 0.4793\n",
      "2/388, train_loss: 0.6840, step time: 0.4866\n",
      "3/388, train_loss: 0.2348, step time: 1.0176\n",
      "4/388, train_loss: 0.3433, step time: 0.5567\n",
      "5/388, train_loss: 0.3547, step time: 0.5337\n",
      "6/388, train_loss: 0.5049, step time: 0.5290\n",
      "7/388, train_loss: 0.3739, step time: 0.5027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/388, train_loss: 0.2999, step time: 0.5070\n",
      "9/388, train_loss: 0.2194, step time: 0.8630\n",
      "10/388, train_loss: 0.4984, step time: 0.5965\n",
      "11/388, train_loss: 0.2433, step time: 0.5712\n",
      "12/388, train_loss: 0.0839, step time: 0.5396\n",
      "13/388, train_loss: 0.3758, step time: 0.5075\n",
      "14/388, train_loss: 0.1198, step time: 0.4898\n",
      "15/388, train_loss: 0.1206, step time: 1.1169\n",
      "16/388, train_loss: 0.5506, step time: 0.5393\n",
      "17/388, train_loss: 0.2920, step time: 0.5173\n",
      "18/388, train_loss: 0.2653, step time: 0.4978\n",
      "19/388, train_loss: 0.6863, step time: 0.7251\n",
      "20/388, train_loss: 0.2917, step time: 0.5753\n",
      "21/388, train_loss: 0.1291, step time: 0.5258\n",
      "22/388, train_loss: 0.1879, step time: 0.5508\n",
      "23/388, train_loss: 0.3416, step time: 0.5114\n",
      "24/388, train_loss: 0.2996, step time: 0.4879\n",
      "25/388, train_loss: 0.2392, step time: 0.5176\n",
      "26/388, train_loss: 0.2167, step time: 0.5109\n",
      "27/388, train_loss: 0.1097, step time: 0.5719\n",
      "28/388, train_loss: 0.1949, step time: 0.5455\n",
      "29/388, train_loss: 0.1945, step time: 0.5112\n",
      "30/388, train_loss: 0.1631, step time: 0.9833\n",
      "31/388, train_loss: 0.1057, step time: 0.5449\n",
      "32/388, train_loss: 0.1813, step time: 0.5161\n",
      "33/388, train_loss: 0.0849, step time: 0.4957\n",
      "34/388, train_loss: 0.1379, step time: 0.4966\n",
      "35/388, train_loss: 0.3745, step time: 0.5099\n",
      "36/388, train_loss: 0.2003, step time: 0.5703\n",
      "37/388, train_loss: 0.6596, step time: 0.5536\n",
      "38/388, train_loss: 0.1942, step time: 0.5060\n",
      "39/388, train_loss: 0.3111, step time: 0.5246\n",
      "40/388, train_loss: 0.6160, step time: 0.4927\n",
      "41/388, train_loss: 0.4248, step time: 1.0244\n",
      "42/388, train_loss: 0.1819, step time: 0.5457\n",
      "43/388, train_loss: 0.2951, step time: 0.5185\n",
      "44/388, train_loss: 0.5301, step time: 0.4898\n",
      "45/388, train_loss: 0.1517, step time: 0.5577\n",
      "46/388, train_loss: 0.1801, step time: 0.5761\n",
      "47/388, train_loss: 0.2251, step time: 0.5370\n",
      "48/388, train_loss: 0.2304, step time: 0.5294\n",
      "49/388, train_loss: 0.3762, step time: 0.5339\n",
      "50/388, train_loss: 0.1967, step time: 0.6243\n",
      "51/388, train_loss: 0.0788, step time: 0.5594\n",
      "52/388, train_loss: 0.1773, step time: 0.5314\n",
      "53/388, train_loss: 0.5899, step time: 0.5086\n",
      "54/388, train_loss: 0.4609, step time: 0.5241\n",
      "55/388, train_loss: 0.1535, step time: 0.6548\n",
      "56/388, train_loss: 0.2432, step time: 0.5482\n",
      "57/388, train_loss: 0.2699, step time: 0.5164\n",
      "58/388, train_loss: 0.1930, step time: 0.5099\n",
      "59/388, train_loss: 0.1910, step time: 0.5266\n",
      "60/388, train_loss: 0.1599, step time: 0.5197\n",
      "61/388, train_loss: 0.2484, step time: 0.4994\n",
      "62/388, train_loss: 0.2920, step time: 0.4966\n",
      "63/388, train_loss: 0.7759, step time: 1.1355\n",
      "64/388, train_loss: 0.3408, step time: 0.5437\n",
      "65/388, train_loss: 0.2094, step time: 0.5117\n",
      "66/388, train_loss: 0.1396, step time: 0.5060\n",
      "67/388, train_loss: 0.2539, step time: 0.4918\n",
      "68/388, train_loss: 0.1073, step time: 1.0101\n",
      "69/388, train_loss: 0.1311, step time: 0.5392\n",
      "70/388, train_loss: 0.3808, step time: 0.5145\n",
      "71/388, train_loss: 0.2221, step time: 0.4920\n",
      "72/388, train_loss: 0.2457, step time: 0.4843\n",
      "73/388, train_loss: 0.2396, step time: 0.4887\n",
      "74/388, train_loss: 0.1518, step time: 0.4835\n",
      "75/388, train_loss: 0.2777, step time: 0.5019\n",
      "76/388, train_loss: 0.6144, step time: 0.4829\n",
      "77/388, train_loss: 0.2254, step time: 1.0485\n",
      "78/388, train_loss: 0.2846, step time: 0.5414\n",
      "79/388, train_loss: 0.1388, step time: 0.5209\n",
      "80/388, train_loss: 0.1580, step time: 0.4884\n",
      "81/388, train_loss: 0.4810, step time: 0.5279\n",
      "82/388, train_loss: 0.2576, step time: 0.5289\n",
      "83/388, train_loss: 0.2547, step time: 0.5037\n",
      "84/388, train_loss: 0.2688, step time: 0.5338\n",
      "85/388, train_loss: 0.1261, step time: 0.5128\n",
      "86/388, train_loss: 0.1551, step time: 0.4955\n",
      "87/388, train_loss: 0.0818, step time: 0.5056\n",
      "88/388, train_loss: 0.0986, step time: 0.4820\n",
      "89/388, train_loss: 0.5073, step time: 0.5162\n",
      "90/388, train_loss: 0.5220, step time: 0.5059\n",
      "91/388, train_loss: 0.1399, step time: 0.5077\n",
      "92/388, train_loss: 0.1862, step time: 0.5028\n",
      "93/388, train_loss: 0.2424, step time: 0.5684\n",
      "94/388, train_loss: 0.0437, step time: 0.5477\n",
      "95/388, train_loss: 0.3263, step time: 0.5322\n",
      "96/388, train_loss: 0.5220, step time: 0.5314\n",
      "97/388, train_loss: 0.7295, step time: 0.5052\n",
      "98/388, train_loss: 0.2096, step time: 0.5011\n",
      "99/388, train_loss: 0.0501, step time: 0.5185\n",
      "100/388, train_loss: 0.1424, step time: 0.5204\n",
      "101/388, train_loss: 0.3491, step time: 0.5106\n",
      "102/388, train_loss: 0.1229, step time: 0.5268\n",
      "103/388, train_loss: 0.0766, step time: 0.5168\n",
      "104/388, train_loss: 0.3243, step time: 0.5193\n",
      "105/388, train_loss: 0.1793, step time: 0.5082\n",
      "106/388, train_loss: 0.1530, step time: 0.4885\n",
      "107/388, train_loss: 0.1964, step time: 0.5140\n",
      "108/388, train_loss: 0.1183, step time: 0.5628\n",
      "109/388, train_loss: 0.1232, step time: 0.5616\n",
      "110/388, train_loss: 0.1168, step time: 0.5479\n",
      "111/388, train_loss: 0.1082, step time: 0.5374\n",
      "112/388, train_loss: 0.1636, step time: 0.5939\n",
      "113/388, train_loss: 0.4526, step time: 0.6848\n",
      "114/388, train_loss: 0.2548, step time: 0.5471\n",
      "115/388, train_loss: 0.1244, step time: 0.5190\n",
      "116/388, train_loss: 0.1724, step time: 0.5036\n",
      "117/388, train_loss: 0.0758, step time: 0.4990\n",
      "118/388, train_loss: 0.3413, step time: 0.4981\n",
      "119/388, train_loss: 0.3868, step time: 0.5388\n",
      "120/388, train_loss: 0.1276, step time: 0.5266\n",
      "121/388, train_loss: 0.1202, step time: 0.5229\n",
      "122/388, train_loss: 0.1856, step time: 0.5602\n",
      "123/388, train_loss: 0.1089, step time: 0.5223\n",
      "124/388, train_loss: 0.3166, step time: 0.4997\n",
      "125/388, train_loss: 0.5509, step time: 0.4977\n",
      "126/388, train_loss: 0.7461, step time: 0.4810\n",
      "127/388, train_loss: 0.1837, step time: 0.5136\n",
      "128/388, train_loss: 0.0591, step time: 0.5292\n",
      "129/388, train_loss: 0.0426, step time: 0.5226\n",
      "130/388, train_loss: 0.2324, step time: 0.5088\n",
      "131/388, train_loss: 0.3783, step time: 0.5099\n",
      "132/388, train_loss: 0.1305, step time: 0.4963\n",
      "133/388, train_loss: 0.2515, step time: 0.4991\n",
      "134/388, train_loss: 0.2318, step time: 1.1150\n",
      "135/388, train_loss: 0.1072, step time: 0.5306\n",
      "136/388, train_loss: 0.2087, step time: 0.5046\n",
      "137/388, train_loss: 0.1043, step time: 0.4773\n",
      "138/388, train_loss: 0.3108, step time: 1.1110\n",
      "139/388, train_loss: 0.1047, step time: 0.5309\n",
      "140/388, train_loss: 0.2700, step time: 0.5069\n",
      "141/388, train_loss: 0.2569, step time: 0.4991\n",
      "142/388, train_loss: 0.1940, step time: 0.5002\n",
      "143/388, train_loss: 0.0866, step time: 0.4804\n",
      "144/388, train_loss: 0.3127, step time: 0.4813\n",
      "145/388, train_loss: 0.3310, step time: 0.4935\n",
      "146/388, train_loss: 0.1392, step time: 0.4857\n",
      "147/388, train_loss: 0.1457, step time: 0.4956\n",
      "148/388, train_loss: 0.6555, step time: 0.4934\n",
      "149/388, train_loss: 0.1069, step time: 0.5109\n",
      "150/388, train_loss: 0.2252, step time: 0.5399\n",
      "151/388, train_loss: 0.1548, step time: 0.5903\n",
      "152/388, train_loss: 0.3204, step time: 0.5300\n",
      "153/388, train_loss: 0.7056, step time: 0.5279\n",
      "154/388, train_loss: 0.0719, step time: 0.5017\n",
      "155/388, train_loss: 0.1853, step time: 0.4880\n",
      "156/388, train_loss: 0.1625, step time: 1.1808\n",
      "157/388, train_loss: 0.1554, step time: 0.5317\n",
      "158/388, train_loss: 0.1251, step time: 0.5135\n",
      "159/388, train_loss: 0.1643, step time: 0.5553\n",
      "160/388, train_loss: 0.1269, step time: 0.5236\n",
      "161/388, train_loss: 0.1789, step time: 0.5126\n",
      "162/388, train_loss: 0.1031, step time: 0.4932\n",
      "163/388, train_loss: 0.2280, step time: 1.1881\n",
      "164/388, train_loss: 0.1433, step time: 0.5311\n",
      "165/388, train_loss: 0.0904, step time: 0.5173\n",
      "166/388, train_loss: 0.2642, step time: 0.4880\n",
      "167/388, train_loss: 0.1254, step time: 0.4860\n",
      "168/388, train_loss: 0.0879, step time: 0.4891\n",
      "169/388, train_loss: 0.3663, step time: 0.4817\n",
      "170/388, train_loss: 0.3349, step time: 0.6885\n",
      "171/388, train_loss: 0.1266, step time: 0.5444\n",
      "172/388, train_loss: 0.0984, step time: 0.5230\n",
      "173/388, train_loss: 0.1279, step time: 0.5031\n",
      "174/388, train_loss: 0.1593, step time: 0.4961\n",
      "175/388, train_loss: 0.3273, step time: 0.5621\n",
      "176/388, train_loss: 0.2628, step time: 0.5185\n",
      "177/388, train_loss: 0.1396, step time: 0.4977\n",
      "178/388, train_loss: 0.2600, step time: 0.4983\n",
      "179/388, train_loss: 0.1521, step time: 0.4807\n",
      "180/388, train_loss: 0.2171, step time: 0.4894\n",
      "181/388, train_loss: 0.2060, step time: 1.1359\n",
      "182/388, train_loss: 0.1368, step time: 0.5351\n",
      "183/388, train_loss: 0.2114, step time: 0.5037\n",
      "184/388, train_loss: 0.0648, step time: 0.4810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/388, train_loss: 0.2431, step time: 0.4821\n",
      "186/388, train_loss: 0.1190, step time: 0.5025\n",
      "187/388, train_loss: 0.2179, step time: 0.4869\n",
      "188/388, train_loss: 0.1867, step time: 0.4917\n",
      "189/388, train_loss: 0.5175, step time: 0.5688\n",
      "190/388, train_loss: 0.4972, step time: 0.5546\n",
      "191/388, train_loss: 0.1247, step time: 0.5274\n",
      "192/388, train_loss: 0.1778, step time: 0.5039\n",
      "193/388, train_loss: 0.4188, step time: 0.4865\n",
      "194/388, train_loss: 0.3815, step time: 0.5208\n",
      "195/388, train_loss: 0.2326, step time: 0.4955\n",
      "196/388, train_loss: 0.4555, step time: 0.4968\n",
      "197/388, train_loss: 0.1856, step time: 0.5165\n",
      "198/388, train_loss: 0.0715, step time: 0.4999\n",
      "199/388, train_loss: 0.1197, step time: 0.4965\n",
      "200/388, train_loss: 0.3290, step time: 0.4883\n",
      "201/388, train_loss: 0.4412, step time: 0.4946\n",
      "202/388, train_loss: 0.0979, step time: 0.4758\n",
      "203/388, train_loss: 0.3431, step time: 0.4956\n",
      "204/388, train_loss: 0.1525, step time: 0.4854\n",
      "205/388, train_loss: 0.1781, step time: 0.4815\n",
      "206/388, train_loss: 0.2519, step time: 1.0159\n",
      "207/388, train_loss: 0.1310, step time: 0.5336\n",
      "208/388, train_loss: 0.1754, step time: 0.5166\n",
      "209/388, train_loss: 0.3377, step time: 0.4918\n",
      "210/388, train_loss: 0.1406, step time: 0.4947\n",
      "211/388, train_loss: 0.2845, step time: 0.4865\n",
      "212/388, train_loss: 0.1641, step time: 0.4830\n",
      "213/388, train_loss: 0.3549, step time: 0.4940\n",
      "214/388, train_loss: 0.3926, step time: 0.4885\n",
      "215/388, train_loss: 0.4464, step time: 0.4878\n",
      "216/388, train_loss: 0.1574, step time: 1.2365\n",
      "217/388, train_loss: 0.4210, step time: 0.5324\n",
      "218/388, train_loss: 0.3240, step time: 0.4986\n",
      "219/388, train_loss: 0.2317, step time: 0.4970\n",
      "220/388, train_loss: 0.1010, step time: 0.4845\n",
      "221/388, train_loss: 0.1365, step time: 0.4788\n",
      "222/388, train_loss: 0.2122, step time: 0.4872\n",
      "223/388, train_loss: 0.1372, step time: 0.4779\n",
      "224/388, train_loss: 0.1045, step time: 0.5991\n",
      "225/388, train_loss: 0.1253, step time: 0.5433\n",
      "226/388, train_loss: 0.0764, step time: 0.5135\n",
      "227/388, train_loss: 0.1647, step time: 0.5117\n",
      "228/388, train_loss: 0.1016, step time: 0.4955\n",
      "229/388, train_loss: 0.1085, step time: 0.4854\n",
      "230/388, train_loss: 0.1081, step time: 0.4884\n",
      "231/388, train_loss: 0.1012, step time: 0.4785\n",
      "232/388, train_loss: 0.2487, step time: 0.7074\n",
      "233/388, train_loss: 0.1344, step time: 0.5470\n",
      "234/388, train_loss: 0.4753, step time: 0.5170\n",
      "235/388, train_loss: 0.1972, step time: 0.5015\n",
      "236/388, train_loss: 0.4363, step time: 0.4940\n",
      "237/388, train_loss: 0.3187, step time: 0.4834\n",
      "238/388, train_loss: 0.5779, step time: 1.0361\n",
      "239/388, train_loss: 0.1824, step time: 0.5294\n",
      "240/388, train_loss: 0.0858, step time: 0.5005\n",
      "241/388, train_loss: 0.1100, step time: 0.4843\n",
      "242/388, train_loss: 0.0862, step time: 0.4852\n",
      "243/388, train_loss: 0.2709, step time: 0.4906\n",
      "244/388, train_loss: 0.3321, step time: 0.4747\n",
      "245/388, train_loss: 0.1459, step time: 0.4747\n",
      "246/388, train_loss: 0.1627, step time: 0.4911\n",
      "247/388, train_loss: 0.0588, step time: 0.5016\n",
      "248/388, train_loss: 0.2468, step time: 0.4967\n",
      "249/388, train_loss: 0.1145, step time: 0.4939\n",
      "250/388, train_loss: 0.3030, step time: 0.4813\n",
      "251/388, train_loss: 0.2476, step time: 0.4958\n",
      "252/388, train_loss: 0.3005, step time: 0.5371\n",
      "253/388, train_loss: 0.3143, step time: 0.5094\n",
      "254/388, train_loss: 0.1231, step time: 0.4957\n",
      "255/388, train_loss: 0.1720, step time: 0.4909\n",
      "256/388, train_loss: 0.1336, step time: 0.4857\n",
      "257/388, train_loss: 0.1491, step time: 1.1334\n",
      "258/388, train_loss: 0.4330, step time: 0.5314\n",
      "259/388, train_loss: 0.1274, step time: 0.5016\n",
      "260/388, train_loss: 0.0876, step time: 0.4959\n",
      "261/388, train_loss: 0.1115, step time: 0.4858\n",
      "262/388, train_loss: 0.0979, step time: 0.4896\n",
      "263/388, train_loss: 0.1563, step time: 0.4933\n",
      "264/388, train_loss: 0.2255, step time: 1.1171\n",
      "265/388, train_loss: 0.2975, step time: 0.5354\n",
      "266/388, train_loss: 0.3474, step time: 0.4970\n",
      "267/388, train_loss: 0.1747, step time: 0.4962\n",
      "268/388, train_loss: 0.0930, step time: 0.4819\n",
      "269/388, train_loss: 0.2377, step time: 0.4761\n",
      "270/388, train_loss: 0.2849, step time: 0.4821\n",
      "271/388, train_loss: 0.2707, step time: 0.4801\n",
      "272/388, train_loss: 0.1876, step time: 0.4827\n",
      "273/388, train_loss: 0.1681, step time: 0.9654\n",
      "274/388, train_loss: 0.1738, step time: 0.5351\n",
      "275/388, train_loss: 0.1438, step time: 0.4982\n",
      "276/388, train_loss: 0.2964, step time: 0.4923\n",
      "277/388, train_loss: 0.0703, step time: 0.4946\n",
      "278/388, train_loss: 0.1116, step time: 0.4820\n",
      "279/388, train_loss: 0.2433, step time: 0.5051\n",
      "280/388, train_loss: 0.3840, step time: 0.5012\n",
      "281/388, train_loss: 0.3614, step time: 0.4928\n",
      "282/388, train_loss: 0.1189, step time: 0.4737\n",
      "283/388, train_loss: 0.1674, step time: 0.9725\n",
      "284/388, train_loss: 0.0855, step time: 0.5278\n",
      "285/388, train_loss: 0.3146, step time: 0.5094\n",
      "286/388, train_loss: 0.1351, step time: 0.4836\n",
      "287/388, train_loss: 0.0737, step time: 0.4772\n",
      "288/388, train_loss: 0.1882, step time: 0.4800\n",
      "289/388, train_loss: 0.1944, step time: 0.4816\n",
      "290/388, train_loss: 0.1300, step time: 0.4861\n",
      "291/388, train_loss: 0.2185, step time: 0.4832\n",
      "292/388, train_loss: 0.1054, step time: 1.1815\n",
      "293/388, train_loss: 0.1541, step time: 0.5244\n",
      "294/388, train_loss: 0.2060, step time: 0.5025\n",
      "295/388, train_loss: 0.2085, step time: 0.4865\n",
      "296/388, train_loss: 0.1761, step time: 0.4856\n",
      "297/388, train_loss: 0.1721, step time: 0.4894\n",
      "298/388, train_loss: 0.2021, step time: 0.5048\n",
      "299/388, train_loss: 0.0950, step time: 0.4969\n",
      "300/388, train_loss: 0.1616, step time: 0.4920\n",
      "301/388, train_loss: 0.1515, step time: 0.4798\n",
      "302/388, train_loss: 0.1181, step time: 0.4746\n",
      "303/388, train_loss: 0.2493, step time: 0.5010\n",
      "304/388, train_loss: 0.0556, step time: 0.4841\n",
      "305/388, train_loss: 0.3505, step time: 0.4916\n",
      "306/388, train_loss: 0.1129, step time: 0.4782\n",
      "307/388, train_loss: 0.4706, step time: 0.4965\n",
      "308/388, train_loss: 0.5221, step time: 0.4857\n",
      "309/388, train_loss: 0.1491, step time: 0.4744\n",
      "310/388, train_loss: 0.1087, step time: 0.6526\n",
      "311/388, train_loss: 0.0962, step time: 0.5589\n",
      "312/388, train_loss: 0.1444, step time: 0.5227\n",
      "313/388, train_loss: 0.2460, step time: 0.4948\n",
      "314/388, train_loss: 0.3231, step time: 0.4958\n",
      "315/388, train_loss: 0.5012, step time: 0.4831\n",
      "316/388, train_loss: 0.1725, step time: 0.4859\n",
      "317/388, train_loss: 0.3025, step time: 1.0583\n",
      "318/388, train_loss: 0.2477, step time: 0.5207\n",
      "319/388, train_loss: 0.2017, step time: 0.4939\n",
      "320/388, train_loss: 0.2160, step time: 0.4900\n",
      "321/388, train_loss: 0.3377, step time: 0.4959\n",
      "322/388, train_loss: 0.0682, step time: 0.4828\n",
      "323/388, train_loss: 0.1298, step time: 0.4838\n",
      "324/388, train_loss: 0.1156, step time: 0.4736\n",
      "325/388, train_loss: 0.3252, step time: 0.4761\n",
      "326/388, train_loss: 0.2250, step time: 0.4892\n",
      "327/388, train_loss: 0.2331, step time: 0.7001\n",
      "328/388, train_loss: 0.2054, step time: 0.5364\n",
      "329/388, train_loss: 0.4164, step time: 0.5191\n",
      "330/388, train_loss: 0.2798, step time: 0.5088\n",
      "331/388, train_loss: 0.1465, step time: 0.4940\n",
      "332/388, train_loss: 0.2059, step time: 0.4996\n",
      "333/388, train_loss: 0.2418, step time: 0.4930\n",
      "334/388, train_loss: 0.0802, step time: 0.4977\n",
      "335/388, train_loss: 0.3908, step time: 0.4946\n",
      "336/388, train_loss: 0.2983, step time: 0.4926\n",
      "337/388, train_loss: 0.1254, step time: 0.4751\n",
      "338/388, train_loss: 0.1568, step time: 0.5014\n",
      "339/388, train_loss: 0.2054, step time: 0.4883\n",
      "340/388, train_loss: 0.2087, step time: 1.0678\n",
      "341/388, train_loss: 0.2871, step time: 0.5508\n",
      "342/388, train_loss: 0.1991, step time: 0.5204\n",
      "343/388, train_loss: 0.2080, step time: 0.5008\n",
      "344/388, train_loss: 0.2160, step time: 0.4885\n",
      "345/388, train_loss: 0.0488, step time: 0.5055\n",
      "346/388, train_loss: 0.1764, step time: 0.4915\n",
      "347/388, train_loss: 0.3459, step time: 0.5033\n",
      "348/388, train_loss: 0.3245, step time: 0.5671\n",
      "349/388, train_loss: 0.1935, step time: 0.5208\n",
      "350/388, train_loss: 0.1416, step time: 0.5023\n",
      "351/388, train_loss: 0.3063, step time: 0.5201\n",
      "352/388, train_loss: 0.3296, step time: 0.5187\n",
      "353/388, train_loss: 0.1418, step time: 0.5114\n",
      "354/388, train_loss: 0.2099, step time: 0.4986\n",
      "355/388, train_loss: 0.1967, step time: 0.4999\n",
      "356/388, train_loss: 0.1857, step time: 0.4784\n",
      "357/388, train_loss: 0.3198, step time: 0.4889\n",
      "358/388, train_loss: 0.1373, step time: 0.4995\n",
      "359/388, train_loss: 0.1054, step time: 1.0494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/388, train_loss: 0.2137, step time: 0.5467\n",
      "361/388, train_loss: 0.2016, step time: 0.5123\n",
      "362/388, train_loss: 0.1200, step time: 0.4933\n",
      "363/388, train_loss: 0.2935, step time: 0.4994\n",
      "364/388, train_loss: 0.0919, step time: 0.4811\n",
      "365/388, train_loss: 0.2129, step time: 0.4830\n",
      "366/388, train_loss: 0.1236, step time: 0.4819\n",
      "367/388, train_loss: 0.2532, step time: 0.5029\n",
      "368/388, train_loss: 0.1714, step time: 0.5010\n",
      "369/388, train_loss: 0.2772, step time: 0.4852\n",
      "370/388, train_loss: 0.3362, step time: 1.0380\n",
      "371/388, train_loss: 0.4212, step time: 0.5520\n",
      "372/388, train_loss: 0.2428, step time: 0.5219\n",
      "373/388, train_loss: 0.1879, step time: 0.5128\n",
      "374/388, train_loss: 0.0503, step time: 0.5041\n",
      "375/388, train_loss: 0.1891, step time: 0.4939\n",
      "376/388, train_loss: 0.5207, step time: 0.4930\n",
      "377/388, train_loss: 0.2461, step time: 0.5165\n",
      "378/388, train_loss: 0.2576, step time: 0.5051\n",
      "379/388, train_loss: 0.1092, step time: 0.4919\n",
      "380/388, train_loss: 0.4800, step time: 1.2050\n",
      "381/388, train_loss: 0.2313, step time: 0.5276\n",
      "382/388, train_loss: 0.1057, step time: 0.4886\n",
      "383/388, train_loss: 0.1045, step time: 0.4859\n",
      "384/388, train_loss: 0.4110, step time: 0.4819\n",
      "385/388, train_loss: 0.1428, step time: 0.4728\n",
      "386/388, train_loss: 0.1305, step time: 0.4785\n",
      "387/388, train_loss: 0.2067, step time: 0.4898\n",
      "388/388, train_loss: 0.2505, step time: 0.4861\n",
      "epoch 18 average loss: 0.2324\n",
      "saved new best metric model\n",
      "current epoch: 18 current mean dice: 0.7289 tc: 0.7843 wt: 0.8790 et: 0.5233\n",
      "best mean dice: 0.7289 at epoch: 18\n",
      "time consuming of epoch 18 is: 301.3717\n",
      "----------\n",
      "epoch 19/300\n",
      "1/388, train_loss: 0.5634, step time: 0.4807\n",
      "2/388, train_loss: 0.2887, step time: 1.0298\n",
      "3/388, train_loss: 0.0961, step time: 0.5505\n",
      "4/388, train_loss: 0.2006, step time: 0.5252\n",
      "5/388, train_loss: 0.1368, step time: 0.5026\n",
      "6/388, train_loss: 0.1422, step time: 0.5711\n",
      "7/388, train_loss: 0.2275, step time: 0.6094\n",
      "8/388, train_loss: 0.1957, step time: 0.5937\n",
      "9/388, train_loss: 0.2253, step time: 0.5370\n",
      "10/388, train_loss: 0.2897, step time: 0.5176\n",
      "11/388, train_loss: 0.0998, step time: 0.4883\n",
      "12/388, train_loss: 0.2854, step time: 0.5080\n",
      "13/388, train_loss: 0.1190, step time: 0.5286\n",
      "14/388, train_loss: 0.1944, step time: 0.5695\n",
      "15/388, train_loss: 0.3860, step time: 0.7191\n",
      "16/388, train_loss: 0.2125, step time: 0.5676\n",
      "17/388, train_loss: 0.3091, step time: 0.5083\n",
      "18/388, train_loss: 0.1343, step time: 0.5063\n",
      "19/388, train_loss: 0.0663, step time: 0.5852\n",
      "20/388, train_loss: 0.1387, step time: 0.5499\n",
      "21/388, train_loss: 0.2300, step time: 0.5040\n",
      "22/388, train_loss: 0.1555, step time: 0.5286\n",
      "23/388, train_loss: 0.1261, step time: 0.5388\n",
      "24/388, train_loss: 0.2967, step time: 0.5311\n",
      "25/388, train_loss: 0.1609, step time: 0.5114\n",
      "26/388, train_loss: 0.1171, step time: 0.5099\n",
      "27/388, train_loss: 0.1306, step time: 0.6462\n",
      "28/388, train_loss: 0.0920, step time: 0.6094\n",
      "29/388, train_loss: 0.2408, step time: 0.5848\n",
      "30/388, train_loss: 0.2632, step time: 0.5391\n",
      "31/388, train_loss: 0.1886, step time: 0.5154\n",
      "32/388, train_loss: 0.2406, step time: 0.5755\n",
      "33/388, train_loss: 0.3029, step time: 0.5880\n",
      "34/388, train_loss: 0.2816, step time: 0.5429\n",
      "35/388, train_loss: 0.4584, step time: 0.5031\n",
      "36/388, train_loss: 0.1743, step time: 0.5229\n",
      "37/388, train_loss: 0.2697, step time: 0.6754\n",
      "38/388, train_loss: 0.2502, step time: 0.5463\n",
      "39/388, train_loss: 0.0742, step time: 0.5219\n",
      "40/388, train_loss: 0.1975, step time: 0.5030\n",
      "41/388, train_loss: 0.1194, step time: 0.6498\n",
      "42/388, train_loss: 0.1365, step time: 0.5754\n",
      "43/388, train_loss: 0.1549, step time: 0.5454\n",
      "44/388, train_loss: 0.3875, step time: 0.5898\n",
      "45/388, train_loss: 0.2558, step time: 0.5634\n",
      "46/388, train_loss: 0.3457, step time: 0.5527\n",
      "47/388, train_loss: 0.1453, step time: 0.5149\n",
      "48/388, train_loss: 0.1150, step time: 0.5655\n",
      "49/388, train_loss: 0.1441, step time: 0.5333\n",
      "50/388, train_loss: 0.2040, step time: 0.5157\n",
      "51/388, train_loss: 0.1507, step time: 0.5046\n",
      "52/388, train_loss: 0.0844, step time: 0.4966\n",
      "53/388, train_loss: 0.2306, step time: 0.4995\n",
      "54/388, train_loss: 0.0725, step time: 0.9097\n",
      "55/388, train_loss: 0.1021, step time: 0.5383\n",
      "56/388, train_loss: 0.2091, step time: 0.5020\n",
      "57/388, train_loss: 0.2577, step time: 0.5279\n",
      "58/388, train_loss: 0.1401, step time: 0.4996\n",
      "59/388, train_loss: 0.5475, step time: 0.5005\n",
      "60/388, train_loss: 0.2141, step time: 1.0138\n",
      "61/388, train_loss: 0.1533, step time: 0.5409\n",
      "62/388, train_loss: 0.1829, step time: 0.5314\n",
      "63/388, train_loss: 0.1596, step time: 0.5017\n",
      "64/388, train_loss: 0.5523, step time: 0.5243\n",
      "65/388, train_loss: 0.4226, step time: 0.4920\n",
      "66/388, train_loss: 0.1128, step time: 1.0647\n",
      "67/388, train_loss: 0.1114, step time: 0.5366\n",
      "68/388, train_loss: 0.3870, step time: 0.5052\n",
      "69/388, train_loss: 0.3230, step time: 0.4788\n",
      "70/388, train_loss: 0.1054, step time: 0.4807\n",
      "71/388, train_loss: 0.2065, step time: 0.4930\n",
      "72/388, train_loss: 0.1126, step time: 0.4856\n",
      "73/388, train_loss: 0.1734, step time: 0.5761\n",
      "74/388, train_loss: 0.6347, step time: 0.5886\n",
      "75/388, train_loss: 0.3090, step time: 0.6598\n",
      "76/388, train_loss: 0.2428, step time: 0.5618\n",
      "77/388, train_loss: 0.0827, step time: 0.5187\n",
      "78/388, train_loss: 0.2522, step time: 0.4953\n",
      "79/388, train_loss: 0.2050, step time: 0.4962\n",
      "80/388, train_loss: 0.2079, step time: 1.1073\n",
      "81/388, train_loss: 0.1353, step time: 0.5386\n",
      "82/388, train_loss: 0.3871, step time: 0.5140\n",
      "83/388, train_loss: 0.2711, step time: 0.4926\n",
      "84/388, train_loss: 0.3188, step time: 0.4989\n",
      "85/388, train_loss: 0.5833, step time: 0.5258\n",
      "86/388, train_loss: 0.2418, step time: 0.5120\n",
      "87/388, train_loss: 0.2302, step time: 0.4868\n",
      "88/388, train_loss: 0.1790, step time: 1.0162\n",
      "89/388, train_loss: 0.1225, step time: 0.5447\n",
      "90/388, train_loss: 0.0821, step time: 0.5161\n",
      "91/388, train_loss: 0.2698, step time: 0.5036\n",
      "92/388, train_loss: 0.1612, step time: 0.4892\n",
      "93/388, train_loss: 0.4040, step time: 0.5002\n",
      "94/388, train_loss: 0.3810, step time: 0.5368\n",
      "95/388, train_loss: 0.0996, step time: 0.5092\n",
      "96/388, train_loss: 0.2612, step time: 0.4890\n",
      "97/388, train_loss: 0.0758, step time: 0.4814\n",
      "98/388, train_loss: 0.3158, step time: 0.4993\n",
      "99/388, train_loss: 0.3463, step time: 0.5011\n",
      "100/388, train_loss: 0.1271, step time: 0.5554\n",
      "101/388, train_loss: 0.2140, step time: 0.5388\n",
      "102/388, train_loss: 0.2061, step time: 0.5118\n",
      "103/388, train_loss: 0.4872, step time: 0.5045\n",
      "104/388, train_loss: 0.3534, step time: 0.4908\n",
      "105/388, train_loss: 0.1239, step time: 0.4941\n",
      "106/388, train_loss: 0.0991, step time: 0.4828\n",
      "107/388, train_loss: 0.1026, step time: 0.5820\n",
      "108/388, train_loss: 0.0334, step time: 0.5597\n",
      "109/388, train_loss: 0.1006, step time: 0.6152\n",
      "110/388, train_loss: 0.1654, step time: 0.5594\n",
      "111/388, train_loss: 0.2337, step time: 0.5250\n",
      "112/388, train_loss: 0.3143, step time: 0.5089\n",
      "113/388, train_loss: 0.3203, step time: 0.5125\n",
      "114/388, train_loss: 0.2544, step time: 0.5986\n",
      "115/388, train_loss: 0.4220, step time: 0.5284\n",
      "116/388, train_loss: 0.1393, step time: 0.4959\n",
      "117/388, train_loss: 0.4278, step time: 0.5306\n",
      "118/388, train_loss: 0.1466, step time: 0.5206\n",
      "119/388, train_loss: 0.1626, step time: 0.5030\n",
      "120/388, train_loss: 0.1334, step time: 0.5520\n",
      "121/388, train_loss: 0.3685, step time: 0.5408\n",
      "122/388, train_loss: 0.1265, step time: 0.5148\n",
      "123/388, train_loss: 0.0884, step time: 0.5170\n",
      "124/388, train_loss: 0.2581, step time: 0.5069\n",
      "125/388, train_loss: 0.2958, step time: 0.5195\n",
      "126/388, train_loss: 0.3256, step time: 0.5000\n",
      "127/388, train_loss: 0.1570, step time: 1.1103\n",
      "128/388, train_loss: 0.5318, step time: 0.5425\n",
      "129/388, train_loss: 0.2323, step time: 0.5114\n",
      "130/388, train_loss: 0.4048, step time: 0.4962\n",
      "131/388, train_loss: 0.4423, step time: 0.5192\n",
      "132/388, train_loss: 0.2343, step time: 0.5931\n",
      "133/388, train_loss: 0.1186, step time: 0.5348\n",
      "134/388, train_loss: 0.2399, step time: 0.5182\n",
      "135/388, train_loss: 0.1138, step time: 0.5003\n",
      "136/388, train_loss: 0.2066, step time: 0.5250\n",
      "137/388, train_loss: 0.1156, step time: 0.5233\n",
      "138/388, train_loss: 0.1576, step time: 0.5011\n",
      "139/388, train_loss: 0.3439, step time: 0.4935\n",
      "140/388, train_loss: 0.0622, step time: 0.9172\n",
      "141/388, train_loss: 0.1182, step time: 0.5460\n",
      "142/388, train_loss: 0.1334, step time: 0.5081\n",
      "143/388, train_loss: 0.1437, step time: 0.4988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/388, train_loss: 0.1455, step time: 0.4841\n",
      "145/388, train_loss: 0.2261, step time: 0.4984\n",
      "146/388, train_loss: 0.1566, step time: 0.5285\n",
      "147/388, train_loss: 0.2034, step time: 0.5017\n",
      "148/388, train_loss: 0.2359, step time: 0.4892\n",
      "149/388, train_loss: 0.1743, step time: 0.5385\n",
      "150/388, train_loss: 0.3170, step time: 0.5918\n",
      "151/388, train_loss: 0.1341, step time: 0.5505\n",
      "152/388, train_loss: 0.4153, step time: 0.5116\n",
      "153/388, train_loss: 0.1825, step time: 0.5480\n",
      "154/388, train_loss: 0.1035, step time: 0.5489\n",
      "155/388, train_loss: 0.4751, step time: 0.5245\n",
      "156/388, train_loss: 0.1695, step time: 0.5030\n",
      "157/388, train_loss: 0.1125, step time: 0.4988\n",
      "158/388, train_loss: 0.1524, step time: 0.5359\n",
      "159/388, train_loss: 0.3440, step time: 0.5191\n",
      "160/388, train_loss: 0.3166, step time: 0.5098\n",
      "161/388, train_loss: 0.0984, step time: 0.5187\n",
      "162/388, train_loss: 0.1045, step time: 0.5049\n",
      "163/388, train_loss: 0.1007, step time: 0.4878\n",
      "164/388, train_loss: 0.2134, step time: 0.4929\n",
      "165/388, train_loss: 0.1827, step time: 0.8798\n",
      "166/388, train_loss: 0.1882, step time: 0.5506\n",
      "167/388, train_loss: 0.1114, step time: 0.5105\n",
      "168/388, train_loss: 0.2932, step time: 0.5035\n",
      "169/388, train_loss: 0.5421, step time: 0.4834\n",
      "170/388, train_loss: 0.2520, step time: 1.0890\n",
      "171/388, train_loss: 0.0705, step time: 0.5459\n",
      "172/388, train_loss: 0.0956, step time: 0.5175\n",
      "173/388, train_loss: 0.1514, step time: 0.5027\n",
      "174/388, train_loss: 0.1660, step time: 0.4967\n",
      "175/388, train_loss: 0.2054, step time: 0.4908\n",
      "176/388, train_loss: 0.2830, step time: 0.4971\n",
      "177/388, train_loss: 0.1258, step time: 0.6130\n",
      "178/388, train_loss: 0.0989, step time: 0.5562\n",
      "179/388, train_loss: 0.2140, step time: 0.5256\n",
      "180/388, train_loss: 0.2031, step time: 0.5159\n",
      "181/388, train_loss: 0.2362, step time: 0.5074\n",
      "182/388, train_loss: 0.2864, step time: 0.4993\n",
      "183/388, train_loss: 0.1328, step time: 0.4754\n",
      "184/388, train_loss: 0.1964, step time: 0.4844\n",
      "185/388, train_loss: 0.0886, step time: 1.1972\n",
      "186/388, train_loss: 0.2956, step time: 0.5324\n",
      "187/388, train_loss: 0.3238, step time: 0.5053\n",
      "188/388, train_loss: 0.2677, step time: 0.4920\n",
      "189/388, train_loss: 0.1959, step time: 0.4830\n",
      "190/388, train_loss: 0.2969, step time: 0.4818\n",
      "191/388, train_loss: 0.2245, step time: 0.4912\n",
      "192/388, train_loss: 0.2441, step time: 0.5027\n",
      "193/388, train_loss: 0.3932, step time: 0.4979\n",
      "194/388, train_loss: 0.1201, step time: 1.1238\n",
      "195/388, train_loss: 0.1214, step time: 0.5216\n",
      "196/388, train_loss: 0.2130, step time: 0.5079\n",
      "197/388, train_loss: 0.1489, step time: 0.4837\n",
      "198/388, train_loss: 0.4650, step time: 0.5275\n",
      "199/388, train_loss: 0.0937, step time: 0.5023\n",
      "200/388, train_loss: 0.1091, step time: 0.5028\n",
      "201/388, train_loss: 0.1806, step time: 0.4873\n",
      "202/388, train_loss: 0.2466, step time: 0.4765\n",
      "203/388, train_loss: 0.2641, step time: 0.4841\n",
      "204/388, train_loss: 0.1973, step time: 0.7673\n",
      "205/388, train_loss: 0.3958, step time: 0.5632\n",
      "206/388, train_loss: 0.2229, step time: 0.5274\n",
      "207/388, train_loss: 0.0972, step time: 0.5049\n",
      "208/388, train_loss: 0.4399, step time: 0.4984\n",
      "209/388, train_loss: 0.1664, step time: 0.4816\n",
      "210/388, train_loss: 0.2579, step time: 0.4825\n",
      "211/388, train_loss: 0.2490, step time: 0.5299\n",
      "212/388, train_loss: 0.5250, step time: 0.5140\n",
      "213/388, train_loss: 0.1232, step time: 0.4980\n",
      "214/388, train_loss: 0.1177, step time: 0.4864\n",
      "215/388, train_loss: 0.3030, step time: 0.4915\n",
      "216/388, train_loss: 0.2291, step time: 1.1623\n",
      "217/388, train_loss: 0.1510, step time: 0.5370\n",
      "218/388, train_loss: 0.0703, step time: 0.5158\n",
      "219/388, train_loss: 0.2469, step time: 0.4972\n",
      "220/388, train_loss: 0.3000, step time: 0.4872\n",
      "221/388, train_loss: 0.6594, step time: 0.4839\n",
      "222/388, train_loss: 0.2160, step time: 0.4791\n",
      "223/388, train_loss: 0.2176, step time: 0.6856\n",
      "224/388, train_loss: 0.1436, step time: 0.5466\n",
      "225/388, train_loss: 0.2175, step time: 0.5275\n",
      "226/388, train_loss: 0.0992, step time: 0.5127\n",
      "227/388, train_loss: 0.1150, step time: 0.4979\n",
      "228/388, train_loss: 0.0853, step time: 0.4934\n",
      "229/388, train_loss: 0.1062, step time: 0.5065\n",
      "230/388, train_loss: 0.2775, step time: 0.5046\n",
      "231/388, train_loss: 0.7195, step time: 1.2047\n",
      "232/388, train_loss: 0.2358, step time: 0.5489\n",
      "233/388, train_loss: 0.1394, step time: 0.5159\n",
      "234/388, train_loss: 0.2736, step time: 0.5081\n",
      "235/388, train_loss: 0.1567, step time: 0.4929\n",
      "236/388, train_loss: 0.1768, step time: 0.4897\n",
      "237/388, train_loss: 0.1012, step time: 0.5006\n",
      "238/388, train_loss: 0.0642, step time: 0.4825\n",
      "239/388, train_loss: 0.4277, step time: 0.4946\n",
      "240/388, train_loss: 0.1198, step time: 0.5232\n",
      "241/388, train_loss: 0.3180, step time: 0.5137\n",
      "242/388, train_loss: 0.1926, step time: 0.5079\n",
      "243/388, train_loss: 0.2625, step time: 0.5001\n",
      "244/388, train_loss: 0.2800, step time: 0.4852\n",
      "245/388, train_loss: 0.2595, step time: 1.1636\n",
      "246/388, train_loss: 0.2479, step time: 0.5469\n",
      "247/388, train_loss: 0.0977, step time: 0.5131\n",
      "248/388, train_loss: 0.1725, step time: 0.4958\n",
      "249/388, train_loss: 0.2804, step time: 0.4918\n",
      "250/388, train_loss: 0.1596, step time: 0.4917\n",
      "251/388, train_loss: 0.0416, step time: 1.0990\n",
      "252/388, train_loss: 0.1876, step time: 0.5261\n",
      "253/388, train_loss: 0.3500, step time: 0.5141\n",
      "254/388, train_loss: 0.6771, step time: 0.4975\n",
      "255/388, train_loss: 0.1542, step time: 0.4930\n",
      "256/388, train_loss: 0.0729, step time: 0.4796\n",
      "257/388, train_loss: 0.1918, step time: 0.4954\n",
      "258/388, train_loss: 0.2664, step time: 0.4954\n",
      "259/388, train_loss: 0.2638, step time: 0.4936\n",
      "260/388, train_loss: 0.1940, step time: 0.5073\n",
      "261/388, train_loss: 0.5886, step time: 0.5063\n",
      "262/388, train_loss: 0.3602, step time: 0.4857\n",
      "263/388, train_loss: 0.2509, step time: 0.5556\n",
      "264/388, train_loss: 0.4841, step time: 0.5896\n",
      "265/388, train_loss: 0.2660, step time: 0.5355\n",
      "266/388, train_loss: 0.2521, step time: 0.5011\n",
      "267/388, train_loss: 0.1500, step time: 0.4956\n",
      "268/388, train_loss: 0.0735, step time: 0.4789\n",
      "269/388, train_loss: 0.3059, step time: 1.0662\n",
      "270/388, train_loss: 0.3767, step time: 0.5321\n",
      "271/388, train_loss: 0.1882, step time: 0.5107\n",
      "272/388, train_loss: 0.3734, step time: 0.4865\n",
      "273/388, train_loss: 0.1409, step time: 0.4854\n",
      "274/388, train_loss: 0.1565, step time: 0.4923\n",
      "275/388, train_loss: 0.1583, step time: 0.4810\n",
      "276/388, train_loss: 0.2344, step time: 0.4942\n",
      "277/388, train_loss: 0.1192, step time: 0.4881\n",
      "278/388, train_loss: 0.1023, step time: 0.4933\n",
      "279/388, train_loss: 0.1616, step time: 0.4796\n",
      "280/388, train_loss: 0.4581, step time: 0.4766\n",
      "281/388, train_loss: 0.2181, step time: 1.0477\n",
      "282/388, train_loss: 0.1894, step time: 0.5325\n",
      "283/388, train_loss: 0.1615, step time: 0.5100\n",
      "284/388, train_loss: 0.2365, step time: 0.5014\n",
      "285/388, train_loss: 0.1393, step time: 0.4939\n",
      "286/388, train_loss: 0.1827, step time: 0.4806\n",
      "287/388, train_loss: 0.1944, step time: 0.6480\n",
      "288/388, train_loss: 0.2654, step time: 0.5297\n",
      "289/388, train_loss: 0.1767, step time: 0.5041\n",
      "290/388, train_loss: 0.0768, step time: 0.5014\n",
      "291/388, train_loss: 0.4256, step time: 0.4941\n",
      "292/388, train_loss: 0.1221, step time: 0.4831\n",
      "293/388, train_loss: 0.1963, step time: 0.7546\n",
      "294/388, train_loss: 0.1598, step time: 0.5410\n",
      "295/388, train_loss: 0.4280, step time: 0.5018\n",
      "296/388, train_loss: 0.2347, step time: 0.5012\n",
      "297/388, train_loss: 0.4197, step time: 0.5041\n",
      "298/388, train_loss: 0.1454, step time: 0.5011\n",
      "299/388, train_loss: 0.2757, step time: 0.4904\n",
      "300/388, train_loss: 0.2589, step time: 0.4818\n",
      "301/388, train_loss: 0.1785, step time: 0.4848\n",
      "302/388, train_loss: 0.1308, step time: 0.5013\n",
      "303/388, train_loss: 0.3485, step time: 0.4908\n",
      "304/388, train_loss: 0.2303, step time: 0.4841\n",
      "305/388, train_loss: 0.1176, step time: 1.1372\n",
      "306/388, train_loss: 0.1829, step time: 0.5293\n",
      "307/388, train_loss: 0.1350, step time: 0.5046\n",
      "308/388, train_loss: 0.1109, step time: 0.4959\n",
      "309/388, train_loss: 0.2260, step time: 0.4872\n",
      "310/388, train_loss: 0.1998, step time: 0.4891\n",
      "311/388, train_loss: 0.5358, step time: 0.4827\n",
      "312/388, train_loss: 0.0978, step time: 0.5019\n",
      "313/388, train_loss: 0.1059, step time: 0.4856\n",
      "314/388, train_loss: 0.1119, step time: 0.4752\n",
      "315/388, train_loss: 0.7220, step time: 0.7844\n",
      "316/388, train_loss: 0.1092, step time: 0.5371\n",
      "317/388, train_loss: 0.1203, step time: 0.5188\n",
      "318/388, train_loss: 0.1264, step time: 0.4890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/388, train_loss: 0.1444, step time: 0.4952\n",
      "320/388, train_loss: 0.1407, step time: 0.4876\n",
      "321/388, train_loss: 0.1542, step time: 0.4999\n",
      "322/388, train_loss: 0.3897, step time: 0.4983\n",
      "323/388, train_loss: 0.2597, step time: 0.5124\n",
      "324/388, train_loss: 0.4238, step time: 0.4956\n",
      "325/388, train_loss: 0.0897, step time: 0.4966\n",
      "326/388, train_loss: 0.3575, step time: 0.4804\n",
      "327/388, train_loss: 0.2689, step time: 0.5035\n",
      "328/388, train_loss: 0.3337, step time: 0.5112\n",
      "329/388, train_loss: 0.2864, step time: 0.5869\n",
      "330/388, train_loss: 0.1859, step time: 0.5328\n",
      "331/388, train_loss: 0.0647, step time: 0.4989\n",
      "332/388, train_loss: 0.1651, step time: 0.5016\n",
      "333/388, train_loss: 0.1698, step time: 0.4841\n",
      "334/388, train_loss: 0.2066, step time: 0.5637\n",
      "335/388, train_loss: 0.0757, step time: 0.5340\n",
      "336/388, train_loss: 0.2784, step time: 0.5113\n",
      "337/388, train_loss: 0.3152, step time: 0.5015\n",
      "338/388, train_loss: 0.5704, step time: 0.5413\n",
      "339/388, train_loss: 0.0857, step time: 0.5414\n",
      "340/388, train_loss: 0.2585, step time: 0.5069\n",
      "341/388, train_loss: 0.3403, step time: 0.5180\n",
      "342/388, train_loss: 0.1782, step time: 0.5328\n",
      "343/388, train_loss: 0.0761, step time: 0.5778\n",
      "344/388, train_loss: 0.3473, step time: 0.5337\n",
      "345/388, train_loss: 0.1594, step time: 0.5096\n",
      "346/388, train_loss: 0.4839, step time: 0.4931\n",
      "347/388, train_loss: 0.1317, step time: 0.4954\n",
      "348/388, train_loss: 0.2017, step time: 0.4830\n",
      "349/388, train_loss: 0.0527, step time: 0.4919\n",
      "350/388, train_loss: 0.1063, step time: 0.4776\n",
      "351/388, train_loss: 0.1447, step time: 0.4984\n",
      "352/388, train_loss: 0.3536, step time: 0.4983\n",
      "353/388, train_loss: 0.1004, step time: 0.5158\n",
      "354/388, train_loss: 0.3092, step time: 0.5477\n",
      "355/388, train_loss: 0.3227, step time: 0.5149\n",
      "356/388, train_loss: 0.1943, step time: 0.4927\n",
      "357/388, train_loss: 0.3284, step time: 0.4956\n",
      "358/388, train_loss: 0.1534, step time: 1.1115\n",
      "359/388, train_loss: 0.6057, step time: 0.5277\n",
      "360/388, train_loss: 0.2076, step time: 0.5106\n",
      "361/388, train_loss: 0.1486, step time: 0.4857\n",
      "362/388, train_loss: 0.3541, step time: 0.4898\n",
      "363/388, train_loss: 0.2630, step time: 0.4898\n",
      "364/388, train_loss: 0.2127, step time: 0.4793\n",
      "365/388, train_loss: 0.1463, step time: 0.4897\n",
      "366/388, train_loss: 0.1391, step time: 0.4835\n",
      "367/388, train_loss: 0.3514, step time: 0.4878\n",
      "368/388, train_loss: 0.1142, step time: 0.4943\n",
      "369/388, train_loss: 0.2466, step time: 0.4881\n",
      "370/388, train_loss: 0.0559, step time: 0.5348\n",
      "371/388, train_loss: 0.3414, step time: 0.5201\n",
      "372/388, train_loss: 0.0981, step time: 0.5021\n",
      "373/388, train_loss: 0.2384, step time: 0.4927\n",
      "374/388, train_loss: 0.1505, step time: 1.1947\n",
      "375/388, train_loss: 0.1102, step time: 0.5204\n",
      "376/388, train_loss: 0.2330, step time: 0.5008\n",
      "377/388, train_loss: 0.1107, step time: 0.4867\n",
      "378/388, train_loss: 0.0541, step time: 0.4956\n",
      "379/388, train_loss: 0.2280, step time: 0.4824\n",
      "380/388, train_loss: 0.1505, step time: 0.4780\n",
      "381/388, train_loss: 0.2801, step time: 0.4711\n",
      "382/388, train_loss: 0.1624, step time: 1.0085\n",
      "383/388, train_loss: 0.4067, step time: 0.5329\n",
      "384/388, train_loss: 0.2606, step time: 0.5061\n",
      "385/388, train_loss: 0.1120, step time: 0.4936\n",
      "386/388, train_loss: 0.1633, step time: 0.4889\n",
      "387/388, train_loss: 0.0828, step time: 0.4839\n",
      "388/388, train_loss: 0.1690, step time: 0.4894\n",
      "epoch 19 average loss: 0.2245\n",
      "current epoch: 19 current mean dice: 0.6900 tc: 0.7168 wt: 0.8457 et: 0.5075\n",
      "best mean dice: 0.7289 at epoch: 18\n",
      "time consuming of epoch 19 is: 302.0164\n",
      "----------\n",
      "epoch 20/300\n",
      "1/388, train_loss: 0.6136, step time: 0.4757\n",
      "2/388, train_loss: 0.1701, step time: 0.4807\n",
      "3/388, train_loss: 0.1532, step time: 0.5923\n",
      "4/388, train_loss: 0.0620, step time: 0.5586\n",
      "5/388, train_loss: 0.2709, step time: 0.4986\n",
      "6/388, train_loss: 0.3250, step time: 0.4903\n",
      "7/388, train_loss: 0.1183, step time: 0.9742\n",
      "8/388, train_loss: 0.2277, step time: 0.5743\n",
      "9/388, train_loss: 0.1280, step time: 0.5257\n",
      "10/388, train_loss: 0.2550, step time: 0.5123\n",
      "11/388, train_loss: 0.1081, step time: 0.5123\n",
      "12/388, train_loss: 0.1908, step time: 0.4971\n",
      "13/388, train_loss: 0.1101, step time: 0.5262\n",
      "14/388, train_loss: 0.2775, step time: 0.5148\n",
      "15/388, train_loss: 0.2226, step time: 0.5164\n",
      "16/388, train_loss: 0.1024, step time: 0.5723\n",
      "17/388, train_loss: 0.2472, step time: 0.5550\n",
      "18/388, train_loss: 0.1288, step time: 0.5228\n",
      "19/388, train_loss: 0.1379, step time: 0.5348\n",
      "20/388, train_loss: 0.2602, step time: 0.5276\n",
      "21/388, train_loss: 0.1770, step time: 0.5221\n",
      "22/388, train_loss: 0.2319, step time: 0.5832\n",
      "23/388, train_loss: 0.2320, step time: 0.5285\n",
      "24/388, train_loss: 0.1421, step time: 0.5009\n",
      "25/388, train_loss: 0.1065, step time: 0.4950\n",
      "26/388, train_loss: 0.0945, step time: 0.5031\n",
      "27/388, train_loss: 0.1219, step time: 0.4947\n",
      "28/388, train_loss: 0.3322, step time: 0.9492\n",
      "29/388, train_loss: 0.0991, step time: 0.5630\n",
      "30/388, train_loss: 0.1825, step time: 0.5376\n",
      "31/388, train_loss: 0.6148, step time: 0.5014\n",
      "32/388, train_loss: 0.0986, step time: 0.4971\n",
      "33/388, train_loss: 0.4459, step time: 0.5032\n",
      "34/388, train_loss: 0.1246, step time: 1.1930\n",
      "35/388, train_loss: 0.1607, step time: 0.5562\n",
      "36/388, train_loss: 0.1121, step time: 0.5243\n",
      "37/388, train_loss: 0.1350, step time: 0.5093\n",
      "38/388, train_loss: 0.1790, step time: 0.4987\n",
      "39/388, train_loss: 0.1892, step time: 0.5430\n",
      "40/388, train_loss: 0.6747, step time: 0.5234\n",
      "41/388, train_loss: 0.1026, step time: 0.5085\n",
      "42/388, train_loss: 0.2414, step time: 0.4879\n",
      "43/388, train_loss: 0.1363, step time: 1.1302\n",
      "44/388, train_loss: 0.3844, step time: 0.5243\n",
      "45/388, train_loss: 0.3015, step time: 0.5016\n",
      "46/388, train_loss: 0.1289, step time: 0.4984\n",
      "47/388, train_loss: 0.3102, step time: 0.4884\n",
      "48/388, train_loss: 0.0860, step time: 0.5225\n",
      "49/388, train_loss: 0.3917, step time: 0.5151\n",
      "50/388, train_loss: 0.3472, step time: 0.5043\n",
      "51/388, train_loss: 0.0950, step time: 0.4852\n",
      "52/388, train_loss: 0.1716, step time: 0.5210\n",
      "53/388, train_loss: 0.1806, step time: 0.5041\n",
      "54/388, train_loss: 0.2567, step time: 0.5212\n",
      "55/388, train_loss: 0.0659, step time: 0.4977\n",
      "56/388, train_loss: 0.2492, step time: 0.5059\n",
      "57/388, train_loss: 0.1578, step time: 0.4923\n",
      "58/388, train_loss: 0.2081, step time: 0.6887\n",
      "59/388, train_loss: 0.0900, step time: 0.5345\n",
      "60/388, train_loss: 0.2697, step time: 0.5142\n",
      "61/388, train_loss: 0.1502, step time: 0.5603\n",
      "62/388, train_loss: 0.3572, step time: 0.5317\n",
      "63/388, train_loss: 0.2578, step time: 0.5127\n",
      "64/388, train_loss: 0.0612, step time: 0.5426\n",
      "65/388, train_loss: 0.1308, step time: 0.5244\n",
      "66/388, train_loss: 0.1096, step time: 0.5066\n",
      "67/388, train_loss: 0.3358, step time: 0.4919\n",
      "68/388, train_loss: 0.1090, step time: 0.5610\n",
      "69/388, train_loss: 0.2078, step time: 0.5320\n",
      "70/388, train_loss: 0.1972, step time: 0.5046\n",
      "71/388, train_loss: 0.1881, step time: 0.5018\n",
      "72/388, train_loss: 0.2601, step time: 0.4821\n",
      "73/388, train_loss: 0.3561, step time: 0.4822\n",
      "74/388, train_loss: 0.2056, step time: 1.0290\n",
      "75/388, train_loss: 0.6656, step time: 0.5542\n",
      "76/388, train_loss: 0.1061, step time: 0.5190\n",
      "77/388, train_loss: 0.2359, step time: 0.5046\n",
      "78/388, train_loss: 0.1940, step time: 0.4866\n",
      "79/388, train_loss: 0.1066, step time: 0.4812\n",
      "80/388, train_loss: 0.2617, step time: 0.4845\n",
      "81/388, train_loss: 0.0743, step time: 0.4800\n",
      "82/388, train_loss: 0.0818, step time: 0.4815\n",
      "83/388, train_loss: 0.1109, step time: 0.4994\n",
      "84/388, train_loss: 0.3620, step time: 0.5010\n",
      "85/388, train_loss: 0.0818, step time: 0.4788\n",
      "86/388, train_loss: 0.2196, step time: 0.5329\n",
      "87/388, train_loss: 0.2452, step time: 0.5346\n",
      "88/388, train_loss: 0.1305, step time: 0.5121\n",
      "89/388, train_loss: 0.2470, step time: 0.4984\n",
      "90/388, train_loss: 0.1179, step time: 1.2268\n",
      "91/388, train_loss: 0.2599, step time: 0.5362\n",
      "92/388, train_loss: 0.1211, step time: 0.4998\n",
      "93/388, train_loss: 0.2075, step time: 0.4895\n",
      "94/388, train_loss: 0.0863, step time: 0.4910\n",
      "95/388, train_loss: 0.2702, step time: 0.4798\n",
      "96/388, train_loss: 0.1197, step time: 1.0907\n",
      "97/388, train_loss: 0.2805, step time: 0.5437\n",
      "98/388, train_loss: 0.4687, step time: 0.5056\n",
      "99/388, train_loss: 0.1222, step time: 0.4951\n",
      "100/388, train_loss: 0.1767, step time: 0.4822\n",
      "101/388, train_loss: 0.3074, step time: 0.5271\n",
      "102/388, train_loss: 0.0816, step time: 0.5041\n",
      "103/388, train_loss: 0.2066, step time: 0.5006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/388, train_loss: 0.1410, step time: 0.4869\n",
      "105/388, train_loss: 0.2066, step time: 0.4823\n",
      "106/388, train_loss: 0.2075, step time: 0.5284\n",
      "107/388, train_loss: 0.1493, step time: 0.5223\n",
      "108/388, train_loss: 0.1356, step time: 0.4876\n",
      "109/388, train_loss: 0.1354, step time: 0.4789\n",
      "110/388, train_loss: 0.2415, step time: 0.5157\n",
      "111/388, train_loss: 0.0934, step time: 0.5833\n",
      "112/388, train_loss: 0.1377, step time: 0.5369\n",
      "113/388, train_loss: 0.1393, step time: 0.5214\n",
      "114/388, train_loss: 0.1596, step time: 0.5118\n",
      "115/388, train_loss: 0.0955, step time: 0.4856\n",
      "116/388, train_loss: 0.4123, step time: 0.4873\n",
      "117/388, train_loss: 0.2355, step time: 0.4829\n",
      "118/388, train_loss: 0.2149, step time: 0.5003\n",
      "119/388, train_loss: 0.1753, step time: 0.5766\n",
      "120/388, train_loss: 0.0998, step time: 0.5363\n",
      "121/388, train_loss: 0.0886, step time: 0.5275\n",
      "122/388, train_loss: 0.1682, step time: 0.5117\n",
      "123/388, train_loss: 0.2407, step time: 0.4863\n",
      "124/388, train_loss: 0.1688, step time: 0.5199\n",
      "125/388, train_loss: 0.2718, step time: 0.5128\n",
      "126/388, train_loss: 0.0776, step time: 0.5081\n",
      "127/388, train_loss: 0.0498, step time: 0.5640\n",
      "128/388, train_loss: 0.1412, step time: 0.5318\n",
      "129/388, train_loss: 0.2305, step time: 0.5103\n",
      "130/388, train_loss: 0.1463, step time: 0.5553\n",
      "131/388, train_loss: 0.1670, step time: 0.5478\n",
      "132/388, train_loss: 0.1043, step time: 0.5235\n",
      "133/388, train_loss: 0.0892, step time: 0.5094\n",
      "134/388, train_loss: 0.2082, step time: 0.4933\n",
      "135/388, train_loss: 0.1200, step time: 0.5174\n",
      "136/388, train_loss: 0.0796, step time: 0.5574\n",
      "137/388, train_loss: 0.4517, step time: 0.5096\n",
      "138/388, train_loss: 0.1742, step time: 0.4955\n",
      "139/388, train_loss: 0.2663, step time: 1.1302\n",
      "140/388, train_loss: 0.2543, step time: 0.5333\n",
      "141/388, train_loss: 0.2550, step time: 0.5082\n",
      "142/388, train_loss: 0.1712, step time: 0.4866\n",
      "143/388, train_loss: 0.6086, step time: 0.4833\n",
      "144/388, train_loss: 0.0699, step time: 0.5022\n",
      "145/388, train_loss: 0.3381, step time: 0.5118\n",
      "146/388, train_loss: 0.1326, step time: 0.5005\n",
      "147/388, train_loss: 0.1594, step time: 0.4934\n",
      "148/388, train_loss: 0.1836, step time: 0.4968\n",
      "149/388, train_loss: 0.4564, step time: 0.4967\n",
      "150/388, train_loss: 0.0928, step time: 1.1074\n",
      "151/388, train_loss: 0.2504, step time: 0.5443\n",
      "152/388, train_loss: 0.2274, step time: 0.5084\n",
      "153/388, train_loss: 0.3360, step time: 0.4952\n",
      "154/388, train_loss: 0.5088, step time: 0.4961\n",
      "155/388, train_loss: 0.5541, step time: 0.4911\n",
      "156/388, train_loss: 0.2479, step time: 0.4773\n",
      "157/388, train_loss: 0.1847, step time: 0.4968\n",
      "158/388, train_loss: 0.0726, step time: 0.4962\n",
      "159/388, train_loss: 0.2101, step time: 1.2364\n",
      "160/388, train_loss: 0.2521, step time: 0.5278\n",
      "161/388, train_loss: 0.1678, step time: 0.5100\n",
      "162/388, train_loss: 0.1169, step time: 0.4980\n",
      "163/388, train_loss: 0.0894, step time: 0.4898\n",
      "164/388, train_loss: 0.4978, step time: 0.4946\n",
      "165/388, train_loss: 0.6196, step time: 0.4980\n",
      "166/388, train_loss: 0.6160, step time: 0.5037\n",
      "167/388, train_loss: 0.2847, step time: 0.4819\n",
      "168/388, train_loss: 0.3608, step time: 0.4834\n",
      "169/388, train_loss: 0.2149, step time: 0.4929\n",
      "170/388, train_loss: 0.2136, step time: 0.9742\n",
      "171/388, train_loss: 0.1569, step time: 0.5399\n",
      "172/388, train_loss: 0.2471, step time: 0.5091\n",
      "173/388, train_loss: 0.0664, step time: 0.4974\n",
      "174/388, train_loss: 0.3467, step time: 0.4951\n",
      "175/388, train_loss: 0.1197, step time: 0.5200\n",
      "176/388, train_loss: 0.2546, step time: 0.4948\n",
      "177/388, train_loss: 0.1610, step time: 0.4815\n",
      "178/388, train_loss: 0.1338, step time: 0.5017\n",
      "179/388, train_loss: 0.0655, step time: 0.4866\n",
      "180/388, train_loss: 0.2318, step time: 0.4981\n",
      "181/388, train_loss: 0.0693, step time: 0.4857\n",
      "182/388, train_loss: 0.0988, step time: 0.4938\n",
      "183/388, train_loss: 0.1753, step time: 0.5321\n",
      "184/388, train_loss: 0.6179, step time: 0.5199\n",
      "185/388, train_loss: 0.1356, step time: 0.4933\n",
      "186/388, train_loss: 0.1347, step time: 0.4981\n",
      "187/388, train_loss: 0.2820, step time: 0.5072\n",
      "188/388, train_loss: 0.2057, step time: 0.5557\n",
      "189/388, train_loss: 0.2337, step time: 0.5266\n",
      "190/388, train_loss: 0.5245, step time: 0.4981\n",
      "191/388, train_loss: 0.1699, step time: 0.5348\n",
      "192/388, train_loss: 0.1058, step time: 0.5282\n",
      "193/388, train_loss: 0.0633, step time: 0.5118\n",
      "194/388, train_loss: 0.5248, step time: 0.5023\n",
      "195/388, train_loss: 0.2466, step time: 0.5007\n",
      "196/388, train_loss: 0.0694, step time: 1.1502\n",
      "197/388, train_loss: 0.1996, step time: 0.5461\n",
      "198/388, train_loss: 0.3391, step time: 0.5024\n",
      "199/388, train_loss: 0.1777, step time: 0.5147\n",
      "200/388, train_loss: 0.2454, step time: 0.4961\n",
      "201/388, train_loss: 0.1726, step time: 0.4964\n",
      "202/388, train_loss: 0.1166, step time: 0.4819\n",
      "203/388, train_loss: 0.1299, step time: 0.4901\n",
      "204/388, train_loss: 0.2854, step time: 0.5840\n",
      "205/388, train_loss: 0.1725, step time: 0.5297\n",
      "206/388, train_loss: 0.3260, step time: 0.4925\n",
      "207/388, train_loss: 0.0768, step time: 0.9558\n",
      "208/388, train_loss: 0.3196, step time: 0.5305\n",
      "209/388, train_loss: 0.2413, step time: 0.5011\n",
      "210/388, train_loss: 0.4731, step time: 0.4965\n",
      "211/388, train_loss: 0.2852, step time: 0.4914\n",
      "212/388, train_loss: 0.3780, step time: 0.6011\n",
      "213/388, train_loss: 0.0935, step time: 0.5600\n",
      "214/388, train_loss: 0.3227, step time: 0.5326\n",
      "215/388, train_loss: 0.3018, step time: 0.5103\n",
      "216/388, train_loss: 0.1358, step time: 0.4972\n",
      "217/388, train_loss: 0.2707, step time: 0.5258\n",
      "218/388, train_loss: 0.2801, step time: 0.5644\n",
      "219/388, train_loss: 0.3892, step time: 0.5347\n",
      "220/388, train_loss: 0.1425, step time: 0.5112\n",
      "221/388, train_loss: 0.0500, step time: 0.5021\n",
      "222/388, train_loss: 0.1909, step time: 0.5664\n",
      "223/388, train_loss: 0.4536, step time: 0.5397\n",
      "224/388, train_loss: 0.0688, step time: 0.5139\n",
      "225/388, train_loss: 0.1061, step time: 0.5019\n",
      "226/388, train_loss: 0.2631, step time: 1.2124\n",
      "227/388, train_loss: 0.0495, step time: 0.5400\n",
      "228/388, train_loss: 0.0980, step time: 0.5130\n",
      "229/388, train_loss: 0.2733, step time: 0.4880\n",
      "230/388, train_loss: 0.1648, step time: 0.4895\n",
      "231/388, train_loss: 0.2189, step time: 0.4868\n",
      "232/388, train_loss: 0.7215, step time: 1.0325\n",
      "233/388, train_loss: 0.2663, step time: 0.5584\n",
      "234/388, train_loss: 0.3129, step time: 0.5133\n",
      "235/388, train_loss: 0.2305, step time: 0.4899\n",
      "236/388, train_loss: 0.1078, step time: 0.4894\n",
      "237/388, train_loss: 0.2645, step time: 0.4927\n",
      "238/388, train_loss: 0.2592, step time: 1.1815\n",
      "239/388, train_loss: 0.1694, step time: 0.5382\n",
      "240/388, train_loss: 0.1743, step time: 0.5009\n",
      "241/388, train_loss: 0.1888, step time: 0.4967\n",
      "242/388, train_loss: 0.1722, step time: 0.4884\n",
      "243/388, train_loss: 0.2802, step time: 0.6939\n",
      "244/388, train_loss: 0.2789, step time: 0.5406\n",
      "245/388, train_loss: 0.3155, step time: 0.5356\n",
      "246/388, train_loss: 0.2298, step time: 0.5157\n",
      "247/388, train_loss: 0.2006, step time: 0.5034\n",
      "248/388, train_loss: 0.2269, step time: 0.4987\n",
      "249/388, train_loss: 0.1537, step time: 0.4949\n",
      "250/388, train_loss: 0.2136, step time: 0.4807\n",
      "251/388, train_loss: 0.0676, step time: 0.6771\n",
      "252/388, train_loss: 0.2522, step time: 0.5709\n",
      "253/388, train_loss: 0.4100, step time: 0.5328\n",
      "254/388, train_loss: 0.5498, step time: 0.5159\n",
      "255/388, train_loss: 0.2654, step time: 0.4986\n",
      "256/388, train_loss: 0.1581, step time: 0.4850\n",
      "257/388, train_loss: 0.2018, step time: 0.4868\n",
      "258/388, train_loss: 0.1273, step time: 0.4862\n",
      "259/388, train_loss: 0.3755, step time: 0.6365\n",
      "260/388, train_loss: 0.1498, step time: 0.5662\n",
      "261/388, train_loss: 0.2261, step time: 0.5480\n",
      "262/388, train_loss: 0.1963, step time: 0.5076\n",
      "263/388, train_loss: 0.1312, step time: 0.4954\n",
      "264/388, train_loss: 0.0948, step time: 0.4907\n",
      "265/388, train_loss: 0.1058, step time: 0.5494\n",
      "266/388, train_loss: 0.1254, step time: 0.5365\n",
      "267/388, train_loss: 0.2492, step time: 0.5037\n",
      "268/388, train_loss: 0.3918, step time: 0.5026\n",
      "269/388, train_loss: 0.3078, step time: 0.4951\n",
      "270/388, train_loss: 0.5336, step time: 1.0115\n",
      "271/388, train_loss: 0.2261, step time: 0.5398\n",
      "272/388, train_loss: 0.4546, step time: 0.5079\n",
      "273/388, train_loss: 0.2684, step time: 0.4907\n",
      "274/388, train_loss: 0.1373, step time: 0.4848\n",
      "275/388, train_loss: 0.4919, step time: 0.5029\n",
      "276/388, train_loss: 0.3564, step time: 0.4869\n",
      "277/388, train_loss: 0.2387, step time: 0.4931\n",
      "278/388, train_loss: 0.2199, step time: 1.0929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/388, train_loss: 0.4088, step time: 0.5432\n",
      "280/388, train_loss: 0.3995, step time: 0.5184\n",
      "281/388, train_loss: 0.1660, step time: 0.5061\n",
      "282/388, train_loss: 0.2963, step time: 0.4975\n",
      "283/388, train_loss: 0.1015, step time: 0.4897\n",
      "284/388, train_loss: 0.1182, step time: 0.4917\n",
      "285/388, train_loss: 0.1535, step time: 0.8702\n",
      "286/388, train_loss: 0.1028, step time: 0.5319\n",
      "287/388, train_loss: 0.2533, step time: 0.5060\n",
      "288/388, train_loss: 0.2120, step time: 0.4972\n",
      "289/388, train_loss: 0.3780, step time: 0.4841\n",
      "290/388, train_loss: 0.2893, step time: 0.5268\n",
      "291/388, train_loss: 0.4387, step time: 0.5063\n",
      "292/388, train_loss: 0.2670, step time: 0.5078\n",
      "293/388, train_loss: 0.3126, step time: 0.4813\n",
      "294/388, train_loss: 0.3255, step time: 0.5623\n",
      "295/388, train_loss: 0.1896, step time: 0.5438\n",
      "296/388, train_loss: 0.2405, step time: 0.5322\n",
      "297/388, train_loss: 0.1764, step time: 0.5180\n",
      "298/388, train_loss: 0.3245, step time: 0.5185\n",
      "299/388, train_loss: 0.1765, step time: 0.4933\n",
      "300/388, train_loss: 0.2483, step time: 0.5138\n",
      "301/388, train_loss: 0.1378, step time: 0.5223\n",
      "302/388, train_loss: 0.2831, step time: 0.5092\n",
      "303/388, train_loss: 0.0920, step time: 0.5153\n",
      "304/388, train_loss: 0.1220, step time: 0.4994\n",
      "305/388, train_loss: 0.2792, step time: 0.5160\n",
      "306/388, train_loss: 0.1771, step time: 0.6157\n",
      "307/388, train_loss: 0.2383, step time: 0.5439\n",
      "308/388, train_loss: 0.1290, step time: 0.5102\n",
      "309/388, train_loss: 0.2568, step time: 0.4907\n",
      "310/388, train_loss: 0.2752, step time: 0.7326\n",
      "311/388, train_loss: 0.2241, step time: 0.5485\n",
      "312/388, train_loss: 0.3512, step time: 0.5197\n",
      "313/388, train_loss: 0.2065, step time: 0.5030\n",
      "314/388, train_loss: 0.1950, step time: 0.4915\n",
      "315/388, train_loss: 0.1027, step time: 0.5002\n",
      "316/388, train_loss: 0.3316, step time: 0.4894\n",
      "317/388, train_loss: 0.1306, step time: 1.2108\n",
      "318/388, train_loss: 0.2707, step time: 0.5327\n",
      "319/388, train_loss: 0.2392, step time: 0.5085\n",
      "320/388, train_loss: 0.2368, step time: 0.4981\n",
      "321/388, train_loss: 0.2856, step time: 0.4954\n",
      "322/388, train_loss: 0.2780, step time: 0.4789\n",
      "323/388, train_loss: 0.2019, step time: 0.4791\n",
      "324/388, train_loss: 0.0986, step time: 0.4988\n",
      "325/388, train_loss: 0.4109, step time: 0.5198\n",
      "326/388, train_loss: 0.1357, step time: 0.6235\n",
      "327/388, train_loss: 0.1501, step time: 0.5501\n",
      "328/388, train_loss: 0.1443, step time: 0.5176\n",
      "329/388, train_loss: 0.1201, step time: 0.5054\n",
      "330/388, train_loss: 0.1434, step time: 0.5004\n",
      "331/388, train_loss: 0.2952, step time: 0.4825\n",
      "332/388, train_loss: 0.2016, step time: 1.1524\n",
      "333/388, train_loss: 0.0404, step time: 0.5376\n",
      "334/388, train_loss: 0.2025, step time: 0.5080\n",
      "335/388, train_loss: 0.2596, step time: 0.4870\n",
      "336/388, train_loss: 0.0951, step time: 0.4826\n",
      "337/388, train_loss: 0.3887, step time: 0.4863\n",
      "338/388, train_loss: 0.1755, step time: 0.8275\n",
      "339/388, train_loss: 0.3612, step time: 0.5436\n",
      "340/388, train_loss: 0.1389, step time: 0.5164\n",
      "341/388, train_loss: 0.1330, step time: 0.5021\n",
      "342/388, train_loss: 0.1232, step time: 0.4916\n",
      "343/388, train_loss: 0.2091, step time: 0.4982\n",
      "344/388, train_loss: 0.0945, step time: 0.4860\n",
      "345/388, train_loss: 0.2125, step time: 0.4978\n",
      "346/388, train_loss: 0.2228, step time: 0.4956\n",
      "347/388, train_loss: 0.1442, step time: 0.4821\n",
      "348/388, train_loss: 0.1447, step time: 0.4863\n",
      "349/388, train_loss: 0.3066, step time: 0.4714\n",
      "350/388, train_loss: 0.2826, step time: 0.4895\n",
      "351/388, train_loss: 0.2305, step time: 1.1453\n",
      "352/388, train_loss: 0.2846, step time: 0.5567\n",
      "353/388, train_loss: 0.1803, step time: 0.5223\n",
      "354/388, train_loss: 0.0820, step time: 0.4902\n",
      "355/388, train_loss: 0.1069, step time: 0.4825\n",
      "356/388, train_loss: 0.2129, step time: 0.4977\n",
      "357/388, train_loss: 0.6222, step time: 0.4939\n",
      "358/388, train_loss: 0.1444, step time: 0.5062\n",
      "359/388, train_loss: 0.4395, step time: 0.5296\n",
      "360/388, train_loss: 0.2842, step time: 0.5074\n",
      "361/388, train_loss: 0.3687, step time: 0.4929\n",
      "362/388, train_loss: 0.1709, step time: 0.4897\n",
      "363/388, train_loss: 0.2867, step time: 0.4781\n",
      "364/388, train_loss: 0.1219, step time: 1.1033\n",
      "365/388, train_loss: 0.3123, step time: 0.5218\n",
      "366/388, train_loss: 0.3281, step time: 0.5030\n",
      "367/388, train_loss: 0.5026, step time: 0.4872\n",
      "368/388, train_loss: 0.1149, step time: 0.4977\n",
      "369/388, train_loss: 0.4160, step time: 0.4773\n",
      "370/388, train_loss: 0.1447, step time: 1.1788\n",
      "371/388, train_loss: 0.2488, step time: 0.5253\n",
      "372/388, train_loss: 0.1783, step time: 0.5036\n",
      "373/388, train_loss: 0.1741, step time: 0.4862\n",
      "374/388, train_loss: 0.3397, step time: 0.5013\n",
      "375/388, train_loss: 0.1805, step time: 0.5061\n",
      "376/388, train_loss: 0.0783, step time: 0.4940\n",
      "377/388, train_loss: 0.3076, step time: 0.4957\n",
      "378/388, train_loss: 0.1781, step time: 0.5109\n",
      "379/388, train_loss: 0.3140, step time: 0.5010\n",
      "380/388, train_loss: 0.0416, step time: 0.4864\n",
      "381/388, train_loss: 0.0940, step time: 0.5160\n",
      "382/388, train_loss: 0.1707, step time: 0.5008\n",
      "383/388, train_loss: 0.1230, step time: 0.4943\n",
      "384/388, train_loss: 0.1353, step time: 0.4736\n",
      "385/388, train_loss: 0.1494, step time: 0.4800\n",
      "386/388, train_loss: 0.2550, step time: 0.5076\n",
      "387/388, train_loss: 0.0910, step time: 0.4845\n",
      "388/388, train_loss: 0.1669, step time: 0.4762\n",
      "epoch 20 average loss: 0.2226\n",
      "saved new best metric model\n",
      "current epoch: 20 current mean dice: 0.7413 tc: 0.7879 wt: 0.8887 et: 0.5474\n",
      "best mean dice: 0.7413 at epoch: 20\n",
      "time consuming of epoch 20 is: 302.3763\n",
      "----------\n",
      "epoch 21/300\n",
      "1/388, train_loss: 0.1413, step time: 0.4762\n",
      "2/388, train_loss: 0.3463, step time: 0.5075\n",
      "3/388, train_loss: 0.1711, step time: 0.9442\n",
      "4/388, train_loss: 0.1405, step time: 0.5662\n",
      "5/388, train_loss: 0.0455, step time: 0.5246\n",
      "6/388, train_loss: 0.1758, step time: 0.5019\n",
      "7/388, train_loss: 0.2188, step time: 0.4963\n",
      "8/388, train_loss: 0.1181, step time: 0.4898\n",
      "9/388, train_loss: 0.2332, step time: 0.6372\n",
      "10/388, train_loss: 0.3349, step time: 0.5385\n",
      "11/388, train_loss: 0.2203, step time: 0.5185\n",
      "12/388, train_loss: 0.0782, step time: 0.4974\n",
      "13/388, train_loss: 0.2972, step time: 0.5008\n",
      "14/388, train_loss: 0.2091, step time: 0.5666\n",
      "15/388, train_loss: 0.1594, step time: 0.5217\n",
      "16/388, train_loss: 0.1242, step time: 0.5680\n",
      "17/388, train_loss: 0.0866, step time: 0.5391\n",
      "18/388, train_loss: 0.4685, step time: 0.6512\n",
      "19/388, train_loss: 0.1273, step time: 0.5541\n",
      "20/388, train_loss: 0.2387, step time: 0.5198\n",
      "21/388, train_loss: 0.2239, step time: 0.5104\n",
      "22/388, train_loss: 0.3348, step time: 0.5040\n",
      "23/388, train_loss: 0.1571, step time: 0.5043\n",
      "24/388, train_loss: 0.1494, step time: 0.5085\n",
      "25/388, train_loss: 0.3432, step time: 0.5005\n",
      "26/388, train_loss: 0.3252, step time: 1.0775\n",
      "27/388, train_loss: 0.1486, step time: 0.5552\n",
      "28/388, train_loss: 0.2310, step time: 0.5254\n",
      "29/388, train_loss: 0.6378, step time: 0.5022\n",
      "30/388, train_loss: 0.4964, step time: 0.4962\n",
      "31/388, train_loss: 0.2763, step time: 0.5007\n",
      "32/388, train_loss: 0.0964, step time: 0.4984\n",
      "33/388, train_loss: 0.2902, step time: 0.4875\n",
      "34/388, train_loss: 0.1231, step time: 0.4900\n",
      "35/388, train_loss: 0.5460, step time: 0.4938\n",
      "36/388, train_loss: 0.4058, step time: 1.1854\n",
      "37/388, train_loss: 0.5457, step time: 0.5583\n",
      "38/388, train_loss: 0.1537, step time: 0.5146\n",
      "39/388, train_loss: 0.2175, step time: 0.5051\n",
      "40/388, train_loss: 0.2735, step time: 0.5090\n",
      "41/388, train_loss: 0.4559, step time: 0.5027\n",
      "42/388, train_loss: 0.4727, step time: 0.5446\n",
      "43/388, train_loss: 0.1242, step time: 0.5178\n",
      "44/388, train_loss: 0.2692, step time: 0.5049\n",
      "45/388, train_loss: 0.2279, step time: 0.4847\n",
      "46/388, train_loss: 0.1276, step time: 1.1688\n",
      "47/388, train_loss: 0.5056, step time: 0.5287\n",
      "48/388, train_loss: 0.2518, step time: 0.5031\n",
      "49/388, train_loss: 0.1229, step time: 0.4850\n",
      "50/388, train_loss: 0.2639, step time: 0.4934\n",
      "51/388, train_loss: 0.1394, step time: 0.5014\n",
      "52/388, train_loss: 0.1113, step time: 0.5015\n",
      "53/388, train_loss: 0.4459, step time: 0.8992\n",
      "54/388, train_loss: 0.2992, step time: 0.5293\n",
      "55/388, train_loss: 0.1654, step time: 0.5076\n",
      "56/388, train_loss: 0.2512, step time: 0.4782\n",
      "57/388, train_loss: 0.0982, step time: 0.4873\n",
      "58/388, train_loss: 0.3427, step time: 0.7515\n",
      "59/388, train_loss: 0.0880, step time: 0.5345\n",
      "60/388, train_loss: 0.0839, step time: 0.5110\n",
      "61/388, train_loss: 0.1100, step time: 0.4883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/388, train_loss: 0.3210, step time: 0.5055\n",
      "63/388, train_loss: 0.2272, step time: 0.4934\n",
      "64/388, train_loss: 0.1465, step time: 0.4961\n",
      "65/388, train_loss: 0.1040, step time: 0.4915\n",
      "66/388, train_loss: 0.1145, step time: 0.4777\n",
      "67/388, train_loss: 0.2588, step time: 0.8087\n",
      "68/388, train_loss: 0.1361, step time: 0.5471\n",
      "69/388, train_loss: 0.3241, step time: 0.5252\n",
      "70/388, train_loss: 0.1014, step time: 0.5065\n",
      "71/388, train_loss: 0.2901, step time: 0.4938\n",
      "72/388, train_loss: 0.2873, step time: 0.4940\n",
      "73/388, train_loss: 0.1401, step time: 0.5030\n",
      "74/388, train_loss: 0.1216, step time: 0.4945\n",
      "75/388, train_loss: 0.4999, step time: 0.4945\n",
      "76/388, train_loss: 0.2535, step time: 0.5187\n",
      "77/388, train_loss: 0.0516, step time: 0.5261\n",
      "78/388, train_loss: 0.1041, step time: 0.5243\n",
      "79/388, train_loss: 0.4684, step time: 0.5063\n",
      "80/388, train_loss: 0.2511, step time: 0.5128\n",
      "81/388, train_loss: 0.1449, step time: 0.4891\n",
      "82/388, train_loss: 0.1003, step time: 0.4968\n",
      "83/388, train_loss: 0.0915, step time: 0.4797\n",
      "84/388, train_loss: 0.2424, step time: 1.0353\n",
      "85/388, train_loss: 0.0917, step time: 0.5454\n",
      "86/388, train_loss: 0.0711, step time: 0.4981\n",
      "87/388, train_loss: 0.0348, step time: 0.4912\n",
      "88/388, train_loss: 0.2543, step time: 0.4792\n",
      "89/388, train_loss: 0.1598, step time: 0.5166\n",
      "90/388, train_loss: 0.2731, step time: 0.5175\n",
      "91/388, train_loss: 0.2623, step time: 0.5064\n",
      "92/388, train_loss: 0.3505, step time: 0.4912\n",
      "93/388, train_loss: 0.1075, step time: 0.4836\n",
      "94/388, train_loss: 0.5401, step time: 0.7250\n",
      "95/388, train_loss: 0.4141, step time: 0.5613\n",
      "96/388, train_loss: 0.1954, step time: 0.5270\n",
      "97/388, train_loss: 0.1109, step time: 0.5019\n",
      "98/388, train_loss: 0.3726, step time: 0.5078\n",
      "99/388, train_loss: 0.1470, step time: 0.4825\n",
      "100/388, train_loss: 0.1366, step time: 0.4957\n",
      "101/388, train_loss: 0.1007, step time: 0.4912\n",
      "102/388, train_loss: 0.1317, step time: 0.6771\n",
      "103/388, train_loss: 0.1265, step time: 0.5363\n",
      "104/388, train_loss: 0.0692, step time: 0.5227\n",
      "105/388, train_loss: 0.0906, step time: 0.4986\n",
      "106/388, train_loss: 0.3060, step time: 0.5004\n",
      "107/388, train_loss: 0.5601, step time: 0.5782\n",
      "108/388, train_loss: 0.2200, step time: 0.5920\n",
      "109/388, train_loss: 0.6465, step time: 0.5368\n",
      "110/388, train_loss: 0.0802, step time: 0.5015\n",
      "111/388, train_loss: 0.1372, step time: 0.5004\n",
      "112/388, train_loss: 0.1472, step time: 0.5317\n",
      "113/388, train_loss: 0.2277, step time: 0.5087\n",
      "114/388, train_loss: 0.2428, step time: 0.5256\n",
      "115/388, train_loss: 0.5295, step time: 0.5056\n",
      "116/388, train_loss: 0.1535, step time: 0.5585\n",
      "117/388, train_loss: 0.3492, step time: 0.5311\n",
      "118/388, train_loss: 0.1333, step time: 0.5081\n",
      "119/388, train_loss: 0.1134, step time: 0.5022\n",
      "120/388, train_loss: 0.2649, step time: 0.4991\n",
      "121/388, train_loss: 0.4809, step time: 0.4812\n",
      "122/388, train_loss: 0.0855, step time: 0.5247\n",
      "123/388, train_loss: 0.2129, step time: 0.4932\n",
      "124/388, train_loss: 0.4451, step time: 0.5136\n",
      "125/388, train_loss: 0.2162, step time: 0.5035\n",
      "126/388, train_loss: 0.1979, step time: 1.1163\n",
      "127/388, train_loss: 0.4827, step time: 0.5351\n",
      "128/388, train_loss: 0.1832, step time: 0.5044\n",
      "129/388, train_loss: 0.1340, step time: 0.5015\n",
      "130/388, train_loss: 0.1469, step time: 0.4880\n",
      "131/388, train_loss: 0.2436, step time: 0.4935\n",
      "132/388, train_loss: 0.1371, step time: 1.1538\n",
      "133/388, train_loss: 0.1817, step time: 0.5357\n",
      "134/388, train_loss: 0.1345, step time: 0.5060\n",
      "135/388, train_loss: 0.1744, step time: 0.4861\n",
      "136/388, train_loss: 0.2915, step time: 0.4973\n",
      "137/388, train_loss: 0.4164, step time: 0.5056\n",
      "138/388, train_loss: 0.4657, step time: 0.5094\n",
      "139/388, train_loss: 0.1813, step time: 0.4882\n",
      "140/388, train_loss: 0.1192, step time: 0.4966\n",
      "141/388, train_loss: 0.3744, step time: 0.4829\n",
      "142/388, train_loss: 0.1678, step time: 0.9356\n",
      "143/388, train_loss: 0.1241, step time: 0.5428\n",
      "144/388, train_loss: 0.4797, step time: 0.5189\n",
      "145/388, train_loss: 0.2173, step time: 0.4958\n",
      "146/388, train_loss: 0.2309, step time: 0.4948\n",
      "147/388, train_loss: 0.0523, step time: 0.4838\n",
      "148/388, train_loss: 0.1624, step time: 0.5124\n",
      "149/388, train_loss: 0.7355, step time: 0.4935\n",
      "150/388, train_loss: 0.1886, step time: 0.4963\n",
      "151/388, train_loss: 0.1509, step time: 1.1473\n",
      "152/388, train_loss: 0.1494, step time: 0.5320\n",
      "153/388, train_loss: 0.1112, step time: 0.5041\n",
      "154/388, train_loss: 0.1333, step time: 0.4872\n",
      "155/388, train_loss: 0.1717, step time: 0.4928\n",
      "156/388, train_loss: 0.3399, step time: 0.4820\n",
      "157/388, train_loss: 0.3441, step time: 0.4897\n",
      "158/388, train_loss: 0.0524, step time: 1.1021\n",
      "159/388, train_loss: 0.0906, step time: 0.5475\n",
      "160/388, train_loss: 0.5848, step time: 0.5302\n",
      "161/388, train_loss: 0.1549, step time: 0.4976\n",
      "162/388, train_loss: 0.1610, step time: 0.4962\n",
      "163/388, train_loss: 0.1328, step time: 0.4810\n",
      "164/388, train_loss: 0.1879, step time: 1.0393\n",
      "165/388, train_loss: 0.1584, step time: 0.5493\n",
      "166/388, train_loss: 0.3862, step time: 0.5155\n",
      "167/388, train_loss: 0.2607, step time: 0.4914\n",
      "168/388, train_loss: 0.1331, step time: 0.4929\n",
      "169/388, train_loss: 0.2460, step time: 0.4790\n",
      "170/388, train_loss: 0.1334, step time: 0.4809\n",
      "171/388, train_loss: 0.6407, step time: 1.1002\n",
      "172/388, train_loss: 0.2471, step time: 0.5333\n",
      "173/388, train_loss: 0.2052, step time: 0.5043\n",
      "174/388, train_loss: 0.1312, step time: 0.4970\n",
      "175/388, train_loss: 0.1160, step time: 0.5050\n",
      "176/388, train_loss: 0.1254, step time: 0.5392\n",
      "177/388, train_loss: 0.1120, step time: 0.5155\n",
      "178/388, train_loss: 0.2358, step time: 0.4968\n",
      "179/388, train_loss: 0.1761, step time: 0.4921\n",
      "180/388, train_loss: 0.1761, step time: 0.4898\n",
      "181/388, train_loss: 0.5055, step time: 0.4968\n",
      "182/388, train_loss: 0.0884, step time: 0.4967\n",
      "183/388, train_loss: 0.2503, step time: 0.4864\n",
      "184/388, train_loss: 0.1662, step time: 0.4896\n",
      "185/388, train_loss: 0.1916, step time: 1.0816\n",
      "186/388, train_loss: 0.2030, step time: 0.5349\n",
      "187/388, train_loss: 0.3172, step time: 0.5017\n",
      "188/388, train_loss: 0.2158, step time: 0.4939\n",
      "189/388, train_loss: 0.5340, step time: 0.4927\n",
      "190/388, train_loss: 0.2383, step time: 0.4969\n",
      "191/388, train_loss: 0.1339, step time: 0.4795\n",
      "192/388, train_loss: 0.2655, step time: 0.4801\n",
      "193/388, train_loss: 0.1251, step time: 1.1447\n",
      "194/388, train_loss: 0.2103, step time: 0.5416\n",
      "195/388, train_loss: 0.2669, step time: 0.5180\n",
      "196/388, train_loss: 0.1368, step time: 0.5085\n",
      "197/388, train_loss: 0.1696, step time: 0.5108\n",
      "198/388, train_loss: 0.1575, step time: 0.5005\n",
      "199/388, train_loss: 0.3543, step time: 0.4980\n",
      "200/388, train_loss: 0.1276, step time: 0.4811\n",
      "201/388, train_loss: 0.3205, step time: 0.5165\n",
      "202/388, train_loss: 0.2803, step time: 0.5086\n",
      "203/388, train_loss: 0.1715, step time: 0.5027\n",
      "204/388, train_loss: 0.0641, step time: 0.4911\n",
      "205/388, train_loss: 0.0521, step time: 0.4914\n",
      "206/388, train_loss: 0.1834, step time: 1.0579\n",
      "207/388, train_loss: 0.3051, step time: 0.5367\n",
      "208/388, train_loss: 0.2840, step time: 0.5144\n",
      "209/388, train_loss: 0.2255, step time: 0.4934\n",
      "210/388, train_loss: 0.2695, step time: 0.4892\n",
      "211/388, train_loss: 0.0704, step time: 0.5081\n",
      "212/388, train_loss: 0.3116, step time: 0.4975\n",
      "213/388, train_loss: 0.3016, step time: 0.4940\n",
      "214/388, train_loss: 0.4189, step time: 1.1951\n",
      "215/388, train_loss: 0.2267, step time: 0.5370\n",
      "216/388, train_loss: 0.4037, step time: 0.5108\n",
      "217/388, train_loss: 0.1318, step time: 0.4949\n",
      "218/388, train_loss: 0.1527, step time: 0.5280\n",
      "219/388, train_loss: 0.0727, step time: 0.5008\n",
      "220/388, train_loss: 0.6846, step time: 0.4932\n",
      "221/388, train_loss: 0.1378, step time: 0.4912\n",
      "222/388, train_loss: 0.1760, step time: 0.4853\n",
      "223/388, train_loss: 0.2297, step time: 0.5641\n",
      "224/388, train_loss: 0.1565, step time: 0.5334\n",
      "225/388, train_loss: 0.3441, step time: 0.5151\n",
      "226/388, train_loss: 0.1811, step time: 0.5547\n",
      "227/388, train_loss: 0.3236, step time: 0.5413\n",
      "228/388, train_loss: 0.1223, step time: 0.5152\n",
      "229/388, train_loss: 0.2690, step time: 0.5476\n",
      "230/388, train_loss: 0.1982, step time: 0.5259\n",
      "231/388, train_loss: 0.1632, step time: 0.6017\n",
      "232/388, train_loss: 0.1187, step time: 0.5131\n",
      "233/388, train_loss: 0.1113, step time: 0.4936\n",
      "234/388, train_loss: 0.1427, step time: 0.4849\n",
      "235/388, train_loss: 0.1507, step time: 1.2434\n",
      "236/388, train_loss: 0.3398, step time: 0.5256\n",
      "237/388, train_loss: 0.2455, step time: 0.4976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/388, train_loss: 0.1925, step time: 0.4979\n",
      "239/388, train_loss: 0.1725, step time: 0.4852\n",
      "240/388, train_loss: 0.1168, step time: 0.4780\n",
      "241/388, train_loss: 0.2356, step time: 1.1374\n",
      "242/388, train_loss: 0.1497, step time: 0.5373\n",
      "243/388, train_loss: 0.1380, step time: 0.5130\n",
      "244/388, train_loss: 0.2936, step time: 0.5022\n",
      "245/388, train_loss: 0.2166, step time: 0.4977\n",
      "246/388, train_loss: 0.2138, step time: 0.4873\n",
      "247/388, train_loss: 0.2229, step time: 0.4955\n",
      "248/388, train_loss: 0.1225, step time: 0.4886\n",
      "249/388, train_loss: 0.0908, step time: 0.4853\n",
      "250/388, train_loss: 0.1121, step time: 0.4928\n",
      "251/388, train_loss: 0.2534, step time: 1.0265\n",
      "252/388, train_loss: 0.1185, step time: 0.5372\n",
      "253/388, train_loss: 0.1500, step time: 0.5221\n",
      "254/388, train_loss: 0.0860, step time: 0.4901\n",
      "255/388, train_loss: 0.1620, step time: 0.4989\n",
      "256/388, train_loss: 0.1519, step time: 0.4908\n",
      "257/388, train_loss: 0.2304, step time: 0.5028\n",
      "258/388, train_loss: 0.1822, step time: 0.4808\n",
      "259/388, train_loss: 0.6124, step time: 1.0288\n",
      "260/388, train_loss: 0.1920, step time: 0.5617\n",
      "261/388, train_loss: 0.2069, step time: 0.5275\n",
      "262/388, train_loss: 0.4591, step time: 0.5176\n",
      "263/388, train_loss: 0.1097, step time: 0.5186\n",
      "264/388, train_loss: 0.1891, step time: 0.4982\n",
      "265/388, train_loss: 0.0622, step time: 0.4973\n",
      "266/388, train_loss: 0.2198, step time: 0.4801\n",
      "267/388, train_loss: 0.2800, step time: 1.1629\n",
      "268/388, train_loss: 0.2766, step time: 0.5403\n",
      "269/388, train_loss: 0.4178, step time: 0.5014\n",
      "270/388, train_loss: 0.0928, step time: 0.4878\n",
      "271/388, train_loss: 0.1478, step time: 0.4853\n",
      "272/388, train_loss: 0.0973, step time: 0.4992\n",
      "273/388, train_loss: 0.1151, step time: 0.4861\n",
      "274/388, train_loss: 0.2562, step time: 0.5386\n",
      "275/388, train_loss: 0.3279, step time: 0.6247\n",
      "276/388, train_loss: 0.2438, step time: 0.5470\n",
      "277/388, train_loss: 0.2434, step time: 0.5192\n",
      "278/388, train_loss: 0.5303, step time: 0.6188\n",
      "279/388, train_loss: 0.2212, step time: 0.5356\n",
      "280/388, train_loss: 0.3205, step time: 0.5071\n",
      "281/388, train_loss: 0.2237, step time: 0.4960\n",
      "282/388, train_loss: 0.6391, step time: 0.5563\n",
      "283/388, train_loss: 0.1154, step time: 0.5472\n",
      "284/388, train_loss: 0.0797, step time: 0.5224\n",
      "285/388, train_loss: 0.2156, step time: 0.5026\n",
      "286/388, train_loss: 0.1912, step time: 0.5907\n",
      "287/388, train_loss: 0.2505, step time: 0.5604\n",
      "288/388, train_loss: 0.2493, step time: 0.6507\n",
      "289/388, train_loss: 0.3636, step time: 0.5458\n",
      "290/388, train_loss: 0.0714, step time: 0.5207\n",
      "291/388, train_loss: 0.2046, step time: 0.5102\n",
      "292/388, train_loss: 0.4771, step time: 1.1455\n",
      "293/388, train_loss: 0.1572, step time: 0.5330\n",
      "294/388, train_loss: 0.2819, step time: 0.5061\n",
      "295/388, train_loss: 0.3724, step time: 0.5019\n",
      "296/388, train_loss: 0.1475, step time: 0.4854\n",
      "297/388, train_loss: 0.0496, step time: 0.4933\n",
      "298/388, train_loss: 0.1987, step time: 0.4987\n",
      "299/388, train_loss: 0.1701, step time: 0.5001\n",
      "300/388, train_loss: 0.1737, step time: 0.4954\n",
      "301/388, train_loss: 0.1326, step time: 0.4818\n",
      "302/388, train_loss: 0.1056, step time: 0.5399\n",
      "303/388, train_loss: 0.1661, step time: 0.4992\n",
      "304/388, train_loss: 0.4084, step time: 0.5458\n",
      "305/388, train_loss: 0.2620, step time: 0.5344\n",
      "306/388, train_loss: 0.2639, step time: 0.5094\n",
      "307/388, train_loss: 0.4757, step time: 0.4940\n",
      "308/388, train_loss: 0.0972, step time: 1.1156\n",
      "309/388, train_loss: 0.1961, step time: 0.5411\n",
      "310/388, train_loss: 0.0862, step time: 0.5057\n",
      "311/388, train_loss: 0.2466, step time: 0.4983\n",
      "312/388, train_loss: 0.2418, step time: 0.9170\n",
      "313/388, train_loss: 0.0777, step time: 0.5372\n",
      "314/388, train_loss: 0.1552, step time: 0.5219\n",
      "315/388, train_loss: 0.1290, step time: 0.4952\n",
      "316/388, train_loss: 0.2583, step time: 0.4998\n",
      "317/388, train_loss: 0.1742, step time: 0.4887\n",
      "318/388, train_loss: 0.2198, step time: 0.5068\n",
      "319/388, train_loss: 0.1960, step time: 0.4906\n",
      "320/388, train_loss: 0.0904, step time: 0.4911\n",
      "321/388, train_loss: 0.3127, step time: 0.5192\n",
      "322/388, train_loss: 0.4320, step time: 0.5159\n",
      "323/388, train_loss: 0.2229, step time: 0.4972\n",
      "324/388, train_loss: 0.1463, step time: 0.8157\n",
      "325/388, train_loss: 0.1004, step time: 0.5489\n",
      "326/388, train_loss: 0.0837, step time: 0.5188\n",
      "327/388, train_loss: 0.0649, step time: 0.5023\n",
      "328/388, train_loss: 0.2835, step time: 0.5087\n",
      "329/388, train_loss: 0.2060, step time: 0.4925\n",
      "330/388, train_loss: 0.2459, step time: 0.4959\n",
      "331/388, train_loss: 0.3108, step time: 0.4821\n",
      "332/388, train_loss: 0.2632, step time: 0.4952\n",
      "333/388, train_loss: 0.1632, step time: 0.5126\n",
      "334/388, train_loss: 0.2999, step time: 0.5201\n",
      "335/388, train_loss: 0.4283, step time: 0.5205\n",
      "336/388, train_loss: 0.3016, step time: 0.5146\n",
      "337/388, train_loss: 0.1093, step time: 0.5419\n",
      "338/388, train_loss: 0.1524, step time: 0.5103\n",
      "339/388, train_loss: 0.1429, step time: 0.5747\n",
      "340/388, train_loss: 0.3623, step time: 0.5319\n",
      "341/388, train_loss: 0.2316, step time: 0.5051\n",
      "342/388, train_loss: 0.2007, step time: 0.5008\n",
      "343/388, train_loss: 0.0863, step time: 0.5018\n",
      "344/388, train_loss: 0.1155, step time: 0.5140\n",
      "345/388, train_loss: 0.1076, step time: 0.4933\n",
      "346/388, train_loss: 0.2763, step time: 0.5046\n",
      "347/388, train_loss: 0.2724, step time: 0.5005\n",
      "348/388, train_loss: 0.2099, step time: 0.5203\n",
      "349/388, train_loss: 0.2906, step time: 0.5982\n",
      "350/388, train_loss: 0.0850, step time: 0.5757\n",
      "351/388, train_loss: 0.2733, step time: 0.5329\n",
      "352/388, train_loss: 0.0691, step time: 0.5123\n",
      "353/388, train_loss: 0.2861, step time: 0.4953\n",
      "354/388, train_loss: 0.2618, step time: 0.4964\n",
      "355/388, train_loss: 0.2404, step time: 0.5634\n",
      "356/388, train_loss: 0.4015, step time: 0.5025\n",
      "357/388, train_loss: 0.1291, step time: 0.5062\n",
      "358/388, train_loss: 0.1833, step time: 0.5052\n",
      "359/388, train_loss: 0.2465, step time: 0.4957\n",
      "360/388, train_loss: 0.2305, step time: 0.5017\n",
      "361/388, train_loss: 0.2083, step time: 0.5021\n",
      "362/388, train_loss: 0.1072, step time: 1.1071\n",
      "363/388, train_loss: 0.1475, step time: 0.5405\n",
      "364/388, train_loss: 0.0698, step time: 0.5102\n",
      "365/388, train_loss: 0.0884, step time: 0.4981\n",
      "366/388, train_loss: 0.2216, step time: 0.4842\n",
      "367/388, train_loss: 0.0962, step time: 0.4968\n",
      "368/388, train_loss: 0.2226, step time: 0.5153\n",
      "369/388, train_loss: 0.0619, step time: 0.5100\n",
      "370/388, train_loss: 0.2232, step time: 0.4889\n",
      "371/388, train_loss: 0.1647, step time: 0.4939\n",
      "372/388, train_loss: 0.4329, step time: 0.5152\n",
      "373/388, train_loss: 0.0534, step time: 0.4997\n",
      "374/388, train_loss: 0.2295, step time: 0.4999\n",
      "375/388, train_loss: 0.2397, step time: 1.1726\n",
      "376/388, train_loss: 0.0947, step time: 0.5437\n",
      "377/388, train_loss: 0.0994, step time: 0.5225\n",
      "378/388, train_loss: 0.2125, step time: 0.4930\n",
      "379/388, train_loss: 0.1037, step time: 0.5055\n",
      "380/388, train_loss: 0.2834, step time: 0.5097\n",
      "381/388, train_loss: 0.0713, step time: 0.5125\n",
      "382/388, train_loss: 0.5694, step time: 0.4993\n",
      "383/388, train_loss: 0.3975, step time: 0.5115\n",
      "384/388, train_loss: 0.2876, step time: 0.5415\n",
      "385/388, train_loss: 0.1889, step time: 0.6817\n",
      "386/388, train_loss: 0.3644, step time: 0.5739\n",
      "387/388, train_loss: 0.4422, step time: 0.5241\n",
      "388/388, train_loss: 0.2717, step time: 0.5420\n",
      "epoch 21 average loss: 0.2251\n",
      "saved new best metric model\n",
      "current epoch: 21 current mean dice: 0.7485 tc: 0.7986 wt: 0.8894 et: 0.5576\n",
      "best mean dice: 0.7485 at epoch: 21\n",
      "time consuming of epoch 21 is: 304.3843\n",
      "----------\n",
      "epoch 22/300\n",
      "1/388, train_loss: 0.1823, step time: 0.4795\n",
      "2/388, train_loss: 0.1257, step time: 0.4936\n",
      "3/388, train_loss: 0.3461, step time: 1.1286\n",
      "4/388, train_loss: 0.1688, step time: 0.5521\n",
      "5/388, train_loss: 0.2107, step time: 0.5225\n",
      "6/388, train_loss: 0.1847, step time: 0.5123\n",
      "7/388, train_loss: 0.1901, step time: 0.5335\n",
      "8/388, train_loss: 0.2059, step time: 0.5154\n",
      "9/388, train_loss: 0.4832, step time: 0.5025\n",
      "10/388, train_loss: 0.4267, step time: 1.1264\n",
      "11/388, train_loss: 0.3761, step time: 0.5438\n",
      "12/388, train_loss: 0.1091, step time: 0.5150\n",
      "13/388, train_loss: 0.1056, step time: 0.4959\n",
      "14/388, train_loss: 0.2814, step time: 0.4907\n",
      "15/388, train_loss: 0.3516, step time: 0.5478\n",
      "16/388, train_loss: 0.5544, step time: 0.5188\n",
      "17/388, train_loss: 0.1295, step time: 0.4932\n",
      "18/388, train_loss: 0.1090, step time: 0.5095\n",
      "19/388, train_loss: 0.1357, step time: 0.5540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/388, train_loss: 0.1705, step time: 0.5447\n",
      "21/388, train_loss: 0.4653, step time: 0.5158\n",
      "22/388, train_loss: 0.0763, step time: 0.4987\n",
      "23/388, train_loss: 0.2553, step time: 0.4986\n",
      "24/388, train_loss: 0.1744, step time: 0.4945\n",
      "25/388, train_loss: 0.1726, step time: 0.4795\n",
      "26/388, train_loss: 0.2024, step time: 0.5008\n",
      "27/388, train_loss: 0.3592, step time: 0.4888\n",
      "28/388, train_loss: 0.1677, step time: 1.1250\n",
      "29/388, train_loss: 0.0991, step time: 0.5388\n",
      "30/388, train_loss: 0.0812, step time: 0.5066\n",
      "31/388, train_loss: 0.0989, step time: 0.5031\n",
      "32/388, train_loss: 0.4714, step time: 0.4920\n",
      "33/388, train_loss: 0.1930, step time: 0.4924\n",
      "34/388, train_loss: 0.0870, step time: 0.5148\n",
      "35/388, train_loss: 0.6143, step time: 0.4975\n",
      "36/388, train_loss: 0.2046, step time: 0.4837\n",
      "37/388, train_loss: 0.2329, step time: 0.7438\n",
      "38/388, train_loss: 0.6308, step time: 0.5476\n",
      "39/388, train_loss: 0.1476, step time: 0.5158\n",
      "40/388, train_loss: 0.0747, step time: 0.4971\n",
      "41/388, train_loss: 0.4823, step time: 0.4912\n",
      "42/388, train_loss: 0.0742, step time: 0.6311\n",
      "43/388, train_loss: 0.1890, step time: 0.5376\n",
      "44/388, train_loss: 0.2244, step time: 0.5035\n",
      "45/388, train_loss: 0.3969, step time: 0.5043\n",
      "46/388, train_loss: 0.1870, step time: 0.4870\n",
      "47/388, train_loss: 0.1940, step time: 0.4808\n",
      "48/388, train_loss: 0.3935, step time: 0.4911\n",
      "49/388, train_loss: 0.3429, step time: 0.5277\n",
      "50/388, train_loss: 0.1383, step time: 0.5114\n",
      "51/388, train_loss: 0.2448, step time: 0.4951\n",
      "52/388, train_loss: 0.1068, step time: 0.4989\n",
      "53/388, train_loss: 0.1304, step time: 0.4814\n",
      "54/388, train_loss: 0.1875, step time: 0.4800\n",
      "55/388, train_loss: 0.1722, step time: 0.5632\n",
      "56/388, train_loss: 0.2849, step time: 0.5499\n",
      "57/388, train_loss: 0.2729, step time: 0.5269\n",
      "58/388, train_loss: 0.2680, step time: 0.5050\n",
      "59/388, train_loss: 0.3273, step time: 0.4924\n",
      "60/388, train_loss: 0.2131, step time: 0.5172\n",
      "61/388, train_loss: 0.0583, step time: 0.5802\n",
      "62/388, train_loss: 0.2084, step time: 0.5161\n",
      "63/388, train_loss: 0.1714, step time: 0.4852\n",
      "64/388, train_loss: 0.1261, step time: 0.4922\n",
      "65/388, train_loss: 0.1341, step time: 0.5093\n",
      "66/388, train_loss: 0.4153, step time: 0.4959\n",
      "67/388, train_loss: 0.1290, step time: 0.4939\n",
      "68/388, train_loss: 0.3119, step time: 0.4870\n",
      "69/388, train_loss: 0.2208, step time: 0.4907\n",
      "70/388, train_loss: 0.3761, step time: 0.4862\n",
      "71/388, train_loss: 0.1792, step time: 0.4979\n",
      "72/388, train_loss: 0.2168, step time: 0.4943\n",
      "73/388, train_loss: 0.5676, step time: 0.5432\n",
      "74/388, train_loss: 0.3454, step time: 0.5973\n",
      "75/388, train_loss: 0.1950, step time: 0.5527\n",
      "76/388, train_loss: 0.2124, step time: 0.5348\n",
      "77/388, train_loss: 0.1228, step time: 0.5204\n",
      "78/388, train_loss: 0.2822, step time: 0.5602\n",
      "79/388, train_loss: 0.1684, step time: 0.5319\n",
      "80/388, train_loss: 0.3586, step time: 0.5163\n",
      "81/388, train_loss: 0.1723, step time: 0.5010\n",
      "82/388, train_loss: 0.2316, step time: 0.4941\n",
      "83/388, train_loss: 0.5444, step time: 0.4892\n",
      "84/388, train_loss: 0.1446, step time: 0.4941\n",
      "85/388, train_loss: 0.5344, step time: 0.4867\n",
      "86/388, train_loss: 0.2228, step time: 0.5365\n",
      "87/388, train_loss: 0.3313, step time: 0.5472\n",
      "88/388, train_loss: 0.1377, step time: 0.5356\n",
      "89/388, train_loss: 0.1090, step time: 0.5187\n",
      "90/388, train_loss: 0.1148, step time: 0.4991\n",
      "91/388, train_loss: 0.2436, step time: 0.4859\n",
      "92/388, train_loss: 0.1342, step time: 0.9612\n",
      "93/388, train_loss: 0.4159, step time: 0.5283\n",
      "94/388, train_loss: 0.6014, step time: 0.5112\n",
      "95/388, train_loss: 0.0976, step time: 0.4888\n",
      "96/388, train_loss: 0.1740, step time: 0.4910\n",
      "97/388, train_loss: 0.2352, step time: 0.4958\n",
      "98/388, train_loss: 0.0926, step time: 0.4799\n",
      "99/388, train_loss: 0.1444, step time: 0.4880\n",
      "100/388, train_loss: 0.1469, step time: 0.4734\n",
      "101/388, train_loss: 0.0963, step time: 0.5069\n",
      "102/388, train_loss: 0.0773, step time: 0.4976\n",
      "103/388, train_loss: 0.3588, step time: 0.4977\n",
      "104/388, train_loss: 0.1518, step time: 0.8517\n",
      "105/388, train_loss: 0.1261, step time: 0.5582\n",
      "106/388, train_loss: 0.2562, step time: 0.5083\n",
      "107/388, train_loss: 0.2233, step time: 0.4965\n",
      "108/388, train_loss: 0.3752, step time: 0.4921\n",
      "109/388, train_loss: 0.3428, step time: 0.5054\n",
      "110/388, train_loss: 0.1182, step time: 0.5138\n",
      "111/388, train_loss: 0.2114, step time: 0.5108\n",
      "112/388, train_loss: 0.1385, step time: 0.4976\n",
      "113/388, train_loss: 0.1598, step time: 0.4821\n",
      "114/388, train_loss: 0.0786, step time: 0.4775\n",
      "115/388, train_loss: 0.1337, step time: 0.5327\n",
      "116/388, train_loss: 0.2918, step time: 0.5259\n",
      "117/388, train_loss: 0.2540, step time: 0.5205\n",
      "118/388, train_loss: 0.1370, step time: 0.5050\n",
      "119/388, train_loss: 0.1848, step time: 0.5019\n",
      "120/388, train_loss: 0.0810, step time: 0.4931\n",
      "121/388, train_loss: 0.2206, step time: 0.4963\n",
      "122/388, train_loss: 0.2292, step time: 0.4941\n",
      "123/388, train_loss: 0.3542, step time: 0.4898\n",
      "124/388, train_loss: 0.0993, step time: 0.4845\n",
      "125/388, train_loss: 0.1522, step time: 0.4961\n",
      "126/388, train_loss: 0.1965, step time: 0.5589\n",
      "127/388, train_loss: 0.2428, step time: 0.5187\n",
      "128/388, train_loss: 0.3002, step time: 0.4907\n",
      "129/388, train_loss: 0.1769, step time: 0.9578\n",
      "130/388, train_loss: 0.1200, step time: 0.5859\n",
      "131/388, train_loss: 0.2210, step time: 0.5191\n",
      "132/388, train_loss: 0.5380, step time: 0.4963\n",
      "133/388, train_loss: 0.1064, step time: 0.4949\n",
      "134/388, train_loss: 0.1627, step time: 1.2144\n",
      "135/388, train_loss: 0.2736, step time: 0.5280\n",
      "136/388, train_loss: 0.1993, step time: 0.5072\n",
      "137/388, train_loss: 0.3637, step time: 0.4988\n",
      "138/388, train_loss: 0.2513, step time: 0.4804\n",
      "139/388, train_loss: 0.2688, step time: 0.4810\n",
      "140/388, train_loss: 0.0806, step time: 0.4792\n",
      "141/388, train_loss: 0.1245, step time: 0.4730\n",
      "142/388, train_loss: 0.3538, step time: 0.9823\n",
      "143/388, train_loss: 0.1400, step time: 0.5345\n",
      "144/388, train_loss: 0.3237, step time: 0.5161\n",
      "145/388, train_loss: 0.1324, step time: 0.5066\n",
      "146/388, train_loss: 0.2928, step time: 0.4845\n",
      "147/388, train_loss: 0.1319, step time: 0.4783\n",
      "148/388, train_loss: 0.1937, step time: 0.4821\n",
      "149/388, train_loss: 0.1337, step time: 0.4870\n",
      "150/388, train_loss: 0.0715, step time: 0.5323\n",
      "151/388, train_loss: 0.4276, step time: 0.5130\n",
      "152/388, train_loss: 0.2765, step time: 0.5101\n",
      "153/388, train_loss: 0.0998, step time: 0.4975\n",
      "154/388, train_loss: 0.3472, step time: 1.0143\n",
      "155/388, train_loss: 0.3377, step time: 0.5454\n",
      "156/388, train_loss: 0.3085, step time: 0.5103\n",
      "157/388, train_loss: 0.1973, step time: 0.5027\n",
      "158/388, train_loss: 0.3044, step time: 0.4852\n",
      "159/388, train_loss: 0.2018, step time: 0.5065\n",
      "160/388, train_loss: 0.1694, step time: 0.5000\n",
      "161/388, train_loss: 0.1493, step time: 0.4825\n",
      "162/388, train_loss: 0.7697, step time: 1.1722\n",
      "163/388, train_loss: 0.3070, step time: 0.5288\n",
      "164/388, train_loss: 0.5843, step time: 0.5071\n",
      "165/388, train_loss: 0.0831, step time: 0.4869\n",
      "166/388, train_loss: 0.1808, step time: 0.4959\n",
      "167/388, train_loss: 0.3290, step time: 0.4836\n",
      "168/388, train_loss: 0.0759, step time: 0.5080\n",
      "169/388, train_loss: 0.1689, step time: 0.5015\n",
      "170/388, train_loss: 0.1189, step time: 0.4881\n",
      "171/388, train_loss: 0.1971, step time: 0.4871\n",
      "172/388, train_loss: 0.1705, step time: 1.1180\n",
      "173/388, train_loss: 0.1130, step time: 0.5404\n",
      "174/388, train_loss: 0.2044, step time: 0.5051\n",
      "175/388, train_loss: 0.2106, step time: 0.4898\n",
      "176/388, train_loss: 0.3386, step time: 0.4971\n",
      "177/388, train_loss: 0.1145, step time: 0.4823\n",
      "178/388, train_loss: 0.1543, step time: 0.4959\n",
      "179/388, train_loss: 0.0704, step time: 0.4826\n",
      "180/388, train_loss: 0.2203, step time: 1.0968\n",
      "181/388, train_loss: 0.0802, step time: 0.5272\n",
      "182/388, train_loss: 0.3104, step time: 0.5019\n",
      "183/388, train_loss: 0.2891, step time: 0.4855\n",
      "184/388, train_loss: 0.0762, step time: 0.4894\n",
      "185/388, train_loss: 0.1933, step time: 0.4936\n",
      "186/388, train_loss: 0.0739, step time: 0.5037\n",
      "187/388, train_loss: 0.0589, step time: 0.5026\n",
      "188/388, train_loss: 0.2388, step time: 0.4831\n",
      "189/388, train_loss: 0.3182, step time: 0.5001\n",
      "190/388, train_loss: 0.0787, step time: 0.4851\n",
      "191/388, train_loss: 0.1055, step time: 0.5362\n",
      "192/388, train_loss: 0.1503, step time: 0.5192\n",
      "193/388, train_loss: 0.2919, step time: 0.4998\n",
      "194/388, train_loss: 0.1207, step time: 0.5127\n",
      "195/388, train_loss: 0.2079, step time: 0.5307\n",
      "196/388, train_loss: 0.1951, step time: 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/388, train_loss: 0.5702, step time: 0.5080\n",
      "198/388, train_loss: 0.1004, step time: 0.5396\n",
      "199/388, train_loss: 0.2438, step time: 0.5324\n",
      "200/388, train_loss: 0.1109, step time: 0.5014\n",
      "201/388, train_loss: 0.2263, step time: 0.4971\n",
      "202/388, train_loss: 0.2464, step time: 0.4817\n",
      "203/388, train_loss: 0.0728, step time: 1.2154\n",
      "204/388, train_loss: 0.5734, step time: 0.5413\n",
      "205/388, train_loss: 0.1164, step time: 0.4997\n",
      "206/388, train_loss: 0.2886, step time: 0.4948\n",
      "207/388, train_loss: 0.3044, step time: 0.4923\n",
      "208/388, train_loss: 0.3099, step time: 0.4854\n",
      "209/388, train_loss: 0.3104, step time: 1.1214\n",
      "210/388, train_loss: 0.1970, step time: 0.5311\n",
      "211/388, train_loss: 0.2967, step time: 0.5010\n",
      "212/388, train_loss: 0.3836, step time: 0.4937\n",
      "213/388, train_loss: 0.1430, step time: 0.4922\n",
      "214/388, train_loss: 0.1521, step time: 0.4940\n",
      "215/388, train_loss: 0.5819, step time: 0.4760\n",
      "216/388, train_loss: 0.1494, step time: 0.4776\n",
      "217/388, train_loss: 0.1058, step time: 0.4827\n",
      "218/388, train_loss: 0.1720, step time: 0.4739\n",
      "219/388, train_loss: 0.2232, step time: 0.7307\n",
      "220/388, train_loss: 0.1030, step time: 0.5429\n",
      "221/388, train_loss: 0.0923, step time: 0.5112\n",
      "222/388, train_loss: 0.2455, step time: 0.5131\n",
      "223/388, train_loss: 0.1405, step time: 0.4939\n",
      "224/388, train_loss: 0.2071, step time: 0.4899\n",
      "225/388, train_loss: 0.2242, step time: 1.0959\n",
      "226/388, train_loss: 0.3626, step time: 0.5348\n",
      "227/388, train_loss: 0.2208, step time: 0.5158\n",
      "228/388, train_loss: 0.0850, step time: 0.4911\n",
      "229/388, train_loss: 0.3317, step time: 0.4983\n",
      "230/388, train_loss: 0.0781, step time: 0.4736\n",
      "231/388, train_loss: 0.0549, step time: 0.4860\n",
      "232/388, train_loss: 0.1248, step time: 0.4915\n",
      "233/388, train_loss: 0.0730, step time: 0.4808\n",
      "234/388, train_loss: 0.2261, step time: 0.5348\n",
      "235/388, train_loss: 0.2126, step time: 0.5047\n",
      "236/388, train_loss: 0.2393, step time: 0.4865\n",
      "237/388, train_loss: 0.2257, step time: 1.0995\n",
      "238/388, train_loss: 0.0987, step time: 0.5341\n",
      "239/388, train_loss: 0.2292, step time: 0.5066\n",
      "240/388, train_loss: 0.1412, step time: 0.4917\n",
      "241/388, train_loss: 0.3191, step time: 0.4909\n",
      "242/388, train_loss: 0.1520, step time: 0.4788\n",
      "243/388, train_loss: 0.1995, step time: 0.4811\n",
      "244/388, train_loss: 0.2868, step time: 0.4827\n",
      "245/388, train_loss: 0.4294, step time: 0.4934\n",
      "246/388, train_loss: 0.1711, step time: 0.5164\n",
      "247/388, train_loss: 0.2097, step time: 0.5182\n",
      "248/388, train_loss: 0.1994, step time: 0.4959\n",
      "249/388, train_loss: 0.1978, step time: 0.4875\n",
      "250/388, train_loss: 0.2240, step time: 0.5222\n",
      "251/388, train_loss: 0.1631, step time: 0.5032\n",
      "252/388, train_loss: 0.3061, step time: 0.4991\n",
      "253/388, train_loss: 0.1569, step time: 0.4811\n",
      "254/388, train_loss: 0.2145, step time: 0.4788\n",
      "255/388, train_loss: 0.1293, step time: 1.2402\n",
      "256/388, train_loss: 0.2770, step time: 0.5378\n",
      "257/388, train_loss: 0.0693, step time: 0.5068\n",
      "258/388, train_loss: 0.2856, step time: 0.5080\n",
      "259/388, train_loss: 0.1351, step time: 0.4894\n",
      "260/388, train_loss: 0.3092, step time: 1.1447\n",
      "261/388, train_loss: 0.1182, step time: 0.5319\n",
      "262/388, train_loss: 0.1066, step time: 0.4966\n",
      "263/388, train_loss: 0.0894, step time: 0.4887\n",
      "264/388, train_loss: 0.3404, step time: 0.4955\n",
      "265/388, train_loss: 0.3332, step time: 0.5143\n",
      "266/388, train_loss: 0.0608, step time: 0.4965\n",
      "267/388, train_loss: 0.1314, step time: 0.4826\n",
      "268/388, train_loss: 0.0852, step time: 0.4869\n",
      "269/388, train_loss: 0.3251, step time: 1.0158\n",
      "270/388, train_loss: 0.1613, step time: 0.5428\n",
      "271/388, train_loss: 0.2665, step time: 0.5076\n",
      "272/388, train_loss: 0.3705, step time: 0.5036\n",
      "273/388, train_loss: 0.2149, step time: 0.4961\n",
      "274/388, train_loss: 0.0441, step time: 0.4818\n",
      "275/388, train_loss: 0.3445, step time: 0.4841\n",
      "276/388, train_loss: 0.1320, step time: 1.0420\n",
      "277/388, train_loss: 0.1758, step time: 0.5396\n",
      "278/388, train_loss: 0.5830, step time: 0.5076\n",
      "279/388, train_loss: 0.1345, step time: 0.4952\n",
      "280/388, train_loss: 0.2149, step time: 0.4827\n",
      "281/388, train_loss: 0.2273, step time: 0.5087\n",
      "282/388, train_loss: 0.3488, step time: 0.5195\n",
      "283/388, train_loss: 0.2281, step time: 0.5709\n",
      "284/388, train_loss: 0.1474, step time: 0.5372\n",
      "285/388, train_loss: 0.3892, step time: 0.5152\n",
      "286/388, train_loss: 0.1413, step time: 0.4998\n",
      "287/388, train_loss: 0.2659, step time: 0.4827\n",
      "288/388, train_loss: 0.3761, step time: 0.4910\n",
      "289/388, train_loss: 0.1311, step time: 0.4807\n",
      "290/388, train_loss: 0.0932, step time: 0.5174\n",
      "291/388, train_loss: 0.0892, step time: 0.4997\n",
      "292/388, train_loss: 0.4194, step time: 0.4960\n",
      "293/388, train_loss: 0.2108, step time: 0.5087\n",
      "294/388, train_loss: 0.3096, step time: 0.5031\n",
      "295/388, train_loss: 0.1916, step time: 0.4898\n",
      "296/388, train_loss: 0.0995, step time: 0.4908\n",
      "297/388, train_loss: 0.5825, step time: 0.9490\n",
      "298/388, train_loss: 0.3968, step time: 0.5448\n",
      "299/388, train_loss: 0.0812, step time: 0.5062\n",
      "300/388, train_loss: 0.1513, step time: 0.4818\n",
      "301/388, train_loss: 0.1380, step time: 0.5146\n",
      "302/388, train_loss: 0.4007, step time: 0.4936\n",
      "303/388, train_loss: 0.1590, step time: 0.4918\n",
      "304/388, train_loss: 0.3839, step time: 1.0841\n",
      "305/388, train_loss: 0.1658, step time: 0.5344\n",
      "306/388, train_loss: 0.1590, step time: 0.5132\n",
      "307/388, train_loss: 0.7288, step time: 0.5234\n",
      "308/388, train_loss: 0.5678, step time: 0.5200\n",
      "309/388, train_loss: 0.2428, step time: 0.5079\n",
      "310/388, train_loss: 0.1147, step time: 0.4966\n",
      "311/388, train_loss: 0.1164, step time: 0.4866\n",
      "312/388, train_loss: 0.3215, step time: 0.5354\n",
      "313/388, train_loss: 0.2402, step time: 0.5001\n",
      "314/388, train_loss: 0.6737, step time: 0.4935\n",
      "315/388, train_loss: 0.2565, step time: 0.4931\n",
      "316/388, train_loss: 0.4371, step time: 0.4817\n",
      "317/388, train_loss: 0.1361, step time: 0.6489\n",
      "318/388, train_loss: 0.1593, step time: 0.5369\n",
      "319/388, train_loss: 0.0419, step time: 0.5020\n",
      "320/388, train_loss: 0.2145, step time: 0.5723\n",
      "321/388, train_loss: 0.7013, step time: 0.5649\n",
      "322/388, train_loss: 0.2275, step time: 0.5267\n",
      "323/388, train_loss: 0.1426, step time: 0.5112\n",
      "324/388, train_loss: 0.3924, step time: 0.5144\n",
      "325/388, train_loss: 0.1471, step time: 0.5076\n",
      "326/388, train_loss: 0.0950, step time: 0.4971\n",
      "327/388, train_loss: 0.3367, step time: 0.4998\n",
      "328/388, train_loss: 0.1825, step time: 0.4962\n",
      "329/388, train_loss: 0.8085, step time: 0.5296\n",
      "330/388, train_loss: 0.2324, step time: 0.5105\n",
      "331/388, train_loss: 0.4177, step time: 0.5062\n",
      "332/388, train_loss: 0.2111, step time: 0.4909\n",
      "333/388, train_loss: 0.2408, step time: 1.1296\n",
      "334/388, train_loss: 0.2813, step time: 0.5640\n",
      "335/388, train_loss: 0.2668, step time: 0.5250\n",
      "336/388, train_loss: 0.4081, step time: 0.5054\n",
      "337/388, train_loss: 0.1360, step time: 0.5322\n",
      "338/388, train_loss: 0.3003, step time: 0.5143\n",
      "339/388, train_loss: 0.1833, step time: 0.5022\n",
      "340/388, train_loss: 0.1364, step time: 0.5489\n",
      "341/388, train_loss: 0.1303, step time: 0.5031\n",
      "342/388, train_loss: 0.1947, step time: 0.4940\n",
      "343/388, train_loss: 0.1493, step time: 0.4919\n",
      "344/388, train_loss: 0.1684, step time: 0.5224\n",
      "345/388, train_loss: 0.2369, step time: 0.5043\n",
      "346/388, train_loss: 0.1009, step time: 0.4985\n",
      "347/388, train_loss: 0.2790, step time: 0.4948\n",
      "348/388, train_loss: 0.2057, step time: 0.4998\n",
      "349/388, train_loss: 0.0495, step time: 0.5633\n",
      "350/388, train_loss: 0.0921, step time: 0.5299\n",
      "351/388, train_loss: 0.4270, step time: 0.4965\n",
      "352/388, train_loss: 0.0960, step time: 0.5198\n",
      "353/388, train_loss: 0.4808, step time: 0.5074\n",
      "354/388, train_loss: 0.2713, step time: 0.4989\n",
      "355/388, train_loss: 0.2148, step time: 0.5192\n",
      "356/388, train_loss: 0.1316, step time: 0.5307\n",
      "357/388, train_loss: 0.4721, step time: 0.5366\n",
      "358/388, train_loss: 0.3484, step time: 0.5798\n",
      "359/388, train_loss: 0.0991, step time: 0.5320\n",
      "360/388, train_loss: 0.4069, step time: 0.4997\n",
      "361/388, train_loss: 0.0546, step time: 0.4972\n",
      "362/388, train_loss: 0.1304, step time: 0.5173\n",
      "363/388, train_loss: 0.3541, step time: 0.5029\n",
      "364/388, train_loss: 0.3833, step time: 0.5584\n",
      "365/388, train_loss: 0.3558, step time: 0.5162\n",
      "366/388, train_loss: 0.2403, step time: 0.4950\n",
      "367/388, train_loss: 0.3339, step time: 0.5047\n",
      "368/388, train_loss: 0.0976, step time: 0.5606\n",
      "369/388, train_loss: 0.1841, step time: 0.5451\n",
      "370/388, train_loss: 0.0912, step time: 0.5097\n",
      "371/388, train_loss: 0.3757, step time: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/388, train_loss: 0.1307, step time: 0.5105\n",
      "373/388, train_loss: 0.2575, step time: 0.5315\n",
      "374/388, train_loss: 0.1194, step time: 0.5152\n",
      "375/388, train_loss: 0.2728, step time: 0.4924\n",
      "376/388, train_loss: 0.4918, step time: 0.5045\n",
      "377/388, train_loss: 0.1984, step time: 1.0904\n",
      "378/388, train_loss: 0.2961, step time: 0.5387\n",
      "379/388, train_loss: 0.2113, step time: 0.5146\n",
      "380/388, train_loss: 0.1214, step time: 0.4909\n",
      "381/388, train_loss: 0.1188, step time: 0.5425\n",
      "382/388, train_loss: 0.1974, step time: 0.5241\n",
      "383/388, train_loss: 0.1152, step time: 0.6105\n",
      "384/388, train_loss: 0.3984, step time: 0.5299\n",
      "385/388, train_loss: 0.1287, step time: 0.5137\n",
      "386/388, train_loss: 0.2742, step time: 0.5002\n",
      "387/388, train_loss: 0.1302, step time: 0.5240\n",
      "388/388, train_loss: 0.1453, step time: 0.5118\n",
      "epoch 22 average loss: 0.2314\n",
      "current epoch: 22 current mean dice: 0.7260 tc: 0.7590 wt: 0.8851 et: 0.5338\n",
      "best mean dice: 0.7485 at epoch: 21\n",
      "time consuming of epoch 22 is: 300.9668\n",
      "----------\n",
      "epoch 23/300\n",
      "1/388, train_loss: 0.0876, step time: 0.4661\n",
      "2/388, train_loss: 0.1954, step time: 0.4796\n",
      "3/388, train_loss: 0.4922, step time: 0.9314\n",
      "4/388, train_loss: 0.3717, step time: 0.5838\n",
      "5/388, train_loss: 0.1881, step time: 0.5325\n",
      "6/388, train_loss: 0.0803, step time: 0.5087\n",
      "7/388, train_loss: 0.4463, step time: 0.5088\n",
      "8/388, train_loss: 0.1268, step time: 0.5247\n",
      "9/388, train_loss: 0.1180, step time: 0.5132\n",
      "10/388, train_loss: 0.1512, step time: 0.5364\n",
      "11/388, train_loss: 0.1184, step time: 0.5179\n",
      "12/388, train_loss: 0.0941, step time: 0.5526\n",
      "13/388, train_loss: 0.3424, step time: 0.5443\n",
      "14/388, train_loss: 0.1153, step time: 0.4919\n",
      "15/388, train_loss: 0.2743, step time: 0.5268\n",
      "16/388, train_loss: 0.2029, step time: 0.5076\n",
      "17/388, train_loss: 0.1244, step time: 1.0844\n",
      "18/388, train_loss: 0.1277, step time: 0.5535\n",
      "19/388, train_loss: 0.5502, step time: 0.5200\n",
      "20/388, train_loss: 0.2498, step time: 0.4918\n",
      "21/388, train_loss: 0.2042, step time: 1.1149\n",
      "22/388, train_loss: 0.0978, step time: 0.5595\n",
      "23/388, train_loss: 0.2016, step time: 0.5204\n",
      "24/388, train_loss: 0.1439, step time: 0.5031\n",
      "25/388, train_loss: 0.4071, step time: 0.4859\n",
      "26/388, train_loss: 0.2697, step time: 0.4974\n",
      "27/388, train_loss: 0.2994, step time: 0.5013\n",
      "28/388, train_loss: 0.1801, step time: 1.0888\n",
      "29/388, train_loss: 0.2785, step time: 0.5268\n",
      "30/388, train_loss: 0.2418, step time: 0.5126\n",
      "31/388, train_loss: 0.1875, step time: 0.4885\n",
      "32/388, train_loss: 0.1598, step time: 0.7248\n",
      "33/388, train_loss: 0.1965, step time: 0.5495\n",
      "34/388, train_loss: 0.0781, step time: 0.5278\n",
      "35/388, train_loss: 0.1642, step time: 0.5079\n",
      "36/388, train_loss: 0.6271, step time: 0.4920\n",
      "37/388, train_loss: 0.1145, step time: 0.5012\n",
      "38/388, train_loss: 0.1577, step time: 0.5439\n",
      "39/388, train_loss: 0.2737, step time: 0.5165\n",
      "40/388, train_loss: 0.2298, step time: 0.4945\n",
      "41/388, train_loss: 0.3271, step time: 1.1386\n",
      "42/388, train_loss: 0.1941, step time: 0.5365\n",
      "43/388, train_loss: 0.3583, step time: 0.5093\n",
      "44/388, train_loss: 0.4008, step time: 0.4848\n",
      "45/388, train_loss: 0.2678, step time: 0.4875\n",
      "46/388, train_loss: 0.5840, step time: 0.4956\n",
      "47/388, train_loss: 0.1709, step time: 0.4829\n",
      "48/388, train_loss: 0.1484, step time: 1.0141\n",
      "49/388, train_loss: 0.2239, step time: 0.5593\n",
      "50/388, train_loss: 0.1415, step time: 0.5241\n",
      "51/388, train_loss: 0.3432, step time: 0.4984\n",
      "52/388, train_loss: 0.1144, step time: 0.4953\n",
      "53/388, train_loss: 0.1450, step time: 0.4797\n",
      "54/388, train_loss: 0.0718, step time: 0.4870\n",
      "55/388, train_loss: 0.2627, step time: 0.4812\n",
      "56/388, train_loss: 0.1519, step time: 0.4899\n",
      "57/388, train_loss: 0.2065, step time: 0.9020\n",
      "58/388, train_loss: 0.1205, step time: 0.5505\n",
      "59/388, train_loss: 0.1040, step time: 0.5073\n",
      "60/388, train_loss: 0.0769, step time: 0.4983\n",
      "61/388, train_loss: 0.2324, step time: 0.4894\n",
      "62/388, train_loss: 0.2905, step time: 0.4877\n",
      "63/388, train_loss: 0.2013, step time: 1.1525\n",
      "64/388, train_loss: 0.2675, step time: 0.5305\n",
      "65/388, train_loss: 0.5731, step time: 0.5080\n",
      "66/388, train_loss: 0.2266, step time: 0.4889\n",
      "67/388, train_loss: 0.2846, step time: 0.5021\n",
      "68/388, train_loss: 0.1354, step time: 0.4850\n",
      "69/388, train_loss: 0.4520, step time: 0.4796\n",
      "70/388, train_loss: 0.3225, step time: 0.4961\n",
      "71/388, train_loss: 0.1401, step time: 0.4805\n",
      "72/388, train_loss: 0.0879, step time: 1.1681\n",
      "73/388, train_loss: 0.2454, step time: 0.5382\n",
      "74/388, train_loss: 0.1743, step time: 0.5063\n",
      "75/388, train_loss: 0.2466, step time: 0.4929\n",
      "76/388, train_loss: 0.1363, step time: 0.4904\n",
      "77/388, train_loss: 0.1864, step time: 0.4980\n",
      "78/388, train_loss: 0.3293, step time: 0.4916\n",
      "79/388, train_loss: 0.0588, step time: 0.4891\n",
      "80/388, train_loss: 0.1013, step time: 0.4748\n",
      "81/388, train_loss: 0.1298, step time: 1.0344\n",
      "82/388, train_loss: 0.2355, step time: 0.5401\n",
      "83/388, train_loss: 0.1308, step time: 0.5145\n",
      "84/388, train_loss: 0.1756, step time: 0.4950\n",
      "85/388, train_loss: 0.6636, step time: 0.4977\n",
      "86/388, train_loss: 0.3968, step time: 0.4810\n",
      "87/388, train_loss: 0.2559, step time: 0.4801\n",
      "88/388, train_loss: 0.3465, step time: 0.5028\n",
      "89/388, train_loss: 0.0919, step time: 0.4864\n",
      "90/388, train_loss: 0.0460, step time: 0.4899\n",
      "91/388, train_loss: 0.2435, step time: 0.4753\n",
      "92/388, train_loss: 0.1634, step time: 0.4803\n",
      "93/388, train_loss: 0.1055, step time: 0.4984\n",
      "94/388, train_loss: 0.2086, step time: 0.4846\n",
      "95/388, train_loss: 0.1158, step time: 0.5129\n",
      "96/388, train_loss: 0.1357, step time: 0.4976\n",
      "97/388, train_loss: 0.1813, step time: 0.4961\n",
      "98/388, train_loss: 0.0685, step time: 0.4803\n",
      "99/388, train_loss: 0.3288, step time: 0.4812\n",
      "100/388, train_loss: 0.1008, step time: 1.2191\n",
      "101/388, train_loss: 0.4824, step time: 0.5228\n",
      "102/388, train_loss: 0.1474, step time: 0.4995\n",
      "103/388, train_loss: 0.5410, step time: 0.4908\n",
      "104/388, train_loss: 0.2140, step time: 0.4977\n",
      "105/388, train_loss: 0.2686, step time: 0.4829\n",
      "106/388, train_loss: 0.1795, step time: 0.4955\n",
      "107/388, train_loss: 0.0784, step time: 0.4833\n",
      "108/388, train_loss: 0.0988, step time: 0.4896\n",
      "109/388, train_loss: 0.3869, step time: 0.4765\n",
      "110/388, train_loss: 0.1365, step time: 0.4962\n",
      "111/388, train_loss: 0.5450, step time: 0.4804\n",
      "112/388, train_loss: 0.2633, step time: 0.5359\n",
      "113/388, train_loss: 0.1467, step time: 0.5161\n",
      "114/388, train_loss: 0.5886, step time: 0.4944\n",
      "115/388, train_loss: 0.1434, step time: 0.4834\n",
      "116/388, train_loss: 0.1246, step time: 0.5041\n",
      "117/388, train_loss: 0.2480, step time: 0.4946\n",
      "118/388, train_loss: 0.2397, step time: 0.4922\n",
      "119/388, train_loss: 0.3248, step time: 0.4990\n",
      "120/388, train_loss: 0.6162, step time: 0.5089\n",
      "121/388, train_loss: 0.1753, step time: 0.4862\n",
      "122/388, train_loss: 0.1891, step time: 0.4960\n",
      "123/388, train_loss: 0.1065, step time: 0.4904\n",
      "124/388, train_loss: 0.2868, step time: 0.5359\n",
      "125/388, train_loss: 0.0907, step time: 0.6004\n",
      "126/388, train_loss: 0.1024, step time: 0.5163\n",
      "127/388, train_loss: 0.0381, step time: 0.4868\n",
      "128/388, train_loss: 0.4343, step time: 0.4970\n",
      "129/388, train_loss: 0.4550, step time: 1.2305\n",
      "130/388, train_loss: 0.1299, step time: 0.5396\n",
      "131/388, train_loss: 0.1464, step time: 0.5205\n",
      "132/388, train_loss: 0.1825, step time: 0.4882\n",
      "133/388, train_loss: 0.2297, step time: 0.4883\n",
      "134/388, train_loss: 0.4283, step time: 0.4898\n",
      "135/388, train_loss: 0.0859, step time: 0.5021\n",
      "136/388, train_loss: 0.2763, step time: 0.4927\n",
      "137/388, train_loss: 0.1156, step time: 0.4823\n",
      "138/388, train_loss: 0.2221, step time: 0.4859\n",
      "139/388, train_loss: 0.0926, step time: 1.0885\n",
      "140/388, train_loss: 0.1542, step time: 0.5208\n",
      "141/388, train_loss: 0.2743, step time: 0.5024\n",
      "142/388, train_loss: 0.0654, step time: 0.4805\n",
      "143/388, train_loss: 0.1546, step time: 0.4779\n",
      "144/388, train_loss: 0.1274, step time: 0.5021\n",
      "145/388, train_loss: 0.1712, step time: 0.4904\n",
      "146/388, train_loss: 0.1717, step time: 0.5120\n",
      "147/388, train_loss: 0.5053, step time: 0.4952\n",
      "148/388, train_loss: 0.1475, step time: 0.4961\n",
      "149/388, train_loss: 0.2379, step time: 0.4829\n",
      "150/388, train_loss: 0.2353, step time: 0.4846\n",
      "151/388, train_loss: 0.4280, step time: 0.5054\n",
      "152/388, train_loss: 0.1417, step time: 0.4864\n",
      "153/388, train_loss: 0.1433, step time: 1.1118\n",
      "154/388, train_loss: 0.2377, step time: 0.5299\n",
      "155/388, train_loss: 0.5763, step time: 0.5026\n",
      "156/388, train_loss: 0.2867, step time: 0.4979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/388, train_loss: 0.1075, step time: 0.4875\n",
      "158/388, train_loss: 0.1121, step time: 0.4934\n",
      "159/388, train_loss: 0.0858, step time: 0.4776\n",
      "160/388, train_loss: 0.2083, step time: 0.4834\n",
      "161/388, train_loss: 0.3755, step time: 0.4828\n",
      "162/388, train_loss: 0.2588, step time: 0.4796\n",
      "163/388, train_loss: 0.3372, step time: 1.0333\n",
      "164/388, train_loss: 0.1711, step time: 0.5255\n",
      "165/388, train_loss: 0.2297, step time: 0.5003\n",
      "166/388, train_loss: 0.3039, step time: 0.4960\n",
      "167/388, train_loss: 0.3445, step time: 0.4859\n",
      "168/388, train_loss: 0.3439, step time: 0.4998\n",
      "169/388, train_loss: 0.3000, step time: 0.5018\n",
      "170/388, train_loss: 0.3655, step time: 0.4900\n",
      "171/388, train_loss: 0.2855, step time: 0.4949\n",
      "172/388, train_loss: 0.1256, step time: 0.4992\n",
      "173/388, train_loss: 0.5583, step time: 0.4968\n",
      "174/388, train_loss: 0.1667, step time: 0.4813\n",
      "175/388, train_loss: 0.2565, step time: 0.4992\n",
      "176/388, train_loss: 0.2819, step time: 0.4897\n",
      "177/388, train_loss: 0.2175, step time: 0.4931\n",
      "178/388, train_loss: 0.1110, step time: 0.5429\n",
      "179/388, train_loss: 0.3266, step time: 0.5287\n",
      "180/388, train_loss: 0.1227, step time: 0.5088\n",
      "181/388, train_loss: 0.3172, step time: 0.4971\n",
      "182/388, train_loss: 0.3426, step time: 0.4884\n",
      "183/388, train_loss: 0.1984, step time: 0.4939\n",
      "184/388, train_loss: 0.1297, step time: 0.4820\n",
      "185/388, train_loss: 0.2334, step time: 0.6077\n",
      "186/388, train_loss: 0.1594, step time: 0.5549\n",
      "187/388, train_loss: 0.1925, step time: 0.5292\n",
      "188/388, train_loss: 0.2178, step time: 0.5029\n",
      "189/388, train_loss: 0.2375, step time: 0.4994\n",
      "190/388, train_loss: 0.0901, step time: 0.5198\n",
      "191/388, train_loss: 0.1710, step time: 0.5048\n",
      "192/388, train_loss: 0.1639, step time: 0.4925\n",
      "193/388, train_loss: 0.3109, step time: 0.4912\n",
      "194/388, train_loss: 0.1092, step time: 0.7324\n",
      "195/388, train_loss: 0.2669, step time: 0.5596\n",
      "196/388, train_loss: 0.3572, step time: 0.5258\n",
      "197/388, train_loss: 0.1797, step time: 0.5110\n",
      "198/388, train_loss: 0.1692, step time: 0.4938\n",
      "199/388, train_loss: 0.2359, step time: 0.5216\n",
      "200/388, train_loss: 0.2728, step time: 0.5030\n",
      "201/388, train_loss: 0.1126, step time: 0.4920\n",
      "202/388, train_loss: 0.1627, step time: 0.4922\n",
      "203/388, train_loss: 0.2945, step time: 0.4942\n",
      "204/388, train_loss: 0.2422, step time: 0.4976\n",
      "205/388, train_loss: 0.1401, step time: 0.4771\n",
      "206/388, train_loss: 0.0822, step time: 0.4766\n",
      "207/388, train_loss: 0.0598, step time: 0.4804\n",
      "208/388, train_loss: 0.1721, step time: 0.4755\n",
      "209/388, train_loss: 0.2998, step time: 0.4865\n",
      "210/388, train_loss: 0.0328, step time: 1.2435\n",
      "211/388, train_loss: 0.2659, step time: 0.5380\n",
      "212/388, train_loss: 0.2172, step time: 0.5034\n",
      "213/388, train_loss: 0.3592, step time: 0.4896\n",
      "214/388, train_loss: 0.2183, step time: 0.4962\n",
      "215/388, train_loss: 0.0624, step time: 0.4794\n",
      "216/388, train_loss: 0.6342, step time: 0.4814\n",
      "217/388, train_loss: 0.2337, step time: 0.4941\n",
      "218/388, train_loss: 0.2150, step time: 0.4998\n",
      "219/388, train_loss: 0.1797, step time: 0.4931\n",
      "220/388, train_loss: 0.3021, step time: 1.0751\n",
      "221/388, train_loss: 0.6443, step time: 0.5312\n",
      "222/388, train_loss: 0.1225, step time: 0.5181\n",
      "223/388, train_loss: 0.2345, step time: 0.5056\n",
      "224/388, train_loss: 0.2559, step time: 0.4972\n",
      "225/388, train_loss: 0.1551, step time: 0.4817\n",
      "226/388, train_loss: 0.2849, step time: 0.4819\n",
      "227/388, train_loss: 0.0975, step time: 0.4729\n",
      "228/388, train_loss: 0.1592, step time: 0.4825\n",
      "229/388, train_loss: 0.2761, step time: 0.4966\n",
      "230/388, train_loss: 0.3055, step time: 0.7369\n",
      "231/388, train_loss: 0.1240, step time: 0.5374\n",
      "232/388, train_loss: 0.0863, step time: 0.5126\n",
      "233/388, train_loss: 0.1154, step time: 0.4980\n",
      "234/388, train_loss: 0.3257, step time: 0.5162\n",
      "235/388, train_loss: 0.1288, step time: 0.5009\n",
      "236/388, train_loss: 0.0469, step time: 0.4988\n",
      "237/388, train_loss: 0.2091, step time: 0.4867\n",
      "238/388, train_loss: 0.5750, step time: 0.4934\n",
      "239/388, train_loss: 0.2000, step time: 0.4939\n",
      "240/388, train_loss: 0.2527, step time: 1.0609\n",
      "241/388, train_loss: 0.3077, step time: 0.5316\n",
      "242/388, train_loss: 0.1586, step time: 0.5188\n",
      "243/388, train_loss: 0.0641, step time: 0.4906\n",
      "244/388, train_loss: 0.1074, step time: 0.4908\n",
      "245/388, train_loss: 0.2248, step time: 0.4794\n",
      "246/388, train_loss: 0.1977, step time: 0.4866\n",
      "247/388, train_loss: 0.2070, step time: 0.4997\n",
      "248/388, train_loss: 0.1942, step time: 0.4799\n",
      "249/388, train_loss: 0.2414, step time: 0.4849\n",
      "250/388, train_loss: 0.3214, step time: 0.4928\n",
      "251/388, train_loss: 0.1044, step time: 0.5164\n",
      "252/388, train_loss: 0.2621, step time: 0.5023\n",
      "253/388, train_loss: 0.2795, step time: 0.4827\n",
      "254/388, train_loss: 0.1657, step time: 0.6682\n",
      "255/388, train_loss: 0.3122, step time: 0.5523\n",
      "256/388, train_loss: 0.3319, step time: 0.5129\n",
      "257/388, train_loss: 0.1229, step time: 0.5022\n",
      "258/388, train_loss: 0.1305, step time: 0.4865\n",
      "259/388, train_loss: 0.1048, step time: 0.4926\n",
      "260/388, train_loss: 0.1687, step time: 0.4794\n",
      "261/388, train_loss: 0.2687, step time: 0.4755\n",
      "262/388, train_loss: 0.0754, step time: 1.0160\n",
      "263/388, train_loss: 0.1456, step time: 0.5500\n",
      "264/388, train_loss: 0.1044, step time: 0.5256\n",
      "265/388, train_loss: 0.1319, step time: 0.4972\n",
      "266/388, train_loss: 0.0764, step time: 0.4948\n",
      "267/388, train_loss: 0.1337, step time: 0.4811\n",
      "268/388, train_loss: 0.0677, step time: 0.4771\n",
      "269/388, train_loss: 0.1289, step time: 0.4818\n",
      "270/388, train_loss: 0.3270, step time: 0.9589\n",
      "271/388, train_loss: 0.3934, step time: 0.5410\n",
      "272/388, train_loss: 0.0950, step time: 0.5226\n",
      "273/388, train_loss: 0.2212, step time: 0.5007\n",
      "274/388, train_loss: 0.2817, step time: 0.4915\n",
      "275/388, train_loss: 0.2352, step time: 0.5151\n",
      "276/388, train_loss: 0.2354, step time: 0.4861\n",
      "277/388, train_loss: 0.1101, step time: 0.5397\n",
      "278/388, train_loss: 0.2431, step time: 0.6474\n",
      "279/388, train_loss: 0.4983, step time: 0.5562\n",
      "280/388, train_loss: 0.3885, step time: 0.5235\n",
      "281/388, train_loss: 0.2083, step time: 0.5040\n",
      "282/388, train_loss: 0.1389, step time: 0.4980\n",
      "283/388, train_loss: 0.1393, step time: 0.4802\n",
      "284/388, train_loss: 0.1056, step time: 0.4764\n",
      "285/388, train_loss: 0.1476, step time: 0.4830\n",
      "286/388, train_loss: 0.2099, step time: 0.4769\n",
      "287/388, train_loss: 0.1337, step time: 0.4824\n",
      "288/388, train_loss: 0.1018, step time: 0.9752\n",
      "289/388, train_loss: 0.2346, step time: 0.5401\n",
      "290/388, train_loss: 0.1363, step time: 0.5120\n",
      "291/388, train_loss: 0.2555, step time: 0.4908\n",
      "292/388, train_loss: 0.0825, step time: 0.4968\n",
      "293/388, train_loss: 0.5030, step time: 0.4809\n",
      "294/388, train_loss: 0.1234, step time: 0.4756\n",
      "295/388, train_loss: 0.2191, step time: 0.5023\n",
      "296/388, train_loss: 0.2324, step time: 0.4890\n",
      "297/388, train_loss: 0.3461, step time: 1.1253\n",
      "298/388, train_loss: 0.1470, step time: 0.5351\n",
      "299/388, train_loss: 0.1440, step time: 0.5006\n",
      "300/388, train_loss: 0.2075, step time: 0.4964\n",
      "301/388, train_loss: 0.1469, step time: 0.4862\n",
      "302/388, train_loss: 0.2476, step time: 0.5265\n",
      "303/388, train_loss: 0.3933, step time: 0.5073\n",
      "304/388, train_loss: 0.0722, step time: 0.5006\n",
      "305/388, train_loss: 0.2349, step time: 0.4909\n",
      "306/388, train_loss: 0.3052, step time: 0.4915\n",
      "307/388, train_loss: 0.2804, step time: 0.4753\n",
      "308/388, train_loss: 0.3647, step time: 0.6223\n",
      "309/388, train_loss: 0.1946, step time: 0.5570\n",
      "310/388, train_loss: 0.1967, step time: 0.5276\n",
      "311/388, train_loss: 0.1103, step time: 0.4997\n",
      "312/388, train_loss: 0.6604, step time: 0.5098\n",
      "313/388, train_loss: 0.2819, step time: 0.5332\n",
      "314/388, train_loss: 0.1953, step time: 0.6147\n",
      "315/388, train_loss: 0.1309, step time: 0.5344\n",
      "316/388, train_loss: 0.1641, step time: 0.5066\n",
      "317/388, train_loss: 0.1636, step time: 0.4999\n",
      "318/388, train_loss: 0.1775, step time: 0.5531\n",
      "319/388, train_loss: 0.0786, step time: 0.5489\n",
      "320/388, train_loss: 0.1787, step time: 0.5232\n",
      "321/388, train_loss: 0.0885, step time: 0.4957\n",
      "322/388, train_loss: 0.1188, step time: 0.4851\n",
      "323/388, train_loss: 0.1059, step time: 0.4966\n",
      "324/388, train_loss: 0.3848, step time: 0.4839\n",
      "325/388, train_loss: 0.1028, step time: 1.0103\n",
      "326/388, train_loss: 0.0995, step time: 0.5403\n",
      "327/388, train_loss: 0.1624, step time: 0.5218\n",
      "328/388, train_loss: 0.1872, step time: 0.4953\n",
      "329/388, train_loss: 0.1991, step time: 0.4994\n",
      "330/388, train_loss: 0.2826, step time: 0.4836\n",
      "331/388, train_loss: 0.2636, step time: 0.4881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/388, train_loss: 0.3078, step time: 0.4851\n",
      "333/388, train_loss: 0.0960, step time: 0.5307\n",
      "334/388, train_loss: 0.0825, step time: 0.5170\n",
      "335/388, train_loss: 0.1879, step time: 0.5019\n",
      "336/388, train_loss: 0.3695, step time: 0.4965\n",
      "337/388, train_loss: 0.1256, step time: 1.1307\n",
      "338/388, train_loss: 0.1615, step time: 0.5309\n",
      "339/388, train_loss: 0.0848, step time: 0.5040\n",
      "340/388, train_loss: 0.2842, step time: 0.4957\n",
      "341/388, train_loss: 0.2056, step time: 0.4886\n",
      "342/388, train_loss: 0.1878, step time: 0.4766\n",
      "343/388, train_loss: 0.0738, step time: 0.4822\n",
      "344/388, train_loss: 0.0682, step time: 0.4846\n",
      "345/388, train_loss: 0.1876, step time: 0.4891\n",
      "346/388, train_loss: 0.0625, step time: 0.4829\n",
      "347/388, train_loss: 0.2988, step time: 0.7373\n",
      "348/388, train_loss: 0.1151, step time: 0.5553\n",
      "349/388, train_loss: 0.1425, step time: 0.5164\n",
      "350/388, train_loss: 0.2630, step time: 0.5054\n",
      "351/388, train_loss: 0.1665, step time: 0.4884\n",
      "352/388, train_loss: 0.1578, step time: 0.4985\n",
      "353/388, train_loss: 0.5695, step time: 0.5458\n",
      "354/388, train_loss: 0.1211, step time: 0.6358\n",
      "355/388, train_loss: 0.1706, step time: 0.5338\n",
      "356/388, train_loss: 0.4115, step time: 0.5154\n",
      "357/388, train_loss: 0.1477, step time: 0.5196\n",
      "358/388, train_loss: 0.1676, step time: 0.6440\n",
      "359/388, train_loss: 0.0873, step time: 0.5588\n",
      "360/388, train_loss: 0.1610, step time: 0.5309\n",
      "361/388, train_loss: 0.1174, step time: 0.4910\n",
      "362/388, train_loss: 0.2216, step time: 0.4973\n",
      "363/388, train_loss: 0.0752, step time: 0.5310\n",
      "364/388, train_loss: 0.3005, step time: 0.5187\n",
      "365/388, train_loss: 0.2176, step time: 0.4963\n",
      "366/388, train_loss: 0.1505, step time: 0.5006\n",
      "367/388, train_loss: 0.1719, step time: 0.5904\n",
      "368/388, train_loss: 0.2279, step time: 0.5433\n",
      "369/388, train_loss: 0.2643, step time: 0.5103\n",
      "370/388, train_loss: 0.1015, step time: 0.4850\n",
      "371/388, train_loss: 0.2160, step time: 0.4864\n",
      "372/388, train_loss: 0.2003, step time: 0.8801\n",
      "373/388, train_loss: 0.1458, step time: 0.5450\n",
      "374/388, train_loss: 0.0438, step time: 0.5102\n",
      "375/388, train_loss: 0.2029, step time: 0.4910\n",
      "376/388, train_loss: 0.0867, step time: 0.4964\n",
      "377/388, train_loss: 0.4267, step time: 0.4793\n",
      "378/388, train_loss: 0.1867, step time: 0.8423\n",
      "379/388, train_loss: 0.1814, step time: 0.5595\n",
      "380/388, train_loss: 0.1406, step time: 0.5177\n",
      "381/388, train_loss: 0.1729, step time: 0.4944\n",
      "382/388, train_loss: 0.0780, step time: 0.5212\n",
      "383/388, train_loss: 0.0962, step time: 0.5064\n",
      "384/388, train_loss: 0.0683, step time: 0.4872\n",
      "385/388, train_loss: 0.1602, step time: 0.4798\n",
      "386/388, train_loss: 0.3201, step time: 0.6343\n",
      "387/388, train_loss: 0.0773, step time: 0.5294\n",
      "388/388, train_loss: 0.7010, step time: 0.5166\n",
      "epoch 23 average loss: 0.2176\n",
      "saved new best metric model\n",
      "current epoch: 23 current mean dice: 0.7501 tc: 0.8052 wt: 0.8876 et: 0.5576\n",
      "best mean dice: 0.7501 at epoch: 23\n",
      "time consuming of epoch 23 is: 302.6855\n",
      "----------\n",
      "epoch 24/300\n",
      "1/388, train_loss: 0.2431, step time: 0.4785\n",
      "2/388, train_loss: 0.0722, step time: 0.4808\n",
      "3/388, train_loss: 0.3278, step time: 0.6824\n",
      "4/388, train_loss: 0.2384, step time: 0.5530\n",
      "5/388, train_loss: 0.2621, step time: 0.5175\n",
      "6/388, train_loss: 0.1430, step time: 0.5009\n",
      "7/388, train_loss: 0.1170, step time: 0.4994\n",
      "8/388, train_loss: 0.1682, step time: 0.5215\n",
      "9/388, train_loss: 0.2276, step time: 0.5178\n",
      "10/388, train_loss: 0.1711, step time: 0.5778\n",
      "11/388, train_loss: 0.2930, step time: 0.5632\n",
      "12/388, train_loss: 0.1559, step time: 0.5717\n",
      "13/388, train_loss: 0.2560, step time: 0.5232\n",
      "14/388, train_loss: 0.1855, step time: 0.5013\n",
      "15/388, train_loss: 0.1029, step time: 0.9490\n",
      "16/388, train_loss: 0.0984, step time: 0.5507\n",
      "17/388, train_loss: 0.1143, step time: 0.5190\n",
      "18/388, train_loss: 0.1509, step time: 0.5013\n",
      "19/388, train_loss: 0.1525, step time: 0.4882\n",
      "20/388, train_loss: 0.5041, step time: 0.5218\n",
      "21/388, train_loss: 0.2009, step time: 0.6008\n",
      "22/388, train_loss: 0.1012, step time: 0.5189\n",
      "23/388, train_loss: 0.1384, step time: 0.5007\n",
      "24/388, train_loss: 0.0603, step time: 0.5570\n",
      "25/388, train_loss: 0.0676, step time: 0.5809\n",
      "26/388, train_loss: 0.2028, step time: 0.5446\n",
      "27/388, train_loss: 0.1313, step time: 0.5400\n",
      "28/388, train_loss: 0.1334, step time: 0.5190\n",
      "29/388, train_loss: 0.1969, step time: 0.4965\n",
      "30/388, train_loss: 0.2315, step time: 0.5348\n",
      "31/388, train_loss: 0.2114, step time: 0.4995\n",
      "32/388, train_loss: 0.0984, step time: 0.5100\n",
      "33/388, train_loss: 0.1454, step time: 0.5297\n",
      "34/388, train_loss: 0.1358, step time: 0.6068\n",
      "35/388, train_loss: 0.5510, step time: 0.5392\n",
      "36/388, train_loss: 0.1054, step time: 0.5041\n",
      "37/388, train_loss: 0.3146, step time: 0.4823\n",
      "38/388, train_loss: 0.1505, step time: 0.5242\n",
      "39/388, train_loss: 0.1734, step time: 0.4969\n",
      "40/388, train_loss: 0.4999, step time: 0.5622\n",
      "41/388, train_loss: 0.3174, step time: 0.5801\n",
      "42/388, train_loss: 0.1840, step time: 0.5532\n",
      "43/388, train_loss: 0.5915, step time: 0.5313\n",
      "44/388, train_loss: 0.1590, step time: 0.5108\n",
      "45/388, train_loss: 0.1893, step time: 0.4895\n",
      "46/388, train_loss: 0.1377, step time: 0.5185\n",
      "47/388, train_loss: 0.3411, step time: 0.5280\n",
      "48/388, train_loss: 0.6082, step time: 0.5030\n",
      "49/388, train_loss: 0.1982, step time: 0.5030\n",
      "50/388, train_loss: 0.1793, step time: 0.5546\n",
      "51/388, train_loss: 0.1063, step time: 0.5263\n",
      "52/388, train_loss: 0.1537, step time: 0.5170\n",
      "53/388, train_loss: 0.3615, step time: 0.5782\n",
      "54/388, train_loss: 0.3557, step time: 0.5356\n",
      "55/388, train_loss: 0.0584, step time: 0.5037\n",
      "56/388, train_loss: 0.1753, step time: 0.5069\n",
      "57/388, train_loss: 0.6204, step time: 0.5599\n",
      "58/388, train_loss: 0.0961, step time: 0.5274\n",
      "59/388, train_loss: 0.0956, step time: 0.5618\n",
      "60/388, train_loss: 0.2127, step time: 0.5340\n",
      "61/388, train_loss: 0.0301, step time: 0.5120\n",
      "62/388, train_loss: 0.1120, step time: 0.5525\n",
      "63/388, train_loss: 0.5654, step time: 0.5523\n",
      "64/388, train_loss: 0.1467, step time: 0.5127\n",
      "65/388, train_loss: 0.0946, step time: 0.6024\n",
      "66/388, train_loss: 0.2218, step time: 0.6231\n",
      "67/388, train_loss: 0.3384, step time: 0.5599\n",
      "68/388, train_loss: 0.1174, step time: 0.5215\n",
      "69/388, train_loss: 0.0805, step time: 0.5382\n",
      "70/388, train_loss: 0.2774, step time: 0.5699\n",
      "71/388, train_loss: 0.0477, step time: 0.5385\n",
      "72/388, train_loss: 0.2984, step time: 0.5119\n",
      "73/388, train_loss: 0.1584, step time: 0.4908\n",
      "74/388, train_loss: 0.1368, step time: 1.0901\n",
      "75/388, train_loss: 0.2327, step time: 0.5357\n",
      "76/388, train_loss: 0.1461, step time: 0.5153\n",
      "77/388, train_loss: 0.1320, step time: 0.5073\n",
      "78/388, train_loss: 0.3002, step time: 0.5756\n",
      "79/388, train_loss: 0.1035, step time: 0.5449\n",
      "80/388, train_loss: 0.2659, step time: 0.5137\n",
      "81/388, train_loss: 0.1274, step time: 0.4947\n",
      "82/388, train_loss: 0.1295, step time: 0.4919\n",
      "83/388, train_loss: 0.1279, step time: 0.4803\n",
      "84/388, train_loss: 0.1349, step time: 1.0400\n",
      "85/388, train_loss: 0.5262, step time: 0.5410\n",
      "86/388, train_loss: 0.1699, step time: 0.5151\n",
      "87/388, train_loss: 0.2792, step time: 0.5080\n",
      "88/388, train_loss: 0.0520, step time: 0.5016\n",
      "89/388, train_loss: 0.2594, step time: 0.4896\n",
      "90/388, train_loss: 0.1976, step time: 0.9460\n",
      "91/388, train_loss: 0.2122, step time: 0.5416\n",
      "92/388, train_loss: 0.1382, step time: 0.5202\n",
      "93/388, train_loss: 0.2892, step time: 0.5051\n",
      "94/388, train_loss: 0.3241, step time: 0.4896\n",
      "95/388, train_loss: 0.2280, step time: 0.9115\n",
      "96/388, train_loss: 0.1911, step time: 0.5198\n",
      "97/388, train_loss: 0.3051, step time: 0.5067\n",
      "98/388, train_loss: 0.1370, step time: 0.4977\n",
      "99/388, train_loss: 0.3449, step time: 0.4898\n",
      "100/388, train_loss: 0.1819, step time: 0.4933\n",
      "101/388, train_loss: 0.1499, step time: 0.4754\n",
      "102/388, train_loss: 0.2441, step time: 0.4751\n",
      "103/388, train_loss: 0.4453, step time: 0.8812\n",
      "104/388, train_loss: 0.6229, step time: 0.5437\n",
      "105/388, train_loss: 0.0806, step time: 0.5159\n",
      "106/388, train_loss: 0.6796, step time: 0.5009\n",
      "107/388, train_loss: 0.3191, step time: 0.5104\n",
      "108/388, train_loss: 0.0733, step time: 0.4860\n",
      "109/388, train_loss: 0.2603, step time: 0.4883\n",
      "110/388, train_loss: 0.1436, step time: 1.1464\n",
      "111/388, train_loss: 0.1701, step time: 0.5205\n",
      "112/388, train_loss: 0.1977, step time: 0.5010\n",
      "113/388, train_loss: 0.1741, step time: 0.4838\n",
      "114/388, train_loss: 0.1303, step time: 0.5143\n",
      "115/388, train_loss: 0.2727, step time: 0.4949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/388, train_loss: 0.0992, step time: 0.5000\n",
      "117/388, train_loss: 0.1388, step time: 0.4885\n",
      "118/388, train_loss: 0.1425, step time: 0.4902\n",
      "119/388, train_loss: 0.4214, step time: 0.4764\n",
      "120/388, train_loss: 0.2217, step time: 0.6708\n",
      "121/388, train_loss: 0.2302, step time: 0.5567\n",
      "122/388, train_loss: 0.0692, step time: 0.5273\n",
      "123/388, train_loss: 0.1578, step time: 0.5078\n",
      "124/388, train_loss: 0.1740, step time: 0.4975\n",
      "125/388, train_loss: 0.4259, step time: 0.4943\n",
      "126/388, train_loss: 0.4852, step time: 0.4916\n",
      "127/388, train_loss: 0.0979, step time: 0.4974\n",
      "128/388, train_loss: 0.2801, step time: 0.4969\n",
      "129/388, train_loss: 0.1190, step time: 0.4957\n",
      "130/388, train_loss: 0.1035, step time: 0.5419\n",
      "131/388, train_loss: 0.2437, step time: 0.5253\n",
      "132/388, train_loss: 0.0978, step time: 0.5094\n",
      "133/388, train_loss: 0.3891, step time: 0.4941\n",
      "134/388, train_loss: 0.3401, step time: 0.4937\n",
      "135/388, train_loss: 0.2447, step time: 1.0632\n",
      "136/388, train_loss: 0.2036, step time: 0.5269\n",
      "137/388, train_loss: 0.2360, step time: 0.5096\n",
      "138/388, train_loss: 0.0728, step time: 0.4887\n",
      "139/388, train_loss: 0.0703, step time: 0.4966\n",
      "140/388, train_loss: 0.2618, step time: 0.4840\n",
      "141/388, train_loss: 0.1063, step time: 0.5031\n",
      "142/388, train_loss: 0.1608, step time: 0.4971\n",
      "143/388, train_loss: 0.1112, step time: 0.6148\n",
      "144/388, train_loss: 0.2135, step time: 0.5414\n",
      "145/388, train_loss: 0.3254, step time: 0.5067\n",
      "146/388, train_loss: 0.6508, step time: 0.5044\n",
      "147/388, train_loss: 0.1211, step time: 0.4886\n",
      "148/388, train_loss: 0.1891, step time: 0.4782\n",
      "149/388, train_loss: 0.3265, step time: 0.4912\n",
      "150/388, train_loss: 0.5396, step time: 0.4920\n",
      "151/388, train_loss: 0.0378, step time: 0.5209\n",
      "152/388, train_loss: 0.1663, step time: 0.4932\n",
      "153/388, train_loss: 0.1182, step time: 0.4875\n",
      "154/388, train_loss: 0.3596, step time: 0.4990\n",
      "155/388, train_loss: 0.1763, step time: 0.5048\n",
      "156/388, train_loss: 0.1837, step time: 0.4980\n",
      "157/388, train_loss: 0.0843, step time: 0.4863\n",
      "158/388, train_loss: 0.3512, step time: 0.4905\n",
      "159/388, train_loss: 0.2147, step time: 0.4880\n",
      "160/388, train_loss: 0.3796, step time: 0.4767\n",
      "161/388, train_loss: 0.2779, step time: 1.1648\n",
      "162/388, train_loss: 0.1000, step time: 0.5430\n",
      "163/388, train_loss: 0.1099, step time: 0.5169\n",
      "164/388, train_loss: 0.1561, step time: 0.4902\n",
      "165/388, train_loss: 0.5661, step time: 0.4931\n",
      "166/388, train_loss: 0.2177, step time: 0.9915\n",
      "167/388, train_loss: 0.3942, step time: 0.5272\n",
      "168/388, train_loss: 0.6118, step time: 0.5082\n",
      "169/388, train_loss: 0.1965, step time: 0.4862\n",
      "170/388, train_loss: 0.3356, step time: 0.4960\n",
      "171/388, train_loss: 0.1057, step time: 0.5573\n",
      "172/388, train_loss: 0.2688, step time: 0.5373\n",
      "173/388, train_loss: 0.4201, step time: 0.5051\n",
      "174/388, train_loss: 0.1030, step time: 0.4903\n",
      "175/388, train_loss: 0.1688, step time: 0.8207\n",
      "176/388, train_loss: 0.1097, step time: 0.5442\n",
      "177/388, train_loss: 0.1585, step time: 0.5174\n",
      "178/388, train_loss: 0.2228, step time: 0.4931\n",
      "179/388, train_loss: 0.1456, step time: 0.4969\n",
      "180/388, train_loss: 0.1900, step time: 0.4836\n",
      "181/388, train_loss: 0.1832, step time: 1.0430\n",
      "182/388, train_loss: 0.1051, step time: 0.5387\n",
      "183/388, train_loss: 0.4383, step time: 0.5052\n",
      "184/388, train_loss: 0.1493, step time: 0.4901\n",
      "185/388, train_loss: 0.0953, step time: 0.4970\n",
      "186/388, train_loss: 0.5349, step time: 0.4799\n",
      "187/388, train_loss: 0.3093, step time: 0.4819\n",
      "188/388, train_loss: 0.1963, step time: 0.4906\n",
      "189/388, train_loss: 0.1131, step time: 0.4815\n",
      "190/388, train_loss: 0.3489, step time: 0.4805\n",
      "191/388, train_loss: 0.0821, step time: 0.4750\n",
      "192/388, train_loss: 0.0458, step time: 0.4760\n",
      "193/388, train_loss: 0.2548, step time: 0.4803\n",
      "194/388, train_loss: 0.2964, step time: 0.5141\n",
      "195/388, train_loss: 0.1922, step time: 0.5171\n",
      "196/388, train_loss: 0.1881, step time: 0.5381\n",
      "197/388, train_loss: 0.2999, step time: 0.5174\n",
      "198/388, train_loss: 0.1476, step time: 0.5044\n",
      "199/388, train_loss: 0.0960, step time: 0.4871\n",
      "200/388, train_loss: 0.1247, step time: 0.4877\n",
      "201/388, train_loss: 0.0593, step time: 0.4891\n",
      "202/388, train_loss: 0.3335, step time: 0.4843\n",
      "203/388, train_loss: 0.1696, step time: 0.4776\n",
      "204/388, train_loss: 0.0734, step time: 0.6730\n",
      "205/388, train_loss: 0.1967, step time: 0.5394\n",
      "206/388, train_loss: 0.1015, step time: 0.5160\n",
      "207/388, train_loss: 0.1622, step time: 0.4958\n",
      "208/388, train_loss: 0.2320, step time: 0.4944\n",
      "209/388, train_loss: 0.4011, step time: 0.4954\n",
      "210/388, train_loss: 0.1549, step time: 0.5083\n",
      "211/388, train_loss: 0.1496, step time: 0.4923\n",
      "212/388, train_loss: 0.1900, step time: 0.5280\n",
      "213/388, train_loss: 0.2667, step time: 0.5344\n",
      "214/388, train_loss: 0.2460, step time: 0.5206\n",
      "215/388, train_loss: 0.0862, step time: 0.5047\n",
      "216/388, train_loss: 0.2413, step time: 0.4881\n",
      "217/388, train_loss: 0.1853, step time: 0.4895\n",
      "218/388, train_loss: 0.1614, step time: 0.5162\n",
      "219/388, train_loss: 0.2484, step time: 0.5053\n",
      "220/388, train_loss: 0.2154, step time: 0.4864\n",
      "221/388, train_loss: 0.2523, step time: 0.4939\n",
      "222/388, train_loss: 0.1501, step time: 0.5048\n",
      "223/388, train_loss: 0.2697, step time: 0.4881\n",
      "224/388, train_loss: 0.1301, step time: 1.1349\n",
      "225/388, train_loss: 0.1149, step time: 0.5252\n",
      "226/388, train_loss: 0.2081, step time: 0.5082\n",
      "227/388, train_loss: 0.2501, step time: 0.4978\n",
      "228/388, train_loss: 0.5478, step time: 0.5113\n",
      "229/388, train_loss: 0.4358, step time: 0.4922\n",
      "230/388, train_loss: 0.1428, step time: 0.5042\n",
      "231/388, train_loss: 0.1183, step time: 0.4908\n",
      "232/388, train_loss: 0.3447, step time: 0.4980\n",
      "233/388, train_loss: 0.2096, step time: 0.4854\n",
      "234/388, train_loss: 0.3299, step time: 0.4956\n",
      "235/388, train_loss: 0.3310, step time: 0.5354\n",
      "236/388, train_loss: 0.0978, step time: 0.5302\n",
      "237/388, train_loss: 0.1691, step time: 0.5182\n",
      "238/388, train_loss: 0.2183, step time: 0.4882\n",
      "239/388, train_loss: 0.3246, step time: 0.4776\n",
      "240/388, train_loss: 0.1868, step time: 0.4751\n",
      "241/388, train_loss: 0.1728, step time: 0.4832\n",
      "242/388, train_loss: 0.1656, step time: 0.5088\n",
      "243/388, train_loss: 0.3878, step time: 0.4874\n",
      "244/388, train_loss: 0.2511, step time: 0.9626\n",
      "245/388, train_loss: 0.2821, step time: 0.5496\n",
      "246/388, train_loss: 0.1866, step time: 0.5118\n",
      "247/388, train_loss: 0.3341, step time: 0.4849\n",
      "248/388, train_loss: 0.5206, step time: 0.4970\n",
      "249/388, train_loss: 0.5408, step time: 0.4959\n",
      "250/388, train_loss: 0.2703, step time: 0.4855\n",
      "251/388, train_loss: 0.4294, step time: 0.4958\n",
      "252/388, train_loss: 0.2468, step time: 0.4836\n",
      "253/388, train_loss: 0.1651, step time: 0.5082\n",
      "254/388, train_loss: 0.0606, step time: 0.4938\n",
      "255/388, train_loss: 0.1160, step time: 0.4820\n",
      "256/388, train_loss: 0.0956, step time: 0.4891\n",
      "257/388, train_loss: 0.5435, step time: 0.4896\n",
      "258/388, train_loss: 0.3137, step time: 0.5191\n",
      "259/388, train_loss: 0.1933, step time: 0.5703\n",
      "260/388, train_loss: 0.2219, step time: 0.5438\n",
      "261/388, train_loss: 0.1587, step time: 0.5217\n",
      "262/388, train_loss: 0.2747, step time: 0.5010\n",
      "263/388, train_loss: 0.4477, step time: 0.4974\n",
      "264/388, train_loss: 0.1700, step time: 0.4863\n",
      "265/388, train_loss: 0.1742, step time: 0.4857\n",
      "266/388, train_loss: 0.0888, step time: 0.4857\n",
      "267/388, train_loss: 0.4516, step time: 0.5298\n",
      "268/388, train_loss: 0.3232, step time: 0.5101\n",
      "269/388, train_loss: 0.3340, step time: 0.5379\n",
      "270/388, train_loss: 0.1467, step time: 0.5452\n",
      "271/388, train_loss: 0.1475, step time: 0.5186\n",
      "272/388, train_loss: 0.2186, step time: 0.4973\n",
      "273/388, train_loss: 0.1408, step time: 0.5008\n",
      "274/388, train_loss: 0.2511, step time: 0.5005\n",
      "275/388, train_loss: 0.0789, step time: 0.5082\n",
      "276/388, train_loss: 0.1966, step time: 0.5030\n",
      "277/388, train_loss: 0.2444, step time: 0.4925\n",
      "278/388, train_loss: 0.2667, step time: 0.5608\n",
      "279/388, train_loss: 0.1344, step time: 0.5392\n",
      "280/388, train_loss: 0.0823, step time: 0.5151\n",
      "281/388, train_loss: 0.1235, step time: 0.5721\n",
      "282/388, train_loss: 0.3506, step time: 0.5315\n",
      "283/388, train_loss: 0.1089, step time: 0.5100\n",
      "284/388, train_loss: 0.3126, step time: 0.5050\n",
      "285/388, train_loss: 0.1785, step time: 0.4847\n",
      "286/388, train_loss: 0.1759, step time: 0.4882\n",
      "287/388, train_loss: 0.3507, step time: 0.4851\n",
      "288/388, train_loss: 0.1035, step time: 0.5776\n",
      "289/388, train_loss: 0.1190, step time: 0.5379\n",
      "290/388, train_loss: 0.2710, step time: 0.5119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/388, train_loss: 0.0667, step time: 0.5159\n",
      "292/388, train_loss: 0.3003, step time: 0.6503\n",
      "293/388, train_loss: 0.3611, step time: 0.5431\n",
      "294/388, train_loss: 0.1180, step time: 0.5169\n",
      "295/388, train_loss: 0.2797, step time: 0.5055\n",
      "296/388, train_loss: 0.3766, step time: 0.5636\n",
      "297/388, train_loss: 0.2775, step time: 0.5326\n",
      "298/388, train_loss: 0.1500, step time: 0.5032\n",
      "299/388, train_loss: 0.1176, step time: 0.5089\n",
      "300/388, train_loss: 0.2118, step time: 0.4967\n",
      "301/388, train_loss: 0.1071, step time: 0.4937\n",
      "302/388, train_loss: 0.3897, step time: 0.5095\n",
      "303/388, train_loss: 0.2007, step time: 0.5166\n",
      "304/388, train_loss: 0.1675, step time: 0.5688\n",
      "305/388, train_loss: 0.0780, step time: 0.5295\n",
      "306/388, train_loss: 0.1051, step time: 0.5127\n",
      "307/388, train_loss: 0.4820, step time: 0.4959\n",
      "308/388, train_loss: 0.0967, step time: 0.5071\n",
      "309/388, train_loss: 0.1756, step time: 0.4893\n",
      "310/388, train_loss: 0.1863, step time: 0.5077\n",
      "311/388, train_loss: 0.2443, step time: 0.4985\n",
      "312/388, train_loss: 0.1111, step time: 1.1856\n",
      "313/388, train_loss: 0.0909, step time: 0.5341\n",
      "314/388, train_loss: 0.2418, step time: 0.5060\n",
      "315/388, train_loss: 0.1265, step time: 0.4962\n",
      "316/388, train_loss: 0.3316, step time: 0.4980\n",
      "317/388, train_loss: 0.0896, step time: 0.4828\n",
      "318/388, train_loss: 0.2027, step time: 0.4882\n",
      "319/388, train_loss: 0.0785, step time: 0.7463\n",
      "320/388, train_loss: 0.2414, step time: 0.5375\n",
      "321/388, train_loss: 0.0942, step time: 0.5074\n",
      "322/388, train_loss: 0.1228, step time: 0.4956\n",
      "323/388, train_loss: 0.2165, step time: 0.4924\n",
      "324/388, train_loss: 0.2091, step time: 0.4836\n",
      "325/388, train_loss: 0.1752, step time: 0.8234\n",
      "326/388, train_loss: 0.0785, step time: 0.5300\n",
      "327/388, train_loss: 0.3161, step time: 0.5010\n",
      "328/388, train_loss: 0.0839, step time: 0.4861\n",
      "329/388, train_loss: 0.1437, step time: 0.4936\n",
      "330/388, train_loss: 0.1043, step time: 0.5401\n",
      "331/388, train_loss: 0.1877, step time: 0.5290\n",
      "332/388, train_loss: 0.2507, step time: 0.5107\n",
      "333/388, train_loss: 0.1189, step time: 0.4989\n",
      "334/388, train_loss: 0.1001, step time: 0.4975\n",
      "335/388, train_loss: 0.1649, step time: 0.4802\n",
      "336/388, train_loss: 0.4026, step time: 0.4815\n",
      "337/388, train_loss: 0.1145, step time: 0.4762\n",
      "338/388, train_loss: 0.0734, step time: 0.4775\n",
      "339/388, train_loss: 0.1293, step time: 0.9417\n",
      "340/388, train_loss: 0.1629, step time: 0.5644\n",
      "341/388, train_loss: 0.1799, step time: 0.5261\n",
      "342/388, train_loss: 0.1136, step time: 0.4941\n",
      "343/388, train_loss: 0.1412, step time: 0.4998\n",
      "344/388, train_loss: 0.2630, step time: 0.4826\n",
      "345/388, train_loss: 0.1193, step time: 0.4793\n",
      "346/388, train_loss: 0.1538, step time: 1.0319\n",
      "347/388, train_loss: 0.2354, step time: 0.5310\n",
      "348/388, train_loss: 0.2581, step time: 0.5095\n",
      "349/388, train_loss: 0.1052, step time: 0.4934\n",
      "350/388, train_loss: 0.3372, step time: 0.4977\n",
      "351/388, train_loss: 0.1734, step time: 0.4836\n",
      "352/388, train_loss: 0.0942, step time: 0.4907\n",
      "353/388, train_loss: 0.1260, step time: 0.4750\n",
      "354/388, train_loss: 0.6264, step time: 0.4778\n",
      "355/388, train_loss: 0.1029, step time: 0.9251\n",
      "356/388, train_loss: 0.3579, step time: 0.5461\n",
      "357/388, train_loss: 0.2875, step time: 0.5212\n",
      "358/388, train_loss: 0.0755, step time: 0.4968\n",
      "359/388, train_loss: 0.1309, step time: 0.5049\n",
      "360/388, train_loss: 0.2736, step time: 0.4924\n",
      "361/388, train_loss: 0.2535, step time: 0.4811\n",
      "362/388, train_loss: 0.1053, step time: 0.4834\n",
      "363/388, train_loss: 0.1492, step time: 0.4719\n",
      "364/388, train_loss: 0.2049, step time: 0.4839\n",
      "365/388, train_loss: 0.1381, step time: 0.4978\n",
      "366/388, train_loss: 0.1467, step time: 0.4890\n",
      "367/388, train_loss: 0.3491, step time: 0.4830\n",
      "368/388, train_loss: 0.5137, step time: 1.1602\n",
      "369/388, train_loss: 0.2566, step time: 0.5240\n",
      "370/388, train_loss: 0.2691, step time: 0.4990\n",
      "371/388, train_loss: 0.0784, step time: 0.4804\n",
      "372/388, train_loss: 0.1524, step time: 0.4847\n",
      "373/388, train_loss: 0.2299, step time: 0.4902\n",
      "374/388, train_loss: 0.0959, step time: 0.4767\n",
      "375/388, train_loss: 0.1380, step time: 0.4881\n",
      "376/388, train_loss: 0.1132, step time: 0.5145\n",
      "377/388, train_loss: 0.1359, step time: 0.5066\n",
      "378/388, train_loss: 0.0719, step time: 0.4925\n",
      "379/388, train_loss: 0.0800, step time: 0.4884\n",
      "380/388, train_loss: 0.2587, step time: 0.4943\n",
      "381/388, train_loss: 0.1949, step time: 0.4869\n",
      "382/388, train_loss: 0.0910, step time: 0.4972\n",
      "383/388, train_loss: 0.4624, step time: 0.4774\n",
      "384/388, train_loss: 0.3502, step time: 0.5099\n",
      "385/388, train_loss: 0.3082, step time: 0.4891\n",
      "386/388, train_loss: 0.2927, step time: 0.4772\n",
      "387/388, train_loss: 0.2045, step time: 0.4787\n",
      "388/388, train_loss: 0.0962, step time: 0.5324\n",
      "epoch 24 average loss: 0.2167\n",
      "current epoch: 24 current mean dice: 0.7418 tc: 0.8023 wt: 0.8931 et: 0.5300\n",
      "best mean dice: 0.7501 at epoch: 23\n",
      "time consuming of epoch 24 is: 298.8067\n",
      "----------\n",
      "epoch 25/300\n",
      "1/388, train_loss: 0.2723, step time: 0.4694\n",
      "2/388, train_loss: 0.0990, step time: 0.4826\n",
      "3/388, train_loss: 0.1893, step time: 0.7467\n",
      "4/388, train_loss: 0.2010, step time: 0.5710\n",
      "5/388, train_loss: 0.1374, step time: 0.5173\n",
      "6/388, train_loss: 0.3577, step time: 0.5001\n",
      "7/388, train_loss: 0.3149, step time: 0.4985\n",
      "8/388, train_loss: 0.1383, step time: 0.6191\n",
      "9/388, train_loss: 0.3277, step time: 0.5410\n",
      "10/388, train_loss: 0.1730, step time: 0.5254\n",
      "11/388, train_loss: 0.2618, step time: 0.4950\n",
      "12/388, train_loss: 0.2298, step time: 0.5060\n",
      "13/388, train_loss: 0.1268, step time: 0.4949\n",
      "14/388, train_loss: 0.0375, step time: 0.5442\n",
      "15/388, train_loss: 0.2747, step time: 0.5084\n",
      "16/388, train_loss: 0.0942, step time: 0.4896\n",
      "17/388, train_loss: 0.4608, step time: 0.4928\n",
      "18/388, train_loss: 0.0921, step time: 0.6385\n",
      "19/388, train_loss: 0.0639, step time: 0.5720\n",
      "20/388, train_loss: 0.4100, step time: 0.5433\n",
      "21/388, train_loss: 0.1356, step time: 0.5370\n",
      "22/388, train_loss: 0.3920, step time: 0.5258\n",
      "23/388, train_loss: 0.1955, step time: 0.4901\n",
      "24/388, train_loss: 0.0927, step time: 0.8568\n",
      "25/388, train_loss: 0.1671, step time: 0.5695\n",
      "26/388, train_loss: 0.3957, step time: 0.5098\n",
      "27/388, train_loss: 0.0721, step time: 0.4985\n",
      "28/388, train_loss: 0.1144, step time: 1.1739\n",
      "29/388, train_loss: 0.0853, step time: 0.5320\n",
      "30/388, train_loss: 0.1447, step time: 0.5063\n",
      "31/388, train_loss: 0.3603, step time: 0.5260\n",
      "32/388, train_loss: 0.0911, step time: 0.5700\n",
      "33/388, train_loss: 0.1597, step time: 0.5301\n",
      "34/388, train_loss: 0.1445, step time: 0.5140\n",
      "35/388, train_loss: 0.1156, step time: 0.5588\n",
      "36/388, train_loss: 0.0806, step time: 0.5236\n",
      "37/388, train_loss: 0.1579, step time: 0.5186\n",
      "38/388, train_loss: 0.1099, step time: 0.4999\n",
      "39/388, train_loss: 0.1979, step time: 0.5315\n",
      "40/388, train_loss: 0.1341, step time: 0.5137\n",
      "41/388, train_loss: 0.3404, step time: 0.5066\n",
      "42/388, train_loss: 0.1735, step time: 0.5089\n",
      "43/388, train_loss: 0.3571, step time: 0.5628\n",
      "44/388, train_loss: 0.1822, step time: 0.5489\n",
      "45/388, train_loss: 0.3037, step time: 0.5170\n",
      "46/388, train_loss: 0.2025, step time: 0.5010\n",
      "47/388, train_loss: 0.2153, step time: 0.5559\n",
      "48/388, train_loss: 0.1198, step time: 0.5319\n",
      "49/388, train_loss: 0.0592, step time: 0.5042\n",
      "50/388, train_loss: 0.1354, step time: 0.4916\n",
      "51/388, train_loss: 0.2491, step time: 1.2277\n",
      "52/388, train_loss: 0.1065, step time: 0.5376\n",
      "53/388, train_loss: 0.1841, step time: 0.5057\n",
      "54/388, train_loss: 0.0695, step time: 0.4979\n",
      "55/388, train_loss: 0.0765, step time: 0.4847\n",
      "56/388, train_loss: 0.1047, step time: 0.4985\n",
      "57/388, train_loss: 0.0609, step time: 0.5663\n",
      "58/388, train_loss: 0.2276, step time: 0.5463\n",
      "59/388, train_loss: 0.0657, step time: 0.5194\n",
      "60/388, train_loss: 0.1450, step time: 0.5123\n",
      "61/388, train_loss: 0.2863, step time: 0.5274\n",
      "62/388, train_loss: 0.1458, step time: 0.5258\n",
      "63/388, train_loss: 0.0967, step time: 0.5056\n",
      "64/388, train_loss: 0.2620, step time: 0.4866\n",
      "65/388, train_loss: 0.3560, step time: 0.9712\n",
      "66/388, train_loss: 0.2006, step time: 0.5418\n",
      "67/388, train_loss: 0.3153, step time: 0.5125\n",
      "68/388, train_loss: 0.1574, step time: 0.5030\n",
      "69/388, train_loss: 0.4107, step time: 0.5021\n",
      "70/388, train_loss: 0.5685, step time: 0.5084\n",
      "71/388, train_loss: 0.1392, step time: 0.8358\n",
      "72/388, train_loss: 0.1230, step time: 0.5726\n",
      "73/388, train_loss: 0.5740, step time: 0.5282\n",
      "74/388, train_loss: 0.1196, step time: 0.5027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/388, train_loss: 0.0907, step time: 0.4958\n",
      "76/388, train_loss: 0.2404, step time: 0.5327\n",
      "77/388, train_loss: 0.1191, step time: 0.5638\n",
      "78/388, train_loss: 0.2279, step time: 0.5348\n",
      "79/388, train_loss: 0.2827, step time: 0.4979\n",
      "80/388, train_loss: 0.0586, step time: 0.5317\n",
      "81/388, train_loss: 0.2220, step time: 0.5222\n",
      "82/388, train_loss: 0.3178, step time: 0.5105\n",
      "83/388, train_loss: 0.1248, step time: 0.5099\n",
      "84/388, train_loss: 0.0878, step time: 0.5011\n",
      "85/388, train_loss: 0.0337, step time: 0.4824\n",
      "86/388, train_loss: 0.2440, step time: 0.5156\n",
      "87/388, train_loss: 0.3172, step time: 0.4961\n",
      "88/388, train_loss: 0.0977, step time: 0.4920\n",
      "89/388, train_loss: 0.1627, step time: 0.4914\n",
      "90/388, train_loss: 0.0611, step time: 0.9313\n",
      "91/388, train_loss: 0.1595, step time: 0.5260\n",
      "92/388, train_loss: 0.4230, step time: 0.5163\n",
      "93/388, train_loss: 0.1056, step time: 0.4918\n",
      "94/388, train_loss: 0.4929, step time: 0.4931\n",
      "95/388, train_loss: 0.0954, step time: 0.5000\n",
      "96/388, train_loss: 0.0759, step time: 0.5602\n",
      "97/388, train_loss: 0.3072, step time: 0.5097\n",
      "98/388, train_loss: 0.1244, step time: 0.4866\n",
      "99/388, train_loss: 0.2948, step time: 1.1159\n",
      "100/388, train_loss: 0.2515, step time: 0.5443\n",
      "101/388, train_loss: 0.2566, step time: 0.5050\n",
      "102/388, train_loss: 0.2158, step time: 0.4954\n",
      "103/388, train_loss: 0.2180, step time: 0.5061\n",
      "104/388, train_loss: 0.0849, step time: 0.5041\n",
      "105/388, train_loss: 0.3024, step time: 0.4885\n",
      "106/388, train_loss: 0.1265, step time: 0.4897\n",
      "107/388, train_loss: 0.2983, step time: 0.4804\n",
      "108/388, train_loss: 0.2267, step time: 0.5235\n",
      "109/388, train_loss: 0.2583, step time: 0.4919\n",
      "110/388, train_loss: 0.7166, step time: 1.1497\n",
      "111/388, train_loss: 0.1386, step time: 0.5446\n",
      "112/388, train_loss: 0.2084, step time: 0.5134\n",
      "113/388, train_loss: 0.1933, step time: 0.5051\n",
      "114/388, train_loss: 0.0935, step time: 0.4869\n",
      "115/388, train_loss: 0.0992, step time: 0.4837\n",
      "116/388, train_loss: 0.2644, step time: 1.2191\n",
      "117/388, train_loss: 0.0842, step time: 0.5461\n",
      "118/388, train_loss: 0.0778, step time: 0.5115\n",
      "119/388, train_loss: 0.1610, step time: 0.4968\n",
      "120/388, train_loss: 0.1885, step time: 0.4851\n",
      "121/388, train_loss: 0.1806, step time: 0.4910\n",
      "122/388, train_loss: 0.3672, step time: 0.4980\n",
      "123/388, train_loss: 0.1127, step time: 1.1857\n",
      "124/388, train_loss: 0.1519, step time: 0.5368\n",
      "125/388, train_loss: 0.3862, step time: 0.5116\n",
      "126/388, train_loss: 0.1047, step time: 0.4869\n",
      "127/388, train_loss: 0.2150, step time: 0.4889\n",
      "128/388, train_loss: 0.1406, step time: 0.4904\n",
      "129/388, train_loss: 0.1251, step time: 0.4812\n",
      "130/388, train_loss: 0.0860, step time: 0.4799\n",
      "131/388, train_loss: 0.2733, step time: 0.4804\n",
      "132/388, train_loss: 0.1964, step time: 0.4941\n",
      "133/388, train_loss: 0.2763, step time: 0.4989\n",
      "134/388, train_loss: 0.3043, step time: 0.5039\n",
      "135/388, train_loss: 0.1209, step time: 0.4945\n",
      "136/388, train_loss: 0.1808, step time: 0.7210\n",
      "137/388, train_loss: 0.4032, step time: 0.5651\n",
      "138/388, train_loss: 0.1689, step time: 0.5373\n",
      "139/388, train_loss: 0.3974, step time: 0.5079\n",
      "140/388, train_loss: 0.2506, step time: 0.4994\n",
      "141/388, train_loss: 0.1101, step time: 0.4808\n",
      "142/388, train_loss: 0.3019, step time: 0.4917\n",
      "143/388, train_loss: 0.4093, step time: 0.4838\n",
      "144/388, train_loss: 0.1967, step time: 0.5157\n",
      "145/388, train_loss: 0.0929, step time: 0.4935\n",
      "146/388, train_loss: 0.4439, step time: 0.5832\n",
      "147/388, train_loss: 0.2771, step time: 0.6116\n",
      "148/388, train_loss: 0.1276, step time: 0.5441\n",
      "149/388, train_loss: 0.1322, step time: 0.5081\n",
      "150/388, train_loss: 0.1415, step time: 0.5049\n",
      "151/388, train_loss: 0.4295, step time: 0.5122\n",
      "152/388, train_loss: 0.4914, step time: 0.5105\n",
      "153/388, train_loss: 0.0935, step time: 0.5034\n",
      "154/388, train_loss: 0.3561, step time: 0.5801\n",
      "155/388, train_loss: 0.0885, step time: 0.5555\n",
      "156/388, train_loss: 0.2338, step time: 0.5198\n",
      "157/388, train_loss: 0.3930, step time: 0.4946\n",
      "158/388, train_loss: 0.3098, step time: 0.4984\n",
      "159/388, train_loss: 0.0813, step time: 0.4984\n",
      "160/388, train_loss: 0.1600, step time: 0.5021\n",
      "161/388, train_loss: 0.1077, step time: 0.4905\n",
      "162/388, train_loss: 0.1977, step time: 0.4899\n",
      "163/388, train_loss: 0.1785, step time: 0.5335\n",
      "164/388, train_loss: 0.2002, step time: 0.5106\n",
      "165/388, train_loss: 0.1557, step time: 0.5102\n",
      "166/388, train_loss: 0.3399, step time: 0.5397\n",
      "167/388, train_loss: 0.2529, step time: 0.4978\n",
      "168/388, train_loss: 0.1636, step time: 0.4972\n",
      "169/388, train_loss: 0.8303, step time: 0.4812\n",
      "170/388, train_loss: 0.0943, step time: 0.4995\n",
      "171/388, train_loss: 0.1408, step time: 0.4986\n",
      "172/388, train_loss: 0.6204, step time: 0.5013\n",
      "173/388, train_loss: 0.4701, step time: 0.4937\n",
      "174/388, train_loss: 0.0766, step time: 1.0069\n",
      "175/388, train_loss: 0.6806, step time: 0.5557\n",
      "176/388, train_loss: 0.0963, step time: 0.5292\n",
      "177/388, train_loss: 0.4733, step time: 0.5110\n",
      "178/388, train_loss: 0.5270, step time: 0.4943\n",
      "179/388, train_loss: 0.2729, step time: 1.1556\n",
      "180/388, train_loss: 0.3102, step time: 0.5463\n",
      "181/388, train_loss: 0.6668, step time: 0.5166\n",
      "182/388, train_loss: 0.1017, step time: 0.4988\n",
      "183/388, train_loss: 0.0720, step time: 0.4890\n",
      "184/388, train_loss: 0.0836, step time: 0.4934\n",
      "185/388, train_loss: 0.1219, step time: 0.4775\n",
      "186/388, train_loss: 0.2576, step time: 0.6513\n",
      "187/388, train_loss: 0.1420, step time: 0.5775\n",
      "188/388, train_loss: 0.2412, step time: 0.5556\n",
      "189/388, train_loss: 0.2434, step time: 0.5193\n",
      "190/388, train_loss: 0.5326, step time: 0.4939\n",
      "191/388, train_loss: 0.1170, step time: 0.4848\n",
      "192/388, train_loss: 0.2168, step time: 0.4895\n",
      "193/388, train_loss: 0.2693, step time: 0.4846\n",
      "194/388, train_loss: 0.2125, step time: 0.5119\n",
      "195/388, train_loss: 0.1268, step time: 0.5060\n",
      "196/388, train_loss: 0.2984, step time: 0.4984\n",
      "197/388, train_loss: 0.1823, step time: 0.9865\n",
      "198/388, train_loss: 0.1886, step time: 0.5326\n",
      "199/388, train_loss: 0.2454, step time: 0.5085\n",
      "200/388, train_loss: 0.2392, step time: 0.4952\n",
      "201/388, train_loss: 0.2823, step time: 0.4839\n",
      "202/388, train_loss: 0.2319, step time: 0.4861\n",
      "203/388, train_loss: 0.2361, step time: 0.5534\n",
      "204/388, train_loss: 0.2054, step time: 0.5312\n",
      "205/388, train_loss: 0.2285, step time: 0.5107\n",
      "206/388, train_loss: 0.1455, step time: 0.5129\n",
      "207/388, train_loss: 0.0895, step time: 0.5066\n",
      "208/388, train_loss: 0.2040, step time: 0.4803\n",
      "209/388, train_loss: 0.1566, step time: 1.0289\n",
      "210/388, train_loss: 0.1939, step time: 0.5432\n",
      "211/388, train_loss: 0.1479, step time: 0.5083\n",
      "212/388, train_loss: 0.2115, step time: 0.4849\n",
      "213/388, train_loss: 0.0931, step time: 0.4957\n",
      "214/388, train_loss: 0.5161, step time: 0.4855\n",
      "215/388, train_loss: 0.0478, step time: 0.4755\n",
      "216/388, train_loss: 0.1561, step time: 0.4966\n",
      "217/388, train_loss: 0.0934, step time: 0.5322\n",
      "218/388, train_loss: 0.1180, step time: 0.5148\n",
      "219/388, train_loss: 0.6352, step time: 0.4933\n",
      "220/388, train_loss: 0.2291, step time: 0.5644\n",
      "221/388, train_loss: 0.1249, step time: 0.5090\n",
      "222/388, train_loss: 0.1004, step time: 0.4947\n",
      "223/388, train_loss: 0.2735, step time: 0.4836\n",
      "224/388, train_loss: 0.3114, step time: 0.4804\n",
      "225/388, train_loss: 0.1385, step time: 0.9391\n",
      "226/388, train_loss: 0.1192, step time: 0.5644\n",
      "227/388, train_loss: 0.1421, step time: 0.5066\n",
      "228/388, train_loss: 0.0705, step time: 0.4804\n",
      "229/388, train_loss: 0.6318, step time: 0.4916\n",
      "230/388, train_loss: 0.4396, step time: 1.1316\n",
      "231/388, train_loss: 0.1560, step time: 0.5176\n",
      "232/388, train_loss: 0.2246, step time: 0.5015\n",
      "233/388, train_loss: 0.3939, step time: 0.4849\n",
      "234/388, train_loss: 0.3685, step time: 0.4920\n",
      "235/388, train_loss: 0.1082, step time: 0.4823\n",
      "236/388, train_loss: 0.2150, step time: 0.4722\n",
      "237/388, train_loss: 0.4613, step time: 0.5072\n",
      "238/388, train_loss: 0.1451, step time: 0.4962\n",
      "239/388, train_loss: 0.1834, step time: 0.5007\n",
      "240/388, train_loss: 0.2200, step time: 1.0975\n",
      "241/388, train_loss: 0.1540, step time: 0.5238\n",
      "242/388, train_loss: 0.2360, step time: 0.5126\n",
      "243/388, train_loss: 0.1266, step time: 0.4896\n",
      "244/388, train_loss: 0.2456, step time: 0.4978\n",
      "245/388, train_loss: 0.2308, step time: 0.4997\n",
      "246/388, train_loss: 0.1781, step time: 0.5548\n",
      "247/388, train_loss: 0.0995, step time: 0.6443\n",
      "248/388, train_loss: 0.3957, step time: 0.5225\n",
      "249/388, train_loss: 0.2385, step time: 0.5027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/388, train_loss: 0.2330, step time: 0.5856\n",
      "251/388, train_loss: 0.1858, step time: 0.5385\n",
      "252/388, train_loss: 0.1309, step time: 0.5192\n",
      "253/388, train_loss: 0.3247, step time: 0.4978\n",
      "254/388, train_loss: 0.5452, step time: 0.4935\n",
      "255/388, train_loss: 0.2489, step time: 0.4931\n",
      "256/388, train_loss: 0.6149, step time: 0.7215\n",
      "257/388, train_loss: 0.1069, step time: 0.5447\n",
      "258/388, train_loss: 0.2959, step time: 0.5097\n",
      "259/388, train_loss: 0.2677, step time: 0.4946\n",
      "260/388, train_loss: 0.3052, step time: 0.4971\n",
      "261/388, train_loss: 0.1512, step time: 0.4808\n",
      "262/388, train_loss: 0.3861, step time: 0.4828\n",
      "263/388, train_loss: 0.1482, step time: 0.4883\n",
      "264/388, train_loss: 0.1938, step time: 0.4886\n",
      "265/388, train_loss: 0.1836, step time: 0.7724\n",
      "266/388, train_loss: 0.0672, step time: 0.5469\n",
      "267/388, train_loss: 0.1592, step time: 0.5107\n",
      "268/388, train_loss: 0.3468, step time: 0.5021\n",
      "269/388, train_loss: 0.1817, step time: 0.4914\n",
      "270/388, train_loss: 0.0981, step time: 0.4949\n",
      "271/388, train_loss: 0.0943, step time: 0.4819\n",
      "272/388, train_loss: 0.1686, step time: 0.4918\n",
      "273/388, train_loss: 0.2688, step time: 0.4749\n",
      "274/388, train_loss: 0.2086, step time: 0.4877\n",
      "275/388, train_loss: 0.2396, step time: 1.1272\n",
      "276/388, train_loss: 0.1974, step time: 0.5448\n",
      "277/388, train_loss: 0.2674, step time: 0.5170\n",
      "278/388, train_loss: 0.2651, step time: 0.5012\n",
      "279/388, train_loss: 0.1592, step time: 0.4990\n",
      "280/388, train_loss: 0.0954, step time: 0.4834\n",
      "281/388, train_loss: 0.2736, step time: 0.4783\n",
      "282/388, train_loss: 0.1678, step time: 0.4819\n",
      "283/388, train_loss: 0.3124, step time: 0.5178\n",
      "284/388, train_loss: 0.0895, step time: 0.5077\n",
      "285/388, train_loss: 0.5862, step time: 0.4837\n",
      "286/388, train_loss: 0.1235, step time: 0.5014\n",
      "287/388, train_loss: 0.2028, step time: 0.4931\n",
      "288/388, train_loss: 0.1113, step time: 0.4841\n",
      "289/388, train_loss: 0.1594, step time: 0.5015\n",
      "290/388, train_loss: 0.2116, step time: 0.5046\n",
      "291/388, train_loss: 0.1759, step time: 0.5191\n",
      "292/388, train_loss: 0.2871, step time: 0.5775\n",
      "293/388, train_loss: 0.3220, step time: 0.5345\n",
      "294/388, train_loss: 0.2650, step time: 0.5108\n",
      "295/388, train_loss: 0.2461, step time: 0.4899\n",
      "296/388, train_loss: 0.2414, step time: 0.5294\n",
      "297/388, train_loss: 0.3280, step time: 0.5155\n",
      "298/388, train_loss: 0.8241, step time: 0.5915\n",
      "299/388, train_loss: 0.1519, step time: 0.5333\n",
      "300/388, train_loss: 0.4202, step time: 0.5335\n",
      "301/388, train_loss: 0.2330, step time: 0.5738\n",
      "302/388, train_loss: 0.0918, step time: 0.5230\n",
      "303/388, train_loss: 0.1973, step time: 0.5169\n",
      "304/388, train_loss: 0.4183, step time: 0.4962\n",
      "305/388, train_loss: 0.1333, step time: 0.4905\n",
      "306/388, train_loss: 0.2520, step time: 0.4800\n",
      "307/388, train_loss: 0.3548, step time: 0.4937\n",
      "308/388, train_loss: 0.2006, step time: 0.5019\n",
      "309/388, train_loss: 0.2177, step time: 0.5016\n",
      "310/388, train_loss: 0.1375, step time: 0.4984\n",
      "311/388, train_loss: 0.2668, step time: 0.5036\n",
      "312/388, train_loss: 0.1021, step time: 0.5844\n",
      "313/388, train_loss: 0.1781, step time: 0.5409\n",
      "314/388, train_loss: 0.3253, step time: 0.5078\n",
      "315/388, train_loss: 0.2963, step time: 0.5005\n",
      "316/388, train_loss: 0.2485, step time: 0.4860\n",
      "317/388, train_loss: 0.1260, step time: 0.4912\n",
      "318/388, train_loss: 0.3947, step time: 0.4766\n",
      "319/388, train_loss: 0.1599, step time: 0.5116\n",
      "320/388, train_loss: 0.0846, step time: 0.5045\n",
      "321/388, train_loss: 0.2346, step time: 0.5186\n",
      "322/388, train_loss: 0.2537, step time: 0.6269\n",
      "323/388, train_loss: 0.1641, step time: 0.5461\n",
      "324/388, train_loss: 0.1148, step time: 0.5262\n",
      "325/388, train_loss: 0.3252, step time: 0.5096\n",
      "326/388, train_loss: 0.1130, step time: 0.4951\n",
      "327/388, train_loss: 0.1263, step time: 0.4820\n",
      "328/388, train_loss: 0.5664, step time: 1.0761\n",
      "329/388, train_loss: 0.0832, step time: 0.5319\n",
      "330/388, train_loss: 0.0700, step time: 0.4964\n",
      "331/388, train_loss: 0.2236, step time: 0.4896\n",
      "332/388, train_loss: 0.1254, step time: 0.4974\n",
      "333/388, train_loss: 0.1202, step time: 0.4838\n",
      "334/388, train_loss: 0.1429, step time: 0.4882\n",
      "335/388, train_loss: 0.2905, step time: 0.4821\n",
      "336/388, train_loss: 0.1779, step time: 0.4942\n",
      "337/388, train_loss: 0.0966, step time: 0.4865\n",
      "338/388, train_loss: 0.2483, step time: 0.5219\n",
      "339/388, train_loss: 0.1794, step time: 0.4922\n",
      "340/388, train_loss: 0.0518, step time: 0.4968\n",
      "341/388, train_loss: 0.2156, step time: 0.4985\n",
      "342/388, train_loss: 0.0560, step time: 0.4903\n",
      "343/388, train_loss: 0.0979, step time: 0.4786\n",
      "344/388, train_loss: 0.2938, step time: 0.6454\n",
      "345/388, train_loss: 0.2059, step time: 0.5450\n",
      "346/388, train_loss: 0.1562, step time: 0.5202\n",
      "347/388, train_loss: 0.1460, step time: 0.4944\n",
      "348/388, train_loss: 0.4939, step time: 0.4946\n",
      "349/388, train_loss: 0.2894, step time: 0.4885\n",
      "350/388, train_loss: 0.5353, step time: 0.4936\n",
      "351/388, train_loss: 0.1948, step time: 0.9799\n",
      "352/388, train_loss: 0.0946, step time: 0.5426\n",
      "353/388, train_loss: 0.1670, step time: 0.5014\n",
      "354/388, train_loss: 0.1664, step time: 0.4938\n",
      "355/388, train_loss: 0.1844, step time: 0.5028\n",
      "356/388, train_loss: 0.2805, step time: 0.4872\n",
      "357/388, train_loss: 0.2441, step time: 0.4973\n",
      "358/388, train_loss: 0.1950, step time: 1.0501\n",
      "359/388, train_loss: 0.2301, step time: 0.5294\n",
      "360/388, train_loss: 0.1147, step time: 0.5077\n",
      "361/388, train_loss: 0.2874, step time: 0.4932\n",
      "362/388, train_loss: 0.1641, step time: 0.4856\n",
      "363/388, train_loss: 0.0986, step time: 0.4916\n",
      "364/388, train_loss: 0.2965, step time: 0.4757\n",
      "365/388, train_loss: 0.1046, step time: 1.0099\n",
      "366/388, train_loss: 0.1093, step time: 0.5389\n",
      "367/388, train_loss: 0.1878, step time: 0.5029\n",
      "368/388, train_loss: 0.1674, step time: 0.4784\n",
      "369/388, train_loss: 0.1428, step time: 0.4776\n",
      "370/388, train_loss: 0.2229, step time: 0.4868\n",
      "371/388, train_loss: 0.2484, step time: 0.4808\n",
      "372/388, train_loss: 0.1426, step time: 0.4899\n",
      "373/388, train_loss: 0.2796, step time: 0.4804\n",
      "374/388, train_loss: 0.1146, step time: 0.4840\n",
      "375/388, train_loss: 0.3207, step time: 0.4725\n",
      "376/388, train_loss: 0.1229, step time: 0.5871\n",
      "377/388, train_loss: 0.1730, step time: 0.5410\n",
      "378/388, train_loss: 0.3021, step time: 0.5027\n",
      "379/388, train_loss: 0.1052, step time: 0.5041\n",
      "380/388, train_loss: 0.2995, step time: 0.4896\n",
      "381/388, train_loss: 0.1459, step time: 1.0861\n",
      "382/388, train_loss: 0.0708, step time: 0.5189\n",
      "383/388, train_loss: 0.3550, step time: 0.4981\n",
      "384/388, train_loss: 0.2154, step time: 0.4802\n",
      "385/388, train_loss: 0.4084, step time: 0.4766\n",
      "386/388, train_loss: 0.1881, step time: 0.4789\n",
      "387/388, train_loss: 0.2453, step time: 0.4731\n",
      "388/388, train_loss: 0.1568, step time: 0.5214\n",
      "epoch 25 average loss: 0.2211\n",
      "current epoch: 25 current mean dice: 0.6964 tc: 0.7302 wt: 0.8346 et: 0.5245\n",
      "best mean dice: 0.7501 at epoch: 23\n",
      "time consuming of epoch 25 is: 299.5125\n",
      "----------\n",
      "epoch 26/300\n",
      "1/388, train_loss: 0.3046, step time: 0.4782\n",
      "2/388, train_loss: 0.4397, step time: 0.4798\n",
      "3/388, train_loss: 0.0840, step time: 1.0062\n",
      "4/388, train_loss: 0.1750, step time: 0.5564\n",
      "5/388, train_loss: 0.1270, step time: 0.5237\n",
      "6/388, train_loss: 0.2800, step time: 0.5049\n",
      "7/388, train_loss: 0.0707, step time: 0.4864\n",
      "8/388, train_loss: 0.2679, step time: 1.0911\n",
      "9/388, train_loss: 0.6326, step time: 0.5398\n",
      "10/388, train_loss: 0.2185, step time: 0.5008\n",
      "11/388, train_loss: 0.0997, step time: 0.4919\n",
      "12/388, train_loss: 0.3903, step time: 0.4961\n",
      "13/388, train_loss: 0.1895, step time: 0.4825\n",
      "14/388, train_loss: 0.3173, step time: 0.4909\n",
      "15/388, train_loss: 0.2199, step time: 1.0238\n",
      "16/388, train_loss: 0.1489, step time: 0.5290\n",
      "17/388, train_loss: 0.2961, step time: 0.5089\n",
      "18/388, train_loss: 0.3374, step time: 0.5252\n",
      "19/388, train_loss: 0.1448, step time: 0.5063\n",
      "20/388, train_loss: 0.2688, step time: 0.4911\n",
      "21/388, train_loss: 0.2626, step time: 1.1511\n",
      "22/388, train_loss: 0.2941, step time: 0.5433\n",
      "23/388, train_loss: 0.4162, step time: 0.5145\n",
      "24/388, train_loss: 0.1206, step time: 0.4963\n",
      "25/388, train_loss: 0.2341, step time: 0.4974\n",
      "26/388, train_loss: 0.6320, step time: 0.4789\n",
      "27/388, train_loss: 0.1189, step time: 0.4814\n",
      "28/388, train_loss: 0.0808, step time: 0.4897\n",
      "29/388, train_loss: 0.2499, step time: 0.4887\n",
      "30/388, train_loss: 0.1149, step time: 1.0591\n",
      "31/388, train_loss: 0.1199, step time: 0.5384\n",
      "32/388, train_loss: 0.1326, step time: 0.5008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/388, train_loss: 0.1105, step time: 0.4950\n",
      "34/388, train_loss: 0.3773, step time: 0.4890\n",
      "35/388, train_loss: 0.1156, step time: 0.8816\n",
      "36/388, train_loss: 0.5146, step time: 0.5254\n",
      "37/388, train_loss: 0.1967, step time: 0.5026\n",
      "38/388, train_loss: 0.4762, step time: 0.5396\n",
      "39/388, train_loss: 0.2129, step time: 0.5144\n",
      "40/388, train_loss: 0.1031, step time: 0.5018\n",
      "41/388, train_loss: 0.1215, step time: 0.5148\n",
      "42/388, train_loss: 0.1463, step time: 0.4978\n",
      "43/388, train_loss: 0.2468, step time: 0.4817\n",
      "44/388, train_loss: 0.3248, step time: 0.4886\n",
      "45/388, train_loss: 0.1215, step time: 0.4998\n",
      "46/388, train_loss: 0.2072, step time: 0.4916\n",
      "47/388, train_loss: 0.1818, step time: 0.4891\n",
      "48/388, train_loss: 0.3164, step time: 0.4729\n",
      "49/388, train_loss: 0.2210, step time: 0.6132\n",
      "50/388, train_loss: 0.2450, step time: 0.5628\n",
      "51/388, train_loss: 0.1341, step time: 0.5224\n",
      "52/388, train_loss: 0.3222, step time: 0.5095\n",
      "53/388, train_loss: 0.3167, step time: 0.8127\n",
      "54/388, train_loss: 0.1088, step time: 0.5496\n",
      "55/388, train_loss: 0.1505, step time: 0.5190\n",
      "56/388, train_loss: 0.1340, step time: 0.5022\n",
      "57/388, train_loss: 0.2327, step time: 0.4870\n",
      "58/388, train_loss: 0.2699, step time: 0.4825\n",
      "59/388, train_loss: 0.6466, step time: 0.5106\n",
      "60/388, train_loss: 0.1299, step time: 0.4952\n",
      "61/388, train_loss: 0.2103, step time: 0.5099\n",
      "62/388, train_loss: 0.5972, step time: 0.5643\n",
      "63/388, train_loss: 0.1828, step time: 0.5275\n",
      "64/388, train_loss: 0.6516, step time: 0.5478\n",
      "65/388, train_loss: 0.3872, step time: 0.5761\n",
      "66/388, train_loss: 0.2950, step time: 0.5334\n",
      "67/388, train_loss: 0.1836, step time: 0.5053\n",
      "68/388, train_loss: 0.2462, step time: 0.4924\n",
      "69/388, train_loss: 0.2427, step time: 0.4956\n",
      "70/388, train_loss: 0.3233, step time: 0.4818\n",
      "71/388, train_loss: 0.2626, step time: 0.4864\n",
      "72/388, train_loss: 0.1138, step time: 0.5040\n",
      "73/388, train_loss: 0.1971, step time: 0.4892\n",
      "74/388, train_loss: 0.3106, step time: 0.4899\n",
      "75/388, train_loss: 0.1793, step time: 1.1121\n",
      "76/388, train_loss: 0.0696, step time: 0.5310\n",
      "77/388, train_loss: 0.3518, step time: 0.5032\n",
      "78/388, train_loss: 0.2933, step time: 0.4926\n",
      "79/388, train_loss: 0.4809, step time: 0.4946\n",
      "80/388, train_loss: 0.0899, step time: 0.4855\n",
      "81/388, train_loss: 0.2926, step time: 0.5148\n",
      "82/388, train_loss: 0.1874, step time: 0.9329\n",
      "83/388, train_loss: 0.2747, step time: 0.5350\n",
      "84/388, train_loss: 0.4362, step time: 0.5067\n",
      "85/388, train_loss: 0.2670, step time: 0.4932\n",
      "86/388, train_loss: 0.1643, step time: 0.4827\n",
      "87/388, train_loss: 0.1329, step time: 0.4891\n",
      "88/388, train_loss: 0.0773, step time: 0.9411\n",
      "89/388, train_loss: 0.2250, step time: 0.5398\n",
      "90/388, train_loss: 0.0395, step time: 0.5110\n",
      "91/388, train_loss: 0.4981, step time: 0.4907\n",
      "92/388, train_loss: 0.1436, step time: 0.4965\n",
      "93/388, train_loss: 0.2973, step time: 0.4947\n",
      "94/388, train_loss: 0.0628, step time: 0.7486\n",
      "95/388, train_loss: 0.2293, step time: 0.5473\n",
      "96/388, train_loss: 0.4413, step time: 0.5104\n",
      "97/388, train_loss: 0.2119, step time: 0.4947\n",
      "98/388, train_loss: 0.0897, step time: 0.4959\n",
      "99/388, train_loss: 0.1705, step time: 0.4880\n",
      "100/388, train_loss: 0.0689, step time: 0.4796\n",
      "101/388, train_loss: 0.1461, step time: 0.4974\n",
      "102/388, train_loss: 0.2800, step time: 0.4934\n",
      "103/388, train_loss: 0.2765, step time: 0.9542\n",
      "104/388, train_loss: 0.3449, step time: 0.5383\n",
      "105/388, train_loss: 0.5746, step time: 0.5004\n",
      "106/388, train_loss: 0.2739, step time: 0.4840\n",
      "107/388, train_loss: 0.2693, step time: 0.4945\n",
      "108/388, train_loss: 0.3379, step time: 0.4808\n",
      "109/388, train_loss: 0.1579, step time: 0.5402\n",
      "110/388, train_loss: 0.1273, step time: 0.5132\n",
      "111/388, train_loss: 0.1489, step time: 0.5187\n",
      "112/388, train_loss: 0.2946, step time: 0.4988\n",
      "113/388, train_loss: 0.1385, step time: 0.6729\n",
      "114/388, train_loss: 0.1627, step time: 0.5438\n",
      "115/388, train_loss: 0.1573, step time: 0.5228\n",
      "116/388, train_loss: 0.2086, step time: 0.5012\n",
      "117/388, train_loss: 0.1779, step time: 0.4891\n",
      "118/388, train_loss: 0.2457, step time: 0.4932\n",
      "119/388, train_loss: 0.1276, step time: 0.4812\n",
      "120/388, train_loss: 0.3143, step time: 0.4851\n",
      "121/388, train_loss: 0.3285, step time: 0.4973\n",
      "122/388, train_loss: 0.1073, step time: 0.4918\n",
      "123/388, train_loss: 0.2592, step time: 0.4758\n",
      "124/388, train_loss: 0.1905, step time: 0.4989\n",
      "125/388, train_loss: 0.1764, step time: 0.4986\n",
      "126/388, train_loss: 0.0878, step time: 0.4816\n",
      "127/388, train_loss: 0.1607, step time: 0.5060\n",
      "128/388, train_loss: 0.2080, step time: 0.4873\n",
      "129/388, train_loss: 0.5921, step time: 0.5117\n",
      "130/388, train_loss: 0.1029, step time: 0.4920\n",
      "131/388, train_loss: 0.1178, step time: 0.4952\n",
      "132/388, train_loss: 0.0696, step time: 0.4976\n",
      "133/388, train_loss: 0.1511, step time: 0.4938\n",
      "134/388, train_loss: 0.1715, step time: 0.4944\n",
      "135/388, train_loss: 0.1620, step time: 0.4859\n",
      "136/388, train_loss: 0.2460, step time: 0.4930\n",
      "137/388, train_loss: 0.2013, step time: 1.0323\n",
      "138/388, train_loss: 0.0915, step time: 0.5469\n",
      "139/388, train_loss: 0.2258, step time: 0.5217\n",
      "140/388, train_loss: 0.3175, step time: 0.5001\n",
      "141/388, train_loss: 0.5994, step time: 0.4840\n",
      "142/388, train_loss: 0.1144, step time: 0.4801\n",
      "143/388, train_loss: 0.0983, step time: 0.4785\n",
      "144/388, train_loss: 0.2730, step time: 0.4827\n",
      "145/388, train_loss: 0.2062, step time: 0.4958\n",
      "146/388, train_loss: 0.0691, step time: 0.9531\n",
      "147/388, train_loss: 0.1627, step time: 0.5257\n",
      "148/388, train_loss: 0.1655, step time: 0.5159\n",
      "149/388, train_loss: 0.1296, step time: 0.4932\n",
      "150/388, train_loss: 0.3008, step time: 0.5017\n",
      "151/388, train_loss: 0.3331, step time: 0.5015\n",
      "152/388, train_loss: 0.0958, step time: 0.6455\n",
      "153/388, train_loss: 0.1210, step time: 0.5485\n",
      "154/388, train_loss: 0.0919, step time: 0.5121\n",
      "155/388, train_loss: 0.4853, step time: 0.4930\n",
      "156/388, train_loss: 0.1088, step time: 0.4978\n",
      "157/388, train_loss: 0.4148, step time: 0.4884\n",
      "158/388, train_loss: 0.4207, step time: 0.7690\n",
      "159/388, train_loss: 0.2730, step time: 0.5545\n",
      "160/388, train_loss: 0.2497, step time: 0.5228\n",
      "161/388, train_loss: 0.2401, step time: 0.5033\n",
      "162/388, train_loss: 0.1897, step time: 0.4863\n",
      "163/388, train_loss: 0.1504, step time: 1.1723\n",
      "164/388, train_loss: 0.1177, step time: 0.5413\n",
      "165/388, train_loss: 0.2139, step time: 0.5126\n",
      "166/388, train_loss: 0.1026, step time: 0.4917\n",
      "167/388, train_loss: 0.2077, step time: 0.4915\n",
      "168/388, train_loss: 0.1111, step time: 0.4945\n",
      "169/388, train_loss: 0.3702, step time: 1.1231\n",
      "170/388, train_loss: 0.1897, step time: 0.5512\n",
      "171/388, train_loss: 0.1782, step time: 0.5152\n",
      "172/388, train_loss: 0.2775, step time: 0.5042\n",
      "173/388, train_loss: 0.4153, step time: 0.4980\n",
      "174/388, train_loss: 0.6158, step time: 0.4874\n",
      "175/388, train_loss: 0.0969, step time: 1.0977\n",
      "176/388, train_loss: 0.2955, step time: 0.5336\n",
      "177/388, train_loss: 0.2501, step time: 0.5045\n",
      "178/388, train_loss: 0.0950, step time: 0.4981\n",
      "179/388, train_loss: 0.1431, step time: 0.4834\n",
      "180/388, train_loss: 0.1102, step time: 0.4848\n",
      "181/388, train_loss: 0.0870, step time: 0.4917\n",
      "182/388, train_loss: 0.3822, step time: 0.4836\n",
      "183/388, train_loss: 0.0769, step time: 0.4914\n",
      "184/388, train_loss: 0.3686, step time: 0.4812\n",
      "185/388, train_loss: 0.2687, step time: 0.4956\n",
      "186/388, train_loss: 0.1272, step time: 0.4831\n",
      "187/388, train_loss: 0.0699, step time: 0.4964\n",
      "188/388, train_loss: 0.1548, step time: 0.4923\n",
      "189/388, train_loss: 0.1301, step time: 0.6418\n",
      "190/388, train_loss: 0.3544, step time: 0.5528\n",
      "191/388, train_loss: 0.0625, step time: 0.5121\n",
      "192/388, train_loss: 0.5257, step time: 0.5125\n",
      "193/388, train_loss: 0.1048, step time: 0.4980\n",
      "194/388, train_loss: 0.4090, step time: 0.5108\n",
      "195/388, train_loss: 0.2948, step time: 0.4947\n",
      "196/388, train_loss: 0.1778, step time: 0.4990\n",
      "197/388, train_loss: 0.1060, step time: 0.5201\n",
      "198/388, train_loss: 0.0690, step time: 0.5100\n",
      "199/388, train_loss: 0.1754, step time: 0.4930\n",
      "200/388, train_loss: 0.0913, step time: 0.4933\n",
      "201/388, train_loss: 0.1926, step time: 0.4924\n",
      "202/388, train_loss: 0.1594, step time: 0.4944\n",
      "203/388, train_loss: 0.2691, step time: 1.1509\n",
      "204/388, train_loss: 0.1806, step time: 0.5416\n",
      "205/388, train_loss: 0.1358, step time: 0.5108\n",
      "206/388, train_loss: 0.4410, step time: 0.4964\n",
      "207/388, train_loss: 0.1449, step time: 0.4864\n",
      "208/388, train_loss: 0.0626, step time: 0.5141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/388, train_loss: 0.4282, step time: 0.5011\n",
      "210/388, train_loss: 0.1078, step time: 0.5010\n",
      "211/388, train_loss: 0.1835, step time: 0.4845\n",
      "212/388, train_loss: 0.2637, step time: 0.4846\n",
      "213/388, train_loss: 0.2662, step time: 0.4939\n",
      "214/388, train_loss: 0.2072, step time: 1.1346\n",
      "215/388, train_loss: 0.1547, step time: 0.5389\n",
      "216/388, train_loss: 0.0741, step time: 0.5114\n",
      "217/388, train_loss: 0.1632, step time: 0.5013\n",
      "218/388, train_loss: 0.2573, step time: 0.4966\n",
      "219/388, train_loss: 0.1284, step time: 0.4802\n",
      "220/388, train_loss: 0.2618, step time: 0.5435\n",
      "221/388, train_loss: 0.2539, step time: 0.5177\n",
      "222/388, train_loss: 0.1935, step time: 0.5025\n",
      "223/388, train_loss: 0.0732, step time: 0.5476\n",
      "224/388, train_loss: 0.3579, step time: 0.5249\n",
      "225/388, train_loss: 0.1440, step time: 0.5176\n",
      "226/388, train_loss: 0.1977, step time: 0.5598\n",
      "227/388, train_loss: 0.1356, step time: 0.5353\n",
      "228/388, train_loss: 0.2309, step time: 0.5056\n",
      "229/388, train_loss: 0.2413, step time: 0.5014\n",
      "230/388, train_loss: 0.2023, step time: 0.4924\n",
      "231/388, train_loss: 0.1038, step time: 0.5223\n",
      "232/388, train_loss: 0.1282, step time: 0.5062\n",
      "233/388, train_loss: 0.1291, step time: 0.4938\n",
      "234/388, train_loss: 0.1243, step time: 0.4961\n",
      "235/388, train_loss: 0.1135, step time: 0.4988\n",
      "236/388, train_loss: 0.3017, step time: 0.4938\n",
      "237/388, train_loss: 0.0880, step time: 0.4990\n",
      "238/388, train_loss: 0.1092, step time: 0.5006\n",
      "239/388, train_loss: 0.1087, step time: 0.4916\n",
      "240/388, train_loss: 0.0872, step time: 0.5187\n",
      "241/388, train_loss: 0.1554, step time: 0.5060\n",
      "242/388, train_loss: 0.1878, step time: 0.4950\n",
      "243/388, train_loss: 0.1325, step time: 1.1334\n",
      "244/388, train_loss: 0.0640, step time: 0.5601\n",
      "245/388, train_loss: 0.2010, step time: 0.5339\n",
      "246/388, train_loss: 0.0588, step time: 0.5018\n",
      "247/388, train_loss: 0.1324, step time: 0.4977\n",
      "248/388, train_loss: 0.2070, step time: 0.4946\n",
      "249/388, train_loss: 0.1440, step time: 0.4870\n",
      "250/388, train_loss: 0.1343, step time: 0.5015\n",
      "251/388, train_loss: 0.0836, step time: 0.4849\n",
      "252/388, train_loss: 0.1170, step time: 0.5104\n",
      "253/388, train_loss: 0.1638, step time: 0.4977\n",
      "254/388, train_loss: 0.1727, step time: 0.7068\n",
      "255/388, train_loss: 0.1775, step time: 0.5579\n",
      "256/388, train_loss: 0.2678, step time: 0.5284\n",
      "257/388, train_loss: 0.0949, step time: 0.5196\n",
      "258/388, train_loss: 0.0816, step time: 0.5046\n",
      "259/388, train_loss: 0.0890, step time: 1.1161\n",
      "260/388, train_loss: 0.2861, step time: 0.5085\n",
      "261/388, train_loss: 0.2484, step time: 0.4896\n",
      "262/388, train_loss: 0.1666, step time: 0.4765\n",
      "263/388, train_loss: 0.2310, step time: 0.4863\n",
      "264/388, train_loss: 0.1130, step time: 0.4855\n",
      "265/388, train_loss: 0.0788, step time: 1.1229\n",
      "266/388, train_loss: 0.0970, step time: 0.5444\n",
      "267/388, train_loss: 0.1713, step time: 0.5065\n",
      "268/388, train_loss: 0.1888, step time: 0.4982\n",
      "269/388, train_loss: 0.1008, step time: 0.4880\n",
      "270/388, train_loss: 0.1774, step time: 0.4881\n",
      "271/388, train_loss: 0.1633, step time: 1.0463\n",
      "272/388, train_loss: 0.1016, step time: 0.5407\n",
      "273/388, train_loss: 0.1308, step time: 0.5269\n",
      "274/388, train_loss: 0.1859, step time: 0.4922\n",
      "275/388, train_loss: 0.1306, step time: 0.4871\n",
      "276/388, train_loss: 0.1038, step time: 0.4967\n",
      "277/388, train_loss: 0.1392, step time: 0.4758\n",
      "278/388, train_loss: 0.2269, step time: 0.8078\n",
      "279/388, train_loss: 0.2359, step time: 0.5390\n",
      "280/388, train_loss: 0.1754, step time: 0.5025\n",
      "281/388, train_loss: 0.0507, step time: 0.4999\n",
      "282/388, train_loss: 0.1014, step time: 0.5287\n",
      "283/388, train_loss: 0.1666, step time: 0.5013\n",
      "284/388, train_loss: 0.0972, step time: 0.5241\n",
      "285/388, train_loss: 0.2132, step time: 0.5071\n",
      "286/388, train_loss: 0.2830, step time: 0.5073\n",
      "287/388, train_loss: 0.1582, step time: 0.4812\n",
      "288/388, train_loss: 0.0930, step time: 0.4903\n",
      "289/388, train_loss: 0.2201, step time: 0.5067\n",
      "290/388, train_loss: 0.0972, step time: 0.5107\n",
      "291/388, train_loss: 0.5270, step time: 0.4955\n",
      "292/388, train_loss: 0.2967, step time: 0.5115\n",
      "293/388, train_loss: 0.3452, step time: 0.5720\n",
      "294/388, train_loss: 0.0869, step time: 0.5439\n",
      "295/388, train_loss: 0.1996, step time: 0.5201\n",
      "296/388, train_loss: 0.1645, step time: 0.5151\n",
      "297/388, train_loss: 0.2035, step time: 0.5421\n",
      "298/388, train_loss: 0.1045, step time: 0.5266\n",
      "299/388, train_loss: 0.1615, step time: 0.5004\n",
      "300/388, train_loss: 0.1916, step time: 0.5011\n",
      "301/388, train_loss: 0.2469, step time: 0.4871\n",
      "302/388, train_loss: 0.1012, step time: 0.8111\n",
      "303/388, train_loss: 0.1005, step time: 0.5702\n",
      "304/388, train_loss: 0.5052, step time: 0.5291\n",
      "305/388, train_loss: 0.3205, step time: 0.5060\n",
      "306/388, train_loss: 0.1467, step time: 0.5145\n",
      "307/388, train_loss: 0.0892, step time: 0.5850\n",
      "308/388, train_loss: 0.1728, step time: 0.5369\n",
      "309/388, train_loss: 0.1186, step time: 0.4881\n",
      "310/388, train_loss: 0.1436, step time: 0.5128\n",
      "311/388, train_loss: 0.1997, step time: 0.4908\n",
      "312/388, train_loss: 0.4552, step time: 1.2116\n",
      "313/388, train_loss: 0.0733, step time: 0.5406\n",
      "314/388, train_loss: 0.3121, step time: 0.5082\n",
      "315/388, train_loss: 0.1837, step time: 0.4859\n",
      "316/388, train_loss: 0.2440, step time: 0.5123\n",
      "317/388, train_loss: 0.1385, step time: 0.5035\n",
      "318/388, train_loss: 0.1070, step time: 0.4915\n",
      "319/388, train_loss: 0.0758, step time: 0.4967\n",
      "320/388, train_loss: 0.2998, step time: 0.4927\n",
      "321/388, train_loss: 0.2848, step time: 0.5683\n",
      "322/388, train_loss: 0.2796, step time: 0.7117\n",
      "323/388, train_loss: 0.0384, step time: 0.5421\n",
      "324/388, train_loss: 0.1146, step time: 0.5077\n",
      "325/388, train_loss: 0.1609, step time: 0.4910\n",
      "326/388, train_loss: 0.0915, step time: 0.4914\n",
      "327/388, train_loss: 0.1385, step time: 0.4802\n",
      "328/388, train_loss: 0.1646, step time: 0.4909\n",
      "329/388, train_loss: 0.2804, step time: 1.1685\n",
      "330/388, train_loss: 0.1778, step time: 0.5334\n",
      "331/388, train_loss: 0.2719, step time: 0.4999\n",
      "332/388, train_loss: 0.0496, step time: 0.5008\n",
      "333/388, train_loss: 0.4495, step time: 0.4990\n",
      "334/388, train_loss: 0.1495, step time: 0.4995\n",
      "335/388, train_loss: 0.2379, step time: 0.4824\n",
      "336/388, train_loss: 0.3160, step time: 1.0232\n",
      "337/388, train_loss: 0.2730, step time: 0.5404\n",
      "338/388, train_loss: 0.1217, step time: 0.5194\n",
      "339/388, train_loss: 0.6495, step time: 0.4966\n",
      "340/388, train_loss: 0.1914, step time: 0.4943\n",
      "341/388, train_loss: 0.3635, step time: 0.5010\n",
      "342/388, train_loss: 0.0670, step time: 0.5015\n",
      "343/388, train_loss: 0.4788, step time: 0.4999\n",
      "344/388, train_loss: 0.1952, step time: 0.5173\n",
      "345/388, train_loss: 0.1577, step time: 0.6531\n",
      "346/388, train_loss: 0.3910, step time: 0.5452\n",
      "347/388, train_loss: 0.1345, step time: 0.5191\n",
      "348/388, train_loss: 0.1204, step time: 0.5116\n",
      "349/388, train_loss: 0.2131, step time: 0.4935\n",
      "350/388, train_loss: 0.1279, step time: 0.4937\n",
      "351/388, train_loss: 0.2702, step time: 0.4933\n",
      "352/388, train_loss: 0.2628, step time: 0.4937\n",
      "353/388, train_loss: 0.2796, step time: 0.4941\n",
      "354/388, train_loss: 0.5408, step time: 0.4855\n",
      "355/388, train_loss: 0.2121, step time: 1.0267\n",
      "356/388, train_loss: 0.2544, step time: 0.5440\n",
      "357/388, train_loss: 0.2587, step time: 0.5196\n",
      "358/388, train_loss: 0.0765, step time: 0.5032\n",
      "359/388, train_loss: 0.2617, step time: 0.4977\n",
      "360/388, train_loss: 0.1502, step time: 0.4890\n",
      "361/388, train_loss: 0.2431, step time: 0.5068\n",
      "362/388, train_loss: 0.1895, step time: 0.5034\n",
      "363/388, train_loss: 0.4820, step time: 0.5382\n",
      "364/388, train_loss: 0.0971, step time: 0.5859\n",
      "365/388, train_loss: 0.3358, step time: 0.5489\n",
      "366/388, train_loss: 0.1751, step time: 0.5248\n",
      "367/388, train_loss: 0.2251, step time: 0.5200\n",
      "368/388, train_loss: 0.0439, step time: 0.5056\n",
      "369/388, train_loss: 0.1317, step time: 0.4881\n",
      "370/388, train_loss: 0.1027, step time: 0.4957\n",
      "371/388, train_loss: 0.2168, step time: 0.5497\n",
      "372/388, train_loss: 0.0360, step time: 0.5310\n",
      "373/388, train_loss: 0.4701, step time: 0.5046\n",
      "374/388, train_loss: 0.2019, step time: 0.4927\n",
      "375/388, train_loss: 0.0634, step time: 1.1332\n",
      "376/388, train_loss: 0.2339, step time: 0.5382\n",
      "377/388, train_loss: 0.1485, step time: 0.5116\n",
      "378/388, train_loss: 0.0881, step time: 0.5038\n",
      "379/388, train_loss: 0.0744, step time: 0.4848\n",
      "380/388, train_loss: 0.4187, step time: 0.4879\n",
      "381/388, train_loss: 0.2673, step time: 0.4957\n",
      "382/388, train_loss: 0.3012, step time: 0.5265\n",
      "383/388, train_loss: 0.4028, step time: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/388, train_loss: 0.2209, step time: 0.5322\n",
      "385/388, train_loss: 0.1439, step time: 0.5195\n",
      "386/388, train_loss: 0.3539, step time: 0.4884\n",
      "387/388, train_loss: 0.3247, step time: 0.4756\n",
      "388/388, train_loss: 0.3566, step time: 0.5040\n",
      "epoch 26 average loss: 0.2145\n",
      "saved new best metric model\n",
      "current epoch: 26 current mean dice: 0.7527 tc: 0.7988 wt: 0.8919 et: 0.5674\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 26 is: 302.5642\n",
      "----------\n",
      "epoch 27/300\n",
      "1/388, train_loss: 0.1682, step time: 0.4842\n",
      "2/388, train_loss: 0.0995, step time: 0.4860\n",
      "3/388, train_loss: 0.1228, step time: 0.4878\n",
      "4/388, train_loss: 0.1879, step time: 1.1628\n",
      "5/388, train_loss: 0.6121, step time: 0.5589\n",
      "6/388, train_loss: 0.2455, step time: 0.5280\n",
      "7/388, train_loss: 0.0627, step time: 0.5444\n",
      "8/388, train_loss: 0.4193, step time: 0.5105\n",
      "9/388, train_loss: 0.0972, step time: 0.8883\n",
      "10/388, train_loss: 0.1269, step time: 0.5475\n",
      "11/388, train_loss: 0.1523, step time: 0.5243\n",
      "12/388, train_loss: 0.2587, step time: 0.5039\n",
      "13/388, train_loss: 0.1280, step time: 0.5015\n",
      "14/388, train_loss: 0.2889, step time: 0.4910\n",
      "15/388, train_loss: 0.1871, step time: 0.4823\n",
      "16/388, train_loss: 0.3924, step time: 0.5060\n",
      "17/388, train_loss: 0.1080, step time: 0.5098\n",
      "18/388, train_loss: 0.2349, step time: 0.5727\n",
      "19/388, train_loss: 0.2453, step time: 0.5183\n",
      "20/388, train_loss: 0.2774, step time: 0.4863\n",
      "21/388, train_loss: 0.1666, step time: 0.4833\n",
      "22/388, train_loss: 0.2502, step time: 0.5155\n",
      "23/388, train_loss: 0.4992, step time: 0.5316\n",
      "24/388, train_loss: 0.1061, step time: 0.5358\n",
      "25/388, train_loss: 0.0866, step time: 0.5072\n",
      "26/388, train_loss: 0.1250, step time: 0.5580\n",
      "27/388, train_loss: 0.2845, step time: 0.5475\n",
      "28/388, train_loss: 0.1903, step time: 0.5144\n",
      "29/388, train_loss: 0.2310, step time: 0.4948\n",
      "30/388, train_loss: 0.1754, step time: 0.5181\n",
      "31/388, train_loss: 0.2593, step time: 0.6143\n",
      "32/388, train_loss: 0.2763, step time: 0.5781\n",
      "33/388, train_loss: 0.0961, step time: 0.5224\n",
      "34/388, train_loss: 0.2111, step time: 0.4891\n",
      "35/388, train_loss: 0.2453, step time: 0.5289\n",
      "36/388, train_loss: 0.1908, step time: 0.5316\n",
      "37/388, train_loss: 0.1532, step time: 0.5169\n",
      "38/388, train_loss: 0.0723, step time: 0.5031\n",
      "39/388, train_loss: 0.2858, step time: 0.4985\n",
      "40/388, train_loss: 0.2621, step time: 0.5005\n",
      "41/388, train_loss: 0.1785, step time: 0.5121\n",
      "42/388, train_loss: 0.3978, step time: 0.4951\n",
      "43/388, train_loss: 0.3867, step time: 0.8908\n",
      "44/388, train_loss: 0.5348, step time: 0.5479\n",
      "45/388, train_loss: 0.1189, step time: 0.5169\n",
      "46/388, train_loss: 0.2882, step time: 0.4997\n",
      "47/388, train_loss: 0.0437, step time: 0.4883\n",
      "48/388, train_loss: 0.3972, step time: 0.4897\n",
      "49/388, train_loss: 0.0573, step time: 0.5275\n",
      "50/388, train_loss: 0.1744, step time: 0.4975\n",
      "51/388, train_loss: 0.2624, step time: 0.5140\n",
      "52/388, train_loss: 0.1155, step time: 0.5785\n",
      "53/388, train_loss: 0.1481, step time: 0.5422\n",
      "54/388, train_loss: 0.1891, step time: 0.5128\n",
      "55/388, train_loss: 0.0878, step time: 0.4955\n",
      "56/388, train_loss: 0.3937, step time: 0.4959\n",
      "57/388, train_loss: 0.1681, step time: 0.5452\n",
      "58/388, train_loss: 0.1579, step time: 0.5282\n",
      "59/388, train_loss: 0.0908, step time: 0.5005\n",
      "60/388, train_loss: 0.2366, step time: 0.4991\n",
      "61/388, train_loss: 0.1235, step time: 0.4894\n",
      "62/388, train_loss: 0.0875, step time: 0.5560\n",
      "63/388, train_loss: 0.2270, step time: 0.6401\n",
      "64/388, train_loss: 0.0510, step time: 0.5385\n",
      "65/388, train_loss: 0.1536, step time: 0.5159\n",
      "66/388, train_loss: 0.0889, step time: 0.5058\n",
      "67/388, train_loss: 0.2621, step time: 0.4997\n",
      "68/388, train_loss: 0.2172, step time: 0.5014\n",
      "69/388, train_loss: 0.2891, step time: 0.5673\n",
      "70/388, train_loss: 0.0991, step time: 0.5757\n",
      "71/388, train_loss: 0.0688, step time: 0.5260\n",
      "72/388, train_loss: 0.1259, step time: 0.5156\n",
      "73/388, train_loss: 0.0775, step time: 0.4940\n",
      "74/388, train_loss: 0.2165, step time: 0.4924\n",
      "75/388, train_loss: 0.2310, step time: 1.0792\n",
      "76/388, train_loss: 0.1610, step time: 0.5414\n",
      "77/388, train_loss: 0.1521, step time: 0.5033\n",
      "78/388, train_loss: 0.1488, step time: 0.4895\n",
      "79/388, train_loss: 0.2536, step time: 0.4923\n",
      "80/388, train_loss: 0.2366, step time: 0.4900\n",
      "81/388, train_loss: 0.2838, step time: 0.4756\n",
      "82/388, train_loss: 0.2404, step time: 0.6352\n",
      "83/388, train_loss: 0.1116, step time: 0.5378\n",
      "84/388, train_loss: 0.2725, step time: 0.5119\n",
      "85/388, train_loss: 0.4083, step time: 0.5076\n",
      "86/388, train_loss: 0.1839, step time: 0.4868\n",
      "87/388, train_loss: 0.2500, step time: 0.5006\n",
      "88/388, train_loss: 0.0922, step time: 0.4801\n",
      "89/388, train_loss: 0.3281, step time: 0.5094\n",
      "90/388, train_loss: 0.1092, step time: 0.5120\n",
      "91/388, train_loss: 0.1842, step time: 0.5049\n",
      "92/388, train_loss: 0.1070, step time: 0.5106\n",
      "93/388, train_loss: 0.0792, step time: 0.5127\n",
      "94/388, train_loss: 0.5426, step time: 0.4968\n",
      "95/388, train_loss: 0.1920, step time: 0.5121\n",
      "96/388, train_loss: 0.1073, step time: 0.5190\n",
      "97/388, train_loss: 0.3488, step time: 0.4967\n",
      "98/388, train_loss: 0.4949, step time: 0.4803\n",
      "99/388, train_loss: 0.2364, step time: 1.0692\n",
      "100/388, train_loss: 0.2799, step time: 0.5514\n",
      "101/388, train_loss: 0.2489, step time: 0.5154\n",
      "102/388, train_loss: 0.4732, step time: 0.5027\n",
      "103/388, train_loss: 0.0628, step time: 0.4880\n",
      "104/388, train_loss: 0.1982, step time: 0.4926\n",
      "105/388, train_loss: 0.3490, step time: 0.4879\n",
      "106/388, train_loss: 0.4440, step time: 0.4982\n",
      "107/388, train_loss: 0.1402, step time: 0.5128\n",
      "108/388, train_loss: 0.1033, step time: 0.5016\n",
      "109/388, train_loss: 0.1717, step time: 0.4952\n",
      "110/388, train_loss: 0.1529, step time: 1.1266\n",
      "111/388, train_loss: 0.2088, step time: 0.5427\n",
      "112/388, train_loss: 0.0593, step time: 0.5112\n",
      "113/388, train_loss: 0.3399, step time: 0.4992\n",
      "114/388, train_loss: 0.1249, step time: 0.4845\n",
      "115/388, train_loss: 0.2026, step time: 0.4766\n",
      "116/388, train_loss: 0.0381, step time: 0.8089\n",
      "117/388, train_loss: 0.1006, step time: 0.5395\n",
      "118/388, train_loss: 0.3957, step time: 0.5032\n",
      "119/388, train_loss: 0.2713, step time: 0.4922\n",
      "120/388, train_loss: 0.0741, step time: 0.4828\n",
      "121/388, train_loss: 0.1053, step time: 0.4872\n",
      "122/388, train_loss: 0.1899, step time: 0.4879\n",
      "123/388, train_loss: 0.1171, step time: 0.4912\n",
      "124/388, train_loss: 0.0458, step time: 0.4828\n",
      "125/388, train_loss: 0.2548, step time: 0.4841\n",
      "126/388, train_loss: 0.0761, step time: 0.4867\n",
      "127/388, train_loss: 0.1169, step time: 0.4892\n",
      "128/388, train_loss: 0.0718, step time: 0.4898\n",
      "129/388, train_loss: 0.3245, step time: 0.4771\n",
      "130/388, train_loss: 0.4887, step time: 0.7956\n",
      "131/388, train_loss: 0.2301, step time: 0.5537\n",
      "132/388, train_loss: 0.1277, step time: 0.5358\n",
      "133/388, train_loss: 0.2181, step time: 0.5015\n",
      "134/388, train_loss: 0.2983, step time: 0.5038\n",
      "135/388, train_loss: 0.4547, step time: 0.4907\n",
      "136/388, train_loss: 0.2853, step time: 0.4821\n",
      "137/388, train_loss: 0.0842, step time: 1.1779\n",
      "138/388, train_loss: 0.1044, step time: 0.5196\n",
      "139/388, train_loss: 0.1953, step time: 0.4964\n",
      "140/388, train_loss: 0.1623, step time: 0.4929\n",
      "141/388, train_loss: 0.1645, step time: 0.4856\n",
      "142/388, train_loss: 0.1915, step time: 0.4965\n",
      "143/388, train_loss: 0.4157, step time: 0.4823\n",
      "144/388, train_loss: 0.1651, step time: 1.0321\n",
      "145/388, train_loss: 0.1609, step time: 0.5485\n",
      "146/388, train_loss: 0.1755, step time: 0.5169\n",
      "147/388, train_loss: 0.1501, step time: 0.5017\n",
      "148/388, train_loss: 0.1914, step time: 0.4919\n",
      "149/388, train_loss: 0.1278, step time: 0.4817\n",
      "150/388, train_loss: 0.6011, step time: 0.4916\n",
      "151/388, train_loss: 0.2385, step time: 0.4975\n",
      "152/388, train_loss: 0.3486, step time: 0.5493\n",
      "153/388, train_loss: 0.0767, step time: 0.5175\n",
      "154/388, train_loss: 0.1866, step time: 0.5123\n",
      "155/388, train_loss: 0.3334, step time: 0.4919\n",
      "156/388, train_loss: 0.1873, step time: 0.4955\n",
      "157/388, train_loss: 0.1674, step time: 0.4832\n",
      "158/388, train_loss: 0.1620, step time: 0.5067\n",
      "159/388, train_loss: 0.1352, step time: 0.5466\n",
      "160/388, train_loss: 0.1180, step time: 0.5452\n",
      "161/388, train_loss: 0.3401, step time: 0.5220\n",
      "162/388, train_loss: 0.6212, step time: 0.4958\n",
      "163/388, train_loss: 0.3533, step time: 0.4938\n",
      "164/388, train_loss: 0.1271, step time: 0.4799\n",
      "165/388, train_loss: 0.2316, step time: 0.4759\n",
      "166/388, train_loss: 0.1288, step time: 0.4810\n",
      "167/388, train_loss: 0.2667, step time: 0.4749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/388, train_loss: 0.1306, step time: 0.4921\n",
      "169/388, train_loss: 0.2209, step time: 0.7406\n",
      "170/388, train_loss: 0.1432, step time: 0.5702\n",
      "171/388, train_loss: 0.1054, step time: 0.5246\n",
      "172/388, train_loss: 0.1044, step time: 0.4982\n",
      "173/388, train_loss: 0.3908, step time: 0.4973\n",
      "174/388, train_loss: 0.1904, step time: 0.4816\n",
      "175/388, train_loss: 0.1818, step time: 0.5142\n",
      "176/388, train_loss: 0.1050, step time: 0.5168\n",
      "177/388, train_loss: 0.1648, step time: 0.5099\n",
      "178/388, train_loss: 0.1114, step time: 0.4898\n",
      "179/388, train_loss: 0.2148, step time: 0.7495\n",
      "180/388, train_loss: 0.4834, step time: 0.5471\n",
      "181/388, train_loss: 0.2728, step time: 0.5132\n",
      "182/388, train_loss: 0.3369, step time: 0.5050\n",
      "183/388, train_loss: 0.1162, step time: 0.4901\n",
      "184/388, train_loss: 0.1214, step time: 0.4861\n",
      "185/388, train_loss: 0.1525, step time: 0.4906\n",
      "186/388, train_loss: 0.1932, step time: 0.4854\n",
      "187/388, train_loss: 0.1154, step time: 0.4903\n",
      "188/388, train_loss: 0.1314, step time: 1.2364\n",
      "189/388, train_loss: 0.7050, step time: 0.5498\n",
      "190/388, train_loss: 0.2180, step time: 0.5137\n",
      "191/388, train_loss: 0.2803, step time: 0.4959\n",
      "192/388, train_loss: 0.1192, step time: 0.4926\n",
      "193/388, train_loss: 0.0935, step time: 0.4914\n",
      "194/388, train_loss: 0.1076, step time: 0.4811\n",
      "195/388, train_loss: 0.3373, step time: 0.4901\n",
      "196/388, train_loss: 0.3074, step time: 1.2107\n",
      "197/388, train_loss: 0.1950, step time: 0.5182\n",
      "198/388, train_loss: 0.1598, step time: 0.4970\n",
      "199/388, train_loss: 0.1725, step time: 0.4946\n",
      "200/388, train_loss: 0.0713, step time: 0.4771\n",
      "201/388, train_loss: 0.3157, step time: 0.4765\n",
      "202/388, train_loss: 0.2562, step time: 0.4831\n",
      "203/388, train_loss: 0.0474, step time: 0.4796\n",
      "204/388, train_loss: 0.5021, step time: 0.4763\n",
      "205/388, train_loss: 0.1728, step time: 0.8923\n",
      "206/388, train_loss: 0.1473, step time: 0.5568\n",
      "207/388, train_loss: 0.3738, step time: 0.5281\n",
      "208/388, train_loss: 0.6018, step time: 0.5057\n",
      "209/388, train_loss: 0.4382, step time: 0.5046\n",
      "210/388, train_loss: 0.4078, step time: 0.4893\n",
      "211/388, train_loss: 0.3232, step time: 0.4884\n",
      "212/388, train_loss: 0.0952, step time: 0.4970\n",
      "213/388, train_loss: 0.2697, step time: 0.4829\n",
      "214/388, train_loss: 0.1676, step time: 0.4805\n",
      "215/388, train_loss: 0.3011, step time: 0.4911\n",
      "216/388, train_loss: 0.1658, step time: 0.4932\n",
      "217/388, train_loss: 0.3467, step time: 0.5024\n",
      "218/388, train_loss: 0.3221, step time: 0.5268\n",
      "219/388, train_loss: 0.1029, step time: 0.5100\n",
      "220/388, train_loss: 0.4504, step time: 0.5067\n",
      "221/388, train_loss: 0.0882, step time: 0.5583\n",
      "222/388, train_loss: 0.1846, step time: 0.5354\n",
      "223/388, train_loss: 0.1873, step time: 0.5237\n",
      "224/388, train_loss: 0.1694, step time: 0.5363\n",
      "225/388, train_loss: 0.1995, step time: 0.6108\n",
      "226/388, train_loss: 0.0969, step time: 0.5515\n",
      "227/388, train_loss: 0.5850, step time: 0.5085\n",
      "228/388, train_loss: 0.1257, step time: 0.5045\n",
      "229/388, train_loss: 0.1432, step time: 1.1328\n",
      "230/388, train_loss: 0.0808, step time: 0.5388\n",
      "231/388, train_loss: 0.4815, step time: 0.4971\n",
      "232/388, train_loss: 0.0966, step time: 0.4925\n",
      "233/388, train_loss: 0.1937, step time: 0.4988\n",
      "234/388, train_loss: 0.1245, step time: 0.4807\n",
      "235/388, train_loss: 0.1041, step time: 0.4748\n",
      "236/388, train_loss: 0.0834, step time: 0.4781\n",
      "237/388, train_loss: 0.1001, step time: 0.4943\n",
      "238/388, train_loss: 0.3140, step time: 0.4989\n",
      "239/388, train_loss: 0.1444, step time: 1.1761\n",
      "240/388, train_loss: 0.1891, step time: 0.5402\n",
      "241/388, train_loss: 0.2150, step time: 0.5083\n",
      "242/388, train_loss: 0.0878, step time: 0.4963\n",
      "243/388, train_loss: 0.1138, step time: 0.4862\n",
      "244/388, train_loss: 0.2776, step time: 0.4908\n",
      "245/388, train_loss: 0.2223, step time: 0.4811\n",
      "246/388, train_loss: 0.1321, step time: 0.4779\n",
      "247/388, train_loss: 0.4786, step time: 0.4821\n",
      "248/388, train_loss: 0.1151, step time: 0.4976\n",
      "249/388, train_loss: 0.2596, step time: 0.5030\n",
      "250/388, train_loss: 0.0853, step time: 0.4946\n",
      "251/388, train_loss: 0.2214, step time: 0.4929\n",
      "252/388, train_loss: 0.1762, step time: 1.1245\n",
      "253/388, train_loss: 0.0658, step time: 0.5420\n",
      "254/388, train_loss: 0.1085, step time: 0.5086\n",
      "255/388, train_loss: 0.3014, step time: 0.4964\n",
      "256/388, train_loss: 0.2064, step time: 0.4828\n",
      "257/388, train_loss: 0.4237, step time: 0.4952\n",
      "258/388, train_loss: 0.2378, step time: 0.9874\n",
      "259/388, train_loss: 0.3037, step time: 0.5399\n",
      "260/388, train_loss: 0.0898, step time: 0.5059\n",
      "261/388, train_loss: 0.1356, step time: 0.4925\n",
      "262/388, train_loss: 0.1640, step time: 0.4884\n",
      "263/388, train_loss: 0.6099, step time: 0.4827\n",
      "264/388, train_loss: 0.1510, step time: 0.9777\n",
      "265/388, train_loss: 0.1892, step time: 0.5360\n",
      "266/388, train_loss: 0.1463, step time: 0.5079\n",
      "267/388, train_loss: 0.1413, step time: 0.4883\n",
      "268/388, train_loss: 0.2193, step time: 0.4897\n",
      "269/388, train_loss: 0.2175, step time: 0.5004\n",
      "270/388, train_loss: 0.1365, step time: 0.4815\n",
      "271/388, train_loss: 0.2086, step time: 0.4783\n",
      "272/388, train_loss: 0.2357, step time: 0.4761\n",
      "273/388, train_loss: 0.2055, step time: 0.4818\n",
      "274/388, train_loss: 0.2411, step time: 1.1431\n",
      "275/388, train_loss: 0.2587, step time: 0.5371\n",
      "276/388, train_loss: 0.1677, step time: 0.5047\n",
      "277/388, train_loss: 0.1379, step time: 0.4961\n",
      "278/388, train_loss: 0.1814, step time: 0.4873\n",
      "279/388, train_loss: 0.2238, step time: 0.5102\n",
      "280/388, train_loss: 0.1515, step time: 0.4921\n",
      "281/388, train_loss: 0.1849, step time: 0.4954\n",
      "282/388, train_loss: 0.2965, step time: 0.4805\n",
      "283/388, train_loss: 0.1649, step time: 0.4817\n",
      "284/388, train_loss: 0.1302, step time: 0.7097\n",
      "285/388, train_loss: 0.2556, step time: 0.5443\n",
      "286/388, train_loss: 0.2807, step time: 0.5254\n",
      "287/388, train_loss: 0.2489, step time: 0.5075\n",
      "288/388, train_loss: 0.2875, step time: 0.5051\n",
      "289/388, train_loss: 0.1053, step time: 0.4935\n",
      "290/388, train_loss: 0.1133, step time: 0.4892\n",
      "291/388, train_loss: 0.1390, step time: 1.2843\n",
      "292/388, train_loss: 0.1899, step time: 0.5246\n",
      "293/388, train_loss: 0.1029, step time: 0.4983\n",
      "294/388, train_loss: 0.2437, step time: 0.4962\n",
      "295/388, train_loss: 0.1442, step time: 0.4861\n",
      "296/388, train_loss: 0.3492, step time: 0.5073\n",
      "297/388, train_loss: 0.0728, step time: 0.4834\n",
      "298/388, train_loss: 0.1479, step time: 0.4891\n",
      "299/388, train_loss: 0.0810, step time: 0.4781\n",
      "300/388, train_loss: 0.5461, step time: 0.6087\n",
      "301/388, train_loss: 0.1681, step time: 0.5516\n",
      "302/388, train_loss: 0.1209, step time: 0.5061\n",
      "303/388, train_loss: 0.1874, step time: 0.4953\n",
      "304/388, train_loss: 0.5053, step time: 0.4906\n",
      "305/388, train_loss: 0.2979, step time: 0.4955\n",
      "306/388, train_loss: 0.3576, step time: 0.7397\n",
      "307/388, train_loss: 0.2174, step time: 0.5586\n",
      "308/388, train_loss: 0.1459, step time: 0.5113\n",
      "309/388, train_loss: 0.4103, step time: 0.5010\n",
      "310/388, train_loss: 0.1677, step time: 0.4859\n",
      "311/388, train_loss: 0.1064, step time: 0.4836\n",
      "312/388, train_loss: 0.1323, step time: 0.4886\n",
      "313/388, train_loss: 0.1629, step time: 0.4781\n",
      "314/388, train_loss: 0.2404, step time: 0.4940\n",
      "315/388, train_loss: 0.2176, step time: 0.4797\n",
      "316/388, train_loss: 0.1470, step time: 0.4919\n",
      "317/388, train_loss: 0.1915, step time: 0.4956\n",
      "318/388, train_loss: 0.1997, step time: 0.5205\n",
      "319/388, train_loss: 0.0831, step time: 0.5115\n",
      "320/388, train_loss: 0.1904, step time: 0.4927\n",
      "321/388, train_loss: 0.2872, step time: 0.7536\n",
      "322/388, train_loss: 0.1089, step time: 0.5545\n",
      "323/388, train_loss: 0.3652, step time: 0.5175\n",
      "324/388, train_loss: 0.1518, step time: 0.5105\n",
      "325/388, train_loss: 0.1425, step time: 0.4970\n",
      "326/388, train_loss: 0.3904, step time: 0.4864\n",
      "327/388, train_loss: 0.0864, step time: 0.8622\n",
      "328/388, train_loss: 0.3614, step time: 0.5402\n",
      "329/388, train_loss: 0.0641, step time: 0.5070\n",
      "330/388, train_loss: 0.1016, step time: 0.4916\n",
      "331/388, train_loss: 0.2521, step time: 0.4958\n",
      "332/388, train_loss: 0.3373, step time: 0.5504\n",
      "333/388, train_loss: 0.0931, step time: 0.5256\n",
      "334/388, train_loss: 0.3248, step time: 0.5011\n",
      "335/388, train_loss: 0.1850, step time: 0.4986\n",
      "336/388, train_loss: 0.2098, step time: 1.1605\n",
      "337/388, train_loss: 0.5425, step time: 0.5501\n",
      "338/388, train_loss: 0.1628, step time: 0.5078\n",
      "339/388, train_loss: 0.0979, step time: 0.4919\n",
      "340/388, train_loss: 0.0721, step time: 0.4970\n",
      "341/388, train_loss: 0.3145, step time: 0.4816\n",
      "342/388, train_loss: 0.2348, step time: 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343/388, train_loss: 0.0948, step time: 1.1463\n",
      "344/388, train_loss: 0.1726, step time: 0.5349\n",
      "345/388, train_loss: 0.1898, step time: 0.5078\n",
      "346/388, train_loss: 0.1534, step time: 0.4878\n",
      "347/388, train_loss: 0.1921, step time: 0.4941\n",
      "348/388, train_loss: 0.1054, step time: 0.4809\n",
      "349/388, train_loss: 0.1370, step time: 0.4860\n",
      "350/388, train_loss: 0.0416, step time: 0.4851\n",
      "351/388, train_loss: 0.2225, step time: 0.6331\n",
      "352/388, train_loss: 0.0546, step time: 0.5446\n",
      "353/388, train_loss: 0.2016, step time: 0.5087\n",
      "354/388, train_loss: 0.1796, step time: 0.5031\n",
      "355/388, train_loss: 0.2909, step time: 0.4890\n",
      "356/388, train_loss: 0.1329, step time: 0.4901\n",
      "357/388, train_loss: 0.2288, step time: 0.4764\n",
      "358/388, train_loss: 0.1267, step time: 0.4993\n",
      "359/388, train_loss: 0.0806, step time: 0.4811\n",
      "360/388, train_loss: 0.3981, step time: 0.4902\n",
      "361/388, train_loss: 0.1974, step time: 0.5080\n",
      "362/388, train_loss: 0.1946, step time: 0.4969\n",
      "363/388, train_loss: 0.1088, step time: 0.4951\n",
      "364/388, train_loss: 0.2814, step time: 0.4992\n",
      "365/388, train_loss: 0.1375, step time: 0.4850\n",
      "366/388, train_loss: 0.6393, step time: 0.4992\n",
      "367/388, train_loss: 0.0675, step time: 0.5267\n",
      "368/388, train_loss: 0.2006, step time: 0.5013\n",
      "369/388, train_loss: 0.1512, step time: 1.0153\n",
      "370/388, train_loss: 0.5404, step time: 0.5605\n",
      "371/388, train_loss: 0.0809, step time: 0.5288\n",
      "372/388, train_loss: 0.3307, step time: 0.5027\n",
      "373/388, train_loss: 0.1043, step time: 0.5051\n",
      "374/388, train_loss: 0.2604, step time: 0.4917\n",
      "375/388, train_loss: 0.2565, step time: 0.4822\n",
      "376/388, train_loss: 0.1824, step time: 0.4959\n",
      "377/388, train_loss: 0.1976, step time: 0.4806\n",
      "378/388, train_loss: 0.1650, step time: 1.0348\n",
      "379/388, train_loss: 0.1742, step time: 0.5290\n",
      "380/388, train_loss: 0.1119, step time: 0.5150\n",
      "381/388, train_loss: 0.2124, step time: 0.4964\n",
      "382/388, train_loss: 0.0646, step time: 0.4943\n",
      "383/388, train_loss: 0.0688, step time: 0.5348\n",
      "384/388, train_loss: 0.2075, step time: 0.5094\n",
      "385/388, train_loss: 0.1093, step time: 0.4855\n",
      "386/388, train_loss: 0.1945, step time: 0.4747\n",
      "387/388, train_loss: 0.3454, step time: 0.6475\n",
      "388/388, train_loss: 0.4203, step time: 0.5683\n",
      "epoch 27 average loss: 0.2122\n",
      "current epoch: 27 current mean dice: 0.7326 tc: 0.7814 wt: 0.8848 et: 0.5316\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 27 is: 302.2936\n",
      "----------\n",
      "epoch 28/300\n",
      "1/388, train_loss: 0.1069, step time: 0.4814\n",
      "2/388, train_loss: 0.0654, step time: 0.4916\n",
      "3/388, train_loss: 0.1084, step time: 0.5108\n",
      "4/388, train_loss: 0.2031, step time: 0.5220\n",
      "5/388, train_loss: 0.2618, step time: 0.5595\n",
      "6/388, train_loss: 0.2667, step time: 0.5286\n",
      "7/388, train_loss: 0.1612, step time: 0.5082\n",
      "8/388, train_loss: 0.2102, step time: 0.5395\n",
      "9/388, train_loss: 0.1923, step time: 0.5344\n",
      "10/388, train_loss: 0.2231, step time: 0.5126\n",
      "11/388, train_loss: 0.1332, step time: 0.5051\n",
      "12/388, train_loss: 0.1930, step time: 0.6714\n",
      "13/388, train_loss: 0.1334, step time: 0.5462\n",
      "14/388, train_loss: 0.2267, step time: 0.5060\n",
      "15/388, train_loss: 0.1202, step time: 0.4975\n",
      "16/388, train_loss: 0.2393, step time: 0.5088\n",
      "17/388, train_loss: 0.1376, step time: 0.6004\n",
      "18/388, train_loss: 0.2679, step time: 0.5648\n",
      "19/388, train_loss: 0.2806, step time: 0.5265\n",
      "20/388, train_loss: 0.4121, step time: 0.5005\n",
      "21/388, train_loss: 0.2800, step time: 0.8262\n",
      "22/388, train_loss: 0.1373, step time: 0.6072\n",
      "23/388, train_loss: 0.1326, step time: 0.5413\n",
      "24/388, train_loss: 0.1580, step time: 0.5112\n",
      "25/388, train_loss: 0.5699, step time: 0.5812\n",
      "26/388, train_loss: 0.2367, step time: 0.5584\n",
      "27/388, train_loss: 0.2901, step time: 0.5427\n",
      "28/388, train_loss: 0.3414, step time: 0.5270\n",
      "29/388, train_loss: 0.2112, step time: 0.5307\n",
      "30/388, train_loss: 0.1138, step time: 0.5365\n",
      "31/388, train_loss: 0.1209, step time: 0.5160\n",
      "32/388, train_loss: 0.2237, step time: 0.5013\n",
      "33/388, train_loss: 0.5600, step time: 0.5299\n",
      "34/388, train_loss: 0.2764, step time: 0.5142\n",
      "35/388, train_loss: 0.1126, step time: 0.5806\n",
      "36/388, train_loss: 0.1314, step time: 0.5691\n",
      "37/388, train_loss: 0.2170, step time: 0.5323\n",
      "38/388, train_loss: 0.0432, step time: 0.5182\n",
      "39/388, train_loss: 0.1056, step time: 0.5090\n",
      "40/388, train_loss: 0.1824, step time: 0.4943\n",
      "41/388, train_loss: 0.1804, step time: 0.4864\n",
      "42/388, train_loss: 0.1338, step time: 0.5039\n",
      "43/388, train_loss: 0.3000, step time: 0.5708\n",
      "44/388, train_loss: 0.1233, step time: 0.5388\n",
      "45/388, train_loss: 0.3716, step time: 0.6223\n",
      "46/388, train_loss: 0.2029, step time: 0.5732\n",
      "47/388, train_loss: 0.1706, step time: 0.5358\n",
      "48/388, train_loss: 0.1072, step time: 0.5211\n",
      "49/388, train_loss: 0.4531, step time: 0.4982\n",
      "50/388, train_loss: 0.3455, step time: 0.5261\n",
      "51/388, train_loss: 0.2558, step time: 0.5800\n",
      "52/388, train_loss: 0.1695, step time: 0.5317\n",
      "53/388, train_loss: 0.0987, step time: 0.5038\n",
      "54/388, train_loss: 0.1252, step time: 0.5054\n",
      "55/388, train_loss: 0.5049, step time: 0.5540\n",
      "56/388, train_loss: 0.3366, step time: 0.7001\n",
      "57/388, train_loss: 0.2961, step time: 0.5781\n",
      "58/388, train_loss: 0.1325, step time: 0.5295\n",
      "59/388, train_loss: 0.1967, step time: 0.5164\n",
      "60/388, train_loss: 0.3064, step time: 0.5177\n",
      "61/388, train_loss: 0.3213, step time: 0.5595\n",
      "62/388, train_loss: 0.0761, step time: 0.6664\n",
      "63/388, train_loss: 0.1555, step time: 0.5502\n",
      "64/388, train_loss: 0.1641, step time: 0.5147\n",
      "65/388, train_loss: 0.0763, step time: 0.5480\n",
      "66/388, train_loss: 0.2136, step time: 0.5168\n",
      "67/388, train_loss: 0.2220, step time: 0.5027\n",
      "68/388, train_loss: 0.1091, step time: 0.4983\n",
      "69/388, train_loss: 0.2386, step time: 0.4995\n",
      "70/388, train_loss: 0.1276, step time: 0.5452\n",
      "71/388, train_loss: 0.4832, step time: 0.6010\n",
      "72/388, train_loss: 0.0968, step time: 0.5325\n",
      "73/388, train_loss: 0.3230, step time: 0.5044\n",
      "74/388, train_loss: 0.1171, step time: 0.4869\n",
      "75/388, train_loss: 0.2148, step time: 0.4947\n",
      "76/388, train_loss: 0.2992, step time: 1.2469\n",
      "77/388, train_loss: 0.1594, step time: 0.5496\n",
      "78/388, train_loss: 0.2928, step time: 0.5292\n",
      "79/388, train_loss: 0.1231, step time: 0.5044\n",
      "80/388, train_loss: 0.3275, step time: 0.4992\n",
      "81/388, train_loss: 0.2572, step time: 0.4855\n",
      "82/388, train_loss: 0.0813, step time: 0.4822\n",
      "83/388, train_loss: 0.0895, step time: 0.4848\n",
      "84/388, train_loss: 0.1005, step time: 0.4742\n",
      "85/388, train_loss: 0.3035, step time: 0.7423\n",
      "86/388, train_loss: 0.0874, step time: 0.5548\n",
      "87/388, train_loss: 0.1461, step time: 0.5267\n",
      "88/388, train_loss: 0.1474, step time: 0.4984\n",
      "89/388, train_loss: 0.3867, step time: 0.4962\n",
      "90/388, train_loss: 0.1800, step time: 0.5021\n",
      "91/388, train_loss: 0.5085, step time: 0.4839\n",
      "92/388, train_loss: 0.0919, step time: 1.0424\n",
      "93/388, train_loss: 0.1882, step time: 0.5444\n",
      "94/388, train_loss: 0.4282, step time: 0.5080\n",
      "95/388, train_loss: 0.2172, step time: 0.4918\n",
      "96/388, train_loss: 0.0950, step time: 0.4777\n",
      "97/388, train_loss: 0.0832, step time: 0.5463\n",
      "98/388, train_loss: 0.0933, step time: 0.5206\n",
      "99/388, train_loss: 0.1254, step time: 0.5041\n",
      "100/388, train_loss: 0.2757, step time: 0.4900\n",
      "101/388, train_loss: 0.1725, step time: 0.4932\n",
      "102/388, train_loss: 0.0860, step time: 0.4807\n",
      "103/388, train_loss: 0.3092, step time: 1.1156\n",
      "104/388, train_loss: 0.1275, step time: 0.5363\n",
      "105/388, train_loss: 0.2395, step time: 0.4977\n",
      "106/388, train_loss: 0.0760, step time: 0.4944\n",
      "107/388, train_loss: 0.0906, step time: 0.9957\n",
      "108/388, train_loss: 0.1515, step time: 0.5387\n",
      "109/388, train_loss: 0.1991, step time: 0.4977\n",
      "110/388, train_loss: 0.1798, step time: 0.4858\n",
      "111/388, train_loss: 0.1127, step time: 0.4894\n",
      "112/388, train_loss: 0.2234, step time: 0.4785\n",
      "113/388, train_loss: 0.2167, step time: 0.4774\n",
      "114/388, train_loss: 0.1887, step time: 0.4704\n",
      "115/388, train_loss: 0.4714, step time: 0.4693\n",
      "116/388, train_loss: 0.2912, step time: 0.4777\n",
      "117/388, train_loss: 0.0899, step time: 0.5684\n",
      "118/388, train_loss: 0.3361, step time: 0.5321\n",
      "119/388, train_loss: 0.3387, step time: 0.5084\n",
      "120/388, train_loss: 0.1642, step time: 0.5073\n",
      "121/388, train_loss: 0.1342, step time: 0.4953\n",
      "122/388, train_loss: 0.1863, step time: 0.4860\n",
      "123/388, train_loss: 0.1854, step time: 0.4927\n",
      "124/388, train_loss: 0.2233, step time: 0.4852\n",
      "125/388, train_loss: 0.2769, step time: 1.1923\n",
      "126/388, train_loss: 0.2022, step time: 0.5425\n",
      "127/388, train_loss: 0.1279, step time: 0.5130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/388, train_loss: 0.0717, step time: 0.4863\n",
      "129/388, train_loss: 0.1520, step time: 0.4937\n",
      "130/388, train_loss: 0.0876, step time: 0.4764\n",
      "131/388, train_loss: 0.4905, step time: 0.4746\n",
      "132/388, train_loss: 0.0938, step time: 0.4749\n",
      "133/388, train_loss: 0.1785, step time: 0.4734\n",
      "134/388, train_loss: 0.1642, step time: 0.4748\n",
      "135/388, train_loss: 0.2188, step time: 0.4834\n",
      "136/388, train_loss: 0.3036, step time: 0.4992\n",
      "137/388, train_loss: 0.1682, step time: 0.4986\n",
      "138/388, train_loss: 0.3600, step time: 0.5103\n",
      "139/388, train_loss: 0.1057, step time: 0.4930\n",
      "140/388, train_loss: 0.2096, step time: 0.5309\n",
      "141/388, train_loss: 0.2236, step time: 0.5337\n",
      "142/388, train_loss: 0.1242, step time: 0.5136\n",
      "143/388, train_loss: 0.1423, step time: 0.4934\n",
      "144/388, train_loss: 0.1285, step time: 0.4985\n",
      "145/388, train_loss: 0.3371, step time: 0.4827\n",
      "146/388, train_loss: 0.2638, step time: 0.5155\n",
      "147/388, train_loss: 0.1494, step time: 0.4964\n",
      "148/388, train_loss: 0.2885, step time: 0.5027\n",
      "149/388, train_loss: 0.4018, step time: 0.4848\n",
      "150/388, train_loss: 0.1409, step time: 0.4798\n",
      "151/388, train_loss: 0.1991, step time: 0.5161\n",
      "152/388, train_loss: 0.2384, step time: 0.5226\n",
      "153/388, train_loss: 0.3614, step time: 0.5268\n",
      "154/388, train_loss: 0.1452, step time: 0.5332\n",
      "155/388, train_loss: 0.1163, step time: 0.5041\n",
      "156/388, train_loss: 0.3280, step time: 0.5045\n",
      "157/388, train_loss: 0.1267, step time: 0.4899\n",
      "158/388, train_loss: 0.5419, step time: 0.5158\n",
      "159/388, train_loss: 0.2768, step time: 0.5835\n",
      "160/388, train_loss: 0.1034, step time: 0.5378\n",
      "161/388, train_loss: 0.1630, step time: 0.5174\n",
      "162/388, train_loss: 0.1263, step time: 0.4883\n",
      "163/388, train_loss: 0.1379, step time: 0.5321\n",
      "164/388, train_loss: 0.6728, step time: 0.5251\n",
      "165/388, train_loss: 0.6457, step time: 0.5074\n",
      "166/388, train_loss: 0.1360, step time: 0.4992\n",
      "167/388, train_loss: 0.2656, step time: 0.5629\n",
      "168/388, train_loss: 0.0434, step time: 0.5253\n",
      "169/388, train_loss: 0.0951, step time: 0.4961\n",
      "170/388, train_loss: 0.0635, step time: 0.5009\n",
      "171/388, train_loss: 0.6106, step time: 0.5087\n",
      "172/388, train_loss: 0.1874, step time: 0.5490\n",
      "173/388, train_loss: 0.4322, step time: 0.5426\n",
      "174/388, train_loss: 0.1302, step time: 0.5151\n",
      "175/388, train_loss: 0.1187, step time: 0.4988\n",
      "176/388, train_loss: 0.1692, step time: 0.5179\n",
      "177/388, train_loss: 0.1267, step time: 0.5255\n",
      "178/388, train_loss: 0.2535, step time: 0.5029\n",
      "179/388, train_loss: 0.2288, step time: 0.4953\n",
      "180/388, train_loss: 0.2109, step time: 0.4824\n",
      "181/388, train_loss: 0.2372, step time: 0.5301\n",
      "182/388, train_loss: 0.2164, step time: 0.4971\n",
      "183/388, train_loss: 0.1129, step time: 0.4890\n",
      "184/388, train_loss: 0.2059, step time: 1.2081\n",
      "185/388, train_loss: 0.2431, step time: 0.5369\n",
      "186/388, train_loss: 0.2958, step time: 0.5089\n",
      "187/388, train_loss: 0.4151, step time: 0.4980\n",
      "188/388, train_loss: 0.3459, step time: 0.5018\n",
      "189/388, train_loss: 0.0638, step time: 0.4865\n",
      "190/388, train_loss: 0.3884, step time: 0.4917\n",
      "191/388, train_loss: 0.0962, step time: 1.1567\n",
      "192/388, train_loss: 0.2554, step time: 0.5524\n",
      "193/388, train_loss: 0.1866, step time: 0.5127\n",
      "194/388, train_loss: 0.1284, step time: 0.4908\n",
      "195/388, train_loss: 0.2188, step time: 0.5221\n",
      "196/388, train_loss: 0.2893, step time: 0.5043\n",
      "197/388, train_loss: 0.2528, step time: 0.5071\n",
      "198/388, train_loss: 0.2202, step time: 0.4836\n",
      "199/388, train_loss: 0.3843, step time: 0.4938\n",
      "200/388, train_loss: 0.1677, step time: 0.4893\n",
      "201/388, train_loss: 0.1817, step time: 0.4913\n",
      "202/388, train_loss: 0.3101, step time: 0.6993\n",
      "203/388, train_loss: 0.0679, step time: 0.5613\n",
      "204/388, train_loss: 0.2410, step time: 0.5194\n",
      "205/388, train_loss: 0.2456, step time: 0.4925\n",
      "206/388, train_loss: 0.1444, step time: 0.4925\n",
      "207/388, train_loss: 0.1238, step time: 0.4994\n",
      "208/388, train_loss: 0.1758, step time: 0.4821\n",
      "209/388, train_loss: 0.2605, step time: 1.2269\n",
      "210/388, train_loss: 0.1004, step time: 0.5249\n",
      "211/388, train_loss: 0.1284, step time: 0.4925\n",
      "212/388, train_loss: 0.1746, step time: 0.5003\n",
      "213/388, train_loss: 0.1203, step time: 0.4816\n",
      "214/388, train_loss: 0.0744, step time: 0.4808\n",
      "215/388, train_loss: 0.1830, step time: 0.4799\n",
      "216/388, train_loss: 0.1063, step time: 0.4926\n",
      "217/388, train_loss: 0.1591, step time: 0.5011\n",
      "218/388, train_loss: 0.0988, step time: 0.4866\n",
      "219/388, train_loss: 0.1282, step time: 0.4975\n",
      "220/388, train_loss: 0.1773, step time: 0.4973\n",
      "221/388, train_loss: 0.2888, step time: 0.5971\n",
      "222/388, train_loss: 0.0704, step time: 0.5555\n",
      "223/388, train_loss: 0.2272, step time: 0.5315\n",
      "224/388, train_loss: 0.3803, step time: 0.5045\n",
      "225/388, train_loss: 0.2279, step time: 0.4920\n",
      "226/388, train_loss: 0.2398, step time: 0.4960\n",
      "227/388, train_loss: 0.1065, step time: 0.4941\n",
      "228/388, train_loss: 0.0986, step time: 0.5021\n",
      "229/388, train_loss: 0.3663, step time: 0.4876\n",
      "230/388, train_loss: 0.2763, step time: 0.4855\n",
      "231/388, train_loss: 0.1882, step time: 0.9589\n",
      "232/388, train_loss: 0.5017, step time: 0.5328\n",
      "233/388, train_loss: 0.3995, step time: 0.4987\n",
      "234/388, train_loss: 0.2265, step time: 0.4915\n",
      "235/388, train_loss: 0.1836, step time: 0.4811\n",
      "236/388, train_loss: 0.0807, step time: 0.4825\n",
      "237/388, train_loss: 0.1129, step time: 0.4998\n",
      "238/388, train_loss: 0.1567, step time: 0.4800\n",
      "239/388, train_loss: 0.1095, step time: 0.9688\n",
      "240/388, train_loss: 0.1705, step time: 0.5432\n",
      "241/388, train_loss: 0.0659, step time: 0.5215\n",
      "242/388, train_loss: 0.1244, step time: 0.5024\n",
      "243/388, train_loss: 0.1563, step time: 0.5040\n",
      "244/388, train_loss: 0.1354, step time: 0.4853\n",
      "245/388, train_loss: 0.1433, step time: 0.4921\n",
      "246/388, train_loss: 0.1857, step time: 0.4819\n",
      "247/388, train_loss: 0.3749, step time: 0.4907\n",
      "248/388, train_loss: 0.0644, step time: 0.4913\n",
      "249/388, train_loss: 0.2207, step time: 1.1441\n",
      "250/388, train_loss: 0.2449, step time: 0.5416\n",
      "251/388, train_loss: 0.0361, step time: 0.5125\n",
      "252/388, train_loss: 0.2122, step time: 0.5025\n",
      "253/388, train_loss: 0.1592, step time: 0.4859\n",
      "254/388, train_loss: 0.1143, step time: 0.4914\n",
      "255/388, train_loss: 0.2747, step time: 0.4866\n",
      "256/388, train_loss: 0.1150, step time: 0.4748\n",
      "257/388, train_loss: 0.1910, step time: 1.1637\n",
      "258/388, train_loss: 0.3289, step time: 0.5451\n",
      "259/388, train_loss: 0.1309, step time: 0.5063\n",
      "260/388, train_loss: 0.0885, step time: 0.4853\n",
      "261/388, train_loss: 0.2136, step time: 0.4855\n",
      "262/388, train_loss: 0.2203, step time: 0.4872\n",
      "263/388, train_loss: 0.1866, step time: 0.5284\n",
      "264/388, train_loss: 0.1719, step time: 0.5151\n",
      "265/388, train_loss: 0.3095, step time: 0.4981\n",
      "266/388, train_loss: 0.4823, step time: 0.4921\n",
      "267/388, train_loss: 0.2071, step time: 1.1030\n",
      "268/388, train_loss: 0.3627, step time: 0.5222\n",
      "269/388, train_loss: 0.1104, step time: 0.5058\n",
      "270/388, train_loss: 0.3349, step time: 0.4886\n",
      "271/388, train_loss: 0.3538, step time: 0.4841\n",
      "272/388, train_loss: 0.0995, step time: 0.4855\n",
      "273/388, train_loss: 0.1150, step time: 0.4773\n",
      "274/388, train_loss: 0.0767, step time: 0.9870\n",
      "275/388, train_loss: 0.3437, step time: 0.5343\n",
      "276/388, train_loss: 0.2214, step time: 0.5092\n",
      "277/388, train_loss: 0.1028, step time: 0.4873\n",
      "278/388, train_loss: 0.2346, step time: 0.4853\n",
      "279/388, train_loss: 0.1591, step time: 0.5078\n",
      "280/388, train_loss: 0.2305, step time: 0.4872\n",
      "281/388, train_loss: 0.0536, step time: 0.4909\n",
      "282/388, train_loss: 0.3239, step time: 0.4881\n",
      "283/388, train_loss: 0.3400, step time: 0.9764\n",
      "284/388, train_loss: 0.5181, step time: 0.5328\n",
      "285/388, train_loss: 0.2862, step time: 0.5082\n",
      "286/388, train_loss: 0.6661, step time: 0.4860\n",
      "287/388, train_loss: 0.0973, step time: 0.4840\n",
      "288/388, train_loss: 0.0777, step time: 0.4878\n",
      "289/388, train_loss: 0.4242, step time: 0.4792\n",
      "290/388, train_loss: 0.2074, step time: 1.0148\n",
      "291/388, train_loss: 0.1014, step time: 0.5283\n",
      "292/388, train_loss: 0.1777, step time: 0.4969\n",
      "293/388, train_loss: 0.1959, step time: 0.4787\n",
      "294/388, train_loss: 0.3320, step time: 0.4821\n",
      "295/388, train_loss: 0.1584, step time: 0.5967\n",
      "296/388, train_loss: 0.1214, step time: 0.5310\n",
      "297/388, train_loss: 0.1382, step time: 0.5086\n",
      "298/388, train_loss: 0.1142, step time: 0.4950\n",
      "299/388, train_loss: 0.2907, step time: 0.5671\n",
      "300/388, train_loss: 0.0781, step time: 0.5440\n",
      "301/388, train_loss: 0.0787, step time: 0.5062\n",
      "302/388, train_loss: 0.4487, step time: 0.5058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/388, train_loss: 0.6626, step time: 0.4943\n",
      "304/388, train_loss: 0.2049, step time: 0.5347\n",
      "305/388, train_loss: 0.2248, step time: 0.5107\n",
      "306/388, train_loss: 0.1258, step time: 0.5049\n",
      "307/388, train_loss: 0.2598, step time: 0.4850\n",
      "308/388, train_loss: 0.0961, step time: 0.4918\n",
      "309/388, train_loss: 0.2867, step time: 0.4870\n",
      "310/388, train_loss: 0.0775, step time: 0.9841\n",
      "311/388, train_loss: 0.1681, step time: 0.5390\n",
      "312/388, train_loss: 0.3241, step time: 0.5163\n",
      "313/388, train_loss: 0.1616, step time: 0.5013\n",
      "314/388, train_loss: 0.5362, step time: 0.4862\n",
      "315/388, train_loss: 0.1255, step time: 0.4825\n",
      "316/388, train_loss: 0.1628, step time: 0.4937\n",
      "317/388, train_loss: 0.0805, step time: 0.4886\n",
      "318/388, train_loss: 0.1465, step time: 0.4967\n",
      "319/388, train_loss: 0.2131, step time: 1.1925\n",
      "320/388, train_loss: 0.1442, step time: 0.5317\n",
      "321/388, train_loss: 0.2364, step time: 0.5083\n",
      "322/388, train_loss: 0.1040, step time: 0.4992\n",
      "323/388, train_loss: 0.2855, step time: 0.4812\n",
      "324/388, train_loss: 0.1250, step time: 0.4776\n",
      "325/388, train_loss: 0.2244, step time: 0.4912\n",
      "326/388, train_loss: 0.1757, step time: 0.8194\n",
      "327/388, train_loss: 0.0456, step time: 0.5627\n",
      "328/388, train_loss: 0.0859, step time: 0.5255\n",
      "329/388, train_loss: 0.1183, step time: 0.5135\n",
      "330/388, train_loss: 0.1585, step time: 0.4983\n",
      "331/388, train_loss: 0.0981, step time: 0.4991\n",
      "332/388, train_loss: 0.2243, step time: 0.4872\n",
      "333/388, train_loss: 0.0527, step time: 0.5017\n",
      "334/388, train_loss: 0.1440, step time: 0.4884\n",
      "335/388, train_loss: 0.0653, step time: 0.4907\n",
      "336/388, train_loss: 0.2532, step time: 0.4848\n",
      "337/388, train_loss: 0.2704, step time: 0.4796\n",
      "338/388, train_loss: 0.1917, step time: 0.5091\n",
      "339/388, train_loss: 0.1041, step time: 0.4937\n",
      "340/388, train_loss: 0.0942, step time: 0.4973\n",
      "341/388, train_loss: 0.2101, step time: 1.1211\n",
      "342/388, train_loss: 0.2599, step time: 0.5392\n",
      "343/388, train_loss: 0.1688, step time: 0.5158\n",
      "344/388, train_loss: 0.1093, step time: 0.4951\n",
      "345/388, train_loss: 0.3169, step time: 0.4973\n",
      "346/388, train_loss: 0.0737, step time: 0.4833\n",
      "347/388, train_loss: 0.1049, step time: 0.5908\n",
      "348/388, train_loss: 0.1920, step time: 0.5521\n",
      "349/388, train_loss: 0.1294, step time: 0.5188\n",
      "350/388, train_loss: 0.2340, step time: 0.4930\n",
      "351/388, train_loss: 0.2873, step time: 0.4895\n",
      "352/388, train_loss: 0.1390, step time: 0.5156\n",
      "353/388, train_loss: 0.1088, step time: 0.4960\n",
      "354/388, train_loss: 0.2282, step time: 0.5803\n",
      "355/388, train_loss: 0.4108, step time: 0.5677\n",
      "356/388, train_loss: 0.0523, step time: 0.5340\n",
      "357/388, train_loss: 0.2759, step time: 0.5120\n",
      "358/388, train_loss: 0.0720, step time: 0.5226\n",
      "359/388, train_loss: 0.0654, step time: 0.5488\n",
      "360/388, train_loss: 0.2166, step time: 0.6145\n",
      "361/388, train_loss: 0.1059, step time: 0.5419\n",
      "362/388, train_loss: 0.2075, step time: 0.5192\n",
      "363/388, train_loss: 0.3710, step time: 0.5538\n",
      "364/388, train_loss: 0.3174, step time: 0.5259\n",
      "365/388, train_loss: 0.2984, step time: 0.5150\n",
      "366/388, train_loss: 0.1375, step time: 0.5677\n",
      "367/388, train_loss: 0.1691, step time: 0.5237\n",
      "368/388, train_loss: 0.1461, step time: 0.5080\n",
      "369/388, train_loss: 0.1212, step time: 0.4995\n",
      "370/388, train_loss: 0.2528, step time: 0.4957\n",
      "371/388, train_loss: 0.3057, step time: 0.4802\n",
      "372/388, train_loss: 0.5297, step time: 1.0465\n",
      "373/388, train_loss: 0.1287, step time: 0.5536\n",
      "374/388, train_loss: 0.1296, step time: 0.5244\n",
      "375/388, train_loss: 0.1035, step time: 0.5086\n",
      "376/388, train_loss: 0.1604, step time: 0.4980\n",
      "377/388, train_loss: 0.2039, step time: 0.5022\n",
      "378/388, train_loss: 0.2175, step time: 0.4905\n",
      "379/388, train_loss: 0.1875, step time: 0.4994\n",
      "380/388, train_loss: 0.0863, step time: 0.4921\n",
      "381/388, train_loss: 0.1252, step time: 0.5673\n",
      "382/388, train_loss: 0.2047, step time: 0.5555\n",
      "383/388, train_loss: 0.0927, step time: 0.5253\n",
      "384/388, train_loss: 0.2109, step time: 0.5084\n",
      "385/388, train_loss: 0.3218, step time: 0.4860\n",
      "386/388, train_loss: 0.1164, step time: 0.4863\n",
      "387/388, train_loss: 0.0805, step time: 1.1661\n",
      "388/388, train_loss: 0.4062, step time: 0.5403\n",
      "epoch 28 average loss: 0.2077\n",
      "current epoch: 28 current mean dice: 0.7254 tc: 0.7693 wt: 0.8942 et: 0.5125\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 28 is: 301.4623\n",
      "----------\n",
      "epoch 29/300\n",
      "1/388, train_loss: 0.3493, step time: 0.4870\n",
      "2/388, train_loss: 0.1450, step time: 0.4970\n",
      "3/388, train_loss: 0.0908, step time: 1.1937\n",
      "4/388, train_loss: 0.1223, step time: 0.5548\n",
      "5/388, train_loss: 0.1999, step time: 0.5081\n",
      "6/388, train_loss: 0.0920, step time: 0.4925\n",
      "7/388, train_loss: 0.2721, step time: 0.4965\n",
      "8/388, train_loss: 0.1118, step time: 0.5418\n",
      "9/388, train_loss: 0.1512, step time: 0.6077\n",
      "10/388, train_loss: 0.1604, step time: 0.5611\n",
      "11/388, train_loss: 0.0418, step time: 0.5115\n",
      "12/388, train_loss: 0.0750, step time: 0.5531\n",
      "13/388, train_loss: 0.1041, step time: 0.5603\n",
      "14/388, train_loss: 0.2887, step time: 0.5375\n",
      "15/388, train_loss: 0.1790, step time: 0.5158\n",
      "16/388, train_loss: 0.5532, step time: 0.5082\n",
      "17/388, train_loss: 0.2447, step time: 0.5293\n",
      "18/388, train_loss: 0.1923, step time: 0.5001\n",
      "19/388, train_loss: 0.2097, step time: 0.8118\n",
      "20/388, train_loss: 0.1677, step time: 0.5767\n",
      "21/388, train_loss: 0.1499, step time: 0.5195\n",
      "22/388, train_loss: 0.3853, step time: 0.4913\n",
      "23/388, train_loss: 0.2209, step time: 0.5192\n",
      "24/388, train_loss: 0.2880, step time: 0.6297\n",
      "25/388, train_loss: 0.6040, step time: 0.5602\n",
      "26/388, train_loss: 0.2937, step time: 0.5313\n",
      "27/388, train_loss: 0.3368, step time: 0.5181\n",
      "28/388, train_loss: 0.1119, step time: 1.0865\n",
      "29/388, train_loss: 0.1211, step time: 0.5475\n",
      "30/388, train_loss: 0.3025, step time: 0.5218\n",
      "31/388, train_loss: 0.1667, step time: 0.4988\n",
      "32/388, train_loss: 0.1212, step time: 0.5241\n",
      "33/388, train_loss: 0.1593, step time: 0.5556\n",
      "34/388, train_loss: 0.1534, step time: 0.6807\n",
      "35/388, train_loss: 0.1900, step time: 0.5521\n",
      "36/388, train_loss: 0.2187, step time: 0.5301\n",
      "37/388, train_loss: 0.1576, step time: 0.5251\n",
      "38/388, train_loss: 0.2027, step time: 0.5393\n",
      "39/388, train_loss: 0.0674, step time: 0.5242\n",
      "40/388, train_loss: 0.2385, step time: 0.5664\n",
      "41/388, train_loss: 0.2138, step time: 0.5285\n",
      "42/388, train_loss: 0.1022, step time: 0.5166\n",
      "43/388, train_loss: 0.3085, step time: 0.5161\n",
      "44/388, train_loss: 0.0527, step time: 0.5586\n",
      "45/388, train_loss: 0.5347, step time: 0.5258\n",
      "46/388, train_loss: 0.3483, step time: 0.5057\n",
      "47/388, train_loss: 0.1661, step time: 0.5305\n",
      "48/388, train_loss: 0.1628, step time: 0.6260\n",
      "49/388, train_loss: 0.1686, step time: 0.5754\n",
      "50/388, train_loss: 0.1471, step time: 0.5375\n",
      "51/388, train_loss: 0.1748, step time: 0.5136\n",
      "52/388, train_loss: 0.4400, step time: 0.5016\n",
      "53/388, train_loss: 0.5081, step time: 0.5160\n",
      "54/388, train_loss: 0.1244, step time: 0.5134\n",
      "55/388, train_loss: 0.1539, step time: 0.5704\n",
      "56/388, train_loss: 0.1580, step time: 0.5447\n",
      "57/388, train_loss: 0.2107, step time: 0.5165\n",
      "58/388, train_loss: 0.1375, step time: 0.4967\n",
      "59/388, train_loss: 0.2677, step time: 0.4878\n",
      "60/388, train_loss: 0.0802, step time: 0.5005\n",
      "61/388, train_loss: 0.3854, step time: 0.5269\n",
      "62/388, train_loss: 0.3081, step time: 0.5052\n",
      "63/388, train_loss: 0.3120, step time: 1.0834\n",
      "64/388, train_loss: 0.3065, step time: 0.5303\n",
      "65/388, train_loss: 0.2162, step time: 0.5171\n",
      "66/388, train_loss: 0.3545, step time: 1.0130\n",
      "67/388, train_loss: 0.0624, step time: 0.5516\n",
      "68/388, train_loss: 0.1222, step time: 0.5239\n",
      "69/388, train_loss: 0.4054, step time: 0.5068\n",
      "70/388, train_loss: 0.6019, step time: 0.4888\n",
      "71/388, train_loss: 0.1326, step time: 0.4922\n",
      "72/388, train_loss: 0.2364, step time: 1.0875\n",
      "73/388, train_loss: 0.4660, step time: 0.5433\n",
      "74/388, train_loss: 0.2639, step time: 0.5114\n",
      "75/388, train_loss: 0.1487, step time: 0.4940\n",
      "76/388, train_loss: 0.0810, step time: 0.4993\n",
      "77/388, train_loss: 0.1765, step time: 0.5017\n",
      "78/388, train_loss: 0.1404, step time: 0.4978\n",
      "79/388, train_loss: 0.2323, step time: 0.4884\n",
      "80/388, train_loss: 0.0746, step time: 0.4910\n",
      "81/388, train_loss: 0.2783, step time: 0.4781\n",
      "82/388, train_loss: 0.1628, step time: 0.6909\n",
      "83/388, train_loss: 0.2262, step time: 0.5670\n",
      "84/388, train_loss: 0.0737, step time: 0.5237\n",
      "85/388, train_loss: 0.0923, step time: 0.5097\n",
      "86/388, train_loss: 0.3894, step time: 0.5235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/388, train_loss: 0.3361, step time: 0.6193\n",
      "88/388, train_loss: 0.1038, step time: 0.5454\n",
      "89/388, train_loss: 0.2399, step time: 0.5270\n",
      "90/388, train_loss: 0.2777, step time: 0.5156\n",
      "91/388, train_loss: 0.1129, step time: 0.5141\n",
      "92/388, train_loss: 0.6003, step time: 0.5044\n",
      "93/388, train_loss: 0.1151, step time: 0.5365\n",
      "94/388, train_loss: 0.1788, step time: 0.5114\n",
      "95/388, train_loss: 0.1220, step time: 0.5039\n",
      "96/388, train_loss: 0.2633, step time: 0.4954\n",
      "97/388, train_loss: 0.1180, step time: 0.4930\n",
      "98/388, train_loss: 0.1081, step time: 0.4767\n",
      "99/388, train_loss: 0.1358, step time: 0.5231\n",
      "100/388, train_loss: 0.0680, step time: 0.5234\n",
      "101/388, train_loss: 0.1295, step time: 0.5130\n",
      "102/388, train_loss: 0.6663, step time: 0.5004\n",
      "103/388, train_loss: 0.1092, step time: 0.5023\n",
      "104/388, train_loss: 0.0954, step time: 0.4873\n",
      "105/388, train_loss: 0.1723, step time: 0.5442\n",
      "106/388, train_loss: 0.2346, step time: 0.5276\n",
      "107/388, train_loss: 0.2297, step time: 0.5995\n",
      "108/388, train_loss: 0.1867, step time: 0.5531\n",
      "109/388, train_loss: 0.2058, step time: 0.5242\n",
      "110/388, train_loss: 0.1579, step time: 0.5049\n",
      "111/388, train_loss: 0.1002, step time: 0.5042\n",
      "112/388, train_loss: 0.2085, step time: 0.4835\n",
      "113/388, train_loss: 0.3765, step time: 0.5001\n",
      "114/388, train_loss: 0.0749, step time: 1.1250\n",
      "115/388, train_loss: 0.4618, step time: 0.5522\n",
      "116/388, train_loss: 0.6193, step time: 0.5183\n",
      "117/388, train_loss: 0.0773, step time: 0.5012\n",
      "118/388, train_loss: 0.3029, step time: 0.4860\n",
      "119/388, train_loss: 0.7056, step time: 0.4760\n",
      "120/388, train_loss: 0.1099, step time: 1.1900\n",
      "121/388, train_loss: 0.2852, step time: 0.5493\n",
      "122/388, train_loss: 0.4743, step time: 0.5112\n",
      "123/388, train_loss: 0.1682, step time: 0.4931\n",
      "124/388, train_loss: 0.1044, step time: 0.4978\n",
      "125/388, train_loss: 0.3195, step time: 0.4827\n",
      "126/388, train_loss: 0.2312, step time: 0.4876\n",
      "127/388, train_loss: 0.2845, step time: 0.5050\n",
      "128/388, train_loss: 0.2015, step time: 0.4912\n",
      "129/388, train_loss: 0.1837, step time: 0.4933\n",
      "130/388, train_loss: 0.3174, step time: 0.4857\n",
      "131/388, train_loss: 0.3264, step time: 0.4900\n",
      "132/388, train_loss: 0.3554, step time: 0.9439\n",
      "133/388, train_loss: 0.1262, step time: 0.5310\n",
      "134/388, train_loss: 0.3165, step time: 0.5114\n",
      "135/388, train_loss: 0.0729, step time: 0.4934\n",
      "136/388, train_loss: 0.2066, step time: 0.5013\n",
      "137/388, train_loss: 0.1510, step time: 0.4879\n",
      "138/388, train_loss: 0.4290, step time: 0.4804\n",
      "139/388, train_loss: 0.3266, step time: 0.6218\n",
      "140/388, train_loss: 0.0899, step time: 0.5423\n",
      "141/388, train_loss: 0.2921, step time: 0.5179\n",
      "142/388, train_loss: 0.1562, step time: 0.5016\n",
      "143/388, train_loss: 0.1387, step time: 0.5628\n",
      "144/388, train_loss: 0.2147, step time: 0.5238\n",
      "145/388, train_loss: 0.6042, step time: 0.5006\n",
      "146/388, train_loss: 0.0658, step time: 0.4985\n",
      "147/388, train_loss: 0.2754, step time: 0.4795\n",
      "148/388, train_loss: 0.1128, step time: 0.4935\n",
      "149/388, train_loss: 0.1064, step time: 0.4943\n",
      "150/388, train_loss: 0.1585, step time: 0.4793\n",
      "151/388, train_loss: 0.1822, step time: 0.4835\n",
      "152/388, train_loss: 0.1172, step time: 1.0083\n",
      "153/388, train_loss: 0.1604, step time: 0.5504\n",
      "154/388, train_loss: 0.4783, step time: 0.5243\n",
      "155/388, train_loss: 0.1423, step time: 0.5079\n",
      "156/388, train_loss: 0.1699, step time: 0.4938\n",
      "157/388, train_loss: 0.1291, step time: 0.4787\n",
      "158/388, train_loss: 0.1013, step time: 0.4836\n",
      "159/388, train_loss: 0.2353, step time: 0.4828\n",
      "160/388, train_loss: 0.1056, step time: 0.4867\n",
      "161/388, train_loss: 0.1201, step time: 0.4724\n",
      "162/388, train_loss: 0.2221, step time: 0.8220\n",
      "163/388, train_loss: 0.1979, step time: 0.5639\n",
      "164/388, train_loss: 0.6050, step time: 0.5273\n",
      "165/388, train_loss: 0.1635, step time: 0.5030\n",
      "166/388, train_loss: 0.1243, step time: 0.5049\n",
      "167/388, train_loss: 0.3138, step time: 0.4832\n",
      "168/388, train_loss: 0.1150, step time: 0.5068\n",
      "169/388, train_loss: 0.2833, step time: 0.4981\n",
      "170/388, train_loss: 0.1985, step time: 0.5146\n",
      "171/388, train_loss: 0.1968, step time: 0.6008\n",
      "172/388, train_loss: 0.3725, step time: 0.5452\n",
      "173/388, train_loss: 0.0863, step time: 0.5231\n",
      "174/388, train_loss: 0.1100, step time: 0.4955\n",
      "175/388, train_loss: 0.0873, step time: 0.9401\n",
      "176/388, train_loss: 0.0647, step time: 0.5208\n",
      "177/388, train_loss: 0.2836, step time: 0.5024\n",
      "178/388, train_loss: 0.0924, step time: 0.4934\n",
      "179/388, train_loss: 0.2302, step time: 0.4806\n",
      "180/388, train_loss: 0.2813, step time: 0.5073\n",
      "181/388, train_loss: 0.1242, step time: 0.4942\n",
      "182/388, train_loss: 0.1394, step time: 0.9447\n",
      "183/388, train_loss: 0.3282, step time: 0.5373\n",
      "184/388, train_loss: 0.0997, step time: 0.5076\n",
      "185/388, train_loss: 0.1147, step time: 0.4940\n",
      "186/388, train_loss: 0.0794, step time: 0.4955\n",
      "187/388, train_loss: 0.4949, step time: 0.4838\n",
      "188/388, train_loss: 0.2063, step time: 0.4982\n",
      "189/388, train_loss: 0.1685, step time: 0.4938\n",
      "190/388, train_loss: 0.1519, step time: 0.4814\n",
      "191/388, train_loss: 0.1051, step time: 0.4850\n",
      "192/388, train_loss: 0.2833, step time: 0.7453\n",
      "193/388, train_loss: 0.2140, step time: 0.5611\n",
      "194/388, train_loss: 0.4647, step time: 0.5173\n",
      "195/388, train_loss: 0.4527, step time: 0.5006\n",
      "196/388, train_loss: 0.1006, step time: 0.4999\n",
      "197/388, train_loss: 0.1846, step time: 0.4840\n",
      "198/388, train_loss: 0.1791, step time: 0.4945\n",
      "199/388, train_loss: 0.2626, step time: 0.4997\n",
      "200/388, train_loss: 0.1224, step time: 0.4902\n",
      "201/388, train_loss: 0.1172, step time: 0.4796\n",
      "202/388, train_loss: 0.0983, step time: 0.4764\n",
      "203/388, train_loss: 0.3182, step time: 0.4929\n",
      "204/388, train_loss: 0.1160, step time: 0.5399\n",
      "205/388, train_loss: 0.5491, step time: 0.5118\n",
      "206/388, train_loss: 0.0840, step time: 0.4922\n",
      "207/388, train_loss: 0.0824, step time: 0.4885\n",
      "208/388, train_loss: 0.2392, step time: 1.1059\n",
      "209/388, train_loss: 0.1535, step time: 0.5490\n",
      "210/388, train_loss: 0.1779, step time: 0.5111\n",
      "211/388, train_loss: 0.3018, step time: 0.5046\n",
      "212/388, train_loss: 0.1210, step time: 0.4893\n",
      "213/388, train_loss: 0.1298, step time: 0.4983\n",
      "214/388, train_loss: 0.1227, step time: 0.4817\n",
      "215/388, train_loss: 0.1021, step time: 0.4916\n",
      "216/388, train_loss: 0.1158, step time: 0.5129\n",
      "217/388, train_loss: 0.1386, step time: 0.4910\n",
      "218/388, train_loss: 0.1419, step time: 0.4903\n",
      "219/388, train_loss: 0.1871, step time: 0.4777\n",
      "220/388, train_loss: 0.0964, step time: 0.7452\n",
      "221/388, train_loss: 0.3861, step time: 0.5458\n",
      "222/388, train_loss: 0.1384, step time: 0.5314\n",
      "223/388, train_loss: 0.0387, step time: 0.5015\n",
      "224/388, train_loss: 0.1372, step time: 0.4889\n",
      "225/388, train_loss: 0.3857, step time: 0.4939\n",
      "226/388, train_loss: 0.2508, step time: 0.4993\n",
      "227/388, train_loss: 0.1407, step time: 0.5456\n",
      "228/388, train_loss: 0.2210, step time: 0.5156\n",
      "229/388, train_loss: 0.0467, step time: 0.4927\n",
      "230/388, train_loss: 0.1561, step time: 0.4837\n",
      "231/388, train_loss: 0.2271, step time: 0.4892\n",
      "232/388, train_loss: 0.0783, step time: 0.4963\n",
      "233/388, train_loss: 0.2033, step time: 0.4950\n",
      "234/388, train_loss: 0.2154, step time: 0.5045\n",
      "235/388, train_loss: 0.0713, step time: 0.4832\n",
      "236/388, train_loss: 0.0664, step time: 0.4903\n",
      "237/388, train_loss: 0.0713, step time: 1.1829\n",
      "238/388, train_loss: 0.2158, step time: 0.5367\n",
      "239/388, train_loss: 0.0875, step time: 0.5051\n",
      "240/388, train_loss: 0.0985, step time: 0.4860\n",
      "241/388, train_loss: 0.1990, step time: 0.4962\n",
      "242/388, train_loss: 0.1301, step time: 1.0638\n",
      "243/388, train_loss: 0.2500, step time: 0.5237\n",
      "244/388, train_loss: 0.2453, step time: 0.5080\n",
      "245/388, train_loss: 0.2442, step time: 0.4924\n",
      "246/388, train_loss: 0.1530, step time: 0.4858\n",
      "247/388, train_loss: 0.1584, step time: 0.4895\n",
      "248/388, train_loss: 0.1335, step time: 0.4785\n",
      "249/388, train_loss: 0.2387, step time: 0.4758\n",
      "250/388, train_loss: 0.0897, step time: 0.4817\n",
      "251/388, train_loss: 0.2567, step time: 0.4758\n",
      "252/388, train_loss: 0.1493, step time: 1.0609\n",
      "253/388, train_loss: 0.3797, step time: 0.5380\n",
      "254/388, train_loss: 0.1718, step time: 0.5068\n",
      "255/388, train_loss: 0.2299, step time: 0.4891\n",
      "256/388, train_loss: 0.1810, step time: 0.4947\n",
      "257/388, train_loss: 0.3683, step time: 0.4813\n",
      "258/388, train_loss: 0.2301, step time: 0.4816\n",
      "259/388, train_loss: 0.1204, step time: 0.4881\n",
      "260/388, train_loss: 0.1522, step time: 0.4734\n",
      "261/388, train_loss: 0.1869, step time: 0.4842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/388, train_loss: 0.3652, step time: 0.4887\n",
      "263/388, train_loss: 0.2409, step time: 0.4788\n",
      "264/388, train_loss: 0.1066, step time: 0.4892\n",
      "265/388, train_loss: 0.1489, step time: 0.9423\n",
      "266/388, train_loss: 0.0866, step time: 0.5446\n",
      "267/388, train_loss: 0.2057, step time: 0.5163\n",
      "268/388, train_loss: 0.2617, step time: 0.4916\n",
      "269/388, train_loss: 0.1132, step time: 0.4994\n",
      "270/388, train_loss: 0.2135, step time: 0.4873\n",
      "271/388, train_loss: 0.1432, step time: 0.8738\n",
      "272/388, train_loss: 0.1979, step time: 0.5301\n",
      "273/388, train_loss: 0.1980, step time: 0.5000\n",
      "274/388, train_loss: 0.1700, step time: 0.5053\n",
      "275/388, train_loss: 0.0932, step time: 0.4842\n",
      "276/388, train_loss: 0.4042, step time: 0.4911\n",
      "277/388, train_loss: 0.4070, step time: 0.4752\n",
      "278/388, train_loss: 0.3905, step time: 0.9774\n",
      "279/388, train_loss: 0.1321, step time: 0.5403\n",
      "280/388, train_loss: 0.2097, step time: 0.5010\n",
      "281/388, train_loss: 0.0698, step time: 0.4955\n",
      "282/388, train_loss: 0.1936, step time: 0.4803\n",
      "283/388, train_loss: 0.1308, step time: 0.4777\n",
      "284/388, train_loss: 0.2341, step time: 0.5016\n",
      "285/388, train_loss: 0.1760, step time: 0.4976\n",
      "286/388, train_loss: 0.1952, step time: 0.4995\n",
      "287/388, train_loss: 0.2393, step time: 0.4919\n",
      "288/388, train_loss: 0.2819, step time: 0.4936\n",
      "289/388, train_loss: 0.1279, step time: 0.4958\n",
      "290/388, train_loss: 0.1141, step time: 0.4928\n",
      "291/388, train_loss: 0.2192, step time: 0.4843\n",
      "292/388, train_loss: 0.3282, step time: 0.4942\n",
      "293/388, train_loss: 0.2694, step time: 0.6234\n",
      "294/388, train_loss: 0.3111, step time: 0.5609\n",
      "295/388, train_loss: 0.1938, step time: 0.5249\n",
      "296/388, train_loss: 0.2141, step time: 0.5104\n",
      "297/388, train_loss: 0.3002, step time: 0.4966\n",
      "298/388, train_loss: 0.3008, step time: 0.5345\n",
      "299/388, train_loss: 0.1230, step time: 0.5019\n",
      "300/388, train_loss: 0.1562, step time: 0.4999\n",
      "301/388, train_loss: 0.0780, step time: 0.4936\n",
      "302/388, train_loss: 0.1516, step time: 0.4941\n",
      "303/388, train_loss: 0.2446, step time: 0.4812\n",
      "304/388, train_loss: 0.1273, step time: 0.4874\n",
      "305/388, train_loss: 0.3715, step time: 0.4750\n",
      "306/388, train_loss: 0.1458, step time: 0.4737\n",
      "307/388, train_loss: 0.1827, step time: 1.0137\n",
      "308/388, train_loss: 0.3071, step time: 0.5499\n",
      "309/388, train_loss: 0.1799, step time: 0.5198\n",
      "310/388, train_loss: 0.2705, step time: 0.5033\n",
      "311/388, train_loss: 0.2193, step time: 0.4905\n",
      "312/388, train_loss: 0.0910, step time: 0.4841\n",
      "313/388, train_loss: 0.1402, step time: 0.4938\n",
      "314/388, train_loss: 0.0924, step time: 0.4911\n",
      "315/388, train_loss: 0.1804, step time: 0.5374\n",
      "316/388, train_loss: 0.2251, step time: 0.5192\n",
      "317/388, train_loss: 0.1228, step time: 0.5022\n",
      "318/388, train_loss: 0.2074, step time: 0.4893\n",
      "319/388, train_loss: 0.1023, step time: 0.4998\n",
      "320/388, train_loss: 0.3392, step time: 0.4862\n",
      "321/388, train_loss: 0.0675, step time: 0.9374\n",
      "322/388, train_loss: 0.1881, step time: 0.5443\n",
      "323/388, train_loss: 0.1265, step time: 0.5111\n",
      "324/388, train_loss: 0.1656, step time: 0.5000\n",
      "325/388, train_loss: 0.2349, step time: 0.4873\n",
      "326/388, train_loss: 0.5458, step time: 0.4941\n",
      "327/388, train_loss: 0.3030, step time: 0.4777\n",
      "328/388, train_loss: 0.2239, step time: 0.4779\n",
      "329/388, train_loss: 0.0960, step time: 0.7815\n",
      "330/388, train_loss: 0.1713, step time: 0.5463\n",
      "331/388, train_loss: 0.1297, step time: 0.5217\n",
      "332/388, train_loss: 0.1537, step time: 0.4931\n",
      "333/388, train_loss: 0.1011, step time: 0.5039\n",
      "334/388, train_loss: 0.4179, step time: 0.4816\n",
      "335/388, train_loss: 0.3036, step time: 0.4918\n",
      "336/388, train_loss: 0.1002, step time: 0.4831\n",
      "337/388, train_loss: 0.3730, step time: 0.5225\n",
      "338/388, train_loss: 0.6574, step time: 0.4966\n",
      "339/388, train_loss: 0.2256, step time: 0.4911\n",
      "340/388, train_loss: 0.2558, step time: 0.4856\n",
      "341/388, train_loss: 0.2801, step time: 0.5201\n",
      "342/388, train_loss: 0.0945, step time: 0.4914\n",
      "343/388, train_loss: 0.1719, step time: 0.5349\n",
      "344/388, train_loss: 0.2207, step time: 0.5408\n",
      "345/388, train_loss: 0.2110, step time: 0.5112\n",
      "346/388, train_loss: 0.2132, step time: 0.4964\n",
      "347/388, train_loss: 0.0956, step time: 0.5050\n",
      "348/388, train_loss: 0.1733, step time: 0.4920\n",
      "349/388, train_loss: 0.2302, step time: 0.4839\n",
      "350/388, train_loss: 0.2476, step time: 1.1403\n",
      "351/388, train_loss: 0.0725, step time: 0.5340\n",
      "352/388, train_loss: 0.2435, step time: 0.5127\n",
      "353/388, train_loss: 0.1118, step time: 0.4914\n",
      "354/388, train_loss: 0.1815, step time: 0.4892\n",
      "355/388, train_loss: 0.0581, step time: 0.4775\n",
      "356/388, train_loss: 0.2234, step time: 0.4785\n",
      "357/388, train_loss: 0.4665, step time: 0.5234\n",
      "358/388, train_loss: 0.1463, step time: 0.5127\n",
      "359/388, train_loss: 0.1420, step time: 0.5077\n",
      "360/388, train_loss: 0.1848, step time: 0.5053\n",
      "361/388, train_loss: 0.2412, step time: 0.4963\n",
      "362/388, train_loss: 0.0318, step time: 0.4834\n",
      "363/388, train_loss: 0.0970, step time: 0.4933\n",
      "364/388, train_loss: 0.0875, step time: 0.4829\n",
      "365/388, train_loss: 0.2257, step time: 0.5123\n",
      "366/388, train_loss: 0.3767, step time: 0.5014\n",
      "367/388, train_loss: 0.1888, step time: 0.5082\n",
      "368/388, train_loss: 0.0791, step time: 0.4920\n",
      "369/388, train_loss: 0.3939, step time: 0.4927\n",
      "370/388, train_loss: 0.2721, step time: 0.8876\n",
      "371/388, train_loss: 0.0416, step time: 0.5483\n",
      "372/388, train_loss: 0.1525, step time: 0.5279\n",
      "373/388, train_loss: 0.3210, step time: 0.5111\n",
      "374/388, train_loss: 0.3723, step time: 0.4931\n",
      "375/388, train_loss: 0.1881, step time: 0.5062\n",
      "376/388, train_loss: 0.4050, step time: 0.5610\n",
      "377/388, train_loss: 0.3520, step time: 0.5440\n",
      "378/388, train_loss: 0.4053, step time: 0.5178\n",
      "379/388, train_loss: 0.3402, step time: 0.4871\n",
      "380/388, train_loss: 0.3220, step time: 0.4866\n",
      "381/388, train_loss: 0.1298, step time: 0.4907\n",
      "382/388, train_loss: 0.1175, step time: 1.1243\n",
      "383/388, train_loss: 0.1369, step time: 0.5239\n",
      "384/388, train_loss: 0.3037, step time: 0.5006\n",
      "385/388, train_loss: 0.1966, step time: 0.4970\n",
      "386/388, train_loss: 0.2190, step time: 0.5503\n",
      "387/388, train_loss: 0.1649, step time: 0.5095\n",
      "388/388, train_loss: 0.1103, step time: 0.5115\n",
      "epoch 29 average loss: 0.2117\n",
      "current epoch: 29 current mean dice: 0.7286 tc: 0.7756 wt: 0.8611 et: 0.5490\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 29 is: 302.1410\n",
      "----------\n",
      "epoch 30/300\n",
      "1/388, train_loss: 0.1199, step time: 0.4846\n",
      "2/388, train_loss: 0.2748, step time: 0.4950\n",
      "3/388, train_loss: 0.2121, step time: 0.5417\n",
      "4/388, train_loss: 0.4251, step time: 0.5130\n",
      "5/388, train_loss: 0.1972, step time: 0.9957\n",
      "6/388, train_loss: 0.2557, step time: 0.5779\n",
      "7/388, train_loss: 0.3489, step time: 0.5299\n",
      "8/388, train_loss: 0.1722, step time: 0.5187\n",
      "9/388, train_loss: 0.1998, step time: 0.5477\n",
      "10/388, train_loss: 0.2229, step time: 0.5015\n",
      "11/388, train_loss: 0.0723, step time: 0.5464\n",
      "12/388, train_loss: 0.1979, step time: 0.7716\n",
      "13/388, train_loss: 0.3340, step time: 0.5762\n",
      "14/388, train_loss: 0.1324, step time: 0.5566\n",
      "15/388, train_loss: 0.2164, step time: 0.5364\n",
      "16/388, train_loss: 0.2456, step time: 0.4977\n",
      "17/388, train_loss: 0.0921, step time: 0.5417\n",
      "18/388, train_loss: 0.1938, step time: 0.5278\n",
      "19/388, train_loss: 0.1951, step time: 0.4890\n",
      "20/388, train_loss: 0.2242, step time: 0.7734\n",
      "21/388, train_loss: 0.0964, step time: 0.5726\n",
      "22/388, train_loss: 0.3504, step time: 0.5215\n",
      "23/388, train_loss: 0.2884, step time: 0.5019\n",
      "24/388, train_loss: 0.1429, step time: 0.5540\n",
      "25/388, train_loss: 0.1365, step time: 0.5148\n",
      "26/388, train_loss: 0.3426, step time: 0.5509\n",
      "27/388, train_loss: 0.1320, step time: 0.5088\n",
      "28/388, train_loss: 0.2092, step time: 0.5871\n",
      "29/388, train_loss: 0.3011, step time: 0.6121\n",
      "30/388, train_loss: 0.1752, step time: 0.5465\n",
      "31/388, train_loss: 0.0762, step time: 0.5212\n",
      "32/388, train_loss: 0.2204, step time: 0.4970\n",
      "33/388, train_loss: 0.1475, step time: 0.4996\n",
      "34/388, train_loss: 0.1080, step time: 1.0740\n",
      "35/388, train_loss: 0.0865, step time: 0.5397\n",
      "36/388, train_loss: 0.2424, step time: 0.5224\n",
      "37/388, train_loss: 0.2180, step time: 0.9945\n",
      "38/388, train_loss: 0.0519, step time: 0.5627\n",
      "39/388, train_loss: 0.3035, step time: 0.5302\n",
      "40/388, train_loss: 0.5112, step time: 0.5258\n",
      "41/388, train_loss: 0.0941, step time: 0.5043\n",
      "42/388, train_loss: 0.4318, step time: 0.5055\n",
      "43/388, train_loss: 0.1518, step time: 0.4856\n",
      "44/388, train_loss: 0.3633, step time: 1.1409\n",
      "45/388, train_loss: 0.1190, step time: 0.5392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/388, train_loss: 0.1109, step time: 0.4999\n",
      "47/388, train_loss: 0.0640, step time: 0.4917\n",
      "48/388, train_loss: 0.1440, step time: 0.5051\n",
      "49/388, train_loss: 0.1486, step time: 0.5090\n",
      "50/388, train_loss: 0.0798, step time: 0.4963\n",
      "51/388, train_loss: 0.0956, step time: 0.4960\n",
      "52/388, train_loss: 0.1229, step time: 0.5059\n",
      "53/388, train_loss: 0.2012, step time: 0.5001\n",
      "54/388, train_loss: 0.2937, step time: 0.5042\n",
      "55/388, train_loss: 0.1318, step time: 1.1660\n",
      "56/388, train_loss: 0.0713, step time: 0.5420\n",
      "57/388, train_loss: 0.2104, step time: 0.5191\n",
      "58/388, train_loss: 0.2386, step time: 0.5043\n",
      "59/388, train_loss: 0.2622, step time: 0.4863\n",
      "60/388, train_loss: 0.1530, step time: 0.5529\n",
      "61/388, train_loss: 0.1952, step time: 0.5501\n",
      "62/388, train_loss: 0.2366, step time: 0.5335\n",
      "63/388, train_loss: 0.2750, step time: 0.5902\n",
      "64/388, train_loss: 0.3938, step time: 0.5321\n",
      "65/388, train_loss: 0.2561, step time: 0.5108\n",
      "66/388, train_loss: 0.1666, step time: 0.4863\n",
      "67/388, train_loss: 0.1603, step time: 0.4858\n",
      "68/388, train_loss: 0.2975, step time: 0.5170\n",
      "69/388, train_loss: 0.1273, step time: 0.4978\n",
      "70/388, train_loss: 0.2653, step time: 0.5042\n",
      "71/388, train_loss: 0.0648, step time: 0.5049\n",
      "72/388, train_loss: 0.1691, step time: 0.5080\n",
      "73/388, train_loss: 0.0685, step time: 0.4942\n",
      "74/388, train_loss: 0.0973, step time: 0.4808\n",
      "75/388, train_loss: 0.1053, step time: 0.7284\n",
      "76/388, train_loss: 0.2513, step time: 0.5699\n",
      "77/388, train_loss: 0.1482, step time: 0.5227\n",
      "78/388, train_loss: 0.5421, step time: 0.5120\n",
      "79/388, train_loss: 0.1011, step time: 0.4940\n",
      "80/388, train_loss: 0.2821, step time: 1.1696\n",
      "81/388, train_loss: 0.0509, step time: 0.5270\n",
      "82/388, train_loss: 0.1099, step time: 0.5143\n",
      "83/388, train_loss: 0.1632, step time: 0.4960\n",
      "84/388, train_loss: 0.1286, step time: 0.5036\n",
      "85/388, train_loss: 0.2355, step time: 0.4888\n",
      "86/388, train_loss: 0.2364, step time: 0.7405\n",
      "87/388, train_loss: 0.0948, step time: 0.5766\n",
      "88/388, train_loss: 0.1766, step time: 0.5431\n",
      "89/388, train_loss: 0.1661, step time: 0.5092\n",
      "90/388, train_loss: 0.1818, step time: 0.5077\n",
      "91/388, train_loss: 0.0411, step time: 0.5387\n",
      "92/388, train_loss: 0.0708, step time: 0.4963\n",
      "93/388, train_loss: 0.1411, step time: 0.5013\n",
      "94/388, train_loss: 0.1524, step time: 0.4915\n",
      "95/388, train_loss: 0.1987, step time: 0.5390\n",
      "96/388, train_loss: 0.2271, step time: 0.5081\n",
      "97/388, train_loss: 0.4071, step time: 0.5081\n",
      "98/388, train_loss: 0.1842, step time: 0.5769\n",
      "99/388, train_loss: 0.0861, step time: 0.5302\n",
      "100/388, train_loss: 0.1589, step time: 0.5246\n",
      "101/388, train_loss: 0.2811, step time: 0.5220\n",
      "102/388, train_loss: 0.2173, step time: 0.5218\n",
      "103/388, train_loss: 0.1419, step time: 0.5913\n",
      "104/388, train_loss: 0.1691, step time: 0.5431\n",
      "105/388, train_loss: 0.1755, step time: 0.5136\n",
      "106/388, train_loss: 0.1061, step time: 0.4856\n",
      "107/388, train_loss: 0.0599, step time: 0.4840\n",
      "108/388, train_loss: 0.2888, step time: 0.5749\n",
      "109/388, train_loss: 0.1734, step time: 0.5728\n",
      "110/388, train_loss: 0.2092, step time: 0.5432\n",
      "111/388, train_loss: 0.1133, step time: 0.5188\n",
      "112/388, train_loss: 0.2451, step time: 0.5017\n",
      "113/388, train_loss: 0.0690, step time: 0.4922\n",
      "114/388, train_loss: 0.3329, step time: 0.5014\n",
      "115/388, train_loss: 0.1171, step time: 0.8158\n",
      "116/388, train_loss: 0.1437, step time: 0.5548\n",
      "117/388, train_loss: 0.1341, step time: 0.5256\n",
      "118/388, train_loss: 0.5339, step time: 0.5000\n",
      "119/388, train_loss: 0.3909, step time: 0.4920\n",
      "120/388, train_loss: 0.1538, step time: 0.4951\n",
      "121/388, train_loss: 0.1438, step time: 0.7719\n",
      "122/388, train_loss: 0.1549, step time: 0.5431\n",
      "123/388, train_loss: 0.3984, step time: 0.5142\n",
      "124/388, train_loss: 0.2416, step time: 0.5021\n",
      "125/388, train_loss: 0.1094, step time: 0.5361\n",
      "126/388, train_loss: 0.3651, step time: 0.5299\n",
      "127/388, train_loss: 0.1036, step time: 0.5190\n",
      "128/388, train_loss: 0.1836, step time: 0.5105\n",
      "129/388, train_loss: 0.1315, step time: 1.2132\n",
      "130/388, train_loss: 0.1144, step time: 0.5404\n",
      "131/388, train_loss: 0.5297, step time: 0.5075\n",
      "132/388, train_loss: 0.1011, step time: 0.5078\n",
      "133/388, train_loss: 0.1532, step time: 0.4920\n",
      "134/388, train_loss: 0.2872, step time: 0.4997\n",
      "135/388, train_loss: 0.0362, step time: 0.5617\n",
      "136/388, train_loss: 0.2990, step time: 0.5308\n",
      "137/388, train_loss: 0.5747, step time: 0.5061\n",
      "138/388, train_loss: 0.2024, step time: 1.1505\n",
      "139/388, train_loss: 0.0903, step time: 0.5357\n",
      "140/388, train_loss: 0.3157, step time: 0.5215\n",
      "141/388, train_loss: 0.0917, step time: 0.4980\n",
      "142/388, train_loss: 0.2143, step time: 0.4972\n",
      "143/388, train_loss: 0.1913, step time: 0.4981\n",
      "144/388, train_loss: 0.1930, step time: 0.4912\n",
      "145/388, train_loss: 0.3208, step time: 0.4888\n",
      "146/388, train_loss: 0.1001, step time: 0.4988\n",
      "147/388, train_loss: 0.2428, step time: 0.5048\n",
      "148/388, train_loss: 0.1006, step time: 0.5123\n",
      "149/388, train_loss: 0.2880, step time: 0.5013\n",
      "150/388, train_loss: 0.1771, step time: 0.5423\n",
      "151/388, train_loss: 0.3470, step time: 0.5046\n",
      "152/388, train_loss: 0.1805, step time: 0.4943\n",
      "153/388, train_loss: 0.1650, step time: 1.0505\n",
      "154/388, train_loss: 0.1330, step time: 0.5352\n",
      "155/388, train_loss: 0.1206, step time: 0.5102\n",
      "156/388, train_loss: 0.0823, step time: 0.4813\n",
      "157/388, train_loss: 0.0817, step time: 0.4817\n",
      "158/388, train_loss: 0.1622, step time: 0.5002\n",
      "159/388, train_loss: 0.1581, step time: 0.4944\n",
      "160/388, train_loss: 0.1667, step time: 0.5132\n",
      "161/388, train_loss: 0.0924, step time: 0.5105\n",
      "162/388, train_loss: 0.4646, step time: 0.4865\n",
      "163/388, train_loss: 0.3187, step time: 0.5053\n",
      "164/388, train_loss: 0.1820, step time: 0.4940\n",
      "165/388, train_loss: 0.1282, step time: 0.5154\n",
      "166/388, train_loss: 0.2573, step time: 0.5420\n",
      "167/388, train_loss: 0.0960, step time: 0.5140\n",
      "168/388, train_loss: 0.1963, step time: 0.4987\n",
      "169/388, train_loss: 0.1130, step time: 0.4877\n",
      "170/388, train_loss: 0.3518, step time: 1.0668\n",
      "171/388, train_loss: 0.1233, step time: 0.5337\n",
      "172/388, train_loss: 0.1366, step time: 0.5162\n",
      "173/388, train_loss: 0.3294, step time: 0.4869\n",
      "174/388, train_loss: 0.1980, step time: 0.4767\n",
      "175/388, train_loss: 0.1268, step time: 0.4753\n",
      "176/388, train_loss: 0.2786, step time: 0.4911\n",
      "177/388, train_loss: 0.1507, step time: 0.5993\n",
      "178/388, train_loss: 0.2700, step time: 0.5594\n",
      "179/388, train_loss: 0.1256, step time: 0.5395\n",
      "180/388, train_loss: 0.3128, step time: 0.5163\n",
      "181/388, train_loss: 0.3073, step time: 0.5065\n",
      "182/388, train_loss: 0.1212, step time: 0.5136\n",
      "183/388, train_loss: 0.0677, step time: 0.5506\n",
      "184/388, train_loss: 0.2198, step time: 0.5168\n",
      "185/388, train_loss: 0.1238, step time: 0.5193\n",
      "186/388, train_loss: 0.3149, step time: 0.5567\n",
      "187/388, train_loss: 0.1695, step time: 0.5249\n",
      "188/388, train_loss: 0.1528, step time: 0.5073\n",
      "189/388, train_loss: 0.3126, step time: 0.4862\n",
      "190/388, train_loss: 0.2122, step time: 0.7884\n",
      "191/388, train_loss: 0.1000, step time: 0.5461\n",
      "192/388, train_loss: 0.2645, step time: 0.5114\n",
      "193/388, train_loss: 0.5453, step time: 0.5012\n",
      "194/388, train_loss: 0.0788, step time: 0.4874\n",
      "195/388, train_loss: 0.1513, step time: 0.4951\n",
      "196/388, train_loss: 0.2595, step time: 0.4869\n",
      "197/388, train_loss: 0.6489, step time: 0.5396\n",
      "198/388, train_loss: 0.2105, step time: 0.5092\n",
      "199/388, train_loss: 0.1006, step time: 0.5053\n",
      "200/388, train_loss: 0.3010, step time: 0.4827\n",
      "201/388, train_loss: 0.0968, step time: 0.5089\n",
      "202/388, train_loss: 0.3592, step time: 0.5092\n",
      "203/388, train_loss: 0.1152, step time: 0.5574\n",
      "204/388, train_loss: 0.0976, step time: 0.5290\n",
      "205/388, train_loss: 0.3143, step time: 0.5026\n",
      "206/388, train_loss: 0.1245, step time: 0.5396\n",
      "207/388, train_loss: 0.2959, step time: 0.5948\n",
      "208/388, train_loss: 0.2601, step time: 0.5513\n",
      "209/388, train_loss: 0.6027, step time: 0.5127\n",
      "210/388, train_loss: 0.1954, step time: 0.5040\n",
      "211/388, train_loss: 0.1013, step time: 0.8660\n",
      "212/388, train_loss: 0.4739, step time: 0.5707\n",
      "213/388, train_loss: 0.1344, step time: 0.5360\n",
      "214/388, train_loss: 0.0978, step time: 0.4995\n",
      "215/388, train_loss: 0.2287, step time: 0.4943\n",
      "216/388, train_loss: 0.3125, step time: 0.4980\n",
      "217/388, train_loss: 0.1064, step time: 0.4839\n",
      "218/388, train_loss: 0.1178, step time: 0.5957\n",
      "219/388, train_loss: 0.2803, step time: 0.5618\n",
      "220/388, train_loss: 0.1927, step time: 0.5312\n",
      "221/388, train_loss: 0.1192, step time: 0.5167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/388, train_loss: 0.1819, step time: 0.5103\n",
      "223/388, train_loss: 0.1797, step time: 0.4868\n",
      "224/388, train_loss: 0.1734, step time: 0.4733\n",
      "225/388, train_loss: 0.0537, step time: 0.4986\n",
      "226/388, train_loss: 0.5015, step time: 0.5209\n",
      "227/388, train_loss: 0.1659, step time: 0.5099\n",
      "228/388, train_loss: 0.2327, step time: 0.5058\n",
      "229/388, train_loss: 0.1443, step time: 0.4952\n",
      "230/388, train_loss: 0.1018, step time: 0.5932\n",
      "231/388, train_loss: 0.2021, step time: 0.5348\n",
      "232/388, train_loss: 0.1539, step time: 0.5095\n",
      "233/388, train_loss: 0.1538, step time: 0.4976\n",
      "234/388, train_loss: 0.0764, step time: 0.4836\n",
      "235/388, train_loss: 0.0682, step time: 0.4872\n",
      "236/388, train_loss: 0.0898, step time: 0.4768\n",
      "237/388, train_loss: 0.1794, step time: 1.0309\n",
      "238/388, train_loss: 0.1194, step time: 0.5301\n",
      "239/388, train_loss: 0.2631, step time: 0.5183\n",
      "240/388, train_loss: 0.1462, step time: 0.4957\n",
      "241/388, train_loss: 0.2212, step time: 0.4874\n",
      "242/388, train_loss: 0.4185, step time: 0.4883\n",
      "243/388, train_loss: 0.2402, step time: 0.5144\n",
      "244/388, train_loss: 0.1683, step time: 0.5088\n",
      "245/388, train_loss: 0.2042, step time: 0.4945\n",
      "246/388, train_loss: 0.1813, step time: 0.4933\n",
      "247/388, train_loss: 0.0801, step time: 0.4998\n",
      "248/388, train_loss: 0.1549, step time: 0.4998\n",
      "249/388, train_loss: 0.2980, step time: 0.5252\n",
      "250/388, train_loss: 0.2345, step time: 0.5117\n",
      "251/388, train_loss: 0.0921, step time: 0.4945\n",
      "252/388, train_loss: 0.2392, step time: 0.5011\n",
      "253/388, train_loss: 0.1037, step time: 1.1333\n",
      "254/388, train_loss: 0.2941, step time: 0.5317\n",
      "255/388, train_loss: 0.2021, step time: 0.5098\n",
      "256/388, train_loss: 0.1237, step time: 0.4951\n",
      "257/388, train_loss: 0.2171, step time: 0.4935\n",
      "258/388, train_loss: 0.1121, step time: 0.4796\n",
      "259/388, train_loss: 0.4031, step time: 0.4863\n",
      "260/388, train_loss: 0.3654, step time: 1.0323\n",
      "261/388, train_loss: 0.2497, step time: 0.5378\n",
      "262/388, train_loss: 0.2225, step time: 0.5019\n",
      "263/388, train_loss: 0.1831, step time: 0.4961\n",
      "264/388, train_loss: 0.2192, step time: 0.4810\n",
      "265/388, train_loss: 0.2041, step time: 0.4838\n",
      "266/388, train_loss: 0.1024, step time: 0.4923\n",
      "267/388, train_loss: 0.4056, step time: 0.4906\n",
      "268/388, train_loss: 0.1195, step time: 0.5029\n",
      "269/388, train_loss: 0.1041, step time: 0.4903\n",
      "270/388, train_loss: 0.0931, step time: 0.4945\n",
      "271/388, train_loss: 0.4807, step time: 0.4894\n",
      "272/388, train_loss: 0.3553, step time: 0.4882\n",
      "273/388, train_loss: 0.2699, step time: 0.9596\n",
      "274/388, train_loss: 0.1340, step time: 0.5290\n",
      "275/388, train_loss: 0.1964, step time: 0.4998\n",
      "276/388, train_loss: 0.2619, step time: 0.4822\n",
      "277/388, train_loss: 0.2755, step time: 0.5162\n",
      "278/388, train_loss: 0.1188, step time: 0.5007\n",
      "279/388, train_loss: 0.1242, step time: 0.4989\n",
      "280/388, train_loss: 0.1204, step time: 0.4877\n",
      "281/388, train_loss: 0.0879, step time: 0.4927\n",
      "282/388, train_loss: 0.1406, step time: 0.4831\n",
      "283/388, train_loss: 0.1363, step time: 0.9882\n",
      "284/388, train_loss: 0.3761, step time: 0.5400\n",
      "285/388, train_loss: 0.1292, step time: 0.5038\n",
      "286/388, train_loss: 0.1191, step time: 0.4978\n",
      "287/388, train_loss: 0.1307, step time: 0.4952\n",
      "288/388, train_loss: 0.2456, step time: 0.4975\n",
      "289/388, train_loss: 0.0882, step time: 0.4860\n",
      "290/388, train_loss: 0.2384, step time: 0.4924\n",
      "291/388, train_loss: 0.4966, step time: 0.5017\n",
      "292/388, train_loss: 0.0969, step time: 0.4835\n",
      "293/388, train_loss: 0.0666, step time: 0.4796\n",
      "294/388, train_loss: 0.3006, step time: 0.5094\n",
      "295/388, train_loss: 0.2496, step time: 0.4868\n",
      "296/388, train_loss: 0.1732, step time: 0.4863\n",
      "297/388, train_loss: 0.1522, step time: 0.4871\n",
      "298/388, train_loss: 0.6065, step time: 0.4991\n",
      "299/388, train_loss: 0.3092, step time: 0.5210\n",
      "300/388, train_loss: 0.1338, step time: 0.5058\n",
      "301/388, train_loss: 0.4666, step time: 0.5088\n",
      "302/388, train_loss: 0.2088, step time: 0.5421\n",
      "303/388, train_loss: 0.1242, step time: 0.5154\n",
      "304/388, train_loss: 0.1464, step time: 0.5051\n",
      "305/388, train_loss: 0.4016, step time: 0.5364\n",
      "306/388, train_loss: 0.5805, step time: 0.5398\n",
      "307/388, train_loss: 0.2490, step time: 0.5104\n",
      "308/388, train_loss: 0.2187, step time: 0.5063\n",
      "309/388, train_loss: 0.3226, step time: 0.4906\n",
      "310/388, train_loss: 0.0993, step time: 0.4966\n",
      "311/388, train_loss: 0.0969, step time: 0.4857\n",
      "312/388, train_loss: 0.1949, step time: 0.4833\n",
      "313/388, train_loss: 0.0870, step time: 0.4813\n",
      "314/388, train_loss: 0.6050, step time: 0.4911\n",
      "315/388, train_loss: 0.2163, step time: 0.5166\n",
      "316/388, train_loss: 0.2175, step time: 0.4883\n",
      "317/388, train_loss: 0.2151, step time: 0.5102\n",
      "318/388, train_loss: 0.1519, step time: 0.5659\n",
      "319/388, train_loss: 0.0934, step time: 0.5280\n",
      "320/388, train_loss: 0.1107, step time: 0.5092\n",
      "321/388, train_loss: 0.2196, step time: 0.4980\n",
      "322/388, train_loss: 0.0730, step time: 0.4943\n",
      "323/388, train_loss: 0.2681, step time: 0.5142\n",
      "324/388, train_loss: 0.0928, step time: 0.5023\n",
      "325/388, train_loss: 0.0867, step time: 0.5049\n",
      "326/388, train_loss: 0.2120, step time: 0.5068\n",
      "327/388, train_loss: 0.2887, step time: 1.1792\n",
      "328/388, train_loss: 0.2201, step time: 0.5441\n",
      "329/388, train_loss: 0.1382, step time: 0.5115\n",
      "330/388, train_loss: 0.2918, step time: 0.4982\n",
      "331/388, train_loss: 0.1694, step time: 0.4930\n",
      "332/388, train_loss: 0.1903, step time: 0.4948\n",
      "333/388, train_loss: 0.2619, step time: 0.4806\n",
      "334/388, train_loss: 0.1563, step time: 0.4821\n",
      "335/388, train_loss: 0.1456, step time: 0.8694\n",
      "336/388, train_loss: 0.5070, step time: 0.5501\n",
      "337/388, train_loss: 0.1297, step time: 0.5176\n",
      "338/388, train_loss: 0.3492, step time: 0.4983\n",
      "339/388, train_loss: 0.2834, step time: 0.4983\n",
      "340/388, train_loss: 0.2797, step time: 0.4801\n",
      "341/388, train_loss: 0.2630, step time: 0.4787\n",
      "342/388, train_loss: 0.2091, step time: 0.9607\n",
      "343/388, train_loss: 0.0848, step time: 0.5365\n",
      "344/388, train_loss: 0.2321, step time: 0.5077\n",
      "345/388, train_loss: 0.1403, step time: 0.4910\n",
      "346/388, train_loss: 0.1213, step time: 0.4898\n",
      "347/388, train_loss: 0.0497, step time: 0.5004\n",
      "348/388, train_loss: 0.0785, step time: 0.4941\n",
      "349/388, train_loss: 0.1652, step time: 0.4919\n",
      "350/388, train_loss: 0.2175, step time: 0.4774\n",
      "351/388, train_loss: 0.2183, step time: 1.0871\n",
      "352/388, train_loss: 0.2301, step time: 0.5567\n",
      "353/388, train_loss: 0.1446, step time: 0.5102\n",
      "354/388, train_loss: 0.0310, step time: 0.4992\n",
      "355/388, train_loss: 0.3165, step time: 0.4898\n",
      "356/388, train_loss: 0.2621, step time: 0.4936\n",
      "357/388, train_loss: 0.3403, step time: 0.4789\n",
      "358/388, train_loss: 0.0937, step time: 0.4801\n",
      "359/388, train_loss: 0.1079, step time: 0.4803\n",
      "360/388, train_loss: 0.0726, step time: 0.4733\n",
      "361/388, train_loss: 0.1533, step time: 0.4933\n",
      "362/388, train_loss: 0.3312, step time: 0.4923\n",
      "363/388, train_loss: 0.3988, step time: 0.6766\n",
      "364/388, train_loss: 0.2873, step time: 0.5576\n",
      "365/388, train_loss: 0.1409, step time: 0.5302\n",
      "366/388, train_loss: 0.3539, step time: 0.5333\n",
      "367/388, train_loss: 0.1396, step time: 0.5310\n",
      "368/388, train_loss: 0.4976, step time: 0.5122\n",
      "369/388, train_loss: 0.3812, step time: 0.4893\n",
      "370/388, train_loss: 0.4279, step time: 0.4942\n",
      "371/388, train_loss: 0.0674, step time: 0.8829\n",
      "372/388, train_loss: 0.1523, step time: 0.5459\n",
      "373/388, train_loss: 0.3586, step time: 0.5064\n",
      "374/388, train_loss: 0.3614, step time: 0.4973\n",
      "375/388, train_loss: 0.1240, step time: 0.4835\n",
      "376/388, train_loss: 0.2972, step time: 0.4808\n",
      "377/388, train_loss: 0.4202, step time: 0.4917\n",
      "378/388, train_loss: 0.0424, step time: 0.5290\n",
      "379/388, train_loss: 0.2183, step time: 0.5135\n",
      "380/388, train_loss: 0.0616, step time: 0.4966\n",
      "381/388, train_loss: 0.5386, step time: 0.4809\n",
      "382/388, train_loss: 0.0533, step time: 1.0113\n",
      "383/388, train_loss: 0.1188, step time: 0.5224\n",
      "384/388, train_loss: 0.1638, step time: 0.5027\n",
      "385/388, train_loss: 0.0981, step time: 0.4873\n",
      "386/388, train_loss: 0.5746, step time: 0.4763\n",
      "387/388, train_loss: 0.3654, step time: 0.4716\n",
      "388/388, train_loss: 0.2022, step time: 0.4727\n",
      "epoch 30 average loss: 0.2087\n",
      "current epoch: 30 current mean dice: 0.7394 tc: 0.7845 wt: 0.8815 et: 0.5522\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 30 is: 300.3656\n",
      "----------\n",
      "epoch 31/300\n",
      "1/388, train_loss: 0.2723, step time: 0.4827\n",
      "2/388, train_loss: 0.0990, step time: 0.4784\n",
      "3/388, train_loss: 0.1243, step time: 0.4852\n",
      "4/388, train_loss: 0.1832, step time: 0.5018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/388, train_loss: 0.1798, step time: 1.1098\n",
      "6/388, train_loss: 0.3428, step time: 0.5652\n",
      "7/388, train_loss: 0.2480, step time: 0.5176\n",
      "8/388, train_loss: 0.1490, step time: 0.5100\n",
      "9/388, train_loss: 0.1082, step time: 0.4877\n",
      "10/388, train_loss: 0.0990, step time: 0.5095\n",
      "11/388, train_loss: 0.3382, step time: 0.5094\n",
      "12/388, train_loss: 0.0997, step time: 0.6139\n",
      "13/388, train_loss: 0.1259, step time: 0.5525\n",
      "14/388, train_loss: 0.2004, step time: 0.5161\n",
      "15/388, train_loss: 0.2185, step time: 0.5251\n",
      "16/388, train_loss: 0.2705, step time: 0.6484\n",
      "17/388, train_loss: 0.1033, step time: 0.5652\n",
      "18/388, train_loss: 0.4680, step time: 0.5306\n",
      "19/388, train_loss: 0.2869, step time: 0.5117\n",
      "20/388, train_loss: 0.1826, step time: 0.4932\n",
      "21/388, train_loss: 0.5084, step time: 0.8576\n",
      "22/388, train_loss: 0.1514, step time: 0.5418\n",
      "23/388, train_loss: 0.2175, step time: 0.5111\n",
      "24/388, train_loss: 0.3200, step time: 0.5222\n",
      "25/388, train_loss: 0.1486, step time: 0.5200\n",
      "26/388, train_loss: 0.2511, step time: 0.5379\n",
      "27/388, train_loss: 0.2899, step time: 0.6415\n",
      "28/388, train_loss: 0.2926, step time: 0.5479\n",
      "29/388, train_loss: 0.1329, step time: 0.5126\n",
      "30/388, train_loss: 0.3293, step time: 0.4972\n",
      "31/388, train_loss: 0.0478, step time: 0.5179\n",
      "32/388, train_loss: 0.1039, step time: 0.5049\n",
      "33/388, train_loss: 0.2617, step time: 0.5276\n",
      "34/388, train_loss: 0.1920, step time: 0.5515\n",
      "35/388, train_loss: 0.2693, step time: 0.5403\n",
      "36/388, train_loss: 0.2530, step time: 0.5052\n",
      "37/388, train_loss: 0.1362, step time: 1.1534\n",
      "38/388, train_loss: 0.0941, step time: 0.5388\n",
      "39/388, train_loss: 0.1131, step time: 0.4999\n",
      "40/388, train_loss: 0.1785, step time: 0.4974\n",
      "41/388, train_loss: 0.0931, step time: 0.4817\n",
      "42/388, train_loss: 0.4413, step time: 0.4751\n",
      "43/388, train_loss: 0.1303, step time: 0.9901\n",
      "44/388, train_loss: 0.3716, step time: 0.5446\n",
      "45/388, train_loss: 0.3599, step time: 0.5186\n",
      "46/388, train_loss: 0.0954, step time: 0.4956\n",
      "47/388, train_loss: 0.2032, step time: 0.5280\n",
      "48/388, train_loss: 0.4063, step time: 0.5868\n",
      "49/388, train_loss: 0.5210, step time: 0.5443\n",
      "50/388, train_loss: 0.1397, step time: 0.5184\n",
      "51/388, train_loss: 0.2937, step time: 0.5214\n",
      "52/388, train_loss: 0.4413, step time: 0.4980\n",
      "53/388, train_loss: 0.1963, step time: 0.4962\n",
      "54/388, train_loss: 0.3795, step time: 0.4796\n",
      "55/388, train_loss: 0.1543, step time: 0.5130\n",
      "56/388, train_loss: 0.2073, step time: 0.4997\n",
      "57/388, train_loss: 0.1503, step time: 1.0477\n",
      "58/388, train_loss: 0.2458, step time: 0.5597\n",
      "59/388, train_loss: 0.2802, step time: 0.5378\n",
      "60/388, train_loss: 0.1097, step time: 0.5089\n",
      "61/388, train_loss: 0.2790, step time: 0.5016\n",
      "62/388, train_loss: 0.3085, step time: 0.5058\n",
      "63/388, train_loss: 0.1309, step time: 0.4931\n",
      "64/388, train_loss: 0.1034, step time: 0.5378\n",
      "65/388, train_loss: 0.1775, step time: 0.5387\n",
      "66/388, train_loss: 0.1027, step time: 0.5348\n",
      "67/388, train_loss: 0.1648, step time: 0.5035\n",
      "68/388, train_loss: 0.0881, step time: 0.4815\n",
      "69/388, train_loss: 0.2001, step time: 0.5174\n",
      "70/388, train_loss: 0.3341, step time: 0.5001\n",
      "71/388, train_loss: 0.2690, step time: 0.4900\n",
      "72/388, train_loss: 0.5246, step time: 0.4955\n",
      "73/388, train_loss: 0.1414, step time: 0.4985\n",
      "74/388, train_loss: 0.5195, step time: 0.5174\n",
      "75/388, train_loss: 0.1139, step time: 0.5195\n",
      "76/388, train_loss: 0.1852, step time: 0.5561\n",
      "77/388, train_loss: 0.5116, step time: 0.5786\n",
      "78/388, train_loss: 0.2009, step time: 0.5164\n",
      "79/388, train_loss: 0.1056, step time: 0.4895\n",
      "80/388, train_loss: 0.2821, step time: 0.5171\n",
      "81/388, train_loss: 0.2863, step time: 0.5154\n",
      "82/388, train_loss: 0.2746, step time: 0.5978\n",
      "83/388, train_loss: 0.3277, step time: 0.5611\n",
      "84/388, train_loss: 0.2341, step time: 0.5374\n",
      "85/388, train_loss: 0.3107, step time: 0.5419\n",
      "86/388, train_loss: 0.3331, step time: 0.5581\n",
      "87/388, train_loss: 0.0897, step time: 0.5209\n",
      "88/388, train_loss: 0.3311, step time: 0.5029\n",
      "89/388, train_loss: 0.3445, step time: 1.1674\n",
      "90/388, train_loss: 0.0998, step time: 0.5358\n",
      "91/388, train_loss: 0.4505, step time: 0.5122\n",
      "92/388, train_loss: 0.2549, step time: 0.5627\n",
      "93/388, train_loss: 0.0731, step time: 0.5274\n",
      "94/388, train_loss: 0.1482, step time: 0.5012\n",
      "95/388, train_loss: 0.0801, step time: 0.4984\n",
      "96/388, train_loss: 0.3112, step time: 0.4847\n",
      "97/388, train_loss: 0.1905, step time: 0.5132\n",
      "98/388, train_loss: 0.2994, step time: 0.4993\n",
      "99/388, train_loss: 0.1904, step time: 0.4968\n",
      "100/388, train_loss: 0.1020, step time: 0.4903\n",
      "101/388, train_loss: 0.4705, step time: 0.4961\n",
      "102/388, train_loss: 0.4424, step time: 0.5275\n",
      "103/388, train_loss: 0.1611, step time: 0.5192\n",
      "104/388, train_loss: 0.3399, step time: 0.5953\n",
      "105/388, train_loss: 0.1941, step time: 0.5529\n",
      "106/388, train_loss: 0.2622, step time: 0.5194\n",
      "107/388, train_loss: 0.2584, step time: 0.5045\n",
      "108/388, train_loss: 0.2407, step time: 0.5573\n",
      "109/388, train_loss: 0.0687, step time: 0.5428\n",
      "110/388, train_loss: 0.1447, step time: 0.5244\n",
      "111/388, train_loss: 0.2719, step time: 0.5022\n",
      "112/388, train_loss: 0.1122, step time: 0.4900\n",
      "113/388, train_loss: 0.1104, step time: 0.6284\n",
      "114/388, train_loss: 0.0327, step time: 0.5426\n",
      "115/388, train_loss: 0.0566, step time: 0.5259\n",
      "116/388, train_loss: 0.7605, step time: 0.5052\n",
      "117/388, train_loss: 0.2894, step time: 0.4943\n",
      "118/388, train_loss: 0.2010, step time: 0.4802\n",
      "119/388, train_loss: 0.3839, step time: 1.0394\n",
      "120/388, train_loss: 0.2518, step time: 0.5622\n",
      "121/388, train_loss: 0.2551, step time: 0.5185\n",
      "122/388, train_loss: 0.1096, step time: 0.4985\n",
      "123/388, train_loss: 0.1297, step time: 0.5474\n",
      "124/388, train_loss: 0.3794, step time: 0.5155\n",
      "125/388, train_loss: 0.1592, step time: 0.4987\n",
      "126/388, train_loss: 0.1156, step time: 0.4929\n",
      "127/388, train_loss: 0.1795, step time: 0.4919\n",
      "128/388, train_loss: 0.0937, step time: 0.4806\n",
      "129/388, train_loss: 0.1711, step time: 0.9058\n",
      "130/388, train_loss: 0.0850, step time: 0.5456\n",
      "131/388, train_loss: 0.1723, step time: 0.5054\n",
      "132/388, train_loss: 0.2187, step time: 0.5035\n",
      "133/388, train_loss: 0.1030, step time: 0.4960\n",
      "134/388, train_loss: 0.2916, step time: 0.4913\n",
      "135/388, train_loss: 0.1496, step time: 0.5026\n",
      "136/388, train_loss: 0.2040, step time: 0.4868\n",
      "137/388, train_loss: 0.3704, step time: 1.1105\n",
      "138/388, train_loss: 0.1412, step time: 0.5347\n",
      "139/388, train_loss: 0.1614, step time: 0.5094\n",
      "140/388, train_loss: 0.1841, step time: 0.4891\n",
      "141/388, train_loss: 0.3198, step time: 0.5072\n",
      "142/388, train_loss: 0.0710, step time: 0.4965\n",
      "143/388, train_loss: 0.2888, step time: 0.4934\n",
      "144/388, train_loss: 0.0698, step time: 0.4847\n",
      "145/388, train_loss: 0.1635, step time: 0.5072\n",
      "146/388, train_loss: 0.1150, step time: 0.4911\n",
      "147/388, train_loss: 0.3050, step time: 1.1242\n",
      "148/388, train_loss: 0.2164, step time: 0.5389\n",
      "149/388, train_loss: 0.1262, step time: 0.5149\n",
      "150/388, train_loss: 0.1086, step time: 0.5011\n",
      "151/388, train_loss: 0.2000, step time: 0.4926\n",
      "152/388, train_loss: 0.1209, step time: 0.4960\n",
      "153/388, train_loss: 0.0886, step time: 0.4801\n",
      "154/388, train_loss: 0.1037, step time: 0.5128\n",
      "155/388, train_loss: 0.3617, step time: 0.4921\n",
      "156/388, train_loss: 0.0330, step time: 0.5493\n",
      "157/388, train_loss: 0.3702, step time: 0.5233\n",
      "158/388, train_loss: 0.1298, step time: 0.5867\n",
      "159/388, train_loss: 0.3674, step time: 0.5599\n",
      "160/388, train_loss: 0.2597, step time: 0.5403\n",
      "161/388, train_loss: 0.1627, step time: 0.5128\n",
      "162/388, train_loss: 0.4131, step time: 0.4973\n",
      "163/388, train_loss: 0.1691, step time: 0.4781\n",
      "164/388, train_loss: 0.1125, step time: 1.0096\n",
      "165/388, train_loss: 0.1753, step time: 0.5257\n",
      "166/388, train_loss: 0.1292, step time: 0.4986\n",
      "167/388, train_loss: 0.0933, step time: 0.4865\n",
      "168/388, train_loss: 0.4234, step time: 0.4971\n",
      "169/388, train_loss: 0.6082, step time: 0.5284\n",
      "170/388, train_loss: 0.1814, step time: 0.5023\n",
      "171/388, train_loss: 0.2895, step time: 0.4940\n",
      "172/388, train_loss: 0.2033, step time: 0.5443\n",
      "173/388, train_loss: 0.3527, step time: 0.5692\n",
      "174/388, train_loss: 0.2714, step time: 0.5287\n",
      "175/388, train_loss: 0.1151, step time: 0.5033\n",
      "176/388, train_loss: 0.1193, step time: 0.5192\n",
      "177/388, train_loss: 0.2782, step time: 0.5834\n",
      "178/388, train_loss: 0.1583, step time: 0.5386\n",
      "179/388, train_loss: 0.0964, step time: 0.5151\n",
      "180/388, train_loss: 0.2935, step time: 0.4915\n",
      "181/388, train_loss: 0.1281, step time: 0.5127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/388, train_loss: 0.3278, step time: 0.5973\n",
      "183/388, train_loss: 0.1588, step time: 0.5497\n",
      "184/388, train_loss: 0.1802, step time: 0.5169\n",
      "185/388, train_loss: 0.1378, step time: 0.4904\n",
      "186/388, train_loss: 0.3360, step time: 0.4910\n",
      "187/388, train_loss: 0.1421, step time: 0.4839\n",
      "188/388, train_loss: 0.1304, step time: 0.5401\n",
      "189/388, train_loss: 0.1192, step time: 0.5311\n",
      "190/388, train_loss: 0.1424, step time: 0.5046\n",
      "191/388, train_loss: 0.5040, step time: 0.5372\n",
      "192/388, train_loss: 0.1396, step time: 0.5151\n",
      "193/388, train_loss: 0.3456, step time: 0.4907\n",
      "194/388, train_loss: 0.3827, step time: 0.5010\n",
      "195/388, train_loss: 0.1841, step time: 1.2292\n",
      "196/388, train_loss: 0.1637, step time: 0.5390\n",
      "197/388, train_loss: 0.5235, step time: 0.5144\n",
      "198/388, train_loss: 0.0972, step time: 0.4925\n",
      "199/388, train_loss: 0.0740, step time: 0.4864\n",
      "200/388, train_loss: 0.2479, step time: 0.4931\n",
      "201/388, train_loss: 0.1363, step time: 0.4807\n",
      "202/388, train_loss: 0.1798, step time: 0.7535\n",
      "203/388, train_loss: 0.0437, step time: 0.5497\n",
      "204/388, train_loss: 0.3541, step time: 0.5365\n",
      "205/388, train_loss: 0.6314, step time: 0.5035\n",
      "206/388, train_loss: 0.3759, step time: 0.5027\n",
      "207/388, train_loss: 0.1162, step time: 0.4944\n",
      "208/388, train_loss: 0.0939, step time: 0.5456\n",
      "209/388, train_loss: 0.1174, step time: 0.5349\n",
      "210/388, train_loss: 0.2613, step time: 0.5068\n",
      "211/388, train_loss: 0.1801, step time: 0.5444\n",
      "212/388, train_loss: 0.5701, step time: 0.5311\n",
      "213/388, train_loss: 0.0532, step time: 1.1497\n",
      "214/388, train_loss: 0.2910, step time: 0.5304\n",
      "215/388, train_loss: 0.1587, step time: 0.5067\n",
      "216/388, train_loss: 0.1208, step time: 0.4961\n",
      "217/388, train_loss: 0.0949, step time: 0.4800\n",
      "218/388, train_loss: 0.1790, step time: 0.6574\n",
      "219/388, train_loss: 0.0694, step time: 0.5506\n",
      "220/388, train_loss: 0.1805, step time: 0.5172\n",
      "221/388, train_loss: 0.1580, step time: 0.4876\n",
      "222/388, train_loss: 0.2070, step time: 0.4991\n",
      "223/388, train_loss: 0.2218, step time: 0.5507\n",
      "224/388, train_loss: 0.0811, step time: 0.5245\n",
      "225/388, train_loss: 0.5853, step time: 0.5010\n",
      "226/388, train_loss: 0.1103, step time: 0.4922\n",
      "227/388, train_loss: 0.1632, step time: 0.5043\n",
      "228/388, train_loss: 0.0744, step time: 0.5266\n",
      "229/388, train_loss: 0.0939, step time: 0.5616\n",
      "230/388, train_loss: 0.0944, step time: 0.5355\n",
      "231/388, train_loss: 0.1036, step time: 0.5179\n",
      "232/388, train_loss: 0.1244, step time: 0.5005\n",
      "233/388, train_loss: 0.1475, step time: 0.4976\n",
      "234/388, train_loss: 0.1574, step time: 0.4823\n",
      "235/388, train_loss: 0.0874, step time: 1.0249\n",
      "236/388, train_loss: 0.1322, step time: 0.5357\n",
      "237/388, train_loss: 0.0968, step time: 0.5094\n",
      "238/388, train_loss: 0.1985, step time: 0.4895\n",
      "239/388, train_loss: 0.0738, step time: 1.1332\n",
      "240/388, train_loss: 0.2532, step time: 0.5426\n",
      "241/388, train_loss: 0.4681, step time: 0.5242\n",
      "242/388, train_loss: 0.5800, step time: 0.4971\n",
      "243/388, train_loss: 0.5047, step time: 0.5050\n",
      "244/388, train_loss: 0.2983, step time: 0.5193\n",
      "245/388, train_loss: 0.0630, step time: 0.5030\n",
      "246/388, train_loss: 0.5000, step time: 0.4881\n",
      "247/388, train_loss: 0.2909, step time: 0.5494\n",
      "248/388, train_loss: 0.1535, step time: 0.6335\n",
      "249/388, train_loss: 0.4498, step time: 0.5440\n",
      "250/388, train_loss: 0.2235, step time: 0.4986\n",
      "251/388, train_loss: 0.1293, step time: 0.5003\n",
      "252/388, train_loss: 0.3337, step time: 0.4846\n",
      "253/388, train_loss: 0.1266, step time: 0.4982\n",
      "254/388, train_loss: 0.1804, step time: 0.4987\n",
      "255/388, train_loss: 0.2002, step time: 0.4897\n",
      "256/388, train_loss: 0.2267, step time: 0.5234\n",
      "257/388, train_loss: 0.0882, step time: 0.5007\n",
      "258/388, train_loss: 0.0991, step time: 0.5003\n",
      "259/388, train_loss: 0.1242, step time: 0.4824\n",
      "260/388, train_loss: 0.0534, step time: 0.4915\n",
      "261/388, train_loss: 0.0821, step time: 0.8364\n",
      "262/388, train_loss: 0.1713, step time: 0.5564\n",
      "263/388, train_loss: 0.1246, step time: 0.5097\n",
      "264/388, train_loss: 0.2854, step time: 0.4941\n",
      "265/388, train_loss: 0.0926, step time: 0.5203\n",
      "266/388, train_loss: 0.2051, step time: 0.5008\n",
      "267/388, train_loss: 0.2743, step time: 0.4938\n",
      "268/388, train_loss: 0.0888, step time: 0.4845\n",
      "269/388, train_loss: 0.1173, step time: 0.5480\n",
      "270/388, train_loss: 0.1721, step time: 0.5539\n",
      "271/388, train_loss: 0.2764, step time: 0.5169\n",
      "272/388, train_loss: 0.1479, step time: 0.5040\n",
      "273/388, train_loss: 0.1593, step time: 0.5050\n",
      "274/388, train_loss: 0.1760, step time: 0.5233\n",
      "275/388, train_loss: 0.1569, step time: 0.5263\n",
      "276/388, train_loss: 0.2722, step time: 0.5112\n",
      "277/388, train_loss: 0.2297, step time: 0.4993\n",
      "278/388, train_loss: 0.0712, step time: 0.9442\n",
      "279/388, train_loss: 0.1874, step time: 0.5392\n",
      "280/388, train_loss: 0.1500, step time: 0.5112\n",
      "281/388, train_loss: 0.2163, step time: 0.4995\n",
      "282/388, train_loss: 0.0806, step time: 0.4960\n",
      "283/388, train_loss: 0.3057, step time: 0.4956\n",
      "284/388, train_loss: 0.3196, step time: 0.4883\n",
      "285/388, train_loss: 0.1197, step time: 0.5020\n",
      "286/388, train_loss: 0.2704, step time: 0.4948\n",
      "287/388, train_loss: 0.2578, step time: 0.4938\n",
      "288/388, train_loss: 0.2981, step time: 0.5301\n",
      "289/388, train_loss: 0.1612, step time: 0.5359\n",
      "290/388, train_loss: 0.1798, step time: 0.5186\n",
      "291/388, train_loss: 0.0947, step time: 0.5017\n",
      "292/388, train_loss: 0.1248, step time: 0.5008\n",
      "293/388, train_loss: 0.2189, step time: 0.4870\n",
      "294/388, train_loss: 0.1624, step time: 0.5130\n",
      "295/388, train_loss: 0.0730, step time: 0.5171\n",
      "296/388, train_loss: 0.2024, step time: 0.5720\n",
      "297/388, train_loss: 0.5219, step time: 0.5557\n",
      "298/388, train_loss: 0.0786, step time: 0.5322\n",
      "299/388, train_loss: 0.1621, step time: 0.5207\n",
      "300/388, train_loss: 0.1310, step time: 0.5024\n",
      "301/388, train_loss: 0.2422, step time: 0.5319\n",
      "302/388, train_loss: 0.0655, step time: 0.5154\n",
      "303/388, train_loss: 0.3232, step time: 0.5276\n",
      "304/388, train_loss: 0.1529, step time: 0.5015\n",
      "305/388, train_loss: 0.2449, step time: 0.4845\n",
      "306/388, train_loss: 0.3605, step time: 1.1371\n",
      "307/388, train_loss: 0.2195, step time: 0.5494\n",
      "308/388, train_loss: 0.0579, step time: 0.5232\n",
      "309/388, train_loss: 0.2756, step time: 0.5006\n",
      "310/388, train_loss: 0.2251, step time: 0.4981\n",
      "311/388, train_loss: 0.1350, step time: 0.4967\n",
      "312/388, train_loss: 0.2045, step time: 0.4838\n",
      "313/388, train_loss: 0.0633, step time: 0.4912\n",
      "314/388, train_loss: 0.1180, step time: 1.0906\n",
      "315/388, train_loss: 0.1624, step time: 0.5479\n",
      "316/388, train_loss: 0.2016, step time: 0.5218\n",
      "317/388, train_loss: 0.2304, step time: 0.4949\n",
      "318/388, train_loss: 0.1157, step time: 0.4993\n",
      "319/388, train_loss: 0.2130, step time: 0.4888\n",
      "320/388, train_loss: 0.4282, step time: 0.8110\n",
      "321/388, train_loss: 0.0610, step time: 0.5564\n",
      "322/388, train_loss: 0.1833, step time: 0.5330\n",
      "323/388, train_loss: 0.2472, step time: 0.5083\n",
      "324/388, train_loss: 0.2465, step time: 0.5227\n",
      "325/388, train_loss: 0.1696, step time: 0.5126\n",
      "326/388, train_loss: 0.1134, step time: 0.5046\n",
      "327/388, train_loss: 0.3588, step time: 0.4860\n",
      "328/388, train_loss: 0.2466, step time: 0.4943\n",
      "329/388, train_loss: 0.0872, step time: 0.5104\n",
      "330/388, train_loss: 0.2854, step time: 0.5533\n",
      "331/388, train_loss: 0.4256, step time: 0.5111\n",
      "332/388, train_loss: 0.1915, step time: 0.5014\n",
      "333/388, train_loss: 0.1386, step time: 0.4805\n",
      "334/388, train_loss: 0.2076, step time: 0.4744\n",
      "335/388, train_loss: 0.1913, step time: 0.9904\n",
      "336/388, train_loss: 0.1536, step time: 0.5326\n",
      "337/388, train_loss: 0.0902, step time: 0.5093\n",
      "338/388, train_loss: 0.1098, step time: 0.4900\n",
      "339/388, train_loss: 0.0965, step time: 0.4903\n",
      "340/388, train_loss: 0.0978, step time: 0.4911\n",
      "341/388, train_loss: 0.1022, step time: 0.4776\n",
      "342/388, train_loss: 0.1981, step time: 0.5011\n",
      "343/388, train_loss: 0.0971, step time: 0.4847\n",
      "344/388, train_loss: 0.1891, step time: 0.5055\n",
      "345/388, train_loss: 0.2208, step time: 0.5079\n",
      "346/388, train_loss: 0.3125, step time: 0.5058\n",
      "347/388, train_loss: 0.2156, step time: 0.5165\n",
      "348/388, train_loss: 0.1850, step time: 0.5039\n",
      "349/388, train_loss: 0.1258, step time: 0.5259\n",
      "350/388, train_loss: 0.0782, step time: 0.5094\n",
      "351/388, train_loss: 0.0459, step time: 0.4969\n",
      "352/388, train_loss: 0.1149, step time: 0.5147\n",
      "353/388, train_loss: 0.0932, step time: 0.5130\n",
      "354/388, train_loss: 0.2003, step time: 0.5720\n",
      "355/388, train_loss: 0.2192, step time: 0.5411\n",
      "356/388, train_loss: 0.1187, step time: 0.5103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/388, train_loss: 0.2326, step time: 0.4958\n",
      "358/388, train_loss: 0.3768, step time: 0.4934\n",
      "359/388, train_loss: 0.3692, step time: 0.5191\n",
      "360/388, train_loss: 0.1289, step time: 0.5119\n",
      "361/388, train_loss: 0.1129, step time: 0.5369\n",
      "362/388, train_loss: 0.1469, step time: 0.5108\n",
      "363/388, train_loss: 0.3343, step time: 0.5035\n",
      "364/388, train_loss: 0.1268, step time: 0.4933\n",
      "365/388, train_loss: 0.0704, step time: 0.4906\n",
      "366/388, train_loss: 0.1081, step time: 0.4828\n",
      "367/388, train_loss: 0.3550, step time: 0.4846\n",
      "368/388, train_loss: 0.2238, step time: 0.4962\n",
      "369/388, train_loss: 0.2113, step time: 0.5015\n",
      "370/388, train_loss: 0.8094, step time: 0.5001\n",
      "371/388, train_loss: 0.1107, step time: 0.5772\n",
      "372/388, train_loss: 0.1541, step time: 0.6288\n",
      "373/388, train_loss: 0.2544, step time: 0.5427\n",
      "374/388, train_loss: 0.3084, step time: 0.5265\n",
      "375/388, train_loss: 0.1991, step time: 0.4969\n",
      "376/388, train_loss: 0.1927, step time: 0.4973\n",
      "377/388, train_loss: 0.6225, step time: 1.1008\n",
      "378/388, train_loss: 0.4413, step time: 0.5350\n",
      "379/388, train_loss: 0.7076, step time: 0.5095\n",
      "380/388, train_loss: 0.2473, step time: 0.4890\n",
      "381/388, train_loss: 0.2320, step time: 0.4923\n",
      "382/388, train_loss: 0.1109, step time: 0.4879\n",
      "383/388, train_loss: 0.1956, step time: 0.4812\n",
      "384/388, train_loss: 0.1381, step time: 0.4823\n",
      "385/388, train_loss: 0.2225, step time: 0.4713\n",
      "386/388, train_loss: 0.3770, step time: 0.4717\n",
      "387/388, train_loss: 0.2807, step time: 0.5032\n",
      "388/388, train_loss: 0.0896, step time: 0.5814\n",
      "epoch 31 average loss: 0.2149\n",
      "current epoch: 31 current mean dice: 0.7454 tc: 0.7971 wt: 0.8835 et: 0.5557\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 31 is: 300.3261\n",
      "----------\n",
      "epoch 32/300\n",
      "1/388, train_loss: 0.0825, step time: 0.4767\n",
      "2/388, train_loss: 0.0945, step time: 0.5045\n",
      "3/388, train_loss: 0.2914, step time: 0.7132\n",
      "4/388, train_loss: 0.2516, step time: 0.5694\n",
      "5/388, train_loss: 0.1926, step time: 0.5135\n",
      "6/388, train_loss: 0.0888, step time: 0.4969\n",
      "7/388, train_loss: 0.2218, step time: 0.4961\n",
      "8/388, train_loss: 0.0944, step time: 0.5513\n",
      "9/388, train_loss: 0.2100, step time: 0.5672\n",
      "10/388, train_loss: 0.3393, step time: 0.5439\n",
      "11/388, train_loss: 0.1230, step time: 0.4908\n",
      "12/388, train_loss: 0.1826, step time: 0.5630\n",
      "13/388, train_loss: 0.1842, step time: 0.5425\n",
      "14/388, train_loss: 0.1608, step time: 0.5157\n",
      "15/388, train_loss: 0.1583, step time: 0.4874\n",
      "16/388, train_loss: 0.2272, step time: 0.9561\n",
      "17/388, train_loss: 0.1964, step time: 0.5433\n",
      "18/388, train_loss: 0.1646, step time: 0.5241\n",
      "19/388, train_loss: 0.1220, step time: 0.5130\n",
      "20/388, train_loss: 0.3695, step time: 0.5688\n",
      "21/388, train_loss: 0.0813, step time: 0.5452\n",
      "22/388, train_loss: 0.0919, step time: 0.5194\n",
      "23/388, train_loss: 0.1128, step time: 0.4993\n",
      "24/388, train_loss: 0.1598, step time: 0.5424\n",
      "25/388, train_loss: 0.1003, step time: 0.5687\n",
      "26/388, train_loss: 0.2251, step time: 0.5357\n",
      "27/388, train_loss: 0.4133, step time: 0.5119\n",
      "28/388, train_loss: 0.2722, step time: 0.9192\n",
      "29/388, train_loss: 0.1395, step time: 0.5476\n",
      "30/388, train_loss: 0.0798, step time: 0.5231\n",
      "31/388, train_loss: 0.2756, step time: 0.5025\n",
      "32/388, train_loss: 0.0673, step time: 0.4883\n",
      "33/388, train_loss: 0.2404, step time: 0.4807\n",
      "34/388, train_loss: 0.1144, step time: 1.0339\n",
      "35/388, train_loss: 0.3897, step time: 0.5578\n",
      "36/388, train_loss: 0.1980, step time: 0.5111\n",
      "37/388, train_loss: 0.1300, step time: 0.4985\n",
      "38/388, train_loss: 0.1476, step time: 0.5001\n",
      "39/388, train_loss: 0.0799, step time: 0.4965\n",
      "40/388, train_loss: 0.2155, step time: 0.4849\n",
      "41/388, train_loss: 0.1183, step time: 0.4889\n",
      "42/388, train_loss: 0.2445, step time: 0.4804\n",
      "43/388, train_loss: 0.2540, step time: 0.4838\n",
      "44/388, train_loss: 0.2456, step time: 0.4929\n",
      "45/388, train_loss: 0.0521, step time: 1.0218\n",
      "46/388, train_loss: 0.0961, step time: 0.5343\n",
      "47/388, train_loss: 0.2256, step time: 0.5027\n",
      "48/388, train_loss: 0.2864, step time: 0.4953\n",
      "49/388, train_loss: 0.2269, step time: 0.4845\n",
      "50/388, train_loss: 0.4256, step time: 0.5061\n",
      "51/388, train_loss: 0.0855, step time: 0.4989\n",
      "52/388, train_loss: 0.2240, step time: 0.4970\n",
      "53/388, train_loss: 0.1290, step time: 0.9501\n",
      "54/388, train_loss: 0.2423, step time: 0.5440\n",
      "55/388, train_loss: 0.1719, step time: 0.5170\n",
      "56/388, train_loss: 0.2703, step time: 0.4958\n",
      "57/388, train_loss: 0.1468, step time: 0.5562\n",
      "58/388, train_loss: 0.0793, step time: 0.5286\n",
      "59/388, train_loss: 0.0896, step time: 0.4987\n",
      "60/388, train_loss: 0.0480, step time: 0.4998\n",
      "61/388, train_loss: 0.1893, step time: 0.4824\n",
      "62/388, train_loss: 0.2539, step time: 0.4788\n",
      "63/388, train_loss: 0.3858, step time: 0.4816\n",
      "64/388, train_loss: 0.2040, step time: 0.4851\n",
      "65/388, train_loss: 0.2471, step time: 0.4989\n",
      "66/388, train_loss: 0.1430, step time: 0.6820\n",
      "67/388, train_loss: 0.0955, step time: 0.5353\n",
      "68/388, train_loss: 0.3735, step time: 0.5152\n",
      "69/388, train_loss: 0.0364, step time: 0.4973\n",
      "70/388, train_loss: 0.1339, step time: 0.4880\n",
      "71/388, train_loss: 0.4476, step time: 1.1803\n",
      "72/388, train_loss: 0.1201, step time: 0.5267\n",
      "73/388, train_loss: 0.2248, step time: 0.5070\n",
      "74/388, train_loss: 0.1512, step time: 0.4940\n",
      "75/388, train_loss: 0.1515, step time: 0.4869\n",
      "76/388, train_loss: 0.2695, step time: 0.4917\n",
      "77/388, train_loss: 0.4010, step time: 0.4782\n",
      "78/388, train_loss: 0.1488, step time: 0.8599\n",
      "79/388, train_loss: 0.2253, step time: 0.5376\n",
      "80/388, train_loss: 0.5069, step time: 0.5161\n",
      "81/388, train_loss: 0.1599, step time: 0.5638\n",
      "82/388, train_loss: 0.2183, step time: 0.5242\n",
      "83/388, train_loss: 0.2847, step time: 0.5010\n",
      "84/388, train_loss: 0.1252, step time: 0.4941\n",
      "85/388, train_loss: 0.3640, step time: 0.4841\n",
      "86/388, train_loss: 0.0534, step time: 0.4784\n",
      "87/388, train_loss: 0.5722, step time: 0.4935\n",
      "88/388, train_loss: 0.1623, step time: 0.5271\n",
      "89/388, train_loss: 0.0689, step time: 0.5230\n",
      "90/388, train_loss: 0.1016, step time: 0.4993\n",
      "91/388, train_loss: 0.3254, step time: 0.4935\n",
      "92/388, train_loss: 0.2892, step time: 0.4923\n",
      "93/388, train_loss: 0.4188, step time: 0.4981\n",
      "94/388, train_loss: 0.1657, step time: 0.4966\n",
      "95/388, train_loss: 0.2324, step time: 0.4908\n",
      "96/388, train_loss: 0.2189, step time: 0.4897\n",
      "97/388, train_loss: 0.0795, step time: 0.4856\n",
      "98/388, train_loss: 0.1588, step time: 0.4861\n",
      "99/388, train_loss: 0.1909, step time: 0.6056\n",
      "100/388, train_loss: 0.2076, step time: 0.5726\n",
      "101/388, train_loss: 0.1807, step time: 0.5221\n",
      "102/388, train_loss: 0.0334, step time: 0.5042\n",
      "103/388, train_loss: 0.5817, step time: 0.5052\n",
      "104/388, train_loss: 0.1511, step time: 0.5242\n",
      "105/388, train_loss: 0.2342, step time: 0.5232\n",
      "106/388, train_loss: 0.1283, step time: 0.4919\n",
      "107/388, train_loss: 0.2313, step time: 0.9647\n",
      "108/388, train_loss: 0.3928, step time: 0.5429\n",
      "109/388, train_loss: 0.2447, step time: 0.5178\n",
      "110/388, train_loss: 0.1252, step time: 0.5019\n",
      "111/388, train_loss: 0.3338, step time: 0.5005\n",
      "112/388, train_loss: 0.2592, step time: 0.5045\n",
      "113/388, train_loss: 0.1382, step time: 0.5069\n",
      "114/388, train_loss: 0.1450, step time: 0.5008\n",
      "115/388, train_loss: 0.3387, step time: 0.4868\n",
      "116/388, train_loss: 0.0575, step time: 0.5203\n",
      "117/388, train_loss: 0.2350, step time: 0.4999\n",
      "118/388, train_loss: 0.1357, step time: 0.4935\n",
      "119/388, train_loss: 0.1298, step time: 0.4834\n",
      "120/388, train_loss: 0.1789, step time: 0.4758\n",
      "121/388, train_loss: 0.0807, step time: 1.0392\n",
      "122/388, train_loss: 0.2043, step time: 0.5342\n",
      "123/388, train_loss: 0.0393, step time: 0.5074\n",
      "124/388, train_loss: 0.2062, step time: 0.4894\n",
      "125/388, train_loss: 0.2939, step time: 0.4973\n",
      "126/388, train_loss: 0.3777, step time: 0.4821\n",
      "127/388, train_loss: 0.1619, step time: 0.4752\n",
      "128/388, train_loss: 0.1097, step time: 0.4968\n",
      "129/388, train_loss: 0.4371, step time: 0.4846\n",
      "130/388, train_loss: 0.2610, step time: 0.5012\n",
      "131/388, train_loss: 0.2489, step time: 0.4894\n",
      "132/388, train_loss: 0.0623, step time: 0.4870\n",
      "133/388, train_loss: 0.0516, step time: 0.5004\n",
      "134/388, train_loss: 0.1981, step time: 0.4836\n",
      "135/388, train_loss: 0.0970, step time: 0.4968\n",
      "136/388, train_loss: 0.3209, step time: 0.4797\n",
      "137/388, train_loss: 0.3785, step time: 1.0263\n",
      "138/388, train_loss: 0.1590, step time: 0.5446\n",
      "139/388, train_loss: 0.1573, step time: 0.5118\n",
      "140/388, train_loss: 0.1193, step time: 0.4928\n",
      "141/388, train_loss: 0.0987, step time: 0.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/388, train_loss: 0.4901, step time: 0.4856\n",
      "143/388, train_loss: 0.1409, step time: 0.4958\n",
      "144/388, train_loss: 0.2596, step time: 0.5295\n",
      "145/388, train_loss: 0.1224, step time: 0.6642\n",
      "146/388, train_loss: 0.2083, step time: 0.5462\n",
      "147/388, train_loss: 0.1755, step time: 0.5100\n",
      "148/388, train_loss: 0.0865, step time: 0.4967\n",
      "149/388, train_loss: 0.0850, step time: 0.4878\n",
      "150/388, train_loss: 0.2272, step time: 0.4896\n",
      "151/388, train_loss: 0.1227, step time: 0.4778\n",
      "152/388, train_loss: 0.2104, step time: 0.4774\n",
      "153/388, train_loss: 0.0998, step time: 0.4902\n",
      "154/388, train_loss: 0.3335, step time: 0.4876\n",
      "155/388, train_loss: 0.1588, step time: 0.5370\n",
      "156/388, train_loss: 0.2753, step time: 0.5391\n",
      "157/388, train_loss: 0.2573, step time: 0.5141\n",
      "158/388, train_loss: 0.1042, step time: 0.5015\n",
      "159/388, train_loss: 0.3078, step time: 0.5211\n",
      "160/388, train_loss: 0.1376, step time: 0.5113\n",
      "161/388, train_loss: 0.2900, step time: 0.4966\n",
      "162/388, train_loss: 0.1202, step time: 1.0030\n",
      "163/388, train_loss: 0.3573, step time: 0.5475\n",
      "164/388, train_loss: 0.1860, step time: 0.5230\n",
      "165/388, train_loss: 0.1621, step time: 0.5070\n",
      "166/388, train_loss: 0.0859, step time: 0.5058\n",
      "167/388, train_loss: 0.0700, step time: 0.4995\n",
      "168/388, train_loss: 0.2077, step time: 0.4814\n",
      "169/388, train_loss: 0.4150, step time: 0.7601\n",
      "170/388, train_loss: 0.2746, step time: 0.5341\n",
      "171/388, train_loss: 0.1871, step time: 0.5041\n",
      "172/388, train_loss: 0.1028, step time: -4.8617\n",
      "173/388, train_loss: 0.0970, step time: 0.4919\n",
      "174/388, train_loss: 0.2267, step time: 0.5112\n",
      "175/388, train_loss: 0.1469, step time: 0.4925\n",
      "176/388, train_loss: 0.2295, step time: 0.9873\n",
      "177/388, train_loss: 0.3278, step time: 0.5373\n",
      "178/388, train_loss: 0.3770, step time: 0.5155\n",
      "179/388, train_loss: 0.3304, step time: 0.4972\n",
      "180/388, train_loss: 0.4137, step time: 0.4840\n",
      "181/388, train_loss: 0.1092, step time: 0.4960\n",
      "182/388, train_loss: 0.1316, step time: 0.4808\n",
      "183/388, train_loss: 0.0746, step time: 0.4752\n",
      "184/388, train_loss: 0.5977, step time: 0.5484\n",
      "185/388, train_loss: 0.2869, step time: 0.5240\n",
      "186/388, train_loss: 0.2620, step time: 0.4991\n",
      "187/388, train_loss: 0.0848, step time: 0.4956\n",
      "188/388, train_loss: 0.1829, step time: 1.1912\n",
      "189/388, train_loss: 0.1918, step time: 0.5425\n",
      "190/388, train_loss: 0.2360, step time: 0.5169\n",
      "191/388, train_loss: 0.2826, step time: 0.4990\n",
      "192/388, train_loss: 0.2737, step time: 0.4871\n",
      "193/388, train_loss: 0.2986, step time: 0.5104\n",
      "194/388, train_loss: 0.2646, step time: 0.4899\n",
      "195/388, train_loss: 0.2687, step time: 0.4965\n",
      "196/388, train_loss: 0.0969, step time: 0.4727\n",
      "197/388, train_loss: 0.1092, step time: 0.5252\n",
      "198/388, train_loss: 0.1568, step time: 0.5209\n",
      "199/388, train_loss: 0.0577, step time: 0.5221\n",
      "200/388, train_loss: 0.3415, step time: 0.5231\n",
      "201/388, train_loss: 0.1071, step time: 0.4982\n",
      "202/388, train_loss: 0.3917, step time: 0.4841\n",
      "203/388, train_loss: 0.1171, step time: 0.8564\n",
      "204/388, train_loss: 0.5667, step time: 0.5491\n",
      "205/388, train_loss: 0.3283, step time: 0.5223\n",
      "206/388, train_loss: 0.1826, step time: 0.5138\n",
      "207/388, train_loss: 0.1444, step time: 0.4948\n",
      "208/388, train_loss: 0.1074, step time: 0.4924\n",
      "209/388, train_loss: 0.0996, step time: 0.5094\n",
      "210/388, train_loss: 0.4908, step time: 0.5672\n",
      "211/388, train_loss: 0.2123, step time: 0.5357\n",
      "212/388, train_loss: 0.1599, step time: 0.5145\n",
      "213/388, train_loss: 0.2029, step time: 0.5142\n",
      "214/388, train_loss: 0.2120, step time: 0.5214\n",
      "215/388, train_loss: 0.2735, step time: 0.4988\n",
      "216/388, train_loss: 0.1817, step time: 0.4909\n",
      "217/388, train_loss: 0.1055, step time: 0.4998\n",
      "218/388, train_loss: 0.2948, step time: 0.5176\n",
      "219/388, train_loss: 0.3343, step time: 0.4989\n",
      "220/388, train_loss: 0.2924, step time: 0.5057\n",
      "221/388, train_loss: 0.0982, step time: 0.4919\n",
      "222/388, train_loss: 0.1786, step time: 0.4885\n",
      "223/388, train_loss: 0.0706, step time: 0.4890\n",
      "224/388, train_loss: 0.2541, step time: 0.5104\n",
      "225/388, train_loss: 0.2450, step time: 0.5196\n",
      "226/388, train_loss: 0.0798, step time: 0.4979\n",
      "227/388, train_loss: 0.1631, step time: 0.5002\n",
      "228/388, train_loss: 0.1272, step time: 0.5029\n",
      "229/388, train_loss: 0.3020, step time: 0.4907\n",
      "230/388, train_loss: 0.2895, step time: 0.5008\n",
      "231/388, train_loss: 0.1657, step time: 0.5142\n",
      "232/388, train_loss: 0.2285, step time: 0.5024\n",
      "233/388, train_loss: 0.2089, step time: 1.1412\n",
      "234/388, train_loss: 0.0911, step time: 0.5347\n",
      "235/388, train_loss: 0.2322, step time: 0.5028\n",
      "236/388, train_loss: 0.1776, step time: 0.4833\n",
      "237/388, train_loss: 0.2086, step time: 0.5172\n",
      "238/388, train_loss: 0.1318, step time: 0.6154\n",
      "239/388, train_loss: 0.1073, step time: 0.5192\n",
      "240/388, train_loss: 0.1675, step time: 0.4966\n",
      "241/388, train_loss: 0.1689, step time: 0.4830\n",
      "242/388, train_loss: 0.1545, step time: 1.1603\n",
      "243/388, train_loss: 0.3233, step time: 0.5429\n",
      "244/388, train_loss: 0.1793, step time: 0.5087\n",
      "245/388, train_loss: 0.1704, step time: 0.4941\n",
      "246/388, train_loss: 0.2339, step time: 0.4889\n",
      "247/388, train_loss: 0.2309, step time: 0.4924\n",
      "248/388, train_loss: 0.1958, step time: 0.5444\n",
      "249/388, train_loss: 0.0934, step time: 0.5208\n",
      "250/388, train_loss: 0.1196, step time: 0.5133\n",
      "251/388, train_loss: 0.3265, step time: 0.5086\n",
      "252/388, train_loss: 0.1299, step time: 1.0376\n",
      "253/388, train_loss: 0.3843, step time: 0.5339\n",
      "254/388, train_loss: 0.1700, step time: 0.4994\n",
      "255/388, train_loss: 0.2281, step time: 0.4967\n",
      "256/388, train_loss: 0.1933, step time: 0.4864\n",
      "257/388, train_loss: 0.5203, step time: 0.4887\n",
      "258/388, train_loss: 0.1246, step time: 0.9144\n",
      "259/388, train_loss: 0.1252, step time: 0.5376\n",
      "260/388, train_loss: 0.0808, step time: 0.5140\n",
      "261/388, train_loss: 0.0746, step time: 0.5053\n",
      "262/388, train_loss: 0.1587, step time: 0.4959\n",
      "263/388, train_loss: 0.3024, step time: 0.4923\n",
      "264/388, train_loss: 0.1757, step time: 0.4921\n",
      "265/388, train_loss: 0.3722, step time: 0.4976\n",
      "266/388, train_loss: 0.0697, step time: 0.4961\n",
      "267/388, train_loss: 0.1292, step time: 0.4819\n",
      "268/388, train_loss: 0.0970, step time: 0.9368\n",
      "269/388, train_loss: 0.1676, step time: 0.5683\n",
      "270/388, train_loss: 0.3653, step time: 0.5197\n",
      "271/388, train_loss: 0.2143, step time: 0.4987\n",
      "272/388, train_loss: 0.4897, step time: 0.4859\n",
      "273/388, train_loss: 0.2665, step time: 0.4930\n",
      "274/388, train_loss: 0.6386, step time: 0.9719\n",
      "275/388, train_loss: 0.1432, step time: 0.5374\n",
      "276/388, train_loss: 0.1516, step time: 0.5054\n",
      "277/388, train_loss: 0.2704, step time: 0.4850\n",
      "278/388, train_loss: 0.0950, step time: 0.4965\n",
      "279/388, train_loss: 0.1300, step time: 0.4803\n",
      "280/388, train_loss: 0.1200, step time: 0.5115\n",
      "281/388, train_loss: 0.4918, step time: 0.5019\n",
      "282/388, train_loss: 0.1934, step time: 0.4989\n",
      "283/388, train_loss: 0.3039, step time: 0.4902\n",
      "284/388, train_loss: 0.1195, step time: 0.4986\n",
      "285/388, train_loss: 0.0890, step time: 0.4930\n",
      "286/388, train_loss: 0.2914, step time: 0.4851\n",
      "287/388, train_loss: 0.0800, step time: 0.4839\n",
      "288/388, train_loss: 0.1781, step time: 0.4943\n",
      "289/388, train_loss: 0.1261, step time: 0.4926\n",
      "290/388, train_loss: 0.1466, step time: 0.9262\n",
      "291/388, train_loss: 0.3049, step time: 0.5458\n",
      "292/388, train_loss: 0.3228, step time: 0.5211\n",
      "293/388, train_loss: 0.2238, step time: 0.5076\n",
      "294/388, train_loss: 0.1173, step time: 0.4865\n",
      "295/388, train_loss: 0.5122, step time: 0.5233\n",
      "296/388, train_loss: 0.1767, step time: 0.4990\n",
      "297/388, train_loss: 0.0718, step time: 1.1145\n",
      "298/388, train_loss: 0.1591, step time: 0.5346\n",
      "299/388, train_loss: 0.2744, step time: 0.5197\n",
      "300/388, train_loss: 0.3137, step time: 0.5056\n",
      "301/388, train_loss: 0.1508, step time: 0.5235\n",
      "302/388, train_loss: 0.1219, step time: 0.4973\n",
      "303/388, train_loss: 0.2188, step time: 0.4954\n",
      "304/388, train_loss: 0.1461, step time: 0.5005\n",
      "305/388, train_loss: 0.2099, step time: 0.4847\n",
      "306/388, train_loss: 0.2246, step time: 0.5015\n",
      "307/388, train_loss: 0.0672, step time: 0.4906\n",
      "308/388, train_loss: 0.1253, step time: 0.5080\n",
      "309/388, train_loss: 0.4010, step time: 0.5137\n",
      "310/388, train_loss: 0.2791, step time: 0.9311\n",
      "311/388, train_loss: 0.2167, step time: 0.5377\n",
      "312/388, train_loss: 0.2075, step time: 0.5010\n",
      "313/388, train_loss: 0.0882, step time: 0.4998\n",
      "314/388, train_loss: 0.2233, step time: 0.4935\n",
      "315/388, train_loss: 0.2336, step time: 0.4926\n",
      "316/388, train_loss: 0.1116, step time: 0.4796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/388, train_loss: 0.1598, step time: 0.5820\n",
      "318/388, train_loss: 0.2311, step time: 0.5507\n",
      "319/388, train_loss: 0.1136, step time: 0.5200\n",
      "320/388, train_loss: 0.1131, step time: 0.5049\n",
      "321/388, train_loss: 0.4707, step time: 0.4845\n",
      "322/388, train_loss: 0.1314, step time: 0.5148\n",
      "323/388, train_loss: 0.2377, step time: 0.4928\n",
      "324/388, train_loss: 0.1158, step time: 0.5189\n",
      "325/388, train_loss: 0.1150, step time: 0.6017\n",
      "326/388, train_loss: 0.2050, step time: 0.5269\n",
      "327/388, train_loss: 0.1842, step time: 0.4882\n",
      "328/388, train_loss: 0.1017, step time: 1.1316\n",
      "329/388, train_loss: 0.2795, step time: 0.5429\n",
      "330/388, train_loss: 0.1082, step time: 0.5015\n",
      "331/388, train_loss: 0.2927, step time: 0.4977\n",
      "332/388, train_loss: 0.1503, step time: 0.4866\n",
      "333/388, train_loss: 0.1072, step time: 0.5048\n",
      "334/388, train_loss: 0.2598, step time: 0.5183\n",
      "335/388, train_loss: 0.4880, step time: 0.5704\n",
      "336/388, train_loss: 0.2994, step time: 0.5406\n",
      "337/388, train_loss: 0.1408, step time: 0.5146\n",
      "338/388, train_loss: 0.1030, step time: 0.4980\n",
      "339/388, train_loss: 0.0842, step time: 0.4895\n",
      "340/388, train_loss: 0.1763, step time: 0.4962\n",
      "341/388, train_loss: 0.1208, step time: 0.4864\n",
      "342/388, train_loss: 0.3009, step time: 0.4988\n",
      "343/388, train_loss: 0.1173, step time: 0.8346\n",
      "344/388, train_loss: 0.1825, step time: 0.5579\n",
      "345/388, train_loss: 0.1362, step time: 0.5279\n",
      "346/388, train_loss: 0.1341, step time: 0.4988\n",
      "347/388, train_loss: 0.1071, step time: 0.5058\n",
      "348/388, train_loss: 0.1150, step time: 0.4840\n",
      "349/388, train_loss: 0.1562, step time: 0.4881\n",
      "350/388, train_loss: 0.1050, step time: 1.1905\n",
      "351/388, train_loss: 0.0980, step time: 0.5470\n",
      "352/388, train_loss: 0.3236, step time: 0.5150\n",
      "353/388, train_loss: 0.1241, step time: 0.5032\n",
      "354/388, train_loss: 0.2288, step time: 0.4937\n",
      "355/388, train_loss: 0.3852, step time: 0.4842\n",
      "356/388, train_loss: 0.1914, step time: 0.5038\n",
      "357/388, train_loss: 0.1724, step time: 0.5002\n",
      "358/388, train_loss: 0.3671, step time: 0.4954\n",
      "359/388, train_loss: 0.1468, step time: 0.7768\n",
      "360/388, train_loss: 0.1783, step time: 0.5646\n",
      "361/388, train_loss: 0.1394, step time: 0.5248\n",
      "362/388, train_loss: 0.2766, step time: 0.5098\n",
      "363/388, train_loss: 0.1821, step time: 0.5485\n",
      "364/388, train_loss: 0.4931, step time: 0.5302\n",
      "365/388, train_loss: 0.0689, step time: 0.5020\n",
      "366/388, train_loss: 0.1801, step time: 0.4957\n",
      "367/388, train_loss: 0.6811, step time: 0.5641\n",
      "368/388, train_loss: 0.1728, step time: 0.5355\n",
      "369/388, train_loss: 0.1506, step time: 0.5061\n",
      "370/388, train_loss: 0.3567, step time: 0.4961\n",
      "371/388, train_loss: 0.1542, step time: 0.4978\n",
      "372/388, train_loss: 0.0748, step time: 0.5034\n",
      "373/388, train_loss: 0.2727, step time: 1.0338\n",
      "374/388, train_loss: 0.1055, step time: 0.5330\n",
      "375/388, train_loss: 0.1156, step time: 0.5093\n",
      "376/388, train_loss: 0.1947, step time: 0.4973\n",
      "377/388, train_loss: 0.1450, step time: 0.4891\n",
      "378/388, train_loss: 0.5120, step time: 0.5262\n",
      "379/388, train_loss: 0.1493, step time: 0.5033\n",
      "380/388, train_loss: 0.4254, step time: 0.4927\n",
      "381/388, train_loss: 0.0901, step time: 1.0226\n",
      "382/388, train_loss: 0.0906, step time: 0.5405\n",
      "383/388, train_loss: 0.0848, step time: 0.5015\n",
      "384/388, train_loss: 0.2050, step time: 0.4765\n",
      "385/388, train_loss: 0.3596, step time: 0.4714\n",
      "386/388, train_loss: 0.1161, step time: 0.4921\n",
      "387/388, train_loss: 0.2486, step time: 0.5261\n",
      "388/388, train_loss: 0.2156, step time: 0.4944\n",
      "epoch 32 average loss: 0.2066\n",
      "current epoch: 32 current mean dice: 0.7497 tc: 0.7982 wt: 0.8912 et: 0.5598\n",
      "best mean dice: 0.7527 at epoch: 26\n",
      "time consuming of epoch 32 is: 296.0514\n",
      "----------\n",
      "epoch 33/300\n",
      "1/388, train_loss: 0.0677, step time: 0.4787\n",
      "2/388, train_loss: 0.1645, step time: 0.4874\n",
      "3/388, train_loss: 0.2380, step time: 0.4891\n",
      "4/388, train_loss: 0.1257, step time: 0.6451\n",
      "5/388, train_loss: 0.2304, step time: 0.5645\n",
      "6/388, train_loss: 0.0868, step time: 0.5421\n",
      "7/388, train_loss: 0.1360, step time: 0.5135\n",
      "8/388, train_loss: 0.4652, step time: 0.5254\n",
      "9/388, train_loss: 0.2142, step time: 0.5203\n",
      "10/388, train_loss: 0.3081, step time: 0.5231\n",
      "11/388, train_loss: 0.1102, step time: 0.4995\n",
      "12/388, train_loss: 0.1677, step time: 0.9428\n",
      "13/388, train_loss: 0.0998, step time: 0.5567\n",
      "14/388, train_loss: 0.1301, step time: 0.5225\n",
      "15/388, train_loss: 0.1074, step time: 0.5255\n",
      "16/388, train_loss: 0.4772, step time: 1.1350\n",
      "17/388, train_loss: 0.1338, step time: 0.5426\n",
      "18/388, train_loss: 0.1313, step time: 0.5172\n",
      "19/388, train_loss: 0.2980, step time: 0.5168\n",
      "20/388, train_loss: 0.2222, step time: 0.5058\n",
      "21/388, train_loss: 0.0970, step time: 0.4876\n",
      "22/388, train_loss: 0.2287, step time: 0.5168\n",
      "23/388, train_loss: 0.2302, step time: 0.6184\n",
      "24/388, train_loss: 0.1220, step time: 0.5699\n",
      "25/388, train_loss: 0.1797, step time: 0.5226\n",
      "26/388, train_loss: 0.3164, step time: 0.5062\n",
      "27/388, train_loss: 0.5012, step time: 0.4867\n",
      "28/388, train_loss: 0.2912, step time: 0.5076\n",
      "29/388, train_loss: 0.3196, step time: 0.8873\n",
      "30/388, train_loss: 0.0993, step time: 0.5504\n",
      "31/388, train_loss: 0.0608, step time: 0.5238\n",
      "32/388, train_loss: 0.1961, step time: 0.5292\n",
      "33/388, train_loss: 0.0931, step time: 0.5225\n",
      "34/388, train_loss: 0.0963, step time: 0.4989\n",
      "35/388, train_loss: 0.2669, step time: 0.4816\n",
      "36/388, train_loss: 0.1621, step time: 0.4970\n",
      "37/388, train_loss: 0.2919, step time: 0.4903\n",
      "38/388, train_loss: 0.1333, step time: 0.7417\n",
      "39/388, train_loss: 0.2220, step time: 0.5598\n",
      "40/388, train_loss: 0.2627, step time: 0.5192\n",
      "41/388, train_loss: 0.4488, step time: 0.5113\n",
      "42/388, train_loss: 0.2018, step time: 0.4975\n",
      "43/388, train_loss: 0.3683, step time: 0.5399\n",
      "44/388, train_loss: 0.0941, step time: 0.5230\n",
      "45/388, train_loss: 0.1641, step time: 0.5084\n",
      "46/388, train_loss: 0.1357, step time: 0.5124\n",
      "47/388, train_loss: 0.5174, step time: 0.5038\n",
      "48/388, train_loss: 0.3207, step time: 0.4870\n",
      "49/388, train_loss: 0.1735, step time: 0.5129\n",
      "50/388, train_loss: 0.2260, step time: 0.4939\n",
      "51/388, train_loss: 0.5028, step time: 0.4891\n",
      "52/388, train_loss: 0.1352, step time: 0.5174\n",
      "53/388, train_loss: 0.1617, step time: 0.5075\n",
      "54/388, train_loss: 0.1171, step time: 0.4980\n",
      "55/388, train_loss: 0.1990, step time: 0.4873\n",
      "56/388, train_loss: 0.3606, step time: 0.5221\n",
      "57/388, train_loss: 0.2907, step time: 0.4956\n",
      "58/388, train_loss: 0.2007, step time: 0.5136\n",
      "59/388, train_loss: 0.1231, step time: 0.5712\n",
      "60/388, train_loss: 0.0578, step time: 0.5316\n",
      "61/388, train_loss: 0.1938, step time: 0.5180\n",
      "62/388, train_loss: 0.3352, step time: 0.5569\n",
      "63/388, train_loss: 0.2339, step time: 0.5253\n",
      "64/388, train_loss: 0.2011, step time: 0.5285\n",
      "65/388, train_loss: 0.0313, step time: 0.5169\n",
      "66/388, train_loss: 0.1482, step time: 0.4962\n",
      "67/388, train_loss: 0.3349, step time: 0.4817\n",
      "68/388, train_loss: 0.1929, step time: 1.0578\n",
      "69/388, train_loss: 0.3312, step time: 0.5374\n",
      "70/388, train_loss: 0.0761, step time: 0.4991\n",
      "71/388, train_loss: 0.1518, step time: 0.4928\n",
      "72/388, train_loss: 0.3214, step time: 0.4918\n",
      "73/388, train_loss: 0.1423, step time: 0.4847\n",
      "74/388, train_loss: 0.1672, step time: 0.4868\n",
      "75/388, train_loss: 0.6089, step time: 0.4771\n",
      "76/388, train_loss: 0.2526, step time: 0.4910\n",
      "77/388, train_loss: 0.2206, step time: 0.5411\n",
      "78/388, train_loss: 0.0954, step time: 0.5255\n",
      "79/388, train_loss: 0.2682, step time: 0.6063\n",
      "80/388, train_loss: 0.1537, step time: 0.5345\n",
      "81/388, train_loss: 0.1212, step time: 0.5125\n",
      "82/388, train_loss: 0.1173, step time: 0.5024\n",
      "83/388, train_loss: 0.0811, step time: 0.4878\n",
      "84/388, train_loss: 0.3197, step time: 0.4925\n",
      "85/388, train_loss: 0.4264, step time: 0.4902\n",
      "86/388, train_loss: 0.1927, step time: 0.5187\n",
      "87/388, train_loss: 0.1649, step time: 0.5068\n",
      "88/388, train_loss: 0.1678, step time: 0.4965\n",
      "89/388, train_loss: 0.1666, step time: 0.5686\n",
      "90/388, train_loss: 0.1605, step time: 0.5479\n",
      "91/388, train_loss: 0.1280, step time: 0.5327\n",
      "92/388, train_loss: 0.0966, step time: 0.5132\n",
      "93/388, train_loss: 0.2422, step time: 0.5569\n",
      "94/388, train_loss: 0.0887, step time: 0.5289\n",
      "95/388, train_loss: 0.1107, step time: 0.5070\n",
      "96/388, train_loss: 0.2451, step time: 0.4919\n",
      "97/388, train_loss: 0.1302, step time: 0.5008\n",
      "98/388, train_loss: 0.1569, step time: 0.4904\n",
      "99/388, train_loss: 0.4366, step time: 0.4861\n",
      "100/388, train_loss: 0.0809, step time: 0.4969\n",
      "101/388, train_loss: 0.1023, step time: 0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/388, train_loss: 0.4746, step time: 0.5050\n",
      "103/388, train_loss: 0.0821, step time: 0.4985\n",
      "104/388, train_loss: 0.1919, step time: 0.5078\n",
      "105/388, train_loss: 0.1750, step time: 0.5292\n",
      "106/388, train_loss: 0.1728, step time: 0.5030\n",
      "107/388, train_loss: 0.1935, step time: 0.5008\n",
      "108/388, train_loss: 0.1105, step time: 1.0695\n",
      "109/388, train_loss: 0.2030, step time: 0.5302\n",
      "110/388, train_loss: 0.1384, step time: 0.5040\n",
      "111/388, train_loss: 0.1992, step time: 0.4963\n",
      "112/388, train_loss: 0.2661, step time: 0.4845\n",
      "113/388, train_loss: 0.2505, step time: 0.5018\n",
      "114/388, train_loss: 0.3020, step time: 0.5094\n",
      "115/388, train_loss: 0.2692, step time: 0.5107\n",
      "116/388, train_loss: 0.1627, step time: 0.4852\n",
      "117/388, train_loss: 0.0821, step time: 0.4857\n",
      "118/388, train_loss: 0.2104, step time: 0.4961\n",
      "119/388, train_loss: 0.3777, step time: 0.4849\n",
      "120/388, train_loss: 0.2000, step time: 1.1663\n",
      "121/388, train_loss: 0.2225, step time: 0.5482\n",
      "122/388, train_loss: 0.0767, step time: 0.5394\n",
      "123/388, train_loss: 0.0609, step time: 0.4859\n",
      "124/388, train_loss: 0.1672, step time: 0.4894\n",
      "125/388, train_loss: 0.1052, step time: 0.4787\n",
      "126/388, train_loss: 0.1818, step time: 0.4782\n",
      "127/388, train_loss: 0.1150, step time: 0.4794\n",
      "128/388, train_loss: 0.4867, step time: 1.0152\n",
      "129/388, train_loss: 0.2505, step time: 0.5272\n",
      "130/388, train_loss: 0.1976, step time: 0.4987\n",
      "131/388, train_loss: 0.2083, step time: 0.4985\n",
      "132/388, train_loss: 0.1982, step time: 0.4867\n",
      "133/388, train_loss: 0.3750, step time: 0.4823\n",
      "134/388, train_loss: 0.1633, step time: 0.4965\n",
      "135/388, train_loss: 0.6268, step time: 0.4876\n",
      "136/388, train_loss: 0.1017, step time: 0.4992\n",
      "137/388, train_loss: 0.0888, step time: 0.4823\n",
      "138/388, train_loss: 0.1618, step time: 0.4810\n",
      "139/388, train_loss: 0.0842, step time: 0.4826\n",
      "140/388, train_loss: 0.6256, step time: 0.4990\n",
      "141/388, train_loss: 0.2884, step time: 0.5322\n",
      "142/388, train_loss: 0.1030, step time: 0.5156\n",
      "143/388, train_loss: 0.0856, step time: 0.5013\n",
      "144/388, train_loss: 0.2815, step time: 0.4907\n",
      "145/388, train_loss: 0.1408, step time: 0.4937\n",
      "146/388, train_loss: 0.2169, step time: 0.5093\n",
      "147/388, train_loss: 0.1348, step time: 0.5084\n",
      "148/388, train_loss: 0.2243, step time: 0.5331\n",
      "149/388, train_loss: 0.4098, step time: 0.5301\n",
      "150/388, train_loss: 0.1961, step time: 0.5008\n",
      "151/388, train_loss: 0.2641, step time: 0.5179\n",
      "152/388, train_loss: 0.3493, step time: 0.4947\n",
      "153/388, train_loss: 0.2756, step time: 0.4909\n",
      "154/388, train_loss: 0.1871, step time: 0.5024\n",
      "155/388, train_loss: 0.5043, step time: 0.4959\n",
      "156/388, train_loss: 0.3505, step time: 0.5937\n",
      "157/388, train_loss: 0.0988, step time: 0.5341\n",
      "158/388, train_loss: 0.1000, step time: 0.4910\n",
      "159/388, train_loss: 0.1150, step time: 0.4953\n",
      "160/388, train_loss: 0.1039, step time: 0.4977\n",
      "161/388, train_loss: 0.0942, step time: 0.4976\n",
      "162/388, train_loss: 0.2784, step time: 0.4824\n",
      "163/388, train_loss: 0.1776, step time: 0.5152\n",
      "164/388, train_loss: 0.3471, step time: 0.4881\n",
      "165/388, train_loss: 0.0540, step time: 0.4804\n",
      "166/388, train_loss: 0.2225, step time: 0.4980\n",
      "167/388, train_loss: 0.1119, step time: 0.4912\n",
      "168/388, train_loss: 0.0913, step time: 0.5002\n",
      "169/388, train_loss: 0.2815, step time: 0.5052\n",
      "170/388, train_loss: 0.5920, step time: 0.5389\n",
      "171/388, train_loss: 0.5600, step time: 0.5724\n",
      "172/388, train_loss: 0.2673, step time: 0.5305\n",
      "173/388, train_loss: 0.1101, step time: 0.4975\n",
      "174/388, train_loss: 0.2391, step time: 0.4874\n",
      "175/388, train_loss: 0.1842, step time: 0.4954\n",
      "176/388, train_loss: 0.1907, step time: 0.4953\n",
      "177/388, train_loss: 0.1240, step time: 0.4865\n",
      "178/388, train_loss: 0.1493, step time: 0.5405\n",
      "179/388, train_loss: 0.3320, step time: 0.5141\n",
      "180/388, train_loss: 0.2305, step time: 0.4893\n",
      "181/388, train_loss: 0.5376, step time: 1.0181\n",
      "182/388, train_loss: 0.1704, step time: 0.5400\n",
      "183/388, train_loss: 0.0701, step time: 0.5184\n",
      "184/388, train_loss: 0.1741, step time: 0.5391\n",
      "185/388, train_loss: 0.2233, step time: 0.5199\n",
      "186/388, train_loss: 0.1464, step time: 0.5120\n",
      "187/388, train_loss: 0.1444, step time: 0.4899\n",
      "188/388, train_loss: 0.0469, step time: 0.4857\n",
      "189/388, train_loss: 0.2238, step time: 0.5298\n",
      "190/388, train_loss: 0.1608, step time: 0.5146\n",
      "191/388, train_loss: 0.2850, step time: 0.5050\n",
      "192/388, train_loss: 0.2193, step time: 0.4983\n",
      "193/388, train_loss: 0.0988, step time: 0.4810\n",
      "194/388, train_loss: 0.1217, step time: 0.6683\n",
      "195/388, train_loss: 0.3263, step time: 0.5604\n",
      "196/388, train_loss: 0.3080, step time: 0.5244\n",
      "197/388, train_loss: 0.1956, step time: 0.5958\n",
      "198/388, train_loss: 0.3539, step time: 0.5349\n",
      "199/388, train_loss: 0.0561, step time: 0.5077\n",
      "200/388, train_loss: 0.1718, step time: 0.4965\n",
      "201/388, train_loss: 0.0999, step time: 0.4750\n",
      "202/388, train_loss: 0.1954, step time: 0.4904\n",
      "203/388, train_loss: 0.0980, step time: 0.4802\n",
      "204/388, train_loss: 0.0503, step time: 0.4918\n",
      "205/388, train_loss: 0.1774, step time: 0.4758\n",
      "206/388, train_loss: 0.2205, step time: 0.6713\n",
      "207/388, train_loss: 0.1761, step time: 0.5546\n",
      "208/388, train_loss: 0.1543, step time: 0.5484\n",
      "209/388, train_loss: 0.2398, step time: 0.6355\n",
      "210/388, train_loss: 0.2896, step time: 0.5679\n",
      "211/388, train_loss: 0.1754, step time: 0.5287\n",
      "212/388, train_loss: 0.4937, step time: 0.4992\n",
      "213/388, train_loss: 0.0839, step time: 0.4976\n",
      "214/388, train_loss: 0.1137, step time: 0.4805\n",
      "215/388, train_loss: 0.2359, step time: 0.6180\n",
      "216/388, train_loss: 0.0930, step time: 0.5667\n",
      "217/388, train_loss: 0.1327, step time: 0.5268\n",
      "218/388, train_loss: 0.3648, step time: 0.5107\n",
      "219/388, train_loss: 0.1009, step time: 0.5096\n",
      "220/388, train_loss: 0.2030, step time: 0.5041\n",
      "221/388, train_loss: 0.2352, step time: 0.5106\n",
      "222/388, train_loss: 0.1469, step time: 0.4912\n",
      "223/388, train_loss: 0.3547, step time: 0.4934\n",
      "224/388, train_loss: 0.2859, step time: 1.2172\n",
      "225/388, train_loss: 0.1288, step time: 0.5204\n",
      "226/388, train_loss: 0.2427, step time: 0.5008\n",
      "227/388, train_loss: 0.1217, step time: 0.4931\n",
      "228/388, train_loss: 0.1047, step time: 0.4797\n",
      "229/388, train_loss: 0.3885, step time: 0.4888\n",
      "230/388, train_loss: 0.1838, step time: 0.4934\n",
      "231/388, train_loss: 0.0685, step time: 0.4958\n",
      "232/388, train_loss: 0.2850, step time: 0.5000\n",
      "233/388, train_loss: 0.0679, step time: 0.4979\n",
      "234/388, train_loss: 0.2241, step time: 0.4950\n",
      "235/388, train_loss: 0.1846, step time: 0.5755\n",
      "236/388, train_loss: 0.3792, step time: 0.5392\n",
      "237/388, train_loss: 0.4684, step time: 0.5118\n",
      "238/388, train_loss: 0.1035, step time: 0.5002\n",
      "239/388, train_loss: 0.1216, step time: 0.4898\n",
      "240/388, train_loss: 0.2651, step time: 0.4930\n",
      "241/388, train_loss: 0.0699, step time: 1.1622\n",
      "242/388, train_loss: 0.1013, step time: 0.5355\n",
      "243/388, train_loss: 0.1444, step time: 0.5062\n",
      "244/388, train_loss: 0.1973, step time: 0.5000\n",
      "245/388, train_loss: 0.2820, step time: 0.4986\n",
      "246/388, train_loss: 0.0568, step time: 0.4842\n",
      "247/388, train_loss: 0.2996, step time: 0.4917\n",
      "248/388, train_loss: 0.5373, step time: 0.4787\n",
      "249/388, train_loss: 0.2501, step time: 1.1780\n",
      "250/388, train_loss: 0.1500, step time: 0.5439\n",
      "251/388, train_loss: 0.1753, step time: 0.5113\n",
      "252/388, train_loss: 0.0859, step time: 0.4927\n",
      "253/388, train_loss: 0.4245, step time: 0.4795\n",
      "254/388, train_loss: 0.0686, step time: 0.4819\n",
      "255/388, train_loss: 0.1542, step time: 0.4854\n",
      "256/388, train_loss: 0.2086, step time: 0.4730\n",
      "257/388, train_loss: 0.1370, step time: 0.4755\n",
      "258/388, train_loss: 0.0606, step time: 0.8571\n",
      "259/388, train_loss: 0.1954, step time: 0.5364\n",
      "260/388, train_loss: 0.3183, step time: 0.5064\n",
      "261/388, train_loss: 0.1506, step time: 0.4878\n",
      "262/388, train_loss: 0.1694, step time: 0.4886\n",
      "263/388, train_loss: 0.0677, step time: 0.4899\n",
      "264/388, train_loss: 0.2574, step time: 0.4861\n",
      "265/388, train_loss: 0.2161, step time: 0.4815\n",
      "266/388, train_loss: 0.1783, step time: 0.5090\n",
      "267/388, train_loss: 0.1789, step time: 0.4966\n",
      "268/388, train_loss: 0.1216, step time: 0.4974\n",
      "269/388, train_loss: 0.0768, step time: 0.7533\n",
      "270/388, train_loss: 0.0798, step time: 0.5370\n",
      "271/388, train_loss: 0.2748, step time: 0.5086\n",
      "272/388, train_loss: 0.1715, step time: 0.4969\n",
      "273/388, train_loss: 0.1749, step time: 0.4791\n",
      "274/388, train_loss: 0.1640, step time: 0.4774\n",
      "275/388, train_loss: 0.1581, step time: 1.1077\n",
      "276/388, train_loss: 0.0775, step time: 0.5199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/388, train_loss: 0.1312, step time: 0.4946\n",
      "278/388, train_loss: 0.1730, step time: 0.4896\n",
      "279/388, train_loss: 0.2265, step time: 0.4930\n",
      "280/388, train_loss: 0.5950, step time: 0.4955\n",
      "281/388, train_loss: 0.1988, step time: 0.4911\n",
      "282/388, train_loss: 0.1459, step time: 0.5170\n",
      "283/388, train_loss: 0.1069, step time: 0.4965\n",
      "284/388, train_loss: 0.1557, step time: 0.5090\n",
      "285/388, train_loss: 0.6306, step time: 0.4933\n",
      "286/388, train_loss: 0.1197, step time: 0.4925\n",
      "287/388, train_loss: 0.2014, step time: 0.4783\n",
      "288/388, train_loss: 0.1602, step time: 0.8229\n",
      "289/388, train_loss: 0.3014, step time: 0.5444\n",
      "290/388, train_loss: 0.0786, step time: 0.5112\n",
      "291/388, train_loss: 0.0357, step time: 0.4909\n",
      "292/388, train_loss: 0.1998, step time: 0.4948\n",
      "293/388, train_loss: 0.3600, step time: 0.4832\n",
      "294/388, train_loss: 0.5143, step time: 0.4865\n",
      "295/388, train_loss: 0.0928, step time: 0.4873\n",
      "296/388, train_loss: 0.0722, step time: 0.4853\n",
      "297/388, train_loss: 0.3689, step time: 1.1073\n",
      "298/388, train_loss: 0.1440, step time: 0.5337\n",
      "299/388, train_loss: 0.1293, step time: 0.4992\n",
      "300/388, train_loss: 0.1069, step time: 0.4890\n",
      "301/388, train_loss: 0.2257, step time: 0.4838\n",
      "302/388, train_loss: 0.4072, step time: 0.4912\n",
      "303/388, train_loss: 0.2871, step time: 0.5014\n",
      "304/388, train_loss: 0.1093, step time: 0.6898\n",
      "305/388, train_loss: 0.1374, step time: 0.5332\n",
      "306/388, train_loss: 0.1066, step time: 0.4983\n",
      "307/388, train_loss: 0.4917, step time: 0.4998\n",
      "308/388, train_loss: 0.1785, step time: 0.4847\n",
      "309/388, train_loss: 0.2407, step time: 0.4887\n",
      "310/388, train_loss: 0.4456, step time: 0.4815\n",
      "311/388, train_loss: 0.1930, step time: 0.4904\n",
      "312/388, train_loss: 0.2728, step time: 0.5200\n",
      "313/388, train_loss: 0.2015, step time: 0.5003\n",
      "314/388, train_loss: 0.3704, step time: 0.4898\n",
      "315/388, train_loss: 0.2333, step time: 1.1433\n",
      "316/388, train_loss: 0.2491, step time: 0.5419\n",
      "317/388, train_loss: 0.1143, step time: 0.5007\n",
      "318/388, train_loss: 0.1089, step time: 0.4947\n",
      "319/388, train_loss: 0.2128, step time: 0.4941\n",
      "320/388, train_loss: 0.0905, step time: 0.5203\n",
      "321/388, train_loss: 0.2110, step time: 0.5042\n",
      "322/388, train_loss: 0.2378, step time: 0.4930\n",
      "323/388, train_loss: 0.1692, step time: 0.4880\n",
      "324/388, train_loss: 0.2190, step time: 0.4797\n",
      "325/388, train_loss: 0.3728, step time: 0.9975\n",
      "326/388, train_loss: 0.1850, step time: 0.5313\n",
      "327/388, train_loss: 0.0992, step time: 0.5083\n",
      "328/388, train_loss: 0.2663, step time: 0.4964\n",
      "329/388, train_loss: 0.3122, step time: 0.4905\n",
      "330/388, train_loss: 0.1496, step time: 0.4903\n",
      "331/388, train_loss: 0.1010, step time: 0.4846\n",
      "332/388, train_loss: 0.3772, step time: 0.4893\n",
      "333/388, train_loss: 0.0914, step time: 0.4912\n",
      "334/388, train_loss: 0.0714, step time: 0.4835\n",
      "335/388, train_loss: 0.1215, step time: 0.4784\n",
      "336/388, train_loss: 0.2459, step time: 0.4742\n",
      "337/388, train_loss: 0.3389, step time: 0.9483\n",
      "338/388, train_loss: 0.2101, step time: 0.5445\n",
      "339/388, train_loss: 0.3225, step time: 0.5225\n",
      "340/388, train_loss: 0.1010, step time: 0.4947\n",
      "341/388, train_loss: 0.1990, step time: 0.4958\n",
      "342/388, train_loss: 0.2681, step time: 0.4918\n",
      "343/388, train_loss: 0.1243, step time: 0.4879\n",
      "344/388, train_loss: 0.0925, step time: 0.4812\n",
      "345/388, train_loss: 0.1789, step time: 1.0324\n",
      "346/388, train_loss: 0.5520, step time: 0.5571\n",
      "347/388, train_loss: 0.3232, step time: 0.5211\n",
      "348/388, train_loss: 0.0372, step time: 0.5008\n",
      "349/388, train_loss: 0.2157, step time: 0.4844\n",
      "350/388, train_loss: 0.2351, step time: 1.0245\n",
      "351/388, train_loss: 0.3818, step time: 0.5361\n",
      "352/388, train_loss: 0.1120, step time: 0.5043\n",
      "353/388, train_loss: 0.2031, step time: 0.4880\n",
      "354/388, train_loss: 0.0593, step time: 0.4856\n",
      "355/388, train_loss: 0.1634, step time: 0.4899\n",
      "356/388, train_loss: 0.0999, step time: 0.4736\n",
      "357/388, train_loss: 0.2217, step time: 0.7596\n",
      "358/388, train_loss: 0.2364, step time: 0.5648\n",
      "359/388, train_loss: 0.1278, step time: 0.5322\n",
      "360/388, train_loss: 0.1073, step time: 0.5038\n",
      "361/388, train_loss: 0.1025, step time: 0.4931\n",
      "362/388, train_loss: 0.2243, step time: 0.5441\n",
      "363/388, train_loss: 0.3284, step time: 0.5251\n",
      "364/388, train_loss: 0.1425, step time: 0.4992\n",
      "365/388, train_loss: 0.1245, step time: 0.5434\n",
      "366/388, train_loss: 0.1461, step time: 0.5283\n",
      "367/388, train_loss: 0.1572, step time: 0.5109\n",
      "368/388, train_loss: 0.2126, step time: 0.4946\n",
      "369/388, train_loss: 0.4942, step time: 0.5057\n",
      "370/388, train_loss: 0.0662, step time: 0.5337\n",
      "371/388, train_loss: 0.0546, step time: 0.5094\n",
      "372/388, train_loss: 0.2681, step time: 0.4898\n",
      "373/388, train_loss: 0.0968, step time: 0.4865\n",
      "374/388, train_loss: 0.1014, step time: 0.4843\n",
      "375/388, train_loss: 0.1324, step time: 0.5008\n",
      "376/388, train_loss: 0.2770, step time: 0.9450\n",
      "377/388, train_loss: 0.1048, step time: 0.5467\n",
      "378/388, train_loss: 0.2019, step time: 0.5167\n",
      "379/388, train_loss: 0.0881, step time: 0.4949\n",
      "380/388, train_loss: 0.4230, step time: 0.5207\n",
      "381/388, train_loss: 0.1432, step time: 0.5015\n",
      "382/388, train_loss: 0.1112, step time: 0.4986\n",
      "383/388, train_loss: 0.1693, step time: 0.4872\n",
      "384/388, train_loss: 0.3880, step time: 0.5208\n",
      "385/388, train_loss: 0.1094, step time: 0.5089\n",
      "386/388, train_loss: 0.2126, step time: 0.5131\n",
      "387/388, train_loss: 0.2540, step time: 0.5014\n",
      "388/388, train_loss: 0.2437, step time: 0.4827\n",
      "epoch 33 average loss: 0.2079\n",
      "saved new best metric model\n",
      "current epoch: 33 current mean dice: 0.7529 tc: 0.8095 wt: 0.8929 et: 0.5562\n",
      "best mean dice: 0.7529 at epoch: 33\n",
      "time consuming of epoch 33 is: 299.5224\n",
      "----------\n",
      "epoch 34/300\n",
      "1/388, train_loss: 0.1135, step time: 0.4727\n",
      "2/388, train_loss: 0.2406, step time: 0.4813\n",
      "3/388, train_loss: 0.4092, step time: 1.0366\n",
      "4/388, train_loss: 0.1504, step time: 0.5856\n",
      "5/388, train_loss: 0.1058, step time: 0.5613\n",
      "6/388, train_loss: 0.3489, step time: 0.5629\n",
      "7/388, train_loss: 0.1539, step time: 0.5078\n",
      "8/388, train_loss: 0.0709, step time: 0.6164\n",
      "9/388, train_loss: 0.0547, step time: 0.5899\n",
      "10/388, train_loss: 0.0538, step time: 0.5212\n",
      "11/388, train_loss: 0.2124, step time: 0.5249\n",
      "12/388, train_loss: 0.2834, step time: 0.6249\n",
      "13/388, train_loss: 0.1260, step time: 0.5448\n",
      "14/388, train_loss: 0.1415, step time: 0.5354\n",
      "15/388, train_loss: 0.2770, step time: 0.5202\n",
      "16/388, train_loss: 0.1195, step time: 0.5029\n",
      "17/388, train_loss: 0.1383, step time: 0.5419\n",
      "18/388, train_loss: 0.1605, step time: 0.5013\n",
      "19/388, train_loss: 0.0653, step time: 0.5131\n",
      "20/388, train_loss: 0.1226, step time: 0.5000\n",
      "21/388, train_loss: 0.1678, step time: 1.1107\n",
      "22/388, train_loss: 0.3103, step time: 0.5613\n",
      "23/388, train_loss: 0.4613, step time: 0.5340\n",
      "24/388, train_loss: 0.2182, step time: 0.6032\n",
      "25/388, train_loss: 0.2579, step time: 0.5666\n",
      "26/388, train_loss: 0.2435, step time: 0.5618\n",
      "27/388, train_loss: 0.1531, step time: 0.5403\n",
      "28/388, train_loss: 0.1040, step time: 0.6385\n",
      "29/388, train_loss: 0.1383, step time: 0.5808\n",
      "30/388, train_loss: 0.5097, step time: 0.5396\n",
      "31/388, train_loss: 0.1239, step time: 0.5205\n",
      "32/388, train_loss: 0.2113, step time: 0.5086\n",
      "33/388, train_loss: 0.1915, step time: 0.4920\n",
      "34/388, train_loss: 0.2792, step time: 0.4944\n",
      "35/388, train_loss: 0.1037, step time: 0.4922\n",
      "36/388, train_loss: 0.1397, step time: 0.5417\n",
      "37/388, train_loss: 0.1883, step time: 0.5376\n",
      "38/388, train_loss: 0.2155, step time: 0.5103\n",
      "39/388, train_loss: 0.0994, step time: 0.4997\n",
      "40/388, train_loss: 0.2402, step time: 0.4880\n",
      "41/388, train_loss: 0.2628, step time: 0.5291\n",
      "42/388, train_loss: 0.2320, step time: 0.5255\n",
      "43/388, train_loss: 0.2670, step time: 0.5824\n",
      "44/388, train_loss: 0.2473, step time: 0.6082\n",
      "45/388, train_loss: 0.2129, step time: 0.5398\n",
      "46/388, train_loss: 0.1068, step time: 0.5107\n",
      "47/388, train_loss: 0.1834, step time: 0.4871\n",
      "48/388, train_loss: 0.1088, step time: 1.0783\n",
      "49/388, train_loss: 0.1470, step time: 0.5558\n",
      "50/388, train_loss: 0.1378, step time: 0.5241\n",
      "51/388, train_loss: 0.8697, step time: 0.5087\n",
      "52/388, train_loss: 0.1768, step time: 0.5337\n",
      "53/388, train_loss: 0.1398, step time: 0.4911\n",
      "54/388, train_loss: 0.2095, step time: 0.4993\n",
      "55/388, train_loss: 0.1196, step time: 0.5906\n",
      "56/388, train_loss: 0.1695, step time: 0.5330\n",
      "57/388, train_loss: 0.1279, step time: 0.4998\n",
      "58/388, train_loss: 0.4216, step time: 0.5183\n",
      "59/388, train_loss: 0.3732, step time: 0.5208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/388, train_loss: 0.1188, step time: 0.5084\n",
      "61/388, train_loss: 0.2046, step time: 0.5036\n",
      "62/388, train_loss: 0.1058, step time: 0.5282\n",
      "63/388, train_loss: 0.2594, step time: 0.5083\n",
      "64/388, train_loss: 0.2177, step time: 0.6875\n",
      "65/388, train_loss: 0.2104, step time: 0.5479\n",
      "66/388, train_loss: 0.0989, step time: 0.5197\n",
      "67/388, train_loss: 0.2173, step time: 0.5174\n",
      "68/388, train_loss: 0.1960, step time: 0.5100\n",
      "69/388, train_loss: 0.1047, step time: 0.5250\n",
      "70/388, train_loss: 0.1927, step time: 0.5002\n",
      "71/388, train_loss: 0.1475, step time: 0.5002\n",
      "72/388, train_loss: 0.0967, step time: 0.8961\n",
      "73/388, train_loss: 0.0828, step time: 0.5591\n",
      "74/388, train_loss: 0.2047, step time: 0.5277\n",
      "75/388, train_loss: 0.0796, step time: 0.5223\n",
      "76/388, train_loss: 0.1135, step time: 0.5316\n",
      "77/388, train_loss: 0.1825, step time: 0.6200\n",
      "78/388, train_loss: 0.1275, step time: 0.5402\n",
      "79/388, train_loss: 0.0979, step time: 0.4977\n",
      "80/388, train_loss: 0.2724, step time: 0.4959\n",
      "81/388, train_loss: 0.1113, step time: 0.9567\n",
      "82/388, train_loss: 0.5439, step time: 0.5515\n",
      "83/388, train_loss: 0.3590, step time: 0.5033\n",
      "84/388, train_loss: 0.0347, step time: 0.5111\n",
      "85/388, train_loss: 0.1404, step time: 0.5219\n",
      "86/388, train_loss: 0.1768, step time: 0.5011\n",
      "87/388, train_loss: 0.0992, step time: 0.5060\n",
      "88/388, train_loss: 0.1146, step time: 0.4894\n",
      "89/388, train_loss: 0.5105, step time: 0.4985\n",
      "90/388, train_loss: 0.2503, step time: 0.5234\n",
      "91/388, train_loss: 0.1159, step time: 0.5020\n",
      "92/388, train_loss: 0.1825, step time: 1.0894\n",
      "93/388, train_loss: 0.2528, step time: 0.5423\n",
      "94/388, train_loss: 0.2775, step time: 0.5158\n",
      "95/388, train_loss: 0.3886, step time: 0.4995\n",
      "96/388, train_loss: 0.1912, step time: 0.4970\n",
      "97/388, train_loss: 0.2322, step time: 0.4982\n",
      "98/388, train_loss: 0.2964, step time: 0.5324\n",
      "99/388, train_loss: 0.2983, step time: 0.5084\n",
      "100/388, train_loss: 0.1513, step time: 0.4904\n",
      "101/388, train_loss: 0.2252, step time: 0.4944\n",
      "102/388, train_loss: 0.1788, step time: 0.4792\n",
      "103/388, train_loss: 0.1821, step time: 0.5245\n",
      "104/388, train_loss: 0.1125, step time: 0.5281\n",
      "105/388, train_loss: 0.2134, step time: 0.6003\n",
      "106/388, train_loss: 0.1875, step time: 0.5401\n",
      "107/388, train_loss: 0.1599, step time: 0.5185\n",
      "108/388, train_loss: 0.0978, step time: 0.4992\n",
      "109/388, train_loss: 0.0673, step time: 0.4988\n",
      "110/388, train_loss: 0.1056, step time: 0.4897\n",
      "111/388, train_loss: 0.1539, step time: 0.5887\n",
      "112/388, train_loss: 0.2022, step time: 0.5374\n",
      "113/388, train_loss: 0.0952, step time: 0.5110\n",
      "114/388, train_loss: 0.0830, step time: 0.5072\n",
      "115/388, train_loss: 0.1096, step time: 0.5137\n",
      "116/388, train_loss: 0.2298, step time: 0.5287\n",
      "117/388, train_loss: 0.1388, step time: 0.5013\n",
      "118/388, train_loss: 0.2354, step time: 0.4987\n",
      "119/388, train_loss: 0.3991, step time: 0.5032\n",
      "120/388, train_loss: 0.1749, step time: 0.5232\n",
      "121/388, train_loss: 0.3258, step time: 0.5064\n",
      "122/388, train_loss: 0.2143, step time: 0.4972\n",
      "123/388, train_loss: 0.2004, step time: 0.4922\n",
      "124/388, train_loss: 0.0710, step time: 0.7298\n",
      "125/388, train_loss: 0.3847, step time: 0.5515\n",
      "126/388, train_loss: 0.1279, step time: 0.5267\n",
      "127/388, train_loss: 0.0442, step time: 0.5117\n",
      "128/388, train_loss: 0.3742, step time: 0.5676\n",
      "129/388, train_loss: 0.1527, step time: 0.5314\n",
      "130/388, train_loss: 0.2396, step time: 0.5168\n",
      "131/388, train_loss: 0.2030, step time: 0.4923\n",
      "132/388, train_loss: 0.1525, step time: 0.4887\n",
      "133/388, train_loss: 0.2669, step time: 0.5038\n",
      "134/388, train_loss: 0.2883, step time: 0.4974\n",
      "135/388, train_loss: 0.2283, step time: 0.4934\n",
      "136/388, train_loss: 0.1840, step time: 0.4767\n",
      "137/388, train_loss: 0.0781, step time: 0.5031\n",
      "138/388, train_loss: 0.4514, step time: 0.5022\n",
      "139/388, train_loss: 0.2771, step time: 0.5130\n",
      "140/388, train_loss: 0.0835, step time: 0.4876\n",
      "141/388, train_loss: 0.3289, step time: 0.9367\n",
      "142/388, train_loss: 0.1498, step time: 0.5554\n",
      "143/388, train_loss: 0.1415, step time: 0.5132\n",
      "144/388, train_loss: 0.1701, step time: 0.5072\n",
      "145/388, train_loss: 0.0275, step time: 0.4942\n",
      "146/388, train_loss: 0.5458, step time: 0.4828\n",
      "147/388, train_loss: 0.0733, step time: 1.0333\n",
      "148/388, train_loss: 0.0752, step time: 0.5383\n",
      "149/388, train_loss: 0.1193, step time: 0.5106\n",
      "150/388, train_loss: 0.1258, step time: 0.4869\n",
      "151/388, train_loss: 0.2770, step time: 0.4861\n",
      "152/388, train_loss: 0.0499, step time: 0.4892\n",
      "153/388, train_loss: 0.6581, step time: 0.4797\n",
      "154/388, train_loss: 0.3480, step time: 1.1337\n",
      "155/388, train_loss: 0.2223, step time: 0.5226\n",
      "156/388, train_loss: 0.2087, step time: 0.4984\n",
      "157/388, train_loss: 0.0844, step time: 0.4961\n",
      "158/388, train_loss: 0.1045, step time: 0.4788\n",
      "159/388, train_loss: 0.1689, step time: 0.4924\n",
      "160/388, train_loss: 0.0445, step time: 0.4797\n",
      "161/388, train_loss: 0.2885, step time: 0.4766\n",
      "162/388, train_loss: 0.6042, step time: 0.5218\n",
      "163/388, train_loss: 0.0986, step time: 0.4997\n",
      "164/388, train_loss: 0.1169, step time: 0.4992\n",
      "165/388, train_loss: 0.1445, step time: 0.4939\n",
      "166/388, train_loss: 0.1098, step time: 0.5338\n",
      "167/388, train_loss: 0.1164, step time: 0.5257\n",
      "168/388, train_loss: 0.0914, step time: 0.4981\n",
      "169/388, train_loss: 0.2609, step time: 0.5041\n",
      "170/388, train_loss: 0.1028, step time: 0.4881\n",
      "171/388, train_loss: 0.1538, step time: 0.4906\n",
      "172/388, train_loss: 0.2790, step time: 0.4955\n",
      "173/388, train_loss: 0.2951, step time: 0.5002\n",
      "174/388, train_loss: 0.1730, step time: 0.5492\n",
      "175/388, train_loss: 0.1071, step time: 0.5249\n",
      "176/388, train_loss: 0.1360, step time: 0.4997\n",
      "177/388, train_loss: 0.1241, step time: 0.5005\n",
      "178/388, train_loss: 0.1119, step time: 0.4819\n",
      "179/388, train_loss: 0.1865, step time: 1.0153\n",
      "180/388, train_loss: 0.2021, step time: 0.5417\n",
      "181/388, train_loss: 0.1892, step time: 0.5120\n",
      "182/388, train_loss: 0.1242, step time: 0.4934\n",
      "183/388, train_loss: 0.1632, step time: 0.5078\n",
      "184/388, train_loss: 0.0802, step time: 0.4850\n",
      "185/388, train_loss: 0.2387, step time: 0.4846\n",
      "186/388, train_loss: 0.1015, step time: 1.2013\n",
      "187/388, train_loss: 0.1113, step time: 0.5351\n",
      "188/388, train_loss: 0.1731, step time: 0.5026\n",
      "189/388, train_loss: 0.1790, step time: 0.4970\n",
      "190/388, train_loss: 0.4302, step time: 0.4941\n",
      "191/388, train_loss: 0.4370, step time: 0.4833\n",
      "192/388, train_loss: 0.0900, step time: 0.4843\n",
      "193/388, train_loss: 0.2417, step time: 0.4844\n",
      "194/388, train_loss: 0.1260, step time: 0.4966\n",
      "195/388, train_loss: 0.1243, step time: 0.5066\n",
      "196/388, train_loss: 0.1215, step time: 0.4877\n",
      "197/388, train_loss: 0.1174, step time: 0.4896\n",
      "198/388, train_loss: 0.1874, step time: 0.4954\n",
      "199/388, train_loss: 0.1898, step time: 0.5150\n",
      "200/388, train_loss: 0.2441, step time: 0.4987\n",
      "201/388, train_loss: 0.2654, step time: 0.5144\n",
      "202/388, train_loss: 0.2915, step time: 0.5940\n",
      "203/388, train_loss: 0.0985, step time: 0.5301\n",
      "204/388, train_loss: 0.5235, step time: 0.5223\n",
      "205/388, train_loss: 0.0565, step time: 0.5195\n",
      "206/388, train_loss: 0.1036, step time: 0.5121\n",
      "207/388, train_loss: 0.1259, step time: 0.5001\n",
      "208/388, train_loss: 0.4487, step time: 0.4904\n",
      "209/388, train_loss: 0.0887, step time: 0.4758\n",
      "210/388, train_loss: 0.1665, step time: 0.5106\n",
      "211/388, train_loss: 0.1062, step time: 0.4979\n",
      "212/388, train_loss: 0.0619, step time: 0.4866\n",
      "213/388, train_loss: 0.3406, step time: 0.5567\n",
      "214/388, train_loss: 0.3366, step time: 0.5206\n",
      "215/388, train_loss: 0.1782, step time: 0.4945\n",
      "216/388, train_loss: 0.0463, step time: 0.4948\n",
      "217/388, train_loss: 0.0896, step time: 1.1575\n",
      "218/388, train_loss: 0.1799, step time: 0.5263\n",
      "219/388, train_loss: 0.3024, step time: 0.5200\n",
      "220/388, train_loss: 0.4515, step time: 0.4975\n",
      "221/388, train_loss: 0.3289, step time: 0.4879\n",
      "222/388, train_loss: 0.1581, step time: 0.5143\n",
      "223/388, train_loss: 0.1062, step time: 0.5049\n",
      "224/388, train_loss: 0.3338, step time: 0.4921\n",
      "225/388, train_loss: 0.2224, step time: 0.4918\n",
      "226/388, train_loss: 0.1378, step time: 0.4889\n",
      "227/388, train_loss: 0.3870, step time: 0.4924\n",
      "228/388, train_loss: 0.1137, step time: 0.4946\n",
      "229/388, train_loss: 0.6249, step time: 0.4868\n",
      "230/388, train_loss: 0.3473, step time: 0.4934\n",
      "231/388, train_loss: 0.0882, step time: 0.5040\n",
      "232/388, train_loss: 0.5832, step time: 0.4984\n",
      "233/388, train_loss: 0.1526, step time: 0.4960\n",
      "234/388, train_loss: 0.1189, step time: 0.4877\n",
      "235/388, train_loss: 0.1323, step time: 0.4913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/388, train_loss: 0.1789, step time: 0.4986\n",
      "237/388, train_loss: 0.2029, step time: 0.5004\n",
      "238/388, train_loss: 0.2384, step time: 0.4826\n",
      "239/388, train_loss: 0.2825, step time: 0.4824\n",
      "240/388, train_loss: 0.2713, step time: 0.5074\n",
      "241/388, train_loss: 0.0705, step time: 0.4895\n",
      "242/388, train_loss: 0.2129, step time: 0.4954\n",
      "243/388, train_loss: 0.1406, step time: 0.4802\n",
      "244/388, train_loss: 0.2056, step time: 0.5391\n",
      "245/388, train_loss: 0.1696, step time: 0.5299\n",
      "246/388, train_loss: 0.1873, step time: 0.6093\n",
      "247/388, train_loss: 0.2681, step time: 0.5472\n",
      "248/388, train_loss: 0.3842, step time: 0.5208\n",
      "249/388, train_loss: 0.0941, step time: 0.5095\n",
      "250/388, train_loss: 0.1136, step time: 0.4844\n",
      "251/388, train_loss: 0.3213, step time: 0.4819\n",
      "252/388, train_loss: 0.2716, step time: 0.5103\n",
      "253/388, train_loss: 0.1644, step time: 0.4963\n",
      "254/388, train_loss: 0.3285, step time: 0.9041\n",
      "255/388, train_loss: 0.1697, step time: 0.5335\n",
      "256/388, train_loss: 0.0779, step time: 0.5174\n",
      "257/388, train_loss: 0.5042, step time: 0.4992\n",
      "258/388, train_loss: 0.4499, step time: 0.5204\n",
      "259/388, train_loss: 0.1902, step time: 0.5105\n",
      "260/388, train_loss: 0.4151, step time: 0.4905\n",
      "261/388, train_loss: 0.0675, step time: 0.4986\n",
      "262/388, train_loss: 0.0975, step time: 0.4846\n",
      "263/388, train_loss: 0.3787, step time: 0.4858\n",
      "264/388, train_loss: 0.2784, step time: 0.4888\n",
      "265/388, train_loss: 0.1778, step time: 1.0259\n",
      "266/388, train_loss: 0.3115, step time: 0.5638\n",
      "267/388, train_loss: 0.2027, step time: 0.5233\n",
      "268/388, train_loss: 0.3606, step time: 0.5054\n",
      "269/388, train_loss: 0.4500, step time: 0.5034\n",
      "270/388, train_loss: 0.2339, step time: 0.4824\n",
      "271/388, train_loss: 0.3244, step time: 0.4777\n",
      "272/388, train_loss: 0.0626, step time: 0.7730\n",
      "273/388, train_loss: 0.1697, step time: 0.5410\n",
      "274/388, train_loss: 0.2969, step time: 0.5165\n",
      "275/388, train_loss: 0.1092, step time: 0.5041\n",
      "276/388, train_loss: 0.1905, step time: 0.4899\n",
      "277/388, train_loss: 0.2128, step time: 0.4934\n",
      "278/388, train_loss: 0.1162, step time: 0.4822\n",
      "279/388, train_loss: 0.1781, step time: 0.4869\n",
      "280/388, train_loss: 0.1765, step time: 0.4859\n",
      "281/388, train_loss: 0.2889, step time: 0.4785\n",
      "282/388, train_loss: 0.1275, step time: 0.4804\n",
      "283/388, train_loss: 0.1848, step time: 0.5342\n",
      "284/388, train_loss: 0.2607, step time: 0.5001\n",
      "285/388, train_loss: 0.2036, step time: 0.4995\n",
      "286/388, train_loss: 0.5974, step time: 0.4938\n",
      "287/388, train_loss: 0.6652, step time: 0.4920\n",
      "288/388, train_loss: 0.2426, step time: 0.4937\n",
      "289/388, train_loss: 0.2564, step time: 0.5294\n",
      "290/388, train_loss: 0.3202, step time: 0.5221\n",
      "291/388, train_loss: 0.1766, step time: 0.5223\n",
      "292/388, train_loss: 0.3082, step time: 0.5084\n",
      "293/388, train_loss: 0.2138, step time: 0.4893\n",
      "294/388, train_loss: 0.1997, step time: 0.4915\n",
      "295/388, train_loss: 0.1966, step time: 0.4793\n",
      "296/388, train_loss: 0.3129, step time: 0.9403\n",
      "297/388, train_loss: 0.1478, step time: 0.5393\n",
      "298/388, train_loss: 0.1028, step time: 0.5106\n",
      "299/388, train_loss: 0.2072, step time: 0.5037\n",
      "300/388, train_loss: 0.1049, step time: 0.4895\n",
      "301/388, train_loss: 0.2127, step time: 0.4862\n",
      "302/388, train_loss: 0.1962, step time: 0.4885\n",
      "303/388, train_loss: 0.2746, step time: 0.4771\n",
      "304/388, train_loss: 0.3223, step time: 0.5775\n",
      "305/388, train_loss: 0.2934, step time: 0.5372\n",
      "306/388, train_loss: 0.3315, step time: 0.5129\n",
      "307/388, train_loss: 0.1108, step time: 0.5016\n",
      "308/388, train_loss: 0.2627, step time: 0.4988\n",
      "309/388, train_loss: 0.3345, step time: 0.4841\n",
      "310/388, train_loss: 0.3225, step time: 0.9501\n",
      "311/388, train_loss: 0.0600, step time: 0.5300\n",
      "312/388, train_loss: 0.0616, step time: 0.4991\n",
      "313/388, train_loss: 0.4699, step time: 0.4956\n",
      "314/388, train_loss: 0.1261, step time: 0.4980\n",
      "315/388, train_loss: 0.0779, step time: 0.4827\n",
      "316/388, train_loss: 0.2552, step time: 0.4852\n",
      "317/388, train_loss: 0.5121, step time: 0.4887\n",
      "318/388, train_loss: 0.0469, step time: 0.4775\n",
      "319/388, train_loss: 0.1191, step time: 0.9330\n",
      "320/388, train_loss: 0.1579, step time: 0.5255\n",
      "321/388, train_loss: 0.1877, step time: 0.5037\n",
      "322/388, train_loss: 0.1415, step time: 0.4898\n",
      "323/388, train_loss: 0.1329, step time: 0.4952\n",
      "324/388, train_loss: 0.1236, step time: 0.4912\n",
      "325/388, train_loss: 0.4680, step time: 0.5051\n",
      "326/388, train_loss: 0.2709, step time: 0.5051\n",
      "327/388, train_loss: 0.2928, step time: 0.4958\n",
      "328/388, train_loss: 0.1056, step time: 0.9312\n",
      "329/388, train_loss: 0.1289, step time: 0.5322\n",
      "330/388, train_loss: 0.3307, step time: 0.5057\n",
      "331/388, train_loss: 0.1651, step time: 0.4861\n",
      "332/388, train_loss: 0.0781, step time: 0.4892\n",
      "333/388, train_loss: 0.2910, step time: 0.4792\n",
      "334/388, train_loss: 0.1367, step time: 0.4875\n",
      "335/388, train_loss: 0.1845, step time: 0.4843\n",
      "336/388, train_loss: 0.2299, step time: 0.4814\n",
      "337/388, train_loss: 0.1807, step time: 0.4884\n",
      "338/388, train_loss: 0.0801, step time: 0.4896\n",
      "339/388, train_loss: 0.0808, step time: 0.5400\n",
      "340/388, train_loss: 0.2379, step time: 0.5255\n",
      "341/388, train_loss: 0.1390, step time: 0.4949\n",
      "342/388, train_loss: 0.0949, step time: 0.4947\n",
      "343/388, train_loss: 0.6476, step time: 0.4859\n",
      "344/388, train_loss: 0.0836, step time: 0.4873\n",
      "345/388, train_loss: 0.1066, step time: 0.4921\n",
      "346/388, train_loss: 0.0864, step time: 1.0879\n",
      "347/388, train_loss: 0.0966, step time: 0.5298\n",
      "348/388, train_loss: 0.2859, step time: 0.5097\n",
      "349/388, train_loss: 0.1828, step time: 0.5042\n",
      "350/388, train_loss: 0.5755, step time: 0.5020\n",
      "351/388, train_loss: 0.1269, step time: 0.4893\n",
      "352/388, train_loss: 0.1502, step time: 0.4795\n",
      "353/388, train_loss: 0.1426, step time: 0.5138\n",
      "354/388, train_loss: 0.1532, step time: 0.5615\n",
      "355/388, train_loss: 0.0879, step time: 0.5411\n",
      "356/388, train_loss: 0.2564, step time: 0.6035\n",
      "357/388, train_loss: 0.2421, step time: 0.5337\n",
      "358/388, train_loss: 0.0976, step time: 0.5152\n",
      "359/388, train_loss: 0.0978, step time: 0.4966\n",
      "360/388, train_loss: 0.2011, step time: 0.4955\n",
      "361/388, train_loss: 0.1381, step time: 0.4804\n",
      "362/388, train_loss: 0.2030, step time: 0.4847\n",
      "363/388, train_loss: 0.1249, step time: 0.4917\n",
      "364/388, train_loss: 0.3639, step time: 0.4913\n",
      "365/388, train_loss: 0.1660, step time: 0.7103\n",
      "366/388, train_loss: 0.3033, step time: 0.5592\n",
      "367/388, train_loss: 0.1581, step time: 0.5335\n",
      "368/388, train_loss: 0.0981, step time: 0.5035\n",
      "369/388, train_loss: 0.2726, step time: 0.5007\n",
      "370/388, train_loss: 0.1583, step time: 0.4825\n",
      "371/388, train_loss: 0.0856, step time: 0.4886\n",
      "372/388, train_loss: 0.1463, step time: 0.4760\n",
      "373/388, train_loss: 0.0942, step time: 0.7080\n",
      "374/388, train_loss: 0.1483, step time: 0.5595\n",
      "375/388, train_loss: 0.1558, step time: 0.5216\n",
      "376/388, train_loss: 0.1597, step time: 0.5083\n",
      "377/388, train_loss: 0.1173, step time: 0.5062\n",
      "378/388, train_loss: 0.4214, step time: 0.5065\n",
      "379/388, train_loss: 0.0774, step time: 1.0635\n",
      "380/388, train_loss: 0.2166, step time: 0.5215\n",
      "381/388, train_loss: 0.1020, step time: 0.5020\n",
      "382/388, train_loss: 0.1476, step time: 0.4809\n",
      "383/388, train_loss: 0.2250, step time: 0.4806\n",
      "384/388, train_loss: 0.1507, step time: 0.4802\n",
      "385/388, train_loss: 0.2613, step time: 0.4874\n",
      "386/388, train_loss: 0.1661, step time: 0.9107\n",
      "387/388, train_loss: 0.0581, step time: 0.5358\n",
      "388/388, train_loss: 0.1840, step time: 0.4916\n",
      "epoch 34 average loss: 0.2061\n",
      "saved new best metric model\n",
      "current epoch: 34 current mean dice: 0.7534 tc: 0.8044 wt: 0.8894 et: 0.5664\n",
      "best mean dice: 0.7534 at epoch: 34\n",
      "time consuming of epoch 34 is: 299.8752\n",
      "----------\n",
      "epoch 35/300\n",
      "1/388, train_loss: 0.2066, step time: 0.4761\n",
      "2/388, train_loss: 0.5864, step time: 0.4843\n",
      "3/388, train_loss: 0.2753, step time: 0.4804\n",
      "4/388, train_loss: 0.2149, step time: 1.2571\n",
      "5/388, train_loss: 0.1688, step time: 0.5369\n",
      "6/388, train_loss: 0.2689, step time: 0.5253\n",
      "7/388, train_loss: 0.3002, step time: 0.4840\n",
      "8/388, train_loss: 0.1327, step time: 0.6826\n",
      "9/388, train_loss: 0.1247, step time: 0.5425\n",
      "10/388, train_loss: 0.1490, step time: 0.5194\n",
      "11/388, train_loss: 0.0526, step time: 0.5088\n",
      "12/388, train_loss: 0.0813, step time: 0.4888\n",
      "13/388, train_loss: 0.2931, step time: 0.5693\n",
      "14/388, train_loss: 0.7463, step time: 0.5541\n",
      "15/388, train_loss: 0.1063, step time: 0.5212\n",
      "16/388, train_loss: 0.1486, step time: 0.5241\n",
      "17/388, train_loss: 0.3026, step time: 0.5047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/388, train_loss: 0.1801, step time: 1.0029\n",
      "19/388, train_loss: 0.2026, step time: 0.5500\n",
      "20/388, train_loss: 0.3771, step time: 0.5325\n",
      "21/388, train_loss: 0.3848, step time: 0.5166\n",
      "22/388, train_loss: 0.0899, step time: 0.4945\n",
      "23/388, train_loss: 0.1174, step time: 0.4958\n",
      "24/388, train_loss: 0.1143, step time: 0.9226\n",
      "25/388, train_loss: 0.1173, step time: 0.5376\n",
      "26/388, train_loss: 0.2584, step time: 0.5168\n",
      "27/388, train_loss: 0.2151, step time: 0.4989\n",
      "28/388, train_loss: 0.1120, step time: 0.4990\n",
      "29/388, train_loss: 0.2541, step time: 0.5430\n",
      "30/388, train_loss: 0.1479, step time: 0.5586\n",
      "31/388, train_loss: 0.2788, step time: 0.5323\n",
      "32/388, train_loss: 0.1998, step time: 0.5073\n",
      "33/388, train_loss: 0.2115, step time: 0.4950\n",
      "34/388, train_loss: 0.3614, step time: 0.4996\n",
      "35/388, train_loss: 0.3521, step time: 0.5542\n",
      "36/388, train_loss: 0.5331, step time: 0.5267\n",
      "37/388, train_loss: 0.1151, step time: 0.4893\n",
      "38/388, train_loss: 0.2925, step time: 1.0480\n",
      "39/388, train_loss: 0.1915, step time: 0.5452\n",
      "40/388, train_loss: 0.1352, step time: 0.5043\n",
      "41/388, train_loss: 0.1162, step time: 0.5260\n",
      "42/388, train_loss: 0.0709, step time: 0.5089\n",
      "43/388, train_loss: 0.1614, step time: 0.5096\n",
      "44/388, train_loss: 0.1816, step time: 0.4900\n",
      "45/388, train_loss: 0.2838, step time: 0.4992\n",
      "46/388, train_loss: 0.2368, step time: 0.5067\n",
      "47/388, train_loss: 0.1789, step time: 0.5090\n",
      "48/388, train_loss: 0.0657, step time: 0.4883\n",
      "49/388, train_loss: 0.0591, step time: 0.9258\n",
      "50/388, train_loss: 0.5091, step time: 0.5477\n",
      "51/388, train_loss: 0.1665, step time: 0.5214\n",
      "52/388, train_loss: 0.0967, step time: 0.5181\n",
      "53/388, train_loss: 0.1497, step time: 0.5204\n",
      "54/388, train_loss: 0.3907, step time: 0.5077\n",
      "55/388, train_loss: 0.1226, step time: 0.5620\n",
      "56/388, train_loss: 0.2096, step time: 0.5284\n",
      "57/388, train_loss: 0.1150, step time: 0.5040\n",
      "58/388, train_loss: 0.1353, step time: 0.4845\n",
      "59/388, train_loss: 0.1459, step time: 0.5010\n",
      "60/388, train_loss: 0.1790, step time: 0.4976\n",
      "61/388, train_loss: 0.2382, step time: 0.5322\n",
      "62/388, train_loss: 0.3206, step time: 0.5131\n",
      "63/388, train_loss: 0.1519, step time: 0.5055\n",
      "64/388, train_loss: 0.2011, step time: 0.5444\n",
      "65/388, train_loss: 0.2279, step time: 0.5573\n",
      "66/388, train_loss: 0.2451, step time: 0.6289\n",
      "67/388, train_loss: 0.3775, step time: 0.5569\n",
      "68/388, train_loss: 0.0921, step time: 0.5264\n",
      "69/388, train_loss: 0.0937, step time: 0.5033\n",
      "70/388, train_loss: 0.0586, step time: 0.4930\n",
      "71/388, train_loss: 0.1962, step time: 0.4809\n",
      "72/388, train_loss: 0.1831, step time: 0.9736\n",
      "73/388, train_loss: 0.1728, step time: 0.5519\n",
      "74/388, train_loss: 0.2586, step time: 0.5219\n",
      "75/388, train_loss: 0.0471, step time: 0.4953\n",
      "76/388, train_loss: 0.1166, step time: 0.4981\n",
      "77/388, train_loss: 0.2573, step time: 0.4842\n",
      "78/388, train_loss: 0.5082, step time: 0.8955\n",
      "79/388, train_loss: 0.2580, step time: 0.5625\n",
      "80/388, train_loss: 0.0483, step time: 0.5298\n",
      "81/388, train_loss: 0.1057, step time: 0.5086\n",
      "82/388, train_loss: 0.1136, step time: 0.5024\n",
      "83/388, train_loss: 0.3785, step time: 0.5230\n",
      "84/388, train_loss: 0.1427, step time: 0.5169\n",
      "85/388, train_loss: 0.2284, step time: 0.4989\n",
      "86/388, train_loss: 0.1053, step time: 0.4822\n",
      "87/388, train_loss: 0.3118, step time: 0.5305\n",
      "88/388, train_loss: 0.1332, step time: 0.5634\n",
      "89/388, train_loss: 0.2328, step time: 0.5355\n",
      "90/388, train_loss: 0.1614, step time: 0.5225\n",
      "91/388, train_loss: 0.0669, step time: 0.5199\n",
      "92/388, train_loss: 0.1738, step time: 0.5115\n",
      "93/388, train_loss: 0.0921, step time: 0.4985\n",
      "94/388, train_loss: 0.1111, step time: 0.4885\n",
      "95/388, train_loss: 0.2156, step time: 0.4794\n",
      "96/388, train_loss: 0.3682, step time: 0.4845\n",
      "97/388, train_loss: 0.1618, step time: 0.5068\n",
      "98/388, train_loss: 0.0947, step time: 0.5003\n",
      "99/388, train_loss: 0.1570, step time: 1.1183\n",
      "100/388, train_loss: 0.1413, step time: 0.5305\n",
      "101/388, train_loss: 0.1007, step time: 0.5039\n",
      "102/388, train_loss: 0.2618, step time: 0.4894\n",
      "103/388, train_loss: 0.0993, step time: 0.5444\n",
      "104/388, train_loss: 0.4863, step time: 0.6638\n",
      "105/388, train_loss: 0.1110, step time: 0.5628\n",
      "106/388, train_loss: 0.1561, step time: 0.5359\n",
      "107/388, train_loss: 0.0784, step time: 0.5109\n",
      "108/388, train_loss: 0.1860, step time: 0.5018\n",
      "109/388, train_loss: 0.3005, step time: 0.4890\n",
      "110/388, train_loss: 0.2229, step time: 0.4900\n",
      "111/388, train_loss: 0.1984, step time: 0.5080\n",
      "112/388, train_loss: 0.0505, step time: 0.5332\n",
      "113/388, train_loss: 0.4619, step time: 0.7167\n",
      "114/388, train_loss: 0.0964, step time: 0.5536\n",
      "115/388, train_loss: 0.1910, step time: 0.5173\n",
      "116/388, train_loss: 0.1105, step time: 0.5180\n",
      "117/388, train_loss: 0.1016, step time: 0.5062\n",
      "118/388, train_loss: 0.1424, step time: 0.5060\n",
      "119/388, train_loss: 0.1820, step time: 0.4921\n",
      "120/388, train_loss: 0.5558, step time: 0.5190\n",
      "121/388, train_loss: 0.2918, step time: 1.1660\n",
      "122/388, train_loss: 0.1415, step time: 0.5407\n",
      "123/388, train_loss: 0.1682, step time: 0.5158\n",
      "124/388, train_loss: 0.2135, step time: 0.4982\n",
      "125/388, train_loss: 0.1540, step time: 0.5019\n",
      "126/388, train_loss: 0.1176, step time: 0.4874\n",
      "127/388, train_loss: 0.2545, step time: 0.4856\n",
      "128/388, train_loss: 0.1327, step time: 0.5503\n",
      "129/388, train_loss: 0.4272, step time: 0.5354\n",
      "130/388, train_loss: 0.1510, step time: 0.5027\n",
      "131/388, train_loss: 0.1846, step time: 0.4963\n",
      "132/388, train_loss: 0.0931, step time: 0.4814\n",
      "133/388, train_loss: 0.1656, step time: 0.4822\n",
      "134/388, train_loss: 0.1045, step time: 0.4879\n",
      "135/388, train_loss: 0.1807, step time: 0.5219\n",
      "136/388, train_loss: 0.1135, step time: 0.5013\n",
      "137/388, train_loss: 0.2165, step time: 1.1201\n",
      "138/388, train_loss: 0.1403, step time: 0.5465\n",
      "139/388, train_loss: 0.1640, step time: 0.5143\n",
      "140/388, train_loss: 0.1109, step time: 0.4977\n",
      "141/388, train_loss: 0.1284, step time: 0.4915\n",
      "142/388, train_loss: 0.0907, step time: 0.4873\n",
      "143/388, train_loss: 0.1097, step time: 0.5027\n",
      "144/388, train_loss: 0.0777, step time: 1.1298\n",
      "145/388, train_loss: 0.1150, step time: 0.5417\n",
      "146/388, train_loss: 0.3069, step time: 0.5210\n",
      "147/388, train_loss: 0.1420, step time: 0.5076\n",
      "148/388, train_loss: 0.7858, step time: 0.4980\n",
      "149/388, train_loss: 0.1466, step time: 0.4885\n",
      "150/388, train_loss: 0.1378, step time: 0.5008\n",
      "151/388, train_loss: 0.0553, step time: 0.5090\n",
      "152/388, train_loss: 0.2794, step time: 0.4868\n",
      "153/388, train_loss: 0.0947, step time: 1.0931\n",
      "154/388, train_loss: 0.1700, step time: 0.5508\n",
      "155/388, train_loss: 0.1834, step time: 0.5184\n",
      "156/388, train_loss: 0.2085, step time: 0.5063\n",
      "157/388, train_loss: 0.3020, step time: 0.5847\n",
      "158/388, train_loss: 0.0816, step time: 0.5538\n",
      "159/388, train_loss: 0.0882, step time: 0.5454\n",
      "160/388, train_loss: 0.1767, step time: 0.5230\n",
      "161/388, train_loss: 0.0896, step time: 0.5553\n",
      "162/388, train_loss: 0.2247, step time: 0.5311\n",
      "163/388, train_loss: 0.1305, step time: 0.5085\n",
      "164/388, train_loss: 0.1293, step time: 0.5437\n",
      "165/388, train_loss: 0.3003, step time: 0.5266\n",
      "166/388, train_loss: 0.1543, step time: 0.5261\n",
      "167/388, train_loss: 0.0720, step time: 0.5180\n",
      "168/388, train_loss: 0.2139, step time: 0.5073\n",
      "169/388, train_loss: 0.1011, step time: 0.5461\n",
      "170/388, train_loss: 0.1913, step time: 0.5217\n",
      "171/388, train_loss: 0.0910, step time: 0.4972\n",
      "172/388, train_loss: 0.2332, step time: 0.4958\n",
      "173/388, train_loss: 0.2569, step time: 0.4849\n",
      "174/388, train_loss: 0.0943, step time: 0.9207\n",
      "175/388, train_loss: 0.1507, step time: 0.5697\n",
      "176/388, train_loss: 0.0987, step time: 0.5272\n",
      "177/388, train_loss: 0.1453, step time: 0.5040\n",
      "178/388, train_loss: 0.1042, step time: 0.5482\n",
      "179/388, train_loss: 0.2438, step time: 0.5249\n",
      "180/388, train_loss: 0.0552, step time: 0.5045\n",
      "181/388, train_loss: 0.1621, step time: 0.4944\n",
      "182/388, train_loss: 0.5167, step time: 0.4909\n",
      "183/388, train_loss: 0.1174, step time: 1.1491\n",
      "184/388, train_loss: 0.2046, step time: 0.5487\n",
      "185/388, train_loss: 0.1266, step time: 0.5150\n",
      "186/388, train_loss: 0.3023, step time: 0.5043\n",
      "187/388, train_loss: 0.2012, step time: 0.4915\n",
      "188/388, train_loss: 0.0818, step time: 0.4948\n",
      "189/388, train_loss: 0.2072, step time: 0.4883\n",
      "190/388, train_loss: 0.2505, step time: 0.4891\n",
      "191/388, train_loss: 0.1957, step time: 0.4777\n",
      "192/388, train_loss: 0.2811, step time: 0.4732\n",
      "193/388, train_loss: 0.3061, step time: 0.4843\n",
      "194/388, train_loss: 0.2472, step time: 0.5104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/388, train_loss: 0.1154, step time: 0.4986\n",
      "196/388, train_loss: 0.1188, step time: 0.5385\n",
      "197/388, train_loss: 0.5509, step time: 0.5481\n",
      "198/388, train_loss: 0.1487, step time: 0.5178\n",
      "199/388, train_loss: 0.1700, step time: 0.5068\n",
      "200/388, train_loss: 0.1825, step time: 0.5024\n",
      "201/388, train_loss: 0.2204, step time: 0.5044\n",
      "202/388, train_loss: 0.3476, step time: 0.5191\n",
      "203/388, train_loss: 0.1026, step time: 0.4994\n",
      "204/388, train_loss: 0.1169, step time: 0.5061\n",
      "205/388, train_loss: 0.1452, step time: 0.4997\n",
      "206/388, train_loss: 0.5494, step time: 0.4844\n",
      "207/388, train_loss: 0.2228, step time: 0.4983\n",
      "208/388, train_loss: 0.1503, step time: 0.4992\n",
      "209/388, train_loss: 0.2036, step time: 0.4951\n",
      "210/388, train_loss: 0.0842, step time: 0.4861\n",
      "211/388, train_loss: 0.2577, step time: 0.5218\n",
      "212/388, train_loss: 0.2116, step time: 0.5431\n",
      "213/388, train_loss: 0.2317, step time: 0.5121\n",
      "214/388, train_loss: 0.2837, step time: 0.7862\n",
      "215/388, train_loss: 0.0779, step time: 0.5580\n",
      "216/388, train_loss: 0.4698, step time: 0.5365\n",
      "217/388, train_loss: 0.3520, step time: 0.5257\n",
      "218/388, train_loss: 0.1576, step time: 0.5841\n",
      "219/388, train_loss: 0.1900, step time: 0.5501\n",
      "220/388, train_loss: 0.3784, step time: 0.5348\n",
      "221/388, train_loss: 0.1428, step time: 0.5066\n",
      "222/388, train_loss: 0.0976, step time: 0.8629\n",
      "223/388, train_loss: 0.1315, step time: 0.5451\n",
      "224/388, train_loss: 0.1921, step time: 0.5151\n",
      "225/388, train_loss: 0.1769, step time: 0.4908\n",
      "226/388, train_loss: 0.2701, step time: 0.5599\n",
      "227/388, train_loss: 0.1320, step time: 0.5562\n",
      "228/388, train_loss: 0.3032, step time: 0.5464\n",
      "229/388, train_loss: 0.2211, step time: 0.5352\n",
      "230/388, train_loss: 0.0384, step time: 0.5598\n",
      "231/388, train_loss: 0.0821, step time: 0.5436\n",
      "232/388, train_loss: 0.1307, step time: 0.5157\n",
      "233/388, train_loss: 0.2473, step time: 0.5472\n",
      "234/388, train_loss: 0.1155, step time: 0.5202\n",
      "235/388, train_loss: 0.2712, step time: 0.5278\n",
      "236/388, train_loss: 0.3924, step time: 0.5858\n",
      "237/388, train_loss: 0.0937, step time: 0.5241\n",
      "238/388, train_loss: 0.0886, step time: 0.5025\n",
      "239/388, train_loss: 0.3712, step time: 0.5065\n",
      "240/388, train_loss: 0.1569, step time: 0.5573\n",
      "241/388, train_loss: 0.1284, step time: 0.5343\n",
      "242/388, train_loss: 0.2267, step time: 0.5207\n",
      "243/388, train_loss: 0.0981, step time: 0.5063\n",
      "244/388, train_loss: 0.0506, step time: 0.5134\n",
      "245/388, train_loss: 0.0364, step time: 0.6278\n",
      "246/388, train_loss: 0.3182, step time: 0.5470\n",
      "247/388, train_loss: 0.0645, step time: 0.5222\n",
      "248/388, train_loss: 0.1035, step time: 0.5209\n",
      "249/388, train_loss: 0.1775, step time: 0.5077\n",
      "250/388, train_loss: 0.2402, step time: 0.5016\n",
      "251/388, train_loss: 0.1919, step time: 0.4861\n",
      "252/388, train_loss: 0.2622, step time: 0.5379\n",
      "253/388, train_loss: 0.2711, step time: 0.6460\n",
      "254/388, train_loss: 0.0879, step time: 0.5785\n",
      "255/388, train_loss: 0.3242, step time: 0.5430\n",
      "256/388, train_loss: 0.2147, step time: 0.5175\n",
      "257/388, train_loss: 0.1556, step time: 0.5087\n",
      "258/388, train_loss: 0.1047, step time: 0.4898\n",
      "259/388, train_loss: 0.2519, step time: 0.5251\n",
      "260/388, train_loss: 0.2567, step time: 0.5277\n",
      "261/388, train_loss: 0.0821, step time: 0.5649\n",
      "262/388, train_loss: 0.1101, step time: 0.5468\n",
      "263/388, train_loss: 0.1151, step time: 0.5113\n",
      "264/388, train_loss: 0.3495, step time: 0.5094\n",
      "265/388, train_loss: 0.0878, step time: 0.5683\n",
      "266/388, train_loss: 0.2492, step time: 0.5499\n",
      "267/388, train_loss: 0.1521, step time: 0.5204\n",
      "268/388, train_loss: 0.0668, step time: 0.4955\n",
      "269/388, train_loss: 0.2537, step time: 0.5458\n",
      "270/388, train_loss: 0.1133, step time: 0.5242\n",
      "271/388, train_loss: 0.3161, step time: 0.5129\n",
      "272/388, train_loss: 0.1032, step time: 0.4985\n",
      "273/388, train_loss: 0.3403, step time: 0.4889\n",
      "274/388, train_loss: 0.3693, step time: 0.4836\n",
      "275/388, train_loss: 0.0808, step time: 0.5145\n",
      "276/388, train_loss: 0.0787, step time: 0.4966\n",
      "277/388, train_loss: 0.1351, step time: 0.5062\n",
      "278/388, train_loss: 0.2573, step time: 0.5140\n",
      "279/388, train_loss: 0.1591, step time: 0.5112\n",
      "280/388, train_loss: 0.3108, step time: 0.4960\n",
      "281/388, train_loss: 0.1492, step time: 1.0038\n",
      "282/388, train_loss: 0.2703, step time: 0.5469\n",
      "283/388, train_loss: 0.1170, step time: 0.5183\n",
      "284/388, train_loss: 0.0399, step time: 0.4918\n",
      "285/388, train_loss: 0.4683, step time: 0.5011\n",
      "286/388, train_loss: 0.0847, step time: 0.5013\n",
      "287/388, train_loss: 0.0935, step time: 0.5629\n",
      "288/388, train_loss: 0.1039, step time: 0.5352\n",
      "289/388, train_loss: 0.0940, step time: 0.5110\n",
      "290/388, train_loss: 0.6655, step time: 0.5002\n",
      "291/388, train_loss: 0.4443, step time: 0.4963\n",
      "292/388, train_loss: 0.1371, step time: 0.4934\n",
      "293/388, train_loss: 0.1040, step time: 0.4959\n",
      "294/388, train_loss: 0.2857, step time: 0.4970\n",
      "295/388, train_loss: 0.2476, step time: 0.5393\n",
      "296/388, train_loss: 0.4175, step time: 0.5498\n",
      "297/388, train_loss: 0.1324, step time: 0.5201\n",
      "298/388, train_loss: 0.2539, step time: 0.5023\n",
      "299/388, train_loss: 0.3778, step time: 0.5837\n",
      "300/388, train_loss: 0.1890, step time: 0.5343\n",
      "301/388, train_loss: 0.2545, step time: 0.5104\n",
      "302/388, train_loss: 0.2434, step time: 0.4865\n",
      "303/388, train_loss: 0.4386, step time: 0.4800\n",
      "304/388, train_loss: 0.2852, step time: 0.5425\n",
      "305/388, train_loss: 0.5980, step time: 0.5125\n",
      "306/388, train_loss: 0.0630, step time: 0.4920\n",
      "307/388, train_loss: 0.1923, step time: 1.1185\n",
      "308/388, train_loss: 0.1230, step time: 0.5151\n",
      "309/388, train_loss: 0.1624, step time: 0.4993\n",
      "310/388, train_loss: 0.2810, step time: 0.4815\n",
      "311/388, train_loss: 0.1001, step time: 0.4896\n",
      "312/388, train_loss: 0.1072, step time: 0.4812\n",
      "313/388, train_loss: 0.1859, step time: 0.4928\n",
      "314/388, train_loss: 0.3442, step time: 0.4882\n",
      "315/388, train_loss: 0.2132, step time: 1.1456\n",
      "316/388, train_loss: 0.2837, step time: 0.5458\n",
      "317/388, train_loss: 0.0721, step time: 0.5152\n",
      "318/388, train_loss: 0.3668, step time: 0.4998\n",
      "319/388, train_loss: 0.3393, step time: 0.4897\n",
      "320/388, train_loss: 0.3017, step time: 0.4957\n",
      "321/388, train_loss: 0.1033, step time: 0.4918\n",
      "322/388, train_loss: 0.1715, step time: 0.4781\n",
      "323/388, train_loss: 0.2062, step time: 0.5025\n",
      "324/388, train_loss: 0.2538, step time: 0.4886\n",
      "325/388, train_loss: 0.3660, step time: 0.4973\n",
      "326/388, train_loss: 0.1467, step time: 1.2027\n",
      "327/388, train_loss: 0.2473, step time: 0.5342\n",
      "328/388, train_loss: 0.2584, step time: 0.5073\n",
      "329/388, train_loss: 0.1570, step time: 0.4954\n",
      "330/388, train_loss: 0.1566, step time: 0.4819\n",
      "331/388, train_loss: 0.1949, step time: 0.4807\n",
      "332/388, train_loss: 0.1607, step time: 0.5021\n",
      "333/388, train_loss: 0.3608, step time: 0.4899\n",
      "334/388, train_loss: 0.6190, step time: 0.4916\n",
      "335/388, train_loss: 0.2013, step time: 0.4851\n",
      "336/388, train_loss: 0.1013, step time: 0.5283\n",
      "337/388, train_loss: 0.2376, step time: 0.5242\n",
      "338/388, train_loss: 0.0675, step time: 0.4993\n",
      "339/388, train_loss: 0.2002, step time: 0.4911\n",
      "340/388, train_loss: 0.1351, step time: 0.4946\n",
      "341/388, train_loss: 0.2637, step time: 0.4823\n",
      "342/388, train_loss: 0.0969, step time: 0.4848\n",
      "343/388, train_loss: 0.1730, step time: 0.4795\n",
      "344/388, train_loss: 0.2801, step time: 0.4965\n",
      "345/388, train_loss: 0.1638, step time: 0.5078\n",
      "346/388, train_loss: 0.1759, step time: 0.5840\n",
      "347/388, train_loss: 0.3965, step time: 0.5366\n",
      "348/388, train_loss: 0.1860, step time: 0.5159\n",
      "349/388, train_loss: 0.4193, step time: 0.5064\n",
      "350/388, train_loss: 0.1199, step time: 0.4900\n",
      "351/388, train_loss: 0.2967, step time: 0.5045\n",
      "352/388, train_loss: 0.1050, step time: 0.4964\n",
      "353/388, train_loss: 0.2300, step time: 1.0039\n",
      "354/388, train_loss: 0.2740, step time: 0.5439\n",
      "355/388, train_loss: 0.2879, step time: 0.5115\n",
      "356/388, train_loss: 0.2249, step time: 0.4942\n",
      "357/388, train_loss: 0.4628, step time: 0.4960\n",
      "358/388, train_loss: 0.2241, step time: 0.4788\n",
      "359/388, train_loss: 0.1121, step time: 0.4966\n",
      "360/388, train_loss: 0.2449, step time: 0.5003\n",
      "361/388, train_loss: 0.2029, step time: 0.4831\n",
      "362/388, train_loss: 0.2259, step time: 1.2010\n",
      "363/388, train_loss: 0.2454, step time: 0.5426\n",
      "364/388, train_loss: 0.3581, step time: 0.5092\n",
      "365/388, train_loss: 0.2455, step time: 0.4972\n",
      "366/388, train_loss: 0.1781, step time: 0.4999\n",
      "367/388, train_loss: 0.1562, step time: 0.4816\n",
      "368/388, train_loss: 0.0867, step time: 0.4756\n",
      "369/388, train_loss: 0.5130, step time: 0.4768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/388, train_loss: 0.1547, step time: 0.5572\n",
      "371/388, train_loss: 0.0913, step time: 0.5361\n",
      "372/388, train_loss: 0.2144, step time: 0.5139\n",
      "373/388, train_loss: 0.1166, step time: 0.5160\n",
      "374/388, train_loss: 0.4197, step time: 0.4950\n",
      "375/388, train_loss: 0.1274, step time: 0.4809\n",
      "376/388, train_loss: 0.0766, step time: 0.4851\n",
      "377/388, train_loss: 0.1212, step time: 0.4878\n",
      "378/388, train_loss: 0.3001, step time: 0.4974\n",
      "379/388, train_loss: 0.2128, step time: 0.8051\n",
      "380/388, train_loss: 0.0599, step time: 0.5546\n",
      "381/388, train_loss: 0.7099, step time: 0.5225\n",
      "382/388, train_loss: 0.2610, step time: 0.5038\n",
      "383/388, train_loss: 0.0467, step time: 0.4858\n",
      "384/388, train_loss: 0.3346, step time: 0.4844\n",
      "385/388, train_loss: 0.2406, step time: 0.4789\n",
      "386/388, train_loss: 0.0671, step time: 1.0046\n",
      "387/388, train_loss: 0.1164, step time: 0.5291\n",
      "388/388, train_loss: 0.2309, step time: 0.4990\n",
      "epoch 35 average loss: 0.2060\n",
      "current epoch: 35 current mean dice: 0.7509 tc: 0.8024 wt: 0.8910 et: 0.5594\n",
      "best mean dice: 0.7534 at epoch: 34\n",
      "time consuming of epoch 35 is: 301.7411\n",
      "----------\n",
      "epoch 36/300\n",
      "1/388, train_loss: 0.1892, step time: 0.4780\n",
      "2/388, train_loss: 0.0901, step time: 0.4806\n",
      "3/388, train_loss: 0.0993, step time: 0.4816\n",
      "4/388, train_loss: 0.2201, step time: 0.7724\n",
      "5/388, train_loss: 0.1414, step time: 0.5389\n",
      "6/388, train_loss: 0.2016, step time: 0.5109\n",
      "7/388, train_loss: 0.0837, step time: 0.4972\n",
      "8/388, train_loss: 0.2858, step time: 0.4811\n",
      "9/388, train_loss: 0.1143, step time: 0.5299\n",
      "10/388, train_loss: 0.1422, step time: 0.5113\n",
      "11/388, train_loss: 0.1670, step time: 0.4931\n",
      "12/388, train_loss: 0.2128, step time: 0.5613\n",
      "13/388, train_loss: 0.1060, step time: 0.5264\n",
      "14/388, train_loss: 0.2462, step time: 0.5153\n",
      "15/388, train_loss: 0.1435, step time: 0.4998\n",
      "16/388, train_loss: 0.1890, step time: 0.5413\n",
      "17/388, train_loss: 0.2765, step time: 0.5121\n",
      "18/388, train_loss: 0.1499, step time: 0.5887\n",
      "19/388, train_loss: 0.1865, step time: 0.6346\n",
      "20/388, train_loss: 0.1872, step time: 0.5343\n",
      "21/388, train_loss: 0.2287, step time: 0.5146\n",
      "22/388, train_loss: 0.0822, step time: 0.4935\n",
      "23/388, train_loss: 0.2442, step time: 0.4996\n",
      "24/388, train_loss: 0.2182, step time: 1.0349\n",
      "25/388, train_loss: 0.1638, step time: 0.5432\n",
      "26/388, train_loss: 0.2409, step time: 0.5206\n",
      "27/388, train_loss: 0.0890, step time: 0.5007\n",
      "28/388, train_loss: 0.2806, step time: 0.5057\n",
      "29/388, train_loss: 0.2699, step time: 0.4949\n",
      "30/388, train_loss: 0.1193, step time: 0.4895\n",
      "31/388, train_loss: 0.0594, step time: 0.4963\n",
      "32/388, train_loss: 0.2355, step time: 0.5002\n",
      "33/388, train_loss: 0.1539, step time: 0.4967\n",
      "34/388, train_loss: 0.2076, step time: 1.0536\n",
      "35/388, train_loss: 0.1484, step time: 0.5629\n",
      "36/388, train_loss: 0.1037, step time: 0.5423\n",
      "37/388, train_loss: 0.2702, step time: 0.5227\n",
      "38/388, train_loss: 0.1774, step time: 0.5030\n",
      "39/388, train_loss: 0.2076, step time: 0.4954\n",
      "40/388, train_loss: 0.0914, step time: 0.4911\n",
      "41/388, train_loss: 0.4109, step time: 0.5136\n",
      "42/388, train_loss: 0.0958, step time: 0.4997\n",
      "43/388, train_loss: 0.1699, step time: 0.4840\n",
      "44/388, train_loss: 0.1341, step time: 0.5097\n",
      "45/388, train_loss: 0.0877, step time: 0.5003\n",
      "46/388, train_loss: 0.1552, step time: 1.1997\n",
      "47/388, train_loss: 0.3277, step time: 0.5152\n",
      "48/388, train_loss: 0.7024, step time: 0.5004\n",
      "49/388, train_loss: 0.4462, step time: 0.4778\n",
      "50/388, train_loss: 0.3105, step time: 0.4955\n",
      "51/388, train_loss: 0.1434, step time: 0.4858\n",
      "52/388, train_loss: 0.0924, step time: 0.4812\n",
      "53/388, train_loss: 0.1012, step time: 0.4902\n",
      "54/388, train_loss: 0.1172, step time: 0.9418\n",
      "55/388, train_loss: 0.1650, step time: 0.5497\n",
      "56/388, train_loss: 0.5171, step time: 0.5181\n",
      "57/388, train_loss: 0.1205, step time: 0.5008\n",
      "58/388, train_loss: 0.1114, step time: 0.4953\n",
      "59/388, train_loss: 0.3865, step time: 0.4897\n",
      "60/388, train_loss: 0.2390, step time: 0.5076\n",
      "61/388, train_loss: 0.1478, step time: 0.4978\n",
      "62/388, train_loss: 0.1794, step time: 0.4952\n",
      "63/388, train_loss: 0.1117, step time: 0.4795\n",
      "64/388, train_loss: 0.0683, step time: 0.7367\n",
      "65/388, train_loss: 0.5380, step time: 0.5461\n",
      "66/388, train_loss: 0.0997, step time: 0.5215\n",
      "67/388, train_loss: 0.1375, step time: 0.5099\n",
      "68/388, train_loss: 0.1712, step time: 0.4989\n",
      "69/388, train_loss: 0.2083, step time: 0.4981\n",
      "70/388, train_loss: 0.2658, step time: 0.4811\n",
      "71/388, train_loss: 0.0745, step time: 1.0713\n",
      "72/388, train_loss: 0.3727, step time: 0.5445\n",
      "73/388, train_loss: 0.3286, step time: 0.5226\n",
      "74/388, train_loss: 0.2095, step time: 0.5027\n",
      "75/388, train_loss: 0.0851, step time: 0.4986\n",
      "76/388, train_loss: 0.2891, step time: 0.4900\n",
      "77/388, train_loss: 0.0987, step time: 0.4922\n",
      "78/388, train_loss: 0.4225, step time: 0.4899\n",
      "79/388, train_loss: 0.3323, step time: 0.5127\n",
      "80/388, train_loss: 0.2401, step time: 0.4941\n",
      "81/388, train_loss: 0.1390, step time: 0.4982\n",
      "82/388, train_loss: 0.3163, step time: 0.5121\n",
      "83/388, train_loss: 0.2164, step time: 0.5984\n",
      "84/388, train_loss: 0.2953, step time: 0.5547\n",
      "85/388, train_loss: 0.1577, step time: 0.5223\n",
      "86/388, train_loss: 0.1287, step time: 0.4955\n",
      "87/388, train_loss: 0.5009, step time: 0.4956\n",
      "88/388, train_loss: 0.4292, step time: 0.4839\n",
      "89/388, train_loss: 0.1544, step time: 0.5078\n",
      "90/388, train_loss: 0.1405, step time: 0.4923\n",
      "91/388, train_loss: 0.1617, step time: 0.4955\n",
      "92/388, train_loss: 0.2074, step time: 0.5151\n",
      "93/388, train_loss: 0.1450, step time: 0.5192\n",
      "94/388, train_loss: 0.3202, step time: 0.5079\n",
      "95/388, train_loss: 0.3315, step time: 0.4907\n",
      "96/388, train_loss: 0.6371, step time: 0.5443\n",
      "97/388, train_loss: 0.3703, step time: 0.5566\n",
      "98/388, train_loss: 0.3064, step time: 0.5411\n",
      "99/388, train_loss: 0.2891, step time: 0.5256\n",
      "100/388, train_loss: 0.4436, step time: 0.5075\n",
      "101/388, train_loss: 0.1011, step time: 0.5006\n",
      "102/388, train_loss: 0.1867, step time: 0.4904\n",
      "103/388, train_loss: 0.1865, step time: 1.0737\n",
      "104/388, train_loss: 0.3521, step time: 0.5421\n",
      "105/388, train_loss: 0.0883, step time: 0.5185\n",
      "106/388, train_loss: 0.1066, step time: 0.4916\n",
      "107/388, train_loss: 0.2330, step time: 0.4973\n",
      "108/388, train_loss: 0.0885, step time: 0.4841\n",
      "109/388, train_loss: 0.0782, step time: 0.4752\n",
      "110/388, train_loss: 0.4520, step time: 0.5073\n",
      "111/388, train_loss: 0.1117, step time: 0.4859\n",
      "112/388, train_loss: 0.3821, step time: 0.5152\n",
      "113/388, train_loss: 0.1952, step time: 0.4930\n",
      "114/388, train_loss: 0.1086, step time: 0.4938\n",
      "115/388, train_loss: 0.1686, step time: 1.1384\n",
      "116/388, train_loss: 0.0871, step time: 0.5433\n",
      "117/388, train_loss: 0.2142, step time: 0.5192\n",
      "118/388, train_loss: 0.0701, step time: 0.5018\n",
      "119/388, train_loss: 0.7432, step time: 0.4916\n",
      "120/388, train_loss: 0.1750, step time: 0.5047\n",
      "121/388, train_loss: 0.3341, step time: 0.4950\n",
      "122/388, train_loss: 0.1604, step time: 1.1480\n",
      "123/388, train_loss: 0.1530, step time: 0.5375\n",
      "124/388, train_loss: 0.4332, step time: 0.5025\n",
      "125/388, train_loss: 0.0691, step time: 0.4937\n",
      "126/388, train_loss: 0.1083, step time: 0.4986\n",
      "127/388, train_loss: 0.1827, step time: 0.4856\n",
      "128/388, train_loss: 0.3278, step time: 0.4930\n",
      "129/388, train_loss: 0.1427, step time: 0.4791\n",
      "130/388, train_loss: 0.0759, step time: 0.9429\n",
      "131/388, train_loss: 0.2474, step time: 0.5449\n",
      "132/388, train_loss: 0.4559, step time: 0.5101\n",
      "133/388, train_loss: 0.4711, step time: 0.5041\n",
      "134/388, train_loss: 0.1117, step time: 0.4929\n",
      "135/388, train_loss: 0.3062, step time: 0.4932\n",
      "136/388, train_loss: 0.0869, step time: 0.4845\n",
      "137/388, train_loss: 0.0313, step time: 0.4766\n",
      "138/388, train_loss: 0.0780, step time: 0.4980\n",
      "139/388, train_loss: 0.1784, step time: 0.4789\n",
      "140/388, train_loss: 0.1886, step time: 1.0598\n",
      "141/388, train_loss: 0.0909, step time: 0.5267\n",
      "142/388, train_loss: 0.1161, step time: 0.5045\n",
      "143/388, train_loss: 0.1787, step time: 0.4905\n",
      "144/388, train_loss: 0.2501, step time: 0.4954\n",
      "145/388, train_loss: 0.3254, step time: 0.4804\n",
      "146/388, train_loss: 0.1094, step time: 1.1452\n",
      "147/388, train_loss: 0.1540, step time: 0.5257\n",
      "148/388, train_loss: 0.3283, step time: 0.4941\n",
      "149/388, train_loss: 0.0906, step time: 0.4920\n",
      "150/388, train_loss: 0.0751, step time: 0.4798\n",
      "151/388, train_loss: 0.0376, step time: 0.4889\n",
      "152/388, train_loss: 0.2182, step time: 0.5002\n",
      "153/388, train_loss: 0.2197, step time: 0.4890\n",
      "154/388, train_loss: 0.2057, step time: 1.0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/388, train_loss: 0.1057, step time: 0.5307\n",
      "156/388, train_loss: 0.1086, step time: 0.5180\n",
      "157/388, train_loss: 0.0945, step time: 0.4972\n",
      "158/388, train_loss: 0.1857, step time: 0.4999\n",
      "159/388, train_loss: 0.1831, step time: 0.4809\n",
      "160/388, train_loss: 0.1023, step time: 0.4868\n",
      "161/388, train_loss: 0.3794, step time: 0.4800\n",
      "162/388, train_loss: 0.0783, step time: 0.5098\n",
      "163/388, train_loss: 0.1793, step time: 0.4941\n",
      "164/388, train_loss: 0.0767, step time: 0.4913\n",
      "165/388, train_loss: 0.3282, step time: 1.1278\n",
      "166/388, train_loss: 0.1521, step time: 0.5362\n",
      "167/388, train_loss: 0.1239, step time: 0.5046\n",
      "168/388, train_loss: 0.0923, step time: 0.4950\n",
      "169/388, train_loss: 0.3264, step time: 0.4847\n",
      "170/388, train_loss: 0.1695, step time: 0.4830\n",
      "171/388, train_loss: 0.1812, step time: 0.4902\n",
      "172/388, train_loss: 0.0936, step time: 0.4711\n",
      "173/388, train_loss: 0.0801, step time: 0.4712\n",
      "174/388, train_loss: 0.0906, step time: 0.4792\n",
      "175/388, train_loss: 0.0714, step time: 0.5038\n",
      "176/388, train_loss: 0.1049, step time: 0.4848\n",
      "177/388, train_loss: 0.5812, step time: 0.4830\n",
      "178/388, train_loss: 0.6279, step time: 0.5140\n",
      "179/388, train_loss: 0.1727, step time: 0.4953\n",
      "180/388, train_loss: 0.2013, step time: 1.1712\n",
      "181/388, train_loss: 0.1677, step time: 0.5346\n",
      "182/388, train_loss: 0.1345, step time: 0.5104\n",
      "183/388, train_loss: 0.3326, step time: 0.4943\n",
      "184/388, train_loss: 0.0679, step time: 0.5174\n",
      "185/388, train_loss: 0.1941, step time: 0.5069\n",
      "186/388, train_loss: 0.5664, step time: 0.5015\n",
      "187/388, train_loss: 0.0894, step time: 0.4865\n",
      "188/388, train_loss: 0.2388, step time: 0.4942\n",
      "189/388, train_loss: 0.1162, step time: 0.4813\n",
      "190/388, train_loss: 0.1201, step time: 1.0605\n",
      "191/388, train_loss: 0.2366, step time: 0.5466\n",
      "192/388, train_loss: 0.1254, step time: 0.5050\n",
      "193/388, train_loss: 0.3590, step time: 0.4935\n",
      "194/388, train_loss: 0.0926, step time: 0.4923\n",
      "195/388, train_loss: 0.2684, step time: 0.5047\n",
      "196/388, train_loss: 0.3728, step time: 0.4993\n",
      "197/388, train_loss: 0.1882, step time: 0.4840\n",
      "198/388, train_loss: 0.3495, step time: 0.4936\n",
      "199/388, train_loss: 0.1675, step time: 0.4790\n",
      "200/388, train_loss: 0.2712, step time: 0.4770\n",
      "201/388, train_loss: 0.2746, step time: 0.6766\n",
      "202/388, train_loss: 0.1094, step time: 0.5511\n",
      "203/388, train_loss: 0.1437, step time: 0.5257\n",
      "204/388, train_loss: 0.2137, step time: 0.5048\n",
      "205/388, train_loss: 0.1790, step time: 0.4988\n",
      "206/388, train_loss: 0.1554, step time: 0.5016\n",
      "207/388, train_loss: 0.1541, step time: 0.5015\n",
      "208/388, train_loss: 0.1896, step time: 0.4854\n",
      "209/388, train_loss: 0.2804, step time: 1.1840\n",
      "210/388, train_loss: 0.5200, step time: 0.5499\n",
      "211/388, train_loss: 0.3337, step time: 0.5217\n",
      "212/388, train_loss: 0.2058, step time: 0.4946\n",
      "213/388, train_loss: 0.1634, step time: 0.5395\n",
      "214/388, train_loss: 0.2672, step time: 0.5145\n",
      "215/388, train_loss: 0.2988, step time: 0.5134\n",
      "216/388, train_loss: 0.0661, step time: 0.5014\n",
      "217/388, train_loss: 0.2884, step time: 0.5062\n",
      "218/388, train_loss: 0.1783, step time: 0.4863\n",
      "219/388, train_loss: 0.2815, step time: 0.5024\n",
      "220/388, train_loss: 0.4691, step time: 0.5477\n",
      "221/388, train_loss: 0.4900, step time: 0.5148\n",
      "222/388, train_loss: 0.1906, step time: 0.4919\n",
      "223/388, train_loss: 0.1146, step time: 1.0217\n",
      "224/388, train_loss: 0.1248, step time: 0.5405\n",
      "225/388, train_loss: 0.2967, step time: 0.5223\n",
      "226/388, train_loss: 0.1318, step time: 0.5083\n",
      "227/388, train_loss: 0.1138, step time: 0.4948\n",
      "228/388, train_loss: 0.1831, step time: 0.4938\n",
      "229/388, train_loss: 0.0595, step time: 0.4801\n",
      "230/388, train_loss: 0.1422, step time: 0.4906\n",
      "231/388, train_loss: 0.1662, step time: 0.4928\n",
      "232/388, train_loss: 0.2220, step time: 0.5010\n",
      "233/388, train_loss: 0.0938, step time: 0.5890\n",
      "234/388, train_loss: 0.1376, step time: 0.5623\n",
      "235/388, train_loss: 0.0834, step time: 0.5302\n",
      "236/388, train_loss: 0.0920, step time: 0.5118\n",
      "237/388, train_loss: 0.2487, step time: 0.4946\n",
      "238/388, train_loss: 0.1067, step time: 0.4882\n",
      "239/388, train_loss: 0.3322, step time: 0.4824\n",
      "240/388, train_loss: 0.2899, step time: 0.4973\n",
      "241/388, train_loss: 0.2419, step time: 0.4890\n",
      "242/388, train_loss: 0.2238, step time: 0.4931\n",
      "243/388, train_loss: 0.0691, step time: 0.4879\n",
      "244/388, train_loss: 0.1149, step time: 0.9126\n",
      "245/388, train_loss: 0.2489, step time: 0.5322\n",
      "246/388, train_loss: 0.1990, step time: 0.5053\n",
      "247/388, train_loss: 0.1065, step time: 0.4979\n",
      "248/388, train_loss: 0.1338, step time: 0.4916\n",
      "249/388, train_loss: 0.4072, step time: 0.5305\n",
      "250/388, train_loss: 0.3077, step time: 0.6160\n",
      "251/388, train_loss: 0.1649, step time: 0.5391\n",
      "252/388, train_loss: 0.2607, step time: 0.5191\n",
      "253/388, train_loss: 0.1569, step time: 0.4943\n",
      "254/388, train_loss: 0.2455, step time: 0.4942\n",
      "255/388, train_loss: 0.3124, step time: 0.4838\n",
      "256/388, train_loss: 0.3077, step time: 0.5905\n",
      "257/388, train_loss: 0.1922, step time: 0.5280\n",
      "258/388, train_loss: 0.2365, step time: 0.5089\n",
      "259/388, train_loss: 0.0753, step time: 0.4982\n",
      "260/388, train_loss: 0.0928, step time: 0.6314\n",
      "261/388, train_loss: 0.3461, step time: 0.5520\n",
      "262/388, train_loss: 0.0856, step time: 0.5152\n",
      "263/388, train_loss: 0.2730, step time: 0.5064\n",
      "264/388, train_loss: 0.3190, step time: 0.4891\n",
      "265/388, train_loss: 0.5589, step time: 0.5020\n",
      "266/388, train_loss: 0.1709, step time: 0.5102\n",
      "267/388, train_loss: 0.1379, step time: 0.5591\n",
      "268/388, train_loss: 0.3066, step time: 0.6178\n",
      "269/388, train_loss: 0.1596, step time: 0.5695\n",
      "270/388, train_loss: 0.1952, step time: 0.5462\n",
      "271/388, train_loss: 0.4405, step time: 0.5012\n",
      "272/388, train_loss: 0.1460, step time: 0.4996\n",
      "273/388, train_loss: 0.5008, step time: 0.5109\n",
      "274/388, train_loss: 0.3054, step time: 0.5127\n",
      "275/388, train_loss: 0.1474, step time: 0.4962\n",
      "276/388, train_loss: 0.3142, step time: 0.4983\n",
      "277/388, train_loss: 0.0984, step time: 1.1231\n",
      "278/388, train_loss: 0.5017, step time: 0.5464\n",
      "279/388, train_loss: 0.2223, step time: 0.5147\n",
      "280/388, train_loss: 0.1075, step time: 0.4936\n",
      "281/388, train_loss: 0.3274, step time: 0.4987\n",
      "282/388, train_loss: 0.5250, step time: 0.5394\n",
      "283/388, train_loss: 0.2350, step time: 0.5248\n",
      "284/388, train_loss: 0.1914, step time: 0.5692\n",
      "285/388, train_loss: 0.2243, step time: 0.5358\n",
      "286/388, train_loss: 0.3344, step time: 0.5043\n",
      "287/388, train_loss: 0.2026, step time: 0.5334\n",
      "288/388, train_loss: 0.1253, step time: 0.5262\n",
      "289/388, train_loss: 0.1244, step time: 0.5066\n",
      "290/388, train_loss: 0.1542, step time: 0.4989\n",
      "291/388, train_loss: 0.2113, step time: 0.5003\n",
      "292/388, train_loss: 0.3098, step time: 0.5142\n",
      "293/388, train_loss: 0.0982, step time: 0.5246\n",
      "294/388, train_loss: 0.2142, step time: 0.5545\n",
      "295/388, train_loss: 0.2131, step time: 0.5217\n",
      "296/388, train_loss: 0.1842, step time: 0.5091\n",
      "297/388, train_loss: 0.1308, step time: 1.1227\n",
      "298/388, train_loss: 0.1642, step time: 0.5358\n",
      "299/388, train_loss: 0.1912, step time: 0.5079\n",
      "300/388, train_loss: 0.2388, step time: 0.5021\n",
      "301/388, train_loss: 0.2479, step time: 0.5011\n",
      "302/388, train_loss: 0.0847, step time: 0.7057\n",
      "303/388, train_loss: 0.0828, step time: 0.5445\n",
      "304/388, train_loss: 0.2552, step time: 0.5120\n",
      "305/388, train_loss: 0.0893, step time: 0.5059\n",
      "306/388, train_loss: 0.3897, step time: 0.4925\n",
      "307/388, train_loss: 0.1146, step time: 0.4945\n",
      "308/388, train_loss: 0.1114, step time: 0.4770\n",
      "309/388, train_loss: 0.0562, step time: 0.4830\n",
      "310/388, train_loss: 0.1659, step time: 0.4881\n",
      "311/388, train_loss: 0.3379, step time: 1.1767\n",
      "312/388, train_loss: 0.4115, step time: 0.5467\n",
      "313/388, train_loss: 0.5710, step time: 0.5098\n",
      "314/388, train_loss: 0.2392, step time: 0.5018\n",
      "315/388, train_loss: 0.4651, step time: 0.4830\n",
      "316/388, train_loss: 0.1567, step time: 0.7606\n",
      "317/388, train_loss: 0.5073, step time: 0.5698\n",
      "318/388, train_loss: 0.1606, step time: 0.5354\n",
      "319/388, train_loss: 0.0570, step time: 0.5110\n",
      "320/388, train_loss: 0.1938, step time: 0.4900\n",
      "321/388, train_loss: 0.1835, step time: 0.4848\n",
      "322/388, train_loss: 0.2432, step time: 0.4919\n",
      "323/388, train_loss: 0.1934, step time: 0.5018\n",
      "324/388, train_loss: 0.1043, step time: 0.4985\n",
      "325/388, train_loss: 0.2808, step time: 0.4987\n",
      "326/388, train_loss: 0.1067, step time: 0.5041\n",
      "327/388, train_loss: 0.1016, step time: 0.4932\n",
      "328/388, train_loss: 0.1073, step time: 0.4904\n",
      "329/388, train_loss: 0.0666, step time: 0.4924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/388, train_loss: 0.1502, step time: 0.4921\n",
      "331/388, train_loss: 0.2206, step time: 0.5138\n",
      "332/388, train_loss: 0.1497, step time: 0.4948\n",
      "333/388, train_loss: 0.4618, step time: 0.4991\n",
      "334/388, train_loss: 0.1457, step time: 1.1172\n",
      "335/388, train_loss: 0.2460, step time: 0.5569\n",
      "336/388, train_loss: 0.1976, step time: 0.5318\n",
      "337/388, train_loss: 0.1565, step time: 0.5190\n",
      "338/388, train_loss: 0.1931, step time: 0.4943\n",
      "339/388, train_loss: 0.0744, step time: 0.4935\n",
      "340/388, train_loss: 0.1545, step time: 0.4757\n",
      "341/388, train_loss: 0.1183, step time: 0.4907\n",
      "342/388, train_loss: 0.2400, step time: 0.4949\n",
      "343/388, train_loss: 0.2254, step time: 1.1522\n",
      "344/388, train_loss: 0.2830, step time: 0.5466\n",
      "345/388, train_loss: 0.0645, step time: 0.5170\n",
      "346/388, train_loss: 0.0745, step time: 0.5073\n",
      "347/388, train_loss: 0.3418, step time: 0.4960\n",
      "348/388, train_loss: 0.2428, step time: 0.4990\n",
      "349/388, train_loss: 0.0951, step time: 1.1532\n",
      "350/388, train_loss: 0.2437, step time: 0.5476\n",
      "351/388, train_loss: 0.2195, step time: 0.5143\n",
      "352/388, train_loss: 0.2739, step time: 0.5015\n",
      "353/388, train_loss: 0.1848, step time: 0.4985\n",
      "354/388, train_loss: 0.1507, step time: 0.4991\n",
      "355/388, train_loss: 0.0567, step time: 0.4767\n",
      "356/388, train_loss: 0.2084, step time: 0.4759\n",
      "357/388, train_loss: 0.2352, step time: 0.4767\n",
      "358/388, train_loss: 0.1946, step time: 0.6525\n",
      "359/388, train_loss: 0.0357, step time: 0.5433\n",
      "360/388, train_loss: 0.1064, step time: 0.5026\n",
      "361/388, train_loss: 0.1173, step time: 0.5034\n",
      "362/388, train_loss: 0.3229, step time: 0.4929\n",
      "363/388, train_loss: 0.1111, step time: 0.4959\n",
      "364/388, train_loss: 0.1447, step time: 0.5410\n",
      "365/388, train_loss: 0.1711, step time: 0.5370\n",
      "366/388, train_loss: 0.1450, step time: 0.4974\n",
      "367/388, train_loss: 0.1886, step time: 0.5364\n",
      "368/388, train_loss: 0.4622, step time: 0.5301\n",
      "369/388, train_loss: 0.2865, step time: 0.4961\n",
      "370/388, train_loss: 0.2224, step time: 0.5337\n",
      "371/388, train_loss: 0.2074, step time: 0.5170\n",
      "372/388, train_loss: 0.0671, step time: 0.5166\n",
      "373/388, train_loss: 0.1266, step time: 0.4960\n",
      "374/388, train_loss: 0.1855, step time: 0.4971\n",
      "375/388, train_loss: 0.0936, step time: 0.4880\n",
      "376/388, train_loss: 0.2277, step time: 0.5098\n",
      "377/388, train_loss: 0.3948, step time: 0.5079\n",
      "378/388, train_loss: 0.3159, step time: 0.5805\n",
      "379/388, train_loss: 0.1349, step time: 0.5314\n",
      "380/388, train_loss: 0.1507, step time: 0.5022\n",
      "381/388, train_loss: 0.1205, step time: 0.5069\n",
      "382/388, train_loss: 0.1708, step time: 0.5950\n",
      "383/388, train_loss: 0.2072, step time: 0.5354\n",
      "384/388, train_loss: 0.2015, step time: 0.5003\n",
      "385/388, train_loss: 0.0517, step time: 0.4882\n",
      "386/388, train_loss: 0.1528, step time: 0.5038\n",
      "387/388, train_loss: 0.2071, step time: 0.6334\n",
      "388/388, train_loss: 0.1119, step time: 0.5318\n",
      "epoch 36 average loss: 0.2096\n",
      "saved new best metric model\n",
      "current epoch: 36 current mean dice: 0.7557 tc: 0.8040 wt: 0.8906 et: 0.5724\n",
      "best mean dice: 0.7557 at epoch: 36\n",
      "time consuming of epoch 36 is: 302.4699\n",
      "----------\n",
      "epoch 37/300\n",
      "1/388, train_loss: 0.0881, step time: 0.4799\n",
      "2/388, train_loss: 0.1685, step time: 0.4822\n",
      "3/388, train_loss: 0.0540, step time: 0.7767\n",
      "4/388, train_loss: 0.1014, step time: 0.5859\n",
      "5/388, train_loss: 0.2356, step time: 0.5273\n",
      "6/388, train_loss: 0.1523, step time: 0.5052\n",
      "7/388, train_loss: 0.2256, step time: 0.4899\n",
      "8/388, train_loss: 0.1388, step time: 0.6424\n",
      "9/388, train_loss: 0.1576, step time: 0.5672\n",
      "10/388, train_loss: 0.5258, step time: 0.5486\n",
      "11/388, train_loss: 0.1007, step time: 0.5378\n",
      "12/388, train_loss: 0.1751, step time: 0.6405\n",
      "13/388, train_loss: 0.1790, step time: 0.5612\n",
      "14/388, train_loss: 0.2687, step time: 0.5219\n",
      "15/388, train_loss: 0.2822, step time: 0.5346\n",
      "16/388, train_loss: 0.2019, step time: 0.5579\n",
      "17/388, train_loss: 0.0473, step time: 0.5373\n",
      "18/388, train_loss: 0.1505, step time: 0.5158\n",
      "19/388, train_loss: 0.3248, step time: 0.5040\n",
      "20/388, train_loss: 0.2773, step time: 1.2453\n",
      "21/388, train_loss: 0.1203, step time: 0.5420\n",
      "22/388, train_loss: 0.1901, step time: 0.5167\n",
      "23/388, train_loss: 0.1528, step time: 0.5052\n",
      "24/388, train_loss: 0.1040, step time: 0.4942\n",
      "25/388, train_loss: 0.1357, step time: 0.5221\n",
      "26/388, train_loss: 0.1486, step time: 0.5192\n",
      "27/388, train_loss: 0.2013, step time: 0.5938\n",
      "28/388, train_loss: 0.0620, step time: 0.5830\n",
      "29/388, train_loss: 0.2074, step time: 0.5364\n",
      "30/388, train_loss: 0.1501, step time: 0.5167\n",
      "31/388, train_loss: 0.1170, step time: 0.4897\n",
      "32/388, train_loss: 0.1653, step time: 0.5051\n",
      "33/388, train_loss: 0.1002, step time: 0.8163\n",
      "34/388, train_loss: 0.2438, step time: 0.5505\n",
      "35/388, train_loss: 0.2872, step time: 0.5247\n",
      "36/388, train_loss: 0.2009, step time: 0.5841\n",
      "37/388, train_loss: 0.2310, step time: 0.5588\n",
      "38/388, train_loss: 0.0721, step time: 0.5192\n",
      "39/388, train_loss: 0.0883, step time: 0.4942\n",
      "40/388, train_loss: 0.5833, step time: 0.4923\n",
      "41/388, train_loss: 0.0615, step time: 0.5868\n",
      "42/388, train_loss: 0.2231, step time: 0.5492\n",
      "43/388, train_loss: 0.3629, step time: 0.5183\n",
      "44/388, train_loss: 0.0840, step time: 0.4966\n",
      "45/388, train_loss: 0.2026, step time: 0.4986\n",
      "46/388, train_loss: 0.1010, step time: 0.4836\n",
      "47/388, train_loss: 0.2649, step time: 0.5228\n",
      "48/388, train_loss: 0.6131, step time: 0.5031\n",
      "49/388, train_loss: 0.5829, step time: 0.5291\n",
      "50/388, train_loss: 0.3416, step time: 0.6836\n",
      "51/388, train_loss: 0.3456, step time: 0.5581\n",
      "52/388, train_loss: 0.1502, step time: 0.5285\n",
      "53/388, train_loss: 0.1004, step time: 0.5062\n",
      "54/388, train_loss: 0.2997, step time: 0.4898\n",
      "55/388, train_loss: 0.5755, step time: 1.1261\n",
      "56/388, train_loss: 0.1398, step time: 0.5349\n",
      "57/388, train_loss: 0.0708, step time: 0.5049\n",
      "58/388, train_loss: 0.3895, step time: 0.4954\n",
      "59/388, train_loss: 0.1591, step time: 0.4837\n",
      "60/388, train_loss: 0.2795, step time: 0.4755\n",
      "61/388, train_loss: 0.2075, step time: 0.4994\n",
      "62/388, train_loss: 0.1980, step time: 0.5150\n",
      "63/388, train_loss: 0.1090, step time: 0.5002\n",
      "64/388, train_loss: 0.1014, step time: 0.5106\n",
      "65/388, train_loss: 0.0900, step time: 0.5184\n",
      "66/388, train_loss: 0.0466, step time: 0.4945\n",
      "67/388, train_loss: 0.1299, step time: 0.4971\n",
      "68/388, train_loss: 0.0976, step time: 0.4975\n",
      "69/388, train_loss: 0.1121, step time: 0.5170\n",
      "70/388, train_loss: 0.0930, step time: 0.4908\n",
      "71/388, train_loss: 0.0809, step time: 0.4865\n",
      "72/388, train_loss: 0.1696, step time: 0.4966\n",
      "73/388, train_loss: 0.0831, step time: 0.4803\n",
      "74/388, train_loss: 0.1397, step time: 0.4872\n",
      "75/388, train_loss: 0.1569, step time: 0.9243\n",
      "76/388, train_loss: 0.1261, step time: 0.5723\n",
      "77/388, train_loss: 0.2215, step time: 0.5219\n",
      "78/388, train_loss: 0.2627, step time: 0.5030\n",
      "79/388, train_loss: 0.1852, step time: 0.4863\n",
      "80/388, train_loss: 0.1373, step time: 0.6976\n",
      "81/388, train_loss: 0.4009, step time: 0.5477\n",
      "82/388, train_loss: 0.4168, step time: 0.5220\n",
      "83/388, train_loss: 0.0379, step time: 0.4999\n",
      "84/388, train_loss: 0.3095, step time: 0.4869\n",
      "85/388, train_loss: 0.1162, step time: 0.4841\n",
      "86/388, train_loss: 0.0930, step time: 0.4921\n",
      "87/388, train_loss: 0.2689, step time: 0.5314\n",
      "88/388, train_loss: 0.0954, step time: 0.5129\n",
      "89/388, train_loss: 0.2290, step time: 0.5127\n",
      "90/388, train_loss: 0.1161, step time: 0.4903\n",
      "91/388, train_loss: 0.1208, step time: 0.4903\n",
      "92/388, train_loss: 0.2309, step time: 0.4929\n",
      "93/388, train_loss: 0.2939, step time: 0.5153\n",
      "94/388, train_loss: 0.2062, step time: 0.4945\n",
      "95/388, train_loss: 0.1813, step time: 0.4921\n",
      "96/388, train_loss: 0.3260, step time: 0.4831\n",
      "97/388, train_loss: 0.0900, step time: 0.4772\n",
      "98/388, train_loss: 0.0943, step time: 0.4862\n",
      "99/388, train_loss: 0.1154, step time: 0.4763\n",
      "100/388, train_loss: 0.2414, step time: 0.4753\n",
      "101/388, train_loss: 0.2341, step time: 0.4807\n",
      "102/388, train_loss: 0.1019, step time: 0.8701\n",
      "103/388, train_loss: 0.2132, step time: 0.5669\n",
      "104/388, train_loss: 0.0579, step time: 0.5179\n",
      "105/388, train_loss: 0.3525, step time: 0.5123\n",
      "106/388, train_loss: 0.2983, step time: 0.4978\n",
      "107/388, train_loss: 0.0949, step time: 0.4911\n",
      "108/388, train_loss: 0.3680, step time: 0.4834\n",
      "109/388, train_loss: 0.4712, step time: 0.5100\n",
      "110/388, train_loss: 0.1155, step time: 0.5008\n",
      "111/388, train_loss: 0.1847, step time: 0.4826\n",
      "112/388, train_loss: 0.2034, step time: 0.4931\n",
      "113/388, train_loss: 0.1230, step time: 0.4925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/388, train_loss: 0.0754, step time: 0.4795\n",
      "115/388, train_loss: 0.1875, step time: 0.4818\n",
      "116/388, train_loss: 0.2744, step time: 0.9272\n",
      "117/388, train_loss: 0.1151, step time: 0.5505\n",
      "118/388, train_loss: 0.3891, step time: 0.5302\n",
      "119/388, train_loss: 0.1413, step time: 0.5030\n",
      "120/388, train_loss: 0.2410, step time: 0.4963\n",
      "121/388, train_loss: 0.1236, step time: 0.4849\n",
      "122/388, train_loss: 0.2001, step time: 0.9922\n",
      "123/388, train_loss: 0.3765, step time: 0.5719\n",
      "124/388, train_loss: 0.1964, step time: 0.5110\n",
      "125/388, train_loss: 0.2107, step time: 0.4997\n",
      "126/388, train_loss: 0.3861, step time: 0.5329\n",
      "127/388, train_loss: 0.0725, step time: 0.5240\n",
      "128/388, train_loss: 0.1696, step time: 0.5013\n",
      "129/388, train_loss: 0.1544, step time: 0.5011\n",
      "130/388, train_loss: 0.5795, step time: 0.4831\n",
      "131/388, train_loss: 0.1397, step time: 0.4977\n",
      "132/388, train_loss: 0.1053, step time: 0.4980\n",
      "133/388, train_loss: 0.1751, step time: 0.4928\n",
      "134/388, train_loss: 0.2976, step time: 0.4981\n",
      "135/388, train_loss: 0.2155, step time: 0.4844\n",
      "136/388, train_loss: 0.2453, step time: 0.5056\n",
      "137/388, train_loss: 0.1704, step time: 0.5017\n",
      "138/388, train_loss: 0.0354, step time: 0.4823\n",
      "139/388, train_loss: 0.3212, step time: 0.4913\n",
      "140/388, train_loss: 0.4610, step time: 1.0748\n",
      "141/388, train_loss: 0.4334, step time: 0.5444\n",
      "142/388, train_loss: 0.2227, step time: 0.5213\n",
      "143/388, train_loss: 0.2432, step time: 0.4975\n",
      "144/388, train_loss: 0.1724, step time: 0.4972\n",
      "145/388, train_loss: 0.1952, step time: 0.4991\n",
      "146/388, train_loss: 0.1337, step time: 0.5674\n",
      "147/388, train_loss: 0.3038, step time: 0.5241\n",
      "148/388, train_loss: 0.1931, step time: 0.5054\n",
      "149/388, train_loss: 0.2236, step time: 0.5505\n",
      "150/388, train_loss: 0.2666, step time: 0.5254\n",
      "151/388, train_loss: 0.0685, step time: 0.5083\n",
      "152/388, train_loss: 0.2312, step time: 0.4955\n",
      "153/388, train_loss: 0.0520, step time: 0.4916\n",
      "154/388, train_loss: 0.1507, step time: 0.4965\n",
      "155/388, train_loss: 0.1344, step time: 0.5047\n",
      "156/388, train_loss: 0.2136, step time: 0.4904\n",
      "157/388, train_loss: 0.2041, step time: 0.4982\n",
      "158/388, train_loss: 0.4233, step time: 0.5085\n",
      "159/388, train_loss: 0.2902, step time: 0.5366\n",
      "160/388, train_loss: 0.2325, step time: 0.5180\n",
      "161/388, train_loss: 0.1056, step time: 0.5006\n",
      "162/388, train_loss: 0.0946, step time: 0.4965\n",
      "163/388, train_loss: 0.1239, step time: 0.4837\n",
      "164/388, train_loss: 0.1617, step time: 0.4755\n",
      "165/388, train_loss: 0.0641, step time: 0.4780\n",
      "166/388, train_loss: 0.3370, step time: 0.4889\n",
      "167/388, train_loss: 0.4723, step time: 0.4746\n",
      "168/388, train_loss: 0.1855, step time: 0.5904\n",
      "169/388, train_loss: 0.2760, step time: 0.5575\n",
      "170/388, train_loss: 0.1532, step time: 0.5322\n",
      "171/388, train_loss: 0.1435, step time: 0.5161\n",
      "172/388, train_loss: 0.1975, step time: 0.5597\n",
      "173/388, train_loss: 0.2855, step time: 0.5263\n",
      "174/388, train_loss: 0.2710, step time: 0.5083\n",
      "175/388, train_loss: 0.2441, step time: 0.4893\n",
      "176/388, train_loss: 0.1010, step time: 0.4859\n",
      "177/388, train_loss: 0.1236, step time: 0.7296\n",
      "178/388, train_loss: 0.1440, step time: 0.5422\n",
      "179/388, train_loss: 0.2190, step time: 0.5221\n",
      "180/388, train_loss: 0.0470, step time: 0.5035\n",
      "181/388, train_loss: 0.2752, step time: 0.4967\n",
      "182/388, train_loss: 0.1497, step time: 0.4858\n",
      "183/388, train_loss: 0.1602, step time: 0.5117\n",
      "184/388, train_loss: 0.1195, step time: 0.4933\n",
      "185/388, train_loss: 0.1824, step time: 0.4978\n",
      "186/388, train_loss: 0.3813, step time: 1.0056\n",
      "187/388, train_loss: 0.0895, step time: 0.5336\n",
      "188/388, train_loss: 0.3946, step time: 0.5081\n",
      "189/388, train_loss: 0.0820, step time: 0.4958\n",
      "190/388, train_loss: 0.2284, step time: 0.4822\n",
      "191/388, train_loss: 0.4629, step time: 0.4825\n",
      "192/388, train_loss: 0.1473, step time: 0.5087\n",
      "193/388, train_loss: 0.3121, step time: 0.4884\n",
      "194/388, train_loss: 0.1639, step time: 0.5075\n",
      "195/388, train_loss: 0.2693, step time: 0.4993\n",
      "196/388, train_loss: 0.0675, step time: 0.6118\n",
      "197/388, train_loss: 0.3525, step time: 0.5371\n",
      "198/388, train_loss: 0.1248, step time: 0.5174\n",
      "199/388, train_loss: 0.1105, step time: 0.5031\n",
      "200/388, train_loss: 0.2714, step time: 0.4994\n",
      "201/388, train_loss: 0.5028, step time: 0.4821\n",
      "202/388, train_loss: 0.3080, step time: 0.4895\n",
      "203/388, train_loss: 0.2290, step time: 0.4750\n",
      "204/388, train_loss: 0.1737, step time: 0.4960\n",
      "205/388, train_loss: 0.1268, step time: 0.5162\n",
      "206/388, train_loss: 0.4787, step time: 0.5009\n",
      "207/388, train_loss: 0.1520, step time: 0.4774\n",
      "208/388, train_loss: 0.2104, step time: 0.4961\n",
      "209/388, train_loss: 0.4473, step time: 0.4916\n",
      "210/388, train_loss: 0.3922, step time: 0.8784\n",
      "211/388, train_loss: 0.0907, step time: 0.5420\n",
      "212/388, train_loss: 0.1339, step time: 0.5150\n",
      "213/388, train_loss: 0.1995, step time: 0.4983\n",
      "214/388, train_loss: 0.0504, step time: 0.4962\n",
      "215/388, train_loss: 0.1140, step time: 0.4838\n",
      "216/388, train_loss: 0.1117, step time: 0.5046\n",
      "217/388, train_loss: 0.6111, step time: 0.4896\n",
      "218/388, train_loss: 0.2905, step time: 0.5202\n",
      "219/388, train_loss: 0.1419, step time: 0.5661\n",
      "220/388, train_loss: 0.2956, step time: 0.5394\n",
      "221/388, train_loss: 0.4042, step time: 0.5180\n",
      "222/388, train_loss: 0.1950, step time: 0.4993\n",
      "223/388, train_loss: 0.1070, step time: 0.5254\n",
      "224/388, train_loss: 0.2997, step time: 0.5014\n",
      "225/388, train_loss: 0.1070, step time: 0.4911\n",
      "226/388, train_loss: 0.0792, step time: 0.4796\n",
      "227/388, train_loss: 0.1188, step time: 0.4899\n",
      "228/388, train_loss: 0.2106, step time: 0.4865\n",
      "229/388, train_loss: 0.2233, step time: 0.4868\n",
      "230/388, train_loss: 0.2378, step time: 1.1093\n",
      "231/388, train_loss: 0.1064, step time: 0.5340\n",
      "232/388, train_loss: 0.2792, step time: 0.5058\n",
      "233/388, train_loss: 0.2690, step time: 0.4967\n",
      "234/388, train_loss: 0.1540, step time: 0.4912\n",
      "235/388, train_loss: 0.1832, step time: 0.4824\n",
      "236/388, train_loss: 0.2492, step time: 0.4869\n",
      "237/388, train_loss: 0.2728, step time: 0.4829\n",
      "238/388, train_loss: 0.1720, step time: 0.4890\n",
      "239/388, train_loss: 0.0768, step time: 0.8402\n",
      "240/388, train_loss: 0.1232, step time: 0.5613\n",
      "241/388, train_loss: 0.6358, step time: 0.5180\n",
      "242/388, train_loss: 0.0973, step time: 0.4964\n",
      "243/388, train_loss: 0.0754, step time: 0.4884\n",
      "244/388, train_loss: 0.2521, step time: 0.4967\n",
      "245/388, train_loss: 0.5025, step time: 0.4862\n",
      "246/388, train_loss: 0.1276, step time: 0.9776\n",
      "247/388, train_loss: 0.3946, step time: 0.5372\n",
      "248/388, train_loss: 0.4362, step time: 0.4985\n",
      "249/388, train_loss: 0.1881, step time: 0.4957\n",
      "250/388, train_loss: 0.2045, step time: 0.4833\n",
      "251/388, train_loss: 0.1142, step time: 0.4900\n",
      "252/388, train_loss: 0.1859, step time: 0.5279\n",
      "253/388, train_loss: 0.2052, step time: 0.5176\n",
      "254/388, train_loss: 0.2011, step time: 0.4939\n",
      "255/388, train_loss: 0.2193, step time: 0.5661\n",
      "256/388, train_loss: 0.1076, step time: 0.5389\n",
      "257/388, train_loss: 0.2488, step time: 0.5235\n",
      "258/388, train_loss: 0.1067, step time: 0.4997\n",
      "259/388, train_loss: 0.1564, step time: 0.4781\n",
      "260/388, train_loss: 0.2326, step time: 0.4808\n",
      "261/388, train_loss: 0.2186, step time: 0.6607\n",
      "262/388, train_loss: 0.1458, step time: 0.5427\n",
      "263/388, train_loss: 0.1102, step time: 0.5148\n",
      "264/388, train_loss: 0.1987, step time: 0.5031\n",
      "265/388, train_loss: 0.0832, step time: 0.4863\n",
      "266/388, train_loss: 0.1397, step time: 0.4966\n",
      "267/388, train_loss: 0.0771, step time: 0.5028\n",
      "268/388, train_loss: 0.2778, step time: 0.4831\n",
      "269/388, train_loss: 0.1117, step time: 0.4840\n",
      "270/388, train_loss: 0.4726, step time: 0.4941\n",
      "271/388, train_loss: 0.2474, step time: 0.4976\n",
      "272/388, train_loss: 0.1082, step time: 0.4817\n",
      "273/388, train_loss: 0.1216, step time: 0.5087\n",
      "274/388, train_loss: 0.1630, step time: 0.4978\n",
      "275/388, train_loss: 0.1413, step time: 0.4868\n",
      "276/388, train_loss: 0.1634, step time: 0.5125\n",
      "277/388, train_loss: 0.1368, step time: 0.5100\n",
      "278/388, train_loss: 0.0867, step time: 0.4990\n",
      "279/388, train_loss: 0.0660, step time: 0.5118\n",
      "280/388, train_loss: 0.0576, step time: 0.5090\n",
      "281/388, train_loss: 0.1308, step time: 0.4959\n",
      "282/388, train_loss: 0.1654, step time: 0.4967\n",
      "283/388, train_loss: 0.1369, step time: 0.4933\n",
      "284/388, train_loss: 0.1449, step time: 0.5352\n",
      "285/388, train_loss: 0.2455, step time: 0.5227\n",
      "286/388, train_loss: 0.6421, step time: 0.5068\n",
      "287/388, train_loss: 0.0715, step time: 1.2261\n",
      "288/388, train_loss: 0.2254, step time: 0.5412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/388, train_loss: 0.1953, step time: 0.5057\n",
      "290/388, train_loss: 0.3102, step time: 0.4942\n",
      "291/388, train_loss: 0.2483, step time: 0.4824\n",
      "292/388, train_loss: 0.1052, step time: 0.4968\n",
      "293/388, train_loss: 0.1523, step time: 0.4810\n",
      "294/388, train_loss: 0.1605, step time: 0.4963\n",
      "295/388, train_loss: 0.3215, step time: 0.4901\n",
      "296/388, train_loss: 0.1441, step time: 0.4851\n",
      "297/388, train_loss: 0.1429, step time: 0.5107\n",
      "298/388, train_loss: 0.2974, step time: 0.5629\n",
      "299/388, train_loss: 0.1776, step time: 0.5213\n",
      "300/388, train_loss: 0.1722, step time: 0.4868\n",
      "301/388, train_loss: 0.1624, step time: 0.4980\n",
      "302/388, train_loss: 0.2630, step time: 0.5192\n",
      "303/388, train_loss: 0.1161, step time: 0.5101\n",
      "304/388, train_loss: 0.1537, step time: 0.5040\n",
      "305/388, train_loss: 0.1856, step time: 0.4999\n",
      "306/388, train_loss: 0.1585, step time: 0.4852\n",
      "307/388, train_loss: 0.0906, step time: 0.5247\n",
      "308/388, train_loss: 0.2848, step time: 0.5039\n",
      "309/388, train_loss: 0.2261, step time: 0.5023\n",
      "310/388, train_loss: 0.3026, step time: 0.4822\n",
      "311/388, train_loss: 0.1887, step time: 0.5660\n",
      "312/388, train_loss: 0.1064, step time: 0.5496\n",
      "313/388, train_loss: 0.2259, step time: 0.5257\n",
      "314/388, train_loss: 0.1991, step time: 0.5082\n",
      "315/388, train_loss: 0.1784, step time: 0.4940\n",
      "316/388, train_loss: 0.0821, step time: 0.6808\n",
      "317/388, train_loss: 0.4617, step time: 0.5580\n",
      "318/388, train_loss: 0.2764, step time: 0.5226\n",
      "319/388, train_loss: 0.2457, step time: 0.5164\n",
      "320/388, train_loss: 0.1115, step time: 0.5169\n",
      "321/388, train_loss: 0.1879, step time: 0.5099\n",
      "322/388, train_loss: 0.1073, step time: 0.4922\n",
      "323/388, train_loss: 0.1212, step time: 0.4921\n",
      "324/388, train_loss: 0.0633, step time: 0.5017\n",
      "325/388, train_loss: 0.2223, step time: 0.5008\n",
      "326/388, train_loss: 0.2772, step time: 0.5465\n",
      "327/388, train_loss: 0.5831, step time: 0.5396\n",
      "328/388, train_loss: 0.0285, step time: 0.5054\n",
      "329/388, train_loss: 0.5435, step time: 0.4882\n",
      "330/388, train_loss: 0.1189, step time: 1.1240\n",
      "331/388, train_loss: 0.3688, step time: 0.5390\n",
      "332/388, train_loss: 0.1646, step time: 0.5199\n",
      "333/388, train_loss: 0.1741, step time: 0.4918\n",
      "334/388, train_loss: 0.0940, step time: 0.4923\n",
      "335/388, train_loss: 0.0673, step time: 0.4886\n",
      "336/388, train_loss: 0.1172, step time: 0.4842\n",
      "337/388, train_loss: 0.3734, step time: 0.5223\n",
      "338/388, train_loss: 0.1299, step time: 0.5135\n",
      "339/388, train_loss: 0.2822, step time: 0.4891\n",
      "340/388, train_loss: 0.1967, step time: 0.4954\n",
      "341/388, train_loss: 0.2178, step time: 0.4807\n",
      "342/388, train_loss: 0.0794, step time: 0.5072\n",
      "343/388, train_loss: 0.2432, step time: 0.4910\n",
      "344/388, train_loss: 0.0939, step time: 0.5060\n",
      "345/388, train_loss: 0.0948, step time: 0.5059\n",
      "346/388, train_loss: 0.4424, step time: 0.5382\n",
      "347/388, train_loss: 0.1324, step time: 0.5332\n",
      "348/388, train_loss: 0.1782, step time: 0.5671\n",
      "349/388, train_loss: 0.1441, step time: 0.5392\n",
      "350/388, train_loss: 0.0956, step time: 0.5176\n",
      "351/388, train_loss: 0.1238, step time: 0.4957\n",
      "352/388, train_loss: 0.0928, step time: 0.5075\n",
      "353/388, train_loss: 0.2004, step time: 0.4984\n",
      "354/388, train_loss: 0.1581, step time: 0.4903\n",
      "355/388, train_loss: 0.2294, step time: 0.4849\n",
      "356/388, train_loss: 0.2285, step time: 1.0379\n",
      "357/388, train_loss: 0.1814, step time: 0.5158\n",
      "358/388, train_loss: 0.0942, step time: 0.5052\n",
      "359/388, train_loss: 0.2759, step time: 0.4873\n",
      "360/388, train_loss: 0.2285, step time: 0.4929\n",
      "361/388, train_loss: 0.1479, step time: 1.1192\n",
      "362/388, train_loss: 0.2200, step time: 0.5249\n",
      "363/388, train_loss: 0.1840, step time: 0.5102\n",
      "364/388, train_loss: 0.1220, step time: 0.4846\n",
      "365/388, train_loss: 0.1347, step time: 0.4879\n",
      "366/388, train_loss: 0.2319, step time: 0.4920\n",
      "367/388, train_loss: 0.4407, step time: 0.4875\n",
      "368/388, train_loss: 0.1244, step time: 0.4986\n",
      "369/388, train_loss: 0.0893, step time: 0.4855\n",
      "370/388, train_loss: 0.1297, step time: 0.5110\n",
      "371/388, train_loss: 0.2408, step time: 0.5341\n",
      "372/388, train_loss: 0.1269, step time: 0.5492\n",
      "373/388, train_loss: 0.0679, step time: 0.5252\n",
      "374/388, train_loss: 0.0896, step time: 0.5124\n",
      "375/388, train_loss: 0.0675, step time: 0.5037\n",
      "376/388, train_loss: 0.1502, step time: 0.4838\n",
      "377/388, train_loss: 0.2651, step time: 0.4842\n",
      "378/388, train_loss: 0.1591, step time: 0.4836\n",
      "379/388, train_loss: 0.1283, step time: 0.4948\n",
      "380/388, train_loss: 0.2790, step time: 0.5591\n",
      "381/388, train_loss: 0.2082, step time: 0.5237\n",
      "382/388, train_loss: 0.1601, step time: 0.5162\n",
      "383/388, train_loss: 0.3254, step time: 0.5158\n",
      "384/388, train_loss: 0.2244, step time: 0.4921\n",
      "385/388, train_loss: 0.1033, step time: 0.4846\n",
      "386/388, train_loss: 0.1387, step time: 0.4890\n",
      "387/388, train_loss: 0.2143, step time: 0.4874\n",
      "388/388, train_loss: 0.3290, step time: 0.5674\n",
      "epoch 37 average loss: 0.2028\n",
      "saved new best metric model\n",
      "current epoch: 37 current mean dice: 0.7585 tc: 0.8104 wt: 0.8954 et: 0.5697\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 37 is: 297.9106\n",
      "----------\n",
      "epoch 38/300\n",
      "1/388, train_loss: 0.1184, step time: 0.4876\n",
      "2/388, train_loss: 0.2504, step time: 0.4936\n",
      "3/388, train_loss: 0.1459, step time: 0.6120\n",
      "4/388, train_loss: 0.5286, step time: 0.5800\n",
      "5/388, train_loss: 0.2388, step time: 0.5309\n",
      "6/388, train_loss: 0.1001, step time: 0.5255\n",
      "7/388, train_loss: 0.2014, step time: 0.5331\n",
      "8/388, train_loss: 0.2878, step time: 1.2335\n",
      "9/388, train_loss: 0.1031, step time: 0.5330\n",
      "10/388, train_loss: 0.3016, step time: 0.5230\n",
      "11/388, train_loss: 0.2117, step time: 0.4968\n",
      "12/388, train_loss: 0.2370, step time: 0.5030\n",
      "13/388, train_loss: 0.3460, step time: 0.5028\n",
      "14/388, train_loss: 0.1912, step time: 0.5753\n",
      "15/388, train_loss: 0.2879, step time: 0.5667\n",
      "16/388, train_loss: 0.1147, step time: 0.5438\n",
      "17/388, train_loss: 0.3019, step time: 0.5233\n",
      "18/388, train_loss: 0.3675, step time: 0.5103\n",
      "19/388, train_loss: 0.2209, step time: 0.6839\n",
      "20/388, train_loss: 0.1144, step time: 0.5540\n",
      "21/388, train_loss: 0.1737, step time: 0.5151\n",
      "22/388, train_loss: 0.2075, step time: 0.4947\n",
      "23/388, train_loss: 0.2255, step time: 0.5423\n",
      "24/388, train_loss: 0.2427, step time: 0.5305\n",
      "25/388, train_loss: 0.2185, step time: 0.5163\n",
      "26/388, train_loss: 0.2428, step time: 0.5051\n",
      "27/388, train_loss: 0.0946, step time: 0.5067\n",
      "28/388, train_loss: 0.1726, step time: 0.5011\n",
      "29/388, train_loss: 0.0998, step time: 0.4818\n",
      "30/388, train_loss: 0.1962, step time: 0.9808\n",
      "31/388, train_loss: 0.2828, step time: 0.5374\n",
      "32/388, train_loss: 0.2542, step time: 0.5146\n",
      "33/388, train_loss: 0.1247, step time: 0.4943\n",
      "34/388, train_loss: 0.1597, step time: 0.5187\n",
      "35/388, train_loss: 0.1129, step time: 0.5196\n",
      "36/388, train_loss: 0.1568, step time: 0.5113\n",
      "37/388, train_loss: 0.2085, step time: 0.5010\n",
      "38/388, train_loss: 0.1776, step time: 0.4840\n",
      "39/388, train_loss: 0.1654, step time: 0.9858\n",
      "40/388, train_loss: 0.1020, step time: 0.5560\n",
      "41/388, train_loss: 0.1252, step time: 0.5082\n",
      "42/388, train_loss: 0.1462, step time: 0.4810\n",
      "43/388, train_loss: 0.3143, step time: 0.4849\n",
      "44/388, train_loss: 0.1056, step time: 0.5100\n",
      "45/388, train_loss: 0.1627, step time: 0.4937\n",
      "46/388, train_loss: 0.1963, step time: 0.5007\n",
      "47/388, train_loss: 0.1767, step time: 0.5089\n",
      "48/388, train_loss: 0.3119, step time: 0.4832\n",
      "49/388, train_loss: 0.1390, step time: 0.4952\n",
      "50/388, train_loss: 0.1150, step time: 0.5114\n",
      "51/388, train_loss: 0.1907, step time: 0.5198\n",
      "52/388, train_loss: 0.2155, step time: 0.5046\n",
      "53/388, train_loss: 0.0885, step time: 0.4924\n",
      "54/388, train_loss: 0.1644, step time: 0.5100\n",
      "55/388, train_loss: 0.2356, step time: 0.5755\n",
      "56/388, train_loss: 0.0946, step time: 0.5308\n",
      "57/388, train_loss: 0.1961, step time: 0.7170\n",
      "58/388, train_loss: 0.1560, step time: 0.6146\n",
      "59/388, train_loss: 0.4319, step time: 0.5383\n",
      "60/388, train_loss: 0.2342, step time: 0.5251\n",
      "61/388, train_loss: 0.0944, step time: 0.5049\n",
      "62/388, train_loss: 0.4072, step time: 1.0937\n",
      "63/388, train_loss: 0.2210, step time: 0.5488\n",
      "64/388, train_loss: 0.1049, step time: 0.5135\n",
      "65/388, train_loss: 0.1218, step time: 0.5045\n",
      "66/388, train_loss: 0.5314, step time: 0.4883\n",
      "67/388, train_loss: 0.0982, step time: 0.5329\n",
      "68/388, train_loss: 0.3658, step time: 0.5315\n",
      "69/388, train_loss: 0.2949, step time: 0.5037\n",
      "70/388, train_loss: 0.2187, step time: 0.4836\n",
      "71/388, train_loss: 0.1228, step time: 1.1278\n",
      "72/388, train_loss: 0.3188, step time: 0.5376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/388, train_loss: 0.1924, step time: 0.5119\n",
      "74/388, train_loss: 0.0936, step time: 0.4937\n",
      "75/388, train_loss: 0.3095, step time: 0.5099\n",
      "76/388, train_loss: 0.3440, step time: 0.4992\n",
      "77/388, train_loss: 0.2460, step time: 0.4876\n",
      "78/388, train_loss: 0.1037, step time: 1.2391\n",
      "79/388, train_loss: 0.4177, step time: 0.5303\n",
      "80/388, train_loss: 0.0675, step time: 0.5008\n",
      "81/388, train_loss: 0.1779, step time: 0.4950\n",
      "82/388, train_loss: 0.1989, step time: 0.4831\n",
      "83/388, train_loss: 0.1224, step time: 1.1420\n",
      "84/388, train_loss: 0.2209, step time: 0.5351\n",
      "85/388, train_loss: 0.2086, step time: 0.5175\n",
      "86/388, train_loss: 0.1839, step time: 0.4928\n",
      "87/388, train_loss: 0.0425, step time: 0.4937\n",
      "88/388, train_loss: 0.2861, step time: 0.4874\n",
      "89/388, train_loss: 0.1584, step time: 0.4981\n",
      "90/388, train_loss: 0.0700, step time: 0.4797\n",
      "91/388, train_loss: 0.1704, step time: 0.4837\n",
      "92/388, train_loss: 0.0564, step time: 0.4728\n",
      "93/388, train_loss: 0.3528, step time: 0.4946\n",
      "94/388, train_loss: 0.1082, step time: 0.4887\n",
      "95/388, train_loss: 0.4849, step time: 0.4970\n",
      "96/388, train_loss: 0.0701, step time: 1.1256\n",
      "97/388, train_loss: 0.1234, step time: 0.5305\n",
      "98/388, train_loss: 0.0985, step time: 0.5105\n",
      "99/388, train_loss: 0.2738, step time: 0.4943\n",
      "100/388, train_loss: 0.1542, step time: 0.4957\n",
      "101/388, train_loss: 0.2083, step time: 0.5028\n",
      "102/388, train_loss: 0.3848, step time: 0.5099\n",
      "103/388, train_loss: 0.2739, step time: 0.5135\n",
      "104/388, train_loss: 0.1194, step time: 0.4959\n",
      "105/388, train_loss: 0.1508, step time: 0.4914\n",
      "106/388, train_loss: 0.4016, step time: 1.1547\n",
      "107/388, train_loss: 0.3832, step time: 0.5388\n",
      "108/388, train_loss: 0.2171, step time: 0.5204\n",
      "109/388, train_loss: 0.0650, step time: 0.4993\n",
      "110/388, train_loss: 0.1917, step time: 0.4942\n",
      "111/388, train_loss: 0.1980, step time: 0.4869\n",
      "112/388, train_loss: 0.1944, step time: 0.4846\n",
      "113/388, train_loss: 0.1428, step time: 0.4784\n",
      "114/388, train_loss: 0.0968, step time: 0.4768\n",
      "115/388, train_loss: 0.2382, step time: 0.4721\n",
      "116/388, train_loss: 0.1721, step time: 0.5139\n",
      "117/388, train_loss: 0.0735, step time: 0.5409\n",
      "118/388, train_loss: 0.1313, step time: 0.5485\n",
      "119/388, train_loss: 0.4038, step time: 0.5202\n",
      "120/388, train_loss: 0.1682, step time: 0.4886\n",
      "121/388, train_loss: 0.0455, step time: 0.4947\n",
      "122/388, train_loss: 0.2800, step time: 0.5949\n",
      "123/388, train_loss: 0.3168, step time: 0.6611\n",
      "124/388, train_loss: 0.0750, step time: 0.5406\n",
      "125/388, train_loss: 0.3281, step time: 0.5179\n",
      "126/388, train_loss: 0.0896, step time: 0.4976\n",
      "127/388, train_loss: 0.1673, step time: 0.4985\n",
      "128/388, train_loss: 0.3313, step time: 1.1508\n",
      "129/388, train_loss: 0.2084, step time: 0.5344\n",
      "130/388, train_loss: 0.1142, step time: 0.5111\n",
      "131/388, train_loss: 0.1162, step time: 0.4961\n",
      "132/388, train_loss: 0.3420, step time: 0.4834\n",
      "133/388, train_loss: 0.2756, step time: 0.4861\n",
      "134/388, train_loss: 0.2147, step time: 1.1115\n",
      "135/388, train_loss: 0.3108, step time: 0.5600\n",
      "136/388, train_loss: 0.1515, step time: 0.5158\n",
      "137/388, train_loss: 0.0915, step time: 0.5010\n",
      "138/388, train_loss: 0.1022, step time: 0.4973\n",
      "139/388, train_loss: 0.4233, step time: 0.4891\n",
      "140/388, train_loss: 0.0652, step time: 0.4890\n",
      "141/388, train_loss: 0.0454, step time: 1.1026\n",
      "142/388, train_loss: 0.2447, step time: 0.5386\n",
      "143/388, train_loss: 0.3352, step time: 0.5071\n",
      "144/388, train_loss: 0.1295, step time: 0.4964\n",
      "145/388, train_loss: 0.3476, step time: 0.4938\n",
      "146/388, train_loss: 0.2449, step time: 0.4893\n",
      "147/388, train_loss: 0.0640, step time: 0.4938\n",
      "148/388, train_loss: 0.5837, step time: 0.4846\n",
      "149/388, train_loss: 0.1688, step time: 0.6270\n",
      "150/388, train_loss: 0.2412, step time: 0.5613\n",
      "151/388, train_loss: 0.0884, step time: 0.5261\n",
      "152/388, train_loss: 0.1477, step time: 0.5030\n",
      "153/388, train_loss: 0.0802, step time: 0.5000\n",
      "154/388, train_loss: 0.2228, step time: 0.4806\n",
      "155/388, train_loss: 0.0682, step time: 0.5146\n",
      "156/388, train_loss: 0.3547, step time: 0.5013\n",
      "157/388, train_loss: 0.1451, step time: 0.5659\n",
      "158/388, train_loss: 0.4443, step time: 0.5197\n",
      "159/388, train_loss: 0.4609, step time: 0.5000\n",
      "160/388, train_loss: 0.0893, step time: 0.5021\n",
      "161/388, train_loss: 0.1545, step time: 0.5222\n",
      "162/388, train_loss: 0.1079, step time: 0.5791\n",
      "163/388, train_loss: 0.0579, step time: 0.5435\n",
      "164/388, train_loss: 0.2744, step time: 0.5154\n",
      "165/388, train_loss: 0.0701, step time: 0.5006\n",
      "166/388, train_loss: 0.1752, step time: 0.4910\n",
      "167/388, train_loss: 0.1273, step time: 1.2172\n",
      "168/388, train_loss: 0.3852, step time: 0.5338\n",
      "169/388, train_loss: 0.1287, step time: 0.5039\n",
      "170/388, train_loss: 0.1059, step time: 0.4948\n",
      "171/388, train_loss: 0.3664, step time: 0.4931\n",
      "172/388, train_loss: 0.0689, step time: 0.4971\n",
      "173/388, train_loss: 0.0954, step time: 0.4977\n",
      "174/388, train_loss: 0.1786, step time: 0.4821\n",
      "175/388, train_loss: 0.3040, step time: 0.4882\n",
      "176/388, train_loss: 0.0897, step time: 0.4961\n",
      "177/388, train_loss: 0.1929, step time: 0.5397\n",
      "178/388, train_loss: 0.2010, step time: 0.5471\n",
      "179/388, train_loss: 0.3000, step time: 0.7035\n",
      "180/388, train_loss: 0.1281, step time: 0.5494\n",
      "181/388, train_loss: 0.1454, step time: 0.5210\n",
      "182/388, train_loss: 0.0865, step time: 0.5134\n",
      "183/388, train_loss: 0.1046, step time: 0.5030\n",
      "184/388, train_loss: 0.1263, step time: 0.5060\n",
      "185/388, train_loss: 0.2872, step time: 0.4959\n",
      "186/388, train_loss: 0.1097, step time: 0.4857\n",
      "187/388, train_loss: 0.2028, step time: 0.5048\n",
      "188/388, train_loss: 0.5250, step time: 0.5236\n",
      "189/388, train_loss: 0.0510, step time: 0.5243\n",
      "190/388, train_loss: 0.1413, step time: 0.5030\n",
      "191/388, train_loss: 0.5893, step time: 0.4886\n",
      "192/388, train_loss: 0.2795, step time: 1.0484\n",
      "193/388, train_loss: 0.1346, step time: 0.5379\n",
      "194/388, train_loss: 0.2516, step time: 0.5133\n",
      "195/388, train_loss: 0.1076, step time: 0.4912\n",
      "196/388, train_loss: 0.1527, step time: 0.4965\n",
      "197/388, train_loss: 0.3861, step time: 0.4871\n",
      "198/388, train_loss: 0.2513, step time: 0.5409\n",
      "199/388, train_loss: 0.1131, step time: 0.5486\n",
      "200/388, train_loss: 0.1275, step time: 0.5486\n",
      "201/388, train_loss: 0.0493, step time: 0.5459\n",
      "202/388, train_loss: 0.1962, step time: 0.5081\n",
      "203/388, train_loss: 0.1500, step time: 0.5058\n",
      "204/388, train_loss: 0.2336, step time: 0.4907\n",
      "205/388, train_loss: 0.2468, step time: 0.4913\n",
      "206/388, train_loss: 0.1326, step time: 0.4797\n",
      "207/388, train_loss: 0.2346, step time: 0.5161\n",
      "208/388, train_loss: 0.1191, step time: 0.4969\n",
      "209/388, train_loss: 0.1524, step time: 0.5501\n",
      "210/388, train_loss: 0.2327, step time: 0.5743\n",
      "211/388, train_loss: 0.3078, step time: 0.5164\n",
      "212/388, train_loss: 0.1501, step time: 0.5279\n",
      "213/388, train_loss: 0.1773, step time: 0.4916\n",
      "214/388, train_loss: 0.2762, step time: 0.9870\n",
      "215/388, train_loss: 0.0906, step time: 0.5476\n",
      "216/388, train_loss: 0.1337, step time: 0.5294\n",
      "217/388, train_loss: 0.1601, step time: 0.4994\n",
      "218/388, train_loss: 0.1439, step time: 0.4944\n",
      "219/388, train_loss: 0.4061, step time: 1.1217\n",
      "220/388, train_loss: 0.1554, step time: 0.5365\n",
      "221/388, train_loss: 0.0838, step time: 0.5147\n",
      "222/388, train_loss: 0.1669, step time: 0.4900\n",
      "223/388, train_loss: 0.1205, step time: 1.0411\n",
      "224/388, train_loss: 0.0846, step time: 0.5514\n",
      "225/388, train_loss: 0.2673, step time: 0.5171\n",
      "226/388, train_loss: 0.1870, step time: 0.4919\n",
      "227/388, train_loss: 0.1014, step time: 0.4940\n",
      "228/388, train_loss: 0.0814, step time: 0.4887\n",
      "229/388, train_loss: 0.1524, step time: 0.4762\n",
      "230/388, train_loss: 0.1891, step time: 1.1583\n",
      "231/388, train_loss: 0.0783, step time: 0.5548\n",
      "232/388, train_loss: 0.1210, step time: 0.5191\n",
      "233/388, train_loss: 0.2054, step time: 0.5000\n",
      "234/388, train_loss: 0.1360, step time: 0.5006\n",
      "235/388, train_loss: 0.0976, step time: 0.4874\n",
      "236/388, train_loss: 0.1948, step time: 0.4975\n",
      "237/388, train_loss: 0.1352, step time: 0.4991\n",
      "238/388, train_loss: 0.0639, step time: 0.5000\n",
      "239/388, train_loss: 0.1057, step time: 0.4996\n",
      "240/388, train_loss: 0.0687, step time: 0.5030\n",
      "241/388, train_loss: 0.1621, step time: 0.5021\n",
      "242/388, train_loss: 0.0457, step time: 0.4972\n",
      "243/388, train_loss: 0.0939, step time: 0.4898\n",
      "244/388, train_loss: 0.1986, step time: 0.5680\n",
      "245/388, train_loss: 0.1181, step time: 0.5391\n",
      "246/388, train_loss: 0.1551, step time: 0.5304\n",
      "247/388, train_loss: 0.2693, step time: 0.5534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/388, train_loss: 0.4952, step time: 0.5259\n",
      "249/388, train_loss: 0.1013, step time: 0.5183\n",
      "250/388, train_loss: 0.1635, step time: 0.5248\n",
      "251/388, train_loss: 0.0325, step time: 0.5785\n",
      "252/388, train_loss: 0.0990, step time: 0.5380\n",
      "253/388, train_loss: 0.1026, step time: 0.5045\n",
      "254/388, train_loss: 0.2025, step time: 0.5245\n",
      "255/388, train_loss: 0.1108, step time: 0.6246\n",
      "256/388, train_loss: 0.5036, step time: 0.5627\n",
      "257/388, train_loss: 0.2240, step time: 0.5325\n",
      "258/388, train_loss: 0.1231, step time: 0.5075\n",
      "259/388, train_loss: 0.0703, step time: 0.5107\n",
      "260/388, train_loss: 0.2284, step time: 0.5812\n",
      "261/388, train_loss: 0.3833, step time: 0.5319\n",
      "262/388, train_loss: 0.1483, step time: 0.5040\n",
      "263/388, train_loss: 0.0821, step time: 0.4960\n",
      "264/388, train_loss: 0.6952, step time: 1.1130\n",
      "265/388, train_loss: 0.1344, step time: 0.5415\n",
      "266/388, train_loss: 0.2897, step time: 0.5245\n",
      "267/388, train_loss: 0.0631, step time: 0.5058\n",
      "268/388, train_loss: 0.1065, step time: 0.4887\n",
      "269/388, train_loss: 0.2209, step time: 0.4831\n",
      "270/388, train_loss: 0.1263, step time: 0.5341\n",
      "271/388, train_loss: 0.2192, step time: 0.5104\n",
      "272/388, train_loss: 0.2773, step time: 0.5066\n",
      "273/388, train_loss: 0.0928, step time: 0.9476\n",
      "274/388, train_loss: 0.1425, step time: 0.5333\n",
      "275/388, train_loss: 0.1789, step time: 0.5045\n",
      "276/388, train_loss: 0.1245, step time: 0.4966\n",
      "277/388, train_loss: 0.0745, step time: 0.4935\n",
      "278/388, train_loss: 0.1272, step time: 0.5796\n",
      "279/388, train_loss: 0.1208, step time: 0.5478\n",
      "280/388, train_loss: 0.2278, step time: 0.5109\n",
      "281/388, train_loss: 0.1756, step time: 0.4972\n",
      "282/388, train_loss: 0.2031, step time: 0.5414\n",
      "283/388, train_loss: 0.1853, step time: 0.5292\n",
      "284/388, train_loss: 0.0917, step time: 0.5470\n",
      "285/388, train_loss: 0.3786, step time: 0.5156\n",
      "286/388, train_loss: 0.1178, step time: 0.4911\n",
      "287/388, train_loss: 0.3150, step time: 0.5195\n",
      "288/388, train_loss: 0.2302, step time: 0.5531\n",
      "289/388, train_loss: 0.1487, step time: 0.5062\n",
      "290/388, train_loss: 0.4537, step time: 0.4932\n",
      "291/388, train_loss: 0.0666, step time: 0.5022\n",
      "292/388, train_loss: 0.3122, step time: 0.5260\n",
      "293/388, train_loss: 0.1236, step time: 0.4951\n",
      "294/388, train_loss: 0.2502, step time: 0.4942\n",
      "295/388, train_loss: 0.1954, step time: 0.5107\n",
      "296/388, train_loss: 0.2477, step time: 0.4876\n",
      "297/388, train_loss: 0.1834, step time: 1.0016\n",
      "298/388, train_loss: 0.1358, step time: 0.5527\n",
      "299/388, train_loss: 0.1538, step time: 0.5393\n",
      "300/388, train_loss: 0.2444, step time: 0.5148\n",
      "301/388, train_loss: 0.3478, step time: 0.5039\n",
      "302/388, train_loss: 0.1143, step time: 0.5358\n",
      "303/388, train_loss: 0.2348, step time: 0.5923\n",
      "304/388, train_loss: 0.0694, step time: 0.5363\n",
      "305/388, train_loss: 0.0685, step time: 0.4919\n",
      "306/388, train_loss: 0.0570, step time: 0.4871\n",
      "307/388, train_loss: 0.2249, step time: 1.2106\n",
      "308/388, train_loss: 0.1463, step time: 0.5345\n",
      "309/388, train_loss: 0.1240, step time: 0.5081\n",
      "310/388, train_loss: 0.3222, step time: 0.4936\n",
      "311/388, train_loss: 0.0980, step time: 0.4983\n",
      "312/388, train_loss: 0.4014, step time: 0.4915\n",
      "313/388, train_loss: 0.2257, step time: 0.5487\n",
      "314/388, train_loss: 0.1177, step time: 0.5267\n",
      "315/388, train_loss: 0.2110, step time: 0.5322\n",
      "316/388, train_loss: 0.0928, step time: 0.5096\n",
      "317/388, train_loss: 0.1053, step time: 0.4952\n",
      "318/388, train_loss: 0.0886, step time: 0.4977\n",
      "319/388, train_loss: 0.1628, step time: 0.4809\n",
      "320/388, train_loss: 0.2609, step time: 1.1042\n",
      "321/388, train_loss: 0.3055, step time: 0.5377\n",
      "322/388, train_loss: 0.0978, step time: 0.5138\n",
      "323/388, train_loss: 0.1488, step time: 0.5084\n",
      "324/388, train_loss: 0.5308, step time: 0.4943\n",
      "325/388, train_loss: 0.1606, step time: 0.5125\n",
      "326/388, train_loss: 0.1494, step time: 0.5042\n",
      "327/388, train_loss: 0.1628, step time: 0.4991\n",
      "328/388, train_loss: 0.1748, step time: 0.5045\n",
      "329/388, train_loss: 0.0899, step time: 0.4848\n",
      "330/388, train_loss: 0.2643, step time: 1.0664\n",
      "331/388, train_loss: 0.1882, step time: 0.5471\n",
      "332/388, train_loss: 0.1068, step time: 0.5139\n",
      "333/388, train_loss: 0.1434, step time: 0.4954\n",
      "334/388, train_loss: 0.1053, step time: 0.4959\n",
      "335/388, train_loss: 0.2793, step time: 0.4884\n",
      "336/388, train_loss: 0.0807, step time: 1.1922\n",
      "337/388, train_loss: 0.0947, step time: 0.5189\n",
      "338/388, train_loss: 0.5579, step time: 0.4915\n",
      "339/388, train_loss: 0.1925, step time: 0.5110\n",
      "340/388, train_loss: 0.1149, step time: 0.5091\n",
      "341/388, train_loss: 0.3334, step time: 0.4969\n",
      "342/388, train_loss: 0.2221, step time: 0.4850\n",
      "343/388, train_loss: 0.3522, step time: 0.5253\n",
      "344/388, train_loss: 0.2612, step time: 0.5628\n",
      "345/388, train_loss: 0.6591, step time: 0.5292\n",
      "346/388, train_loss: 0.2513, step time: 0.5063\n",
      "347/388, train_loss: 0.1482, step time: 0.4913\n",
      "348/388, train_loss: 0.1865, step time: 0.4991\n",
      "349/388, train_loss: 0.7102, step time: 0.8915\n",
      "350/388, train_loss: 0.0764, step time: 0.5420\n",
      "351/388, train_loss: 0.1795, step time: 0.5346\n",
      "352/388, train_loss: 0.2757, step time: 0.5026\n",
      "353/388, train_loss: 0.6280, step time: 0.4934\n",
      "354/388, train_loss: 0.2149, step time: 0.7741\n",
      "355/388, train_loss: 0.2422, step time: 0.5636\n",
      "356/388, train_loss: 0.2227, step time: 0.5217\n",
      "357/388, train_loss: 0.3027, step time: 0.5081\n",
      "358/388, train_loss: 0.3529, step time: 0.5006\n",
      "359/388, train_loss: 0.1230, step time: 0.4931\n",
      "360/388, train_loss: 0.1839, step time: 0.4843\n",
      "361/388, train_loss: 0.1286, step time: 0.4947\n",
      "362/388, train_loss: 0.5574, step time: 0.4827\n",
      "363/388, train_loss: 0.2720, step time: 1.1225\n",
      "364/388, train_loss: 0.1090, step time: 0.5340\n",
      "365/388, train_loss: 0.0925, step time: 0.5029\n",
      "366/388, train_loss: 0.1346, step time: 0.4947\n",
      "367/388, train_loss: 0.2895, step time: 0.4946\n",
      "368/388, train_loss: 0.0853, step time: 0.4967\n",
      "369/388, train_loss: 0.2399, step time: 0.4847\n",
      "370/388, train_loss: 0.2102, step time: 0.4927\n",
      "371/388, train_loss: 0.2429, step time: 0.4914\n",
      "372/388, train_loss: 0.3079, step time: 1.0016\n",
      "373/388, train_loss: 0.1420, step time: 0.5293\n",
      "374/388, train_loss: 0.0878, step time: 0.4972\n",
      "375/388, train_loss: 0.2447, step time: 0.4918\n",
      "376/388, train_loss: 0.1136, step time: 0.4772\n",
      "377/388, train_loss: 0.1079, step time: 0.5274\n",
      "378/388, train_loss: 0.2965, step time: 0.4981\n",
      "379/388, train_loss: 0.2784, step time: 0.4995\n",
      "380/388, train_loss: 0.1467, step time: 0.4784\n",
      "381/388, train_loss: 0.0916, step time: 1.0074\n",
      "382/388, train_loss: 0.4848, step time: 0.5587\n",
      "383/388, train_loss: 0.1632, step time: 0.5158\n",
      "384/388, train_loss: 0.1990, step time: 0.5004\n",
      "385/388, train_loss: 0.2808, step time: 0.4828\n",
      "386/388, train_loss: 0.3923, step time: 0.4810\n",
      "387/388, train_loss: 0.2965, step time: 0.7086\n",
      "388/388, train_loss: 0.3290, step time: 0.5441\n",
      "epoch 38 average loss: 0.2020\n",
      "current epoch: 38 current mean dice: 0.7491 tc: 0.7934 wt: 0.8873 et: 0.5665\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 38 is: 305.8445\n",
      "----------\n",
      "epoch 39/300\n",
      "1/388, train_loss: 0.5406, step time: 0.4769\n",
      "2/388, train_loss: 0.2366, step time: 0.4805\n",
      "3/388, train_loss: 0.1876, step time: 0.5175\n",
      "4/388, train_loss: 0.3771, step time: 0.5316\n",
      "5/388, train_loss: 0.5662, step time: 0.5662\n",
      "6/388, train_loss: 0.3128, step time: 0.5759\n",
      "7/388, train_loss: 0.4334, step time: 0.7803\n",
      "8/388, train_loss: 0.2174, step time: 0.5796\n",
      "9/388, train_loss: 0.0857, step time: 0.5299\n",
      "10/388, train_loss: 0.2558, step time: 0.5220\n",
      "11/388, train_loss: 0.6087, step time: 0.5228\n",
      "12/388, train_loss: 0.1844, step time: 0.5261\n",
      "13/388, train_loss: 0.0724, step time: 0.4911\n",
      "14/388, train_loss: 0.0965, step time: 0.4825\n",
      "15/388, train_loss: 0.1632, step time: 0.5075\n",
      "16/388, train_loss: 0.0866, step time: 0.5729\n",
      "17/388, train_loss: 0.1372, step time: 0.5754\n",
      "18/388, train_loss: 0.2796, step time: 0.6976\n",
      "19/388, train_loss: 0.1942, step time: 0.5631\n",
      "20/388, train_loss: 0.1515, step time: 0.5313\n",
      "21/388, train_loss: 0.2459, step time: 0.4972\n",
      "22/388, train_loss: 0.1599, step time: 0.5418\n",
      "23/388, train_loss: 0.3877, step time: 0.5304\n",
      "24/388, train_loss: 0.2247, step time: 0.5125\n",
      "25/388, train_loss: 0.2161, step time: 0.4978\n",
      "26/388, train_loss: 0.5663, step time: 0.9454\n",
      "27/388, train_loss: 0.3197, step time: 0.5428\n",
      "28/388, train_loss: 0.1767, step time: 0.5173\n",
      "29/388, train_loss: 0.1863, step time: 0.5096\n",
      "30/388, train_loss: 0.1597, step time: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/388, train_loss: 0.0751, step time: 0.4880\n",
      "32/388, train_loss: 0.2007, step time: 0.4830\n",
      "33/388, train_loss: 0.2403, step time: 0.4967\n",
      "34/388, train_loss: 0.1970, step time: 0.4925\n",
      "35/388, train_loss: 0.1348, step time: 0.5633\n",
      "36/388, train_loss: 0.0763, step time: 0.6192\n",
      "37/388, train_loss: 0.1049, step time: 0.5578\n",
      "38/388, train_loss: 0.3287, step time: 0.5184\n",
      "39/388, train_loss: 0.1619, step time: 0.5122\n",
      "40/388, train_loss: 0.2885, step time: 0.5714\n",
      "41/388, train_loss: 0.3302, step time: 0.5701\n",
      "42/388, train_loss: 0.0882, step time: 0.5346\n",
      "43/388, train_loss: 0.1189, step time: 0.5171\n",
      "44/388, train_loss: 0.2719, step time: 0.5049\n",
      "45/388, train_loss: 0.2306, step time: 0.5082\n",
      "46/388, train_loss: 0.1647, step time: 0.4901\n",
      "47/388, train_loss: 0.1303, step time: 0.5094\n",
      "48/388, train_loss: 0.2735, step time: 0.5750\n",
      "49/388, train_loss: 0.0959, step time: 0.5324\n",
      "50/388, train_loss: 0.2342, step time: 0.4986\n",
      "51/388, train_loss: 0.0911, step time: 0.4815\n",
      "52/388, train_loss: 0.1379, step time: 0.4996\n",
      "53/388, train_loss: 0.1678, step time: 0.5186\n",
      "54/388, train_loss: 0.1285, step time: 0.6064\n",
      "55/388, train_loss: 0.1755, step time: 0.5459\n",
      "56/388, train_loss: 0.0507, step time: 0.5188\n",
      "57/388, train_loss: 0.0958, step time: 0.4923\n",
      "58/388, train_loss: 0.1118, step time: 0.5334\n",
      "59/388, train_loss: 0.3656, step time: 0.5146\n",
      "60/388, train_loss: 0.1868, step time: 0.5100\n",
      "61/388, train_loss: 0.1043, step time: 0.4914\n",
      "62/388, train_loss: 0.2107, step time: 0.4980\n",
      "63/388, train_loss: 0.1138, step time: 0.4931\n",
      "64/388, train_loss: 0.2794, step time: 0.5207\n",
      "65/388, train_loss: 0.1677, step time: 0.5291\n",
      "66/388, train_loss: 0.1043, step time: 0.4982\n",
      "67/388, train_loss: 0.2448, step time: 0.5130\n",
      "68/388, train_loss: 0.2345, step time: 0.4961\n",
      "69/388, train_loss: 0.0936, step time: 0.4979\n",
      "70/388, train_loss: 0.1107, step time: 0.4827\n",
      "71/388, train_loss: 0.0956, step time: 0.5268\n",
      "72/388, train_loss: 0.2256, step time: 0.5132\n",
      "73/388, train_loss: 0.1765, step time: 0.4952\n",
      "74/388, train_loss: 0.0850, step time: 0.4883\n",
      "75/388, train_loss: 0.2196, step time: 0.4838\n",
      "76/388, train_loss: 0.0505, step time: 0.4937\n",
      "77/388, train_loss: 0.2258, step time: 1.0012\n",
      "78/388, train_loss: 0.1273, step time: 0.5420\n",
      "79/388, train_loss: 0.1587, step time: 0.5158\n",
      "80/388, train_loss: 0.2725, step time: 0.5019\n",
      "81/388, train_loss: 0.1380, step time: 0.4950\n",
      "82/388, train_loss: 0.3096, step time: 1.0752\n",
      "83/388, train_loss: 0.2404, step time: 0.5283\n",
      "84/388, train_loss: 0.1728, step time: 0.5125\n",
      "85/388, train_loss: 0.1636, step time: 0.4991\n",
      "86/388, train_loss: 0.1863, step time: 0.4948\n",
      "87/388, train_loss: 0.3642, step time: 0.4912\n",
      "88/388, train_loss: 0.4277, step time: 0.5207\n",
      "89/388, train_loss: 0.2753, step time: 0.4963\n",
      "90/388, train_loss: 0.1741, step time: 0.5272\n",
      "91/388, train_loss: 0.2719, step time: 0.5037\n",
      "92/388, train_loss: 0.2798, step time: 0.4977\n",
      "93/388, train_loss: 0.2392, step time: 0.4976\n",
      "94/388, train_loss: 0.1537, step time: 0.4921\n",
      "95/388, train_loss: 0.4127, step time: 0.4805\n",
      "96/388, train_loss: 0.0496, step time: 1.0088\n",
      "97/388, train_loss: 0.2117, step time: 0.5458\n",
      "98/388, train_loss: 0.2190, step time: 0.5125\n",
      "99/388, train_loss: 0.1132, step time: 0.5081\n",
      "100/388, train_loss: 0.1926, step time: 0.4961\n",
      "101/388, train_loss: 0.4012, step time: 0.4979\n",
      "102/388, train_loss: 0.3976, step time: 0.4830\n",
      "103/388, train_loss: 0.1745, step time: 0.4932\n",
      "104/388, train_loss: 0.1377, step time: 0.4835\n",
      "105/388, train_loss: 0.1299, step time: 0.4843\n",
      "106/388, train_loss: 0.0366, step time: 0.4812\n",
      "107/388, train_loss: 0.0582, step time: 0.4734\n",
      "108/388, train_loss: 0.1736, step time: 0.9971\n",
      "109/388, train_loss: 0.2673, step time: 0.5322\n",
      "110/388, train_loss: 0.3458, step time: 0.5051\n",
      "111/388, train_loss: 0.3270, step time: 0.4922\n",
      "112/388, train_loss: 0.1285, step time: 0.4965\n",
      "113/388, train_loss: 0.2516, step time: 0.5014\n",
      "114/388, train_loss: 0.2464, step time: 0.4951\n",
      "115/388, train_loss: 0.1046, step time: 0.4880\n",
      "116/388, train_loss: 0.2475, step time: 0.5030\n",
      "117/388, train_loss: 0.0751, step time: 0.4904\n",
      "118/388, train_loss: 0.1732, step time: 0.4941\n",
      "119/388, train_loss: 0.2092, step time: 0.4899\n",
      "120/388, train_loss: 0.1522, step time: 0.4960\n",
      "121/388, train_loss: 0.2106, step time: 0.4873\n",
      "122/388, train_loss: 0.2569, step time: 0.5013\n",
      "123/388, train_loss: 0.1089, step time: 0.5063\n",
      "124/388, train_loss: 0.3126, step time: 0.4976\n",
      "125/388, train_loss: 0.1243, step time: 0.4936\n",
      "126/388, train_loss: 0.2303, step time: 0.4973\n",
      "127/388, train_loss: 0.3733, step time: 0.4791\n",
      "128/388, train_loss: 0.1436, step time: 0.5227\n",
      "129/388, train_loss: 0.2913, step time: 0.5096\n",
      "130/388, train_loss: 0.4495, step time: 0.4987\n",
      "131/388, train_loss: 0.0975, step time: 0.4803\n",
      "132/388, train_loss: 0.1266, step time: 0.5325\n",
      "133/388, train_loss: 0.1754, step time: 0.5377\n",
      "134/388, train_loss: 0.3192, step time: 0.5198\n",
      "135/388, train_loss: 0.2960, step time: 0.5293\n",
      "136/388, train_loss: 0.0894, step time: 0.5137\n",
      "137/388, train_loss: 0.0950, step time: 0.5043\n",
      "138/388, train_loss: 0.2960, step time: 0.4862\n",
      "139/388, train_loss: 0.0885, step time: 0.5877\n",
      "140/388, train_loss: 0.1744, step time: 0.5518\n",
      "141/388, train_loss: 0.2887, step time: 0.5220\n",
      "142/388, train_loss: 0.3976, step time: 0.5050\n",
      "143/388, train_loss: 0.1886, step time: 0.5156\n",
      "144/388, train_loss: 0.1197, step time: 0.5090\n",
      "145/388, train_loss: 0.1124, step time: 0.5062\n",
      "146/388, train_loss: 0.2973, step time: 0.4849\n",
      "147/388, train_loss: 0.1304, step time: 1.0479\n",
      "148/388, train_loss: 0.0888, step time: 0.5325\n",
      "149/388, train_loss: 0.2020, step time: 0.5169\n",
      "150/388, train_loss: 0.1005, step time: 0.4944\n",
      "151/388, train_loss: 0.0740, step time: 0.4984\n",
      "152/388, train_loss: 0.1534, step time: 0.4807\n",
      "153/388, train_loss: 0.1947, step time: 0.4932\n",
      "154/388, train_loss: 0.2043, step time: 0.5344\n",
      "155/388, train_loss: 0.2246, step time: 0.5085\n",
      "156/388, train_loss: 0.1887, step time: 0.4987\n",
      "157/388, train_loss: 0.1320, step time: 0.5185\n",
      "158/388, train_loss: 0.2384, step time: 0.5385\n",
      "159/388, train_loss: 0.2244, step time: 0.5944\n",
      "160/388, train_loss: 0.2275, step time: 0.5618\n",
      "161/388, train_loss: 0.0649, step time: 0.5247\n",
      "162/388, train_loss: 0.1500, step time: 0.5049\n",
      "163/388, train_loss: 0.0692, step time: 0.4952\n",
      "164/388, train_loss: 0.3404, step time: 0.4928\n",
      "165/388, train_loss: 0.1212, step time: 0.5053\n",
      "166/388, train_loss: 0.2790, step time: 0.5042\n",
      "167/388, train_loss: 0.2005, step time: 0.4879\n",
      "168/388, train_loss: 0.3182, step time: 0.4987\n",
      "169/388, train_loss: 0.0706, step time: 0.5437\n",
      "170/388, train_loss: 0.3322, step time: 0.5437\n",
      "171/388, train_loss: 0.1934, step time: 0.5025\n",
      "172/388, train_loss: 0.5037, step time: 0.5075\n",
      "173/388, train_loss: 0.2119, step time: 0.4919\n",
      "174/388, train_loss: 0.2831, step time: 0.4816\n",
      "175/388, train_loss: 0.1092, step time: 0.4956\n",
      "176/388, train_loss: 0.0938, step time: 1.0187\n",
      "177/388, train_loss: 0.0641, step time: 0.5387\n",
      "178/388, train_loss: 0.2575, step time: 0.5112\n",
      "179/388, train_loss: 0.0364, step time: 0.4881\n",
      "180/388, train_loss: 0.1142, step time: 0.4931\n",
      "181/388, train_loss: 0.4388, step time: 0.4794\n",
      "182/388, train_loss: 0.1483, step time: 0.4762\n",
      "183/388, train_loss: 0.2866, step time: 1.0419\n",
      "184/388, train_loss: 0.3204, step time: 0.5372\n",
      "185/388, train_loss: 0.2740, step time: 0.5099\n",
      "186/388, train_loss: 0.3010, step time: 0.5087\n",
      "187/388, train_loss: 0.1739, step time: 0.4911\n",
      "188/388, train_loss: 0.1305, step time: 0.7065\n",
      "189/388, train_loss: 0.2235, step time: 0.5428\n",
      "190/388, train_loss: 0.0720, step time: 0.5173\n",
      "191/388, train_loss: 0.3414, step time: 0.5109\n",
      "192/388, train_loss: 0.0643, step time: 0.4910\n",
      "193/388, train_loss: 0.0668, step time: 0.4929\n",
      "194/388, train_loss: 0.0597, step time: 0.4785\n",
      "195/388, train_loss: 0.3399, step time: 0.4772\n",
      "196/388, train_loss: 0.2262, step time: 0.4741\n",
      "197/388, train_loss: 0.0868, step time: 0.6181\n",
      "198/388, train_loss: 0.0929, step time: 0.5438\n",
      "199/388, train_loss: 0.0967, step time: 0.5119\n",
      "200/388, train_loss: 0.1713, step time: 0.4986\n",
      "201/388, train_loss: 0.0578, step time: 0.4936\n",
      "202/388, train_loss: 0.1066, step time: 0.4940\n",
      "203/388, train_loss: 0.0958, step time: 0.4856\n",
      "204/388, train_loss: 0.1998, step time: 0.4850\n",
      "205/388, train_loss: 0.1042, step time: 0.5054\n",
      "206/388, train_loss: 0.1392, step time: 0.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/388, train_loss: 0.5382, step time: 0.4949\n",
      "208/388, train_loss: 0.2218, step time: 1.1943\n",
      "209/388, train_loss: 0.1459, step time: 0.5296\n",
      "210/388, train_loss: 0.0881, step time: 0.5040\n",
      "211/388, train_loss: 0.1638, step time: 0.4991\n",
      "212/388, train_loss: 0.2612, step time: 0.4901\n",
      "213/388, train_loss: 0.0789, step time: 0.4905\n",
      "214/388, train_loss: 0.1871, step time: 0.4814\n",
      "215/388, train_loss: 0.1915, step time: 0.4854\n",
      "216/388, train_loss: 0.0602, step time: 0.4982\n",
      "217/388, train_loss: 0.0804, step time: 0.4900\n",
      "218/388, train_loss: 0.4513, step time: 0.7872\n",
      "219/388, train_loss: 0.1046, step time: 0.5689\n",
      "220/388, train_loss: 0.2113, step time: 0.5381\n",
      "221/388, train_loss: 0.5685, step time: 0.5151\n",
      "222/388, train_loss: 0.1008, step time: 0.4893\n",
      "223/388, train_loss: 0.2091, step time: 0.4913\n",
      "224/388, train_loss: 0.2145, step time: 0.4950\n",
      "225/388, train_loss: 0.1436, step time: 1.1834\n",
      "226/388, train_loss: 0.1649, step time: 0.5337\n",
      "227/388, train_loss: 0.3333, step time: 0.5111\n",
      "228/388, train_loss: 0.2521, step time: 0.4976\n",
      "229/388, train_loss: 0.5534, step time: 0.4924\n",
      "230/388, train_loss: 0.0872, step time: 0.4904\n",
      "231/388, train_loss: 0.1472, step time: 0.4849\n",
      "232/388, train_loss: 0.3007, step time: 0.4898\n",
      "233/388, train_loss: 0.1151, step time: 0.4836\n",
      "234/388, train_loss: 0.0740, step time: 1.1421\n",
      "235/388, train_loss: 0.1970, step time: 0.5313\n",
      "236/388, train_loss: 0.1066, step time: 0.4965\n",
      "237/388, train_loss: 0.1351, step time: 0.4954\n",
      "238/388, train_loss: 0.2949, step time: 0.4876\n",
      "239/388, train_loss: 0.1497, step time: 0.4794\n",
      "240/388, train_loss: 0.3153, step time: 0.7588\n",
      "241/388, train_loss: 0.1686, step time: 0.5351\n",
      "242/388, train_loss: 0.2288, step time: 0.5200\n",
      "243/388, train_loss: 0.2905, step time: 0.4967\n",
      "244/388, train_loss: 0.1905, step time: 0.4943\n",
      "245/388, train_loss: 0.3092, step time: 0.4904\n",
      "246/388, train_loss: 0.1127, step time: 0.4883\n",
      "247/388, train_loss: 0.1701, step time: 0.4972\n",
      "248/388, train_loss: 0.0511, step time: 0.4782\n",
      "249/388, train_loss: 0.5203, step time: 1.1580\n",
      "250/388, train_loss: 0.1890, step time: 0.5415\n",
      "251/388, train_loss: 0.0930, step time: 0.5037\n",
      "252/388, train_loss: 0.1875, step time: 0.5042\n",
      "253/388, train_loss: 0.1017, step time: 0.4867\n",
      "254/388, train_loss: 0.1991, step time: 0.4976\n",
      "255/388, train_loss: 0.1597, step time: 0.4801\n",
      "256/388, train_loss: 0.1565, step time: 0.4804\n",
      "257/388, train_loss: 0.2867, step time: 0.4813\n",
      "258/388, train_loss: 0.0695, step time: 0.8122\n",
      "259/388, train_loss: 0.1130, step time: 0.5352\n",
      "260/388, train_loss: 0.3473, step time: 0.5144\n",
      "261/388, train_loss: 0.1307, step time: 0.5015\n",
      "262/388, train_loss: 0.1148, step time: 0.4888\n",
      "263/388, train_loss: 0.2423, step time: 0.4880\n",
      "264/388, train_loss: 0.1605, step time: 0.5012\n",
      "265/388, train_loss: 0.1127, step time: 0.4801\n",
      "266/388, train_loss: 0.2768, step time: 0.5024\n",
      "267/388, train_loss: 0.2485, step time: 0.5060\n",
      "268/388, train_loss: 0.3674, step time: 0.5049\n",
      "269/388, train_loss: 0.2900, step time: 0.5220\n",
      "270/388, train_loss: 0.1048, step time: 0.5120\n",
      "271/388, train_loss: 0.1751, step time: 0.4935\n",
      "272/388, train_loss: 0.3269, step time: 0.4820\n",
      "273/388, train_loss: 0.1509, step time: 0.4973\n",
      "274/388, train_loss: 0.0996, step time: 0.4839\n",
      "275/388, train_loss: 0.1472, step time: 1.0555\n",
      "276/388, train_loss: 0.1077, step time: 0.5528\n",
      "277/388, train_loss: 0.0537, step time: 0.5282\n",
      "278/388, train_loss: 0.1176, step time: 0.5007\n",
      "279/388, train_loss: 0.1453, step time: 0.5028\n",
      "280/388, train_loss: 0.2853, step time: 0.5512\n",
      "281/388, train_loss: 0.2796, step time: 0.5297\n",
      "282/388, train_loss: 0.1508, step time: 0.5049\n",
      "283/388, train_loss: 0.2517, step time: 0.4981\n",
      "284/388, train_loss: 0.1010, step time: 0.4842\n",
      "285/388, train_loss: 0.1502, step time: 0.4849\n",
      "286/388, train_loss: 0.1611, step time: 0.4903\n",
      "287/388, train_loss: 0.6550, step time: 0.4805\n",
      "288/388, train_loss: 0.0742, step time: 0.4832\n",
      "289/388, train_loss: 0.1109, step time: 1.0741\n",
      "290/388, train_loss: 0.1244, step time: 0.5501\n",
      "291/388, train_loss: 0.1336, step time: 0.5090\n",
      "292/388, train_loss: 0.1400, step time: 0.5031\n",
      "293/388, train_loss: 0.1638, step time: 0.4956\n",
      "294/388, train_loss: 0.1663, step time: 0.4862\n",
      "295/388, train_loss: 0.2635, step time: 0.4933\n",
      "296/388, train_loss: 0.2701, step time: 0.9228\n",
      "297/388, train_loss: 0.1479, step time: 0.5388\n",
      "298/388, train_loss: 0.1550, step time: 0.5196\n",
      "299/388, train_loss: 0.2411, step time: 0.5416\n",
      "300/388, train_loss: 0.2603, step time: 0.5285\n",
      "301/388, train_loss: 0.1425, step time: 0.5092\n",
      "302/388, train_loss: 0.5159, step time: 0.5057\n",
      "303/388, train_loss: 0.1300, step time: 0.5149\n",
      "304/388, train_loss: 0.4148, step time: 0.5010\n",
      "305/388, train_loss: 0.1100, step time: 0.4938\n",
      "306/388, train_loss: 0.0749, step time: 0.4851\n",
      "307/388, train_loss: 0.1649, step time: 0.4933\n",
      "308/388, train_loss: 0.2482, step time: 0.5572\n",
      "309/388, train_loss: 0.5399, step time: 0.5299\n",
      "310/388, train_loss: 0.1413, step time: 0.5123\n",
      "311/388, train_loss: 0.1038, step time: 0.4968\n",
      "312/388, train_loss: 0.0781, step time: 0.4946\n",
      "313/388, train_loss: 0.0965, step time: 0.4771\n",
      "314/388, train_loss: 0.2135, step time: 0.4935\n",
      "315/388, train_loss: 0.0896, step time: 1.1199\n",
      "316/388, train_loss: 0.1225, step time: 0.5276\n",
      "317/388, train_loss: 0.3583, step time: 0.5152\n",
      "318/388, train_loss: 0.2094, step time: 0.4899\n",
      "319/388, train_loss: 0.2046, step time: 0.5014\n",
      "320/388, train_loss: 0.1347, step time: 0.4977\n",
      "321/388, train_loss: 0.1709, step time: 0.4931\n",
      "322/388, train_loss: 0.0734, step time: 0.5002\n",
      "323/388, train_loss: 0.2889, step time: 0.4977\n",
      "324/388, train_loss: 0.0975, step time: 0.4917\n",
      "325/388, train_loss: 0.1336, step time: 0.4974\n",
      "326/388, train_loss: 0.2862, step time: 0.5128\n",
      "327/388, train_loss: 0.0579, step time: 0.5116\n",
      "328/388, train_loss: 0.3799, step time: 0.5058\n",
      "329/388, train_loss: 0.1818, step time: 0.4915\n",
      "330/388, train_loss: 0.1270, step time: 0.4901\n",
      "331/388, train_loss: 0.1915, step time: 0.5058\n",
      "332/388, train_loss: 0.2296, step time: 0.5941\n",
      "333/388, train_loss: 0.2161, step time: 0.5711\n",
      "334/388, train_loss: 0.0856, step time: 0.5231\n",
      "335/388, train_loss: 0.1657, step time: 0.4937\n",
      "336/388, train_loss: 0.1215, step time: 0.4946\n",
      "337/388, train_loss: 0.0904, step time: 0.6974\n",
      "338/388, train_loss: 0.0573, step time: 0.5605\n",
      "339/388, train_loss: 0.0884, step time: 0.5278\n",
      "340/388, train_loss: 0.0894, step time: 0.5019\n",
      "341/388, train_loss: 0.2793, step time: 0.5026\n",
      "342/388, train_loss: 0.0896, step time: 0.4964\n",
      "343/388, train_loss: 0.3658, step time: 0.4968\n",
      "344/388, train_loss: 0.2764, step time: 1.2333\n",
      "345/388, train_loss: 0.3143, step time: 0.5347\n",
      "346/388, train_loss: 0.1897, step time: 0.5108\n",
      "347/388, train_loss: 0.0679, step time: 0.4998\n",
      "348/388, train_loss: 0.1451, step time: 0.4832\n",
      "349/388, train_loss: 0.1803, step time: 0.5063\n",
      "350/388, train_loss: 0.3177, step time: 0.4967\n",
      "351/388, train_loss: 0.1453, step time: 0.9104\n",
      "352/388, train_loss: 0.0855, step time: 0.5522\n",
      "353/388, train_loss: 0.1066, step time: 0.5093\n",
      "354/388, train_loss: 0.1426, step time: 0.4990\n",
      "355/388, train_loss: 0.3037, step time: 0.4912\n",
      "356/388, train_loss: 0.2379, step time: 0.5129\n",
      "357/388, train_loss: 0.2041, step time: 0.5535\n",
      "358/388, train_loss: 0.2132, step time: 0.5112\n",
      "359/388, train_loss: 0.1799, step time: 0.5077\n",
      "360/388, train_loss: 0.2194, step time: 0.5112\n",
      "361/388, train_loss: 0.0343, step time: 0.5161\n",
      "362/388, train_loss: 0.2488, step time: 0.5020\n",
      "363/388, train_loss: 0.1368, step time: 0.5278\n",
      "364/388, train_loss: 0.1786, step time: 0.5063\n",
      "365/388, train_loss: 0.2433, step time: 0.5054\n",
      "366/388, train_loss: 0.2408, step time: 0.4873\n",
      "367/388, train_loss: 0.1113, step time: 1.2241\n",
      "368/388, train_loss: 0.2110, step time: 0.5321\n",
      "369/388, train_loss: 0.1436, step time: 0.5077\n",
      "370/388, train_loss: 0.0723, step time: 0.4964\n",
      "371/388, train_loss: 0.4505, step time: 0.5112\n",
      "372/388, train_loss: 0.0745, step time: 0.5461\n",
      "373/388, train_loss: 0.3357, step time: 0.5131\n",
      "374/388, train_loss: 0.0971, step time: 0.5031\n",
      "375/388, train_loss: 0.1945, step time: 0.4873\n",
      "376/388, train_loss: 0.1835, step time: 0.4977\n",
      "377/388, train_loss: 0.1578, step time: 0.5378\n",
      "378/388, train_loss: 0.4571, step time: 0.5168\n",
      "379/388, train_loss: 0.1370, step time: 0.4952\n",
      "380/388, train_loss: 0.5588, step time: 0.4993\n",
      "381/388, train_loss: 0.1004, step time: 0.4857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/388, train_loss: 0.0730, step time: 0.4756\n",
      "383/388, train_loss: 0.2697, step time: 1.0948\n",
      "384/388, train_loss: 0.1043, step time: 0.5301\n",
      "385/388, train_loss: 0.1847, step time: 0.5076\n",
      "386/388, train_loss: 0.0914, step time: 0.4883\n",
      "387/388, train_loss: 0.1171, step time: 0.4786\n",
      "388/388, train_loss: 0.1476, step time: 0.4912\n",
      "epoch 39 average loss: 0.1984\n",
      "current epoch: 39 current mean dice: 0.7515 tc: 0.7983 wt: 0.8934 et: 0.5626\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 39 is: 300.0198\n",
      "----------\n",
      "epoch 40/300\n",
      "1/388, train_loss: 0.0982, step time: 0.4807\n",
      "2/388, train_loss: 0.1137, step time: 0.5264\n",
      "3/388, train_loss: 0.1061, step time: 0.5454\n",
      "4/388, train_loss: 0.0871, step time: 0.6805\n",
      "5/388, train_loss: 0.1149, step time: 0.5411\n",
      "6/388, train_loss: 0.2223, step time: 0.5174\n",
      "7/388, train_loss: 0.0989, step time: 0.4983\n",
      "8/388, train_loss: 0.0924, step time: 0.4990\n",
      "9/388, train_loss: 0.1043, step time: 0.5040\n",
      "10/388, train_loss: 0.1787, step time: 1.1005\n",
      "11/388, train_loss: 0.2824, step time: 0.5383\n",
      "12/388, train_loss: 0.1715, step time: 0.5892\n",
      "13/388, train_loss: 0.4803, step time: 0.5279\n",
      "14/388, train_loss: 0.1124, step time: 0.4994\n",
      "15/388, train_loss: 0.1158, step time: 1.1643\n",
      "16/388, train_loss: 0.4115, step time: 0.5255\n",
      "17/388, train_loss: 0.2096, step time: 0.4948\n",
      "18/388, train_loss: 0.1125, step time: 0.4852\n",
      "19/388, train_loss: 0.3032, step time: 0.4854\n",
      "20/388, train_loss: 0.1685, step time: 0.5228\n",
      "21/388, train_loss: 0.1852, step time: 0.5279\n",
      "22/388, train_loss: 0.1079, step time: 0.6140\n",
      "23/388, train_loss: 0.2943, step time: 0.5610\n",
      "24/388, train_loss: 0.1272, step time: 0.5245\n",
      "25/388, train_loss: 0.2877, step time: 0.5311\n",
      "26/388, train_loss: 0.2257, step time: 0.5311\n",
      "27/388, train_loss: 0.1096, step time: 0.5319\n",
      "28/388, train_loss: 0.2195, step time: 0.5828\n",
      "29/388, train_loss: 0.2756, step time: 0.5515\n",
      "30/388, train_loss: 0.1393, step time: 0.5148\n",
      "31/388, train_loss: 0.0410, step time: 0.4906\n",
      "32/388, train_loss: 0.1117, step time: 0.5008\n",
      "33/388, train_loss: 0.2191, step time: 0.4896\n",
      "34/388, train_loss: 0.0966, step time: 0.4991\n",
      "35/388, train_loss: 0.2071, step time: 0.4848\n",
      "36/388, train_loss: 0.3261, step time: 1.0936\n",
      "37/388, train_loss: 0.0797, step time: 0.5543\n",
      "38/388, train_loss: 0.1400, step time: 0.5111\n",
      "39/388, train_loss: 0.0606, step time: 0.4936\n",
      "40/388, train_loss: 0.2532, step time: 0.4783\n",
      "41/388, train_loss: 0.1832, step time: 1.0410\n",
      "42/388, train_loss: 0.2657, step time: 0.5462\n",
      "43/388, train_loss: 0.1155, step time: 0.5162\n",
      "44/388, train_loss: 0.1225, step time: 0.4878\n",
      "45/388, train_loss: 0.2193, step time: 0.5046\n",
      "46/388, train_loss: 0.1932, step time: 0.4890\n",
      "47/388, train_loss: 0.3925, step time: 0.4941\n",
      "48/388, train_loss: 0.0473, step time: 1.0045\n",
      "49/388, train_loss: 0.1649, step time: 0.5490\n",
      "50/388, train_loss: 0.0356, step time: 0.5169\n",
      "51/388, train_loss: 0.1895, step time: 0.5056\n",
      "52/388, train_loss: 0.2022, step time: 0.4867\n",
      "53/388, train_loss: 0.1895, step time: 0.5249\n",
      "54/388, train_loss: 0.1364, step time: 0.5261\n",
      "55/388, train_loss: 0.1595, step time: 0.5940\n",
      "56/388, train_loss: 0.1281, step time: 0.5772\n",
      "57/388, train_loss: 0.3226, step time: 0.5348\n",
      "58/388, train_loss: 0.2734, step time: 0.5082\n",
      "59/388, train_loss: 0.0976, step time: 1.0135\n",
      "60/388, train_loss: 0.0613, step time: 0.5382\n",
      "61/388, train_loss: 0.3636, step time: 0.5047\n",
      "62/388, train_loss: 0.2686, step time: 0.4905\n",
      "63/388, train_loss: 0.2171, step time: 0.8943\n",
      "64/388, train_loss: 0.1264, step time: 0.5505\n",
      "65/388, train_loss: 0.3347, step time: 0.5026\n",
      "66/388, train_loss: 0.0834, step time: 0.4991\n",
      "67/388, train_loss: 0.0521, step time: 0.4843\n",
      "68/388, train_loss: 0.1532, step time: 0.4772\n",
      "69/388, train_loss: 0.0612, step time: 0.4806\n",
      "70/388, train_loss: 0.1652, step time: 0.4999\n",
      "71/388, train_loss: 0.2339, step time: 0.5368\n",
      "72/388, train_loss: 0.2870, step time: 0.5228\n",
      "73/388, train_loss: 0.3546, step time: 0.5015\n",
      "74/388, train_loss: 0.2064, step time: 0.4993\n",
      "75/388, train_loss: 0.1931, step time: 0.4865\n",
      "76/388, train_loss: 0.2005, step time: 1.0424\n",
      "77/388, train_loss: 0.6336, step time: 0.5391\n",
      "78/388, train_loss: 0.1516, step time: 0.5168\n",
      "79/388, train_loss: 0.4011, step time: 0.4837\n",
      "80/388, train_loss: 0.2211, step time: 0.4986\n",
      "81/388, train_loss: 0.1626, step time: 0.4983\n",
      "82/388, train_loss: 0.1621, step time: 0.5224\n",
      "83/388, train_loss: 0.0571, step time: 0.5076\n",
      "84/388, train_loss: 0.0765, step time: 0.4971\n",
      "85/388, train_loss: 0.0649, step time: 0.4871\n",
      "86/388, train_loss: 0.0790, step time: 0.4913\n",
      "87/388, train_loss: 0.0961, step time: 0.4890\n",
      "88/388, train_loss: 0.1620, step time: 0.5693\n",
      "89/388, train_loss: 0.1385, step time: 0.5338\n",
      "90/388, train_loss: 0.1168, step time: 0.5057\n",
      "91/388, train_loss: 0.1936, step time: 0.5070\n",
      "92/388, train_loss: 0.1490, step time: 0.5086\n",
      "93/388, train_loss: 0.1450, step time: 0.5055\n",
      "94/388, train_loss: 0.1493, step time: 0.5049\n",
      "95/388, train_loss: 0.2403, step time: 0.5039\n",
      "96/388, train_loss: 0.5099, step time: 0.5295\n",
      "97/388, train_loss: 0.4527, step time: 0.5083\n",
      "98/388, train_loss: 0.0848, step time: 0.4857\n",
      "99/388, train_loss: 0.2995, step time: 0.4869\n",
      "100/388, train_loss: 0.1280, step time: 0.5118\n",
      "101/388, train_loss: 0.1017, step time: 0.5162\n",
      "102/388, train_loss: 0.0655, step time: 0.5241\n",
      "103/388, train_loss: 0.1466, step time: 0.5230\n",
      "104/388, train_loss: 0.0985, step time: 0.5627\n",
      "105/388, train_loss: 0.3204, step time: 0.5400\n",
      "106/388, train_loss: 0.2628, step time: 0.5213\n",
      "107/388, train_loss: 0.1275, step time: 0.5101\n",
      "108/388, train_loss: 0.2402, step time: 0.5301\n",
      "109/388, train_loss: 0.2283, step time: 0.6237\n",
      "110/388, train_loss: 0.1040, step time: 0.5423\n",
      "111/388, train_loss: 0.0767, step time: 0.5213\n",
      "112/388, train_loss: 0.1023, step time: 0.5110\n",
      "113/388, train_loss: 0.1893, step time: 0.4883\n",
      "114/388, train_loss: 0.2032, step time: 0.4822\n",
      "115/388, train_loss: 0.3126, step time: 0.4952\n",
      "116/388, train_loss: 0.1495, step time: 1.0841\n",
      "117/388, train_loss: 0.3045, step time: 0.5418\n",
      "118/388, train_loss: 0.1151, step time: 0.5046\n",
      "119/388, train_loss: 0.0735, step time: 0.4951\n",
      "120/388, train_loss: 0.2330, step time: 0.4951\n",
      "121/388, train_loss: 0.1940, step time: 0.4815\n",
      "122/388, train_loss: 0.2387, step time: 0.4816\n",
      "123/388, train_loss: 0.2939, step time: 0.4843\n",
      "124/388, train_loss: 0.1382, step time: 0.8164\n",
      "125/388, train_loss: 0.1244, step time: 0.5411\n",
      "126/388, train_loss: 0.1786, step time: 0.5129\n",
      "127/388, train_loss: 0.3881, step time: 0.5033\n",
      "128/388, train_loss: 0.2926, step time: 0.4887\n",
      "129/388, train_loss: 0.1888, step time: 0.4847\n",
      "130/388, train_loss: 0.1261, step time: 0.4892\n",
      "131/388, train_loss: 0.1694, step time: 0.5737\n",
      "132/388, train_loss: 0.0970, step time: 0.5476\n",
      "133/388, train_loss: 0.4362, step time: 0.5282\n",
      "134/388, train_loss: 0.1906, step time: 0.5115\n",
      "135/388, train_loss: 0.0807, step time: 0.4916\n",
      "136/388, train_loss: 0.1037, step time: 0.4820\n",
      "137/388, train_loss: 0.1054, step time: 0.4917\n",
      "138/388, train_loss: 0.2845, step time: 0.4805\n",
      "139/388, train_loss: 0.2476, step time: 0.5053\n",
      "140/388, train_loss: 0.2329, step time: 0.4888\n",
      "141/388, train_loss: 0.0586, step time: 0.4923\n",
      "142/388, train_loss: 0.2977, step time: 0.9421\n",
      "143/388, train_loss: 0.0629, step time: 0.5569\n",
      "144/388, train_loss: 0.1134, step time: 0.5166\n",
      "145/388, train_loss: 0.1948, step time: 0.4957\n",
      "146/388, train_loss: 0.2377, step time: 0.4965\n",
      "147/388, train_loss: 0.0924, step time: 0.4827\n",
      "148/388, train_loss: 0.2982, step time: 0.4885\n",
      "149/388, train_loss: 0.2213, step time: 0.4862\n",
      "150/388, train_loss: 0.2097, step time: 0.4986\n",
      "151/388, train_loss: 0.1225, step time: 0.6486\n",
      "152/388, train_loss: 0.1170, step time: 0.5455\n",
      "153/388, train_loss: 0.1719, step time: 0.5067\n",
      "154/388, train_loss: 0.2409, step time: 0.5027\n",
      "155/388, train_loss: 0.1504, step time: 0.4826\n",
      "156/388, train_loss: 0.1540, step time: 0.4777\n",
      "157/388, train_loss: 0.0300, step time: 0.4871\n",
      "158/388, train_loss: 0.2751, step time: 0.5115\n",
      "159/388, train_loss: 0.1362, step time: 0.5715\n",
      "160/388, train_loss: 0.1260, step time: 0.5403\n",
      "161/388, train_loss: 0.1385, step time: 0.5208\n",
      "162/388, train_loss: 0.1634, step time: 0.4998\n",
      "163/388, train_loss: 0.1171, step time: 0.4987\n",
      "164/388, train_loss: 0.2461, step time: 0.4915\n",
      "165/388, train_loss: 0.1647, step time: 0.4970\n",
      "166/388, train_loss: 0.0932, step time: 0.4856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/388, train_loss: 0.1833, step time: 1.1836\n",
      "168/388, train_loss: 0.0690, step time: 0.5348\n",
      "169/388, train_loss: 0.3784, step time: 0.5141\n",
      "170/388, train_loss: 0.5828, step time: 0.4896\n",
      "171/388, train_loss: 0.1886, step time: 0.4948\n",
      "172/388, train_loss: 0.2548, step time: 0.4853\n",
      "173/388, train_loss: 0.2101, step time: 0.4985\n",
      "174/388, train_loss: 0.0773, step time: 0.4821\n",
      "175/388, train_loss: 0.1876, step time: 0.4824\n",
      "176/388, train_loss: 0.0777, step time: 0.4865\n",
      "177/388, train_loss: 0.1625, step time: 0.4874\n",
      "178/388, train_loss: 0.1660, step time: 0.4898\n",
      "179/388, train_loss: 0.0963, step time: 0.4804\n",
      "180/388, train_loss: 0.1667, step time: 0.4849\n",
      "181/388, train_loss: 0.2723, step time: 0.4929\n",
      "182/388, train_loss: 0.2045, step time: 0.4952\n",
      "183/388, train_loss: 0.1477, step time: 0.4983\n",
      "184/388, train_loss: 0.3972, step time: 0.4846\n",
      "185/388, train_loss: 0.2533, step time: 0.6922\n",
      "186/388, train_loss: 0.1766, step time: 0.5555\n",
      "187/388, train_loss: 0.5592, step time: 0.5215\n",
      "188/388, train_loss: 0.2430, step time: 0.5134\n",
      "189/388, train_loss: 0.1366, step time: 0.5014\n",
      "190/388, train_loss: 0.0946, step time: 0.5042\n",
      "191/388, train_loss: 0.3091, step time: 0.5028\n",
      "192/388, train_loss: 0.1055, step time: 0.4996\n",
      "193/388, train_loss: 0.1746, step time: 0.5509\n",
      "194/388, train_loss: 0.5777, step time: 0.5373\n",
      "195/388, train_loss: 0.2360, step time: 0.5172\n",
      "196/388, train_loss: 0.0931, step time: 0.5165\n",
      "197/388, train_loss: 0.1207, step time: 0.5043\n",
      "198/388, train_loss: 0.1703, step time: 0.4929\n",
      "199/388, train_loss: 0.0767, step time: 0.4888\n",
      "200/388, train_loss: 0.2857, step time: 0.5046\n",
      "201/388, train_loss: 0.2779, step time: 0.4799\n",
      "202/388, train_loss: 0.2232, step time: 0.4887\n",
      "203/388, train_loss: 0.1702, step time: 0.4911\n",
      "204/388, train_loss: 0.4790, step time: 0.9889\n",
      "205/388, train_loss: 0.1176, step time: 0.5577\n",
      "206/388, train_loss: 0.2588, step time: 0.5220\n",
      "207/388, train_loss: 0.2129, step time: 0.5162\n",
      "208/388, train_loss: 0.4972, step time: 0.5070\n",
      "209/388, train_loss: 0.0650, step time: 0.5350\n",
      "210/388, train_loss: 0.1418, step time: 0.5200\n",
      "211/388, train_loss: 0.2106, step time: 0.4978\n",
      "212/388, train_loss: 0.0824, step time: 0.4998\n",
      "213/388, train_loss: 0.0864, step time: 0.4873\n",
      "214/388, train_loss: 0.1645, step time: 0.4814\n",
      "215/388, train_loss: 0.0874, step time: 1.0567\n",
      "216/388, train_loss: 0.1401, step time: 0.5396\n",
      "217/388, train_loss: 0.0995, step time: 0.5138\n",
      "218/388, train_loss: 0.1236, step time: 0.4903\n",
      "219/388, train_loss: 0.1749, step time: 0.4952\n",
      "220/388, train_loss: 0.0547, step time: 0.4796\n",
      "221/388, train_loss: 0.1109, step time: 0.5300\n",
      "222/388, train_loss: 0.3375, step time: 0.5070\n",
      "223/388, train_loss: 0.1496, step time: 0.5508\n",
      "224/388, train_loss: 0.1861, step time: 0.5142\n",
      "225/388, train_loss: 0.2776, step time: 0.5020\n",
      "226/388, train_loss: 0.2174, step time: 0.5050\n",
      "227/388, train_loss: 0.2555, step time: 0.4910\n",
      "228/388, train_loss: 0.1932, step time: 0.4863\n",
      "229/388, train_loss: 0.5291, step time: 0.9871\n",
      "230/388, train_loss: 0.2296, step time: 0.5335\n",
      "231/388, train_loss: 0.1812, step time: 0.5085\n",
      "232/388, train_loss: 0.4180, step time: 0.4884\n",
      "233/388, train_loss: 0.0957, step time: 0.5143\n",
      "234/388, train_loss: 0.1065, step time: 0.5049\n",
      "235/388, train_loss: 0.0938, step time: 0.4902\n",
      "236/388, train_loss: 0.2287, step time: 0.4896\n",
      "237/388, train_loss: 0.0978, step time: 0.4789\n",
      "238/388, train_loss: 0.0991, step time: 0.4839\n",
      "239/388, train_loss: 0.3005, step time: 0.4841\n",
      "240/388, train_loss: 0.0665, step time: 0.9728\n",
      "241/388, train_loss: 0.1360, step time: 0.5377\n",
      "242/388, train_loss: 0.0959, step time: 0.5091\n",
      "243/388, train_loss: 0.1909, step time: 0.4870\n",
      "244/388, train_loss: 0.2371, step time: 0.4832\n",
      "245/388, train_loss: 0.0641, step time: 0.4892\n",
      "246/388, train_loss: 0.1725, step time: 0.4778\n",
      "247/388, train_loss: 0.1032, step time: 0.4786\n",
      "248/388, train_loss: 0.2637, step time: 0.4948\n",
      "249/388, train_loss: 0.2387, step time: 0.4779\n",
      "250/388, train_loss: 0.2186, step time: 0.4919\n",
      "251/388, train_loss: 0.0714, step time: 0.4854\n",
      "252/388, train_loss: 0.1336, step time: 1.1359\n",
      "253/388, train_loss: 0.1166, step time: 0.5230\n",
      "254/388, train_loss: 0.0992, step time: 0.4964\n",
      "255/388, train_loss: 0.0910, step time: 0.4818\n",
      "256/388, train_loss: 0.1207, step time: 0.4924\n",
      "257/388, train_loss: 0.2167, step time: 0.4884\n",
      "258/388, train_loss: 0.1383, step time: 0.4930\n",
      "259/388, train_loss: 0.0425, step time: 0.4900\n",
      "260/388, train_loss: 0.2802, step time: 0.5016\n",
      "261/388, train_loss: 0.2207, step time: 0.4984\n",
      "262/388, train_loss: 0.1082, step time: 0.9396\n",
      "263/388, train_loss: 0.1981, step time: 0.5407\n",
      "264/388, train_loss: 0.5569, step time: 0.5171\n",
      "265/388, train_loss: 0.2170, step time: 0.4950\n",
      "266/388, train_loss: 0.2562, step time: 0.4941\n",
      "267/388, train_loss: 0.1237, step time: 0.4801\n",
      "268/388, train_loss: 0.2039, step time: 0.4767\n",
      "269/388, train_loss: 0.1016, step time: 0.4776\n",
      "270/388, train_loss: 0.1731, step time: 0.4746\n",
      "271/388, train_loss: 0.3982, step time: 1.0076\n",
      "272/388, train_loss: 0.1457, step time: 0.5332\n",
      "273/388, train_loss: 0.4325, step time: 0.5022\n",
      "274/388, train_loss: 0.1122, step time: 0.4904\n",
      "275/388, train_loss: 0.2975, step time: 0.4967\n",
      "276/388, train_loss: 0.0748, step time: 0.4815\n",
      "277/388, train_loss: 0.1511, step time: 0.4926\n",
      "278/388, train_loss: 0.5731, step time: 0.4793\n",
      "279/388, train_loss: 0.2422, step time: 0.5852\n",
      "280/388, train_loss: 0.1330, step time: 0.5443\n",
      "281/388, train_loss: 0.2867, step time: 0.5229\n",
      "282/388, train_loss: 0.0688, step time: 0.5124\n",
      "283/388, train_loss: 0.1148, step time: 0.5043\n",
      "284/388, train_loss: 0.1528, step time: 0.4983\n",
      "285/388, train_loss: 0.2830, step time: 0.4962\n",
      "286/388, train_loss: 0.2020, step time: 0.4850\n",
      "287/388, train_loss: 0.2749, step time: 0.4876\n",
      "288/388, train_loss: 0.3138, step time: 0.4884\n",
      "289/388, train_loss: 0.2840, step time: 0.4809\n",
      "290/388, train_loss: 0.3369, step time: 0.4871\n",
      "291/388, train_loss: 0.2296, step time: 0.4806\n",
      "292/388, train_loss: 0.2635, step time: 0.8676\n",
      "293/388, train_loss: 0.1180, step time: 0.5435\n",
      "294/388, train_loss: 0.2273, step time: 0.5128\n",
      "295/388, train_loss: 0.0627, step time: 0.4971\n",
      "296/388, train_loss: 0.2964, step time: 0.5106\n",
      "297/388, train_loss: 0.4534, step time: 0.6228\n",
      "298/388, train_loss: 0.1355, step time: 0.5482\n",
      "299/388, train_loss: 0.1150, step time: 0.5172\n",
      "300/388, train_loss: 0.2151, step time: 0.4977\n",
      "301/388, train_loss: 0.2266, step time: 0.4979\n",
      "302/388, train_loss: 0.2084, step time: 0.4821\n",
      "303/388, train_loss: 0.0999, step time: 0.4804\n",
      "304/388, train_loss: 0.5370, step time: 0.5045\n",
      "305/388, train_loss: 0.2152, step time: 0.4989\n",
      "306/388, train_loss: 0.0691, step time: 0.4906\n",
      "307/388, train_loss: 0.1508, step time: 0.5163\n",
      "308/388, train_loss: 0.1960, step time: 0.5014\n",
      "309/388, train_loss: 0.3456, step time: 0.5074\n",
      "310/388, train_loss: 0.3329, step time: 0.5011\n",
      "311/388, train_loss: 0.0996, step time: 0.5979\n",
      "312/388, train_loss: 0.1090, step time: 0.5525\n",
      "313/388, train_loss: 0.1976, step time: 0.5183\n",
      "314/388, train_loss: 0.2237, step time: 0.5118\n",
      "315/388, train_loss: 0.2033, step time: 0.5135\n",
      "316/388, train_loss: 0.1107, step time: 0.5009\n",
      "317/388, train_loss: 0.1346, step time: 0.4978\n",
      "318/388, train_loss: 0.2607, step time: 0.5127\n",
      "319/388, train_loss: 0.0956, step time: 0.5223\n",
      "320/388, train_loss: 0.3478, step time: 0.5800\n",
      "321/388, train_loss: 0.1100, step time: 0.5335\n",
      "322/388, train_loss: 0.2393, step time: 0.5117\n",
      "323/388, train_loss: 0.1467, step time: 0.5022\n",
      "324/388, train_loss: 0.2967, step time: 0.5416\n",
      "325/388, train_loss: 0.1727, step time: 0.5256\n",
      "326/388, train_loss: 0.2710, step time: 0.5439\n",
      "327/388, train_loss: 0.2183, step time: 0.6667\n",
      "328/388, train_loss: 0.1998, step time: 0.5470\n",
      "329/388, train_loss: 0.3800, step time: 0.5210\n",
      "330/388, train_loss: 0.1361, step time: 0.5035\n",
      "331/388, train_loss: 0.1684, step time: 0.4888\n",
      "332/388, train_loss: 0.1848, step time: 0.4951\n",
      "333/388, train_loss: 0.4033, step time: 0.5408\n",
      "334/388, train_loss: 0.4206, step time: 0.5213\n",
      "335/388, train_loss: 0.0905, step time: 0.5114\n",
      "336/388, train_loss: 0.2749, step time: 0.4979\n",
      "337/388, train_loss: 0.1823, step time: 0.5242\n",
      "338/388, train_loss: 0.1559, step time: 0.4983\n",
      "339/388, train_loss: 0.2442, step time: 0.4843\n",
      "340/388, train_loss: 0.4327, step time: 0.4968\n",
      "341/388, train_loss: 0.3941, step time: 0.5315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/388, train_loss: 0.0866, step time: 0.5142\n",
      "343/388, train_loss: 0.2306, step time: 0.4867\n",
      "344/388, train_loss: 0.2989, step time: 0.4833\n",
      "345/388, train_loss: 0.1453, step time: 0.4886\n",
      "346/388, train_loss: 0.1133, step time: 0.4765\n",
      "347/388, train_loss: 0.0944, step time: 0.4877\n",
      "348/388, train_loss: 0.0966, step time: 0.4966\n",
      "349/388, train_loss: 0.4726, step time: 0.4906\n",
      "350/388, train_loss: 0.1563, step time: 0.5281\n",
      "351/388, train_loss: 0.3585, step time: 0.5345\n",
      "352/388, train_loss: 0.4488, step time: 0.5147\n",
      "353/388, train_loss: 0.1384, step time: 0.5043\n",
      "354/388, train_loss: 0.3523, step time: 0.5586\n",
      "355/388, train_loss: 0.1156, step time: 0.5744\n",
      "356/388, train_loss: 0.0816, step time: 0.5288\n",
      "357/388, train_loss: 0.0690, step time: 0.5163\n",
      "358/388, train_loss: 0.2671, step time: 0.4936\n",
      "359/388, train_loss: 0.0783, step time: 0.5186\n",
      "360/388, train_loss: 0.2681, step time: 0.5705\n",
      "361/388, train_loss: 0.1526, step time: 0.5297\n",
      "362/388, train_loss: 0.2104, step time: 0.5068\n",
      "363/388, train_loss: 0.3062, step time: 0.5105\n",
      "364/388, train_loss: 0.1379, step time: 0.4982\n",
      "365/388, train_loss: 0.1795, step time: 0.4874\n",
      "366/388, train_loss: 0.1676, step time: 0.4895\n",
      "367/388, train_loss: 0.1360, step time: 1.0928\n",
      "368/388, train_loss: 0.1216, step time: 0.5521\n",
      "369/388, train_loss: 0.3058, step time: 0.5226\n",
      "370/388, train_loss: 0.1016, step time: 0.5094\n",
      "371/388, train_loss: 0.0692, step time: 0.5051\n",
      "372/388, train_loss: 0.3683, step time: 0.4848\n",
      "373/388, train_loss: 0.2381, step time: 0.4810\n",
      "374/388, train_loss: 0.0841, step time: 0.4821\n",
      "375/388, train_loss: 0.3927, step time: 0.4870\n",
      "376/388, train_loss: 0.2450, step time: 1.1558\n",
      "377/388, train_loss: 0.0841, step time: 0.5501\n",
      "378/388, train_loss: 0.1596, step time: 0.5161\n",
      "379/388, train_loss: 0.2433, step time: 0.4997\n",
      "380/388, train_loss: 0.1600, step time: 0.4957\n",
      "381/388, train_loss: 0.2890, step time: 0.4922\n",
      "382/388, train_loss: 0.1409, step time: 0.4842\n",
      "383/388, train_loss: 0.2237, step time: 0.5364\n",
      "384/388, train_loss: 0.1008, step time: 0.5142\n",
      "385/388, train_loss: 0.0639, step time: 0.4925\n",
      "386/388, train_loss: 0.1077, step time: 0.4891\n",
      "387/388, train_loss: 0.3126, step time: 0.4910\n",
      "388/388, train_loss: 0.0857, step time: 0.6076\n",
      "epoch 40 average loss: 0.1968\n",
      "current epoch: 40 current mean dice: 0.7457 tc: 0.7965 wt: 0.8870 et: 0.5537\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 40 is: 299.4774\n",
      "----------\n",
      "epoch 41/300\n",
      "1/388, train_loss: 0.4107, step time: 0.4804\n",
      "2/388, train_loss: 0.2629, step time: 0.4752\n",
      "3/388, train_loss: 0.0732, step time: 0.5383\n",
      "4/388, train_loss: 0.1202, step time: 0.5227\n",
      "5/388, train_loss: 0.1743, step time: 0.4860\n",
      "6/388, train_loss: 0.1404, step time: 0.5137\n",
      "7/388, train_loss: 0.2612, step time: 0.6300\n",
      "8/388, train_loss: 0.0519, step time: 0.5689\n",
      "9/388, train_loss: 0.1440, step time: 0.5201\n",
      "10/388, train_loss: 0.2234, step time: 0.5048\n",
      "11/388, train_loss: 0.0889, step time: 0.5671\n",
      "12/388, train_loss: 0.3220, step time: 0.5613\n",
      "13/388, train_loss: 0.3021, step time: 0.5356\n",
      "14/388, train_loss: 0.0730, step time: 0.5342\n",
      "15/388, train_loss: 0.0954, step time: 0.6464\n",
      "16/388, train_loss: 0.1290, step time: 0.5615\n",
      "17/388, train_loss: 0.1307, step time: 0.5473\n",
      "18/388, train_loss: 0.1198, step time: 0.5253\n",
      "19/388, train_loss: 0.2367, step time: 0.5312\n",
      "20/388, train_loss: 0.4845, step time: 0.6074\n",
      "21/388, train_loss: 0.2337, step time: 0.5425\n",
      "22/388, train_loss: 0.2442, step time: 0.5256\n",
      "23/388, train_loss: 0.1214, step time: 0.5115\n",
      "24/388, train_loss: 0.1034, step time: 0.9084\n",
      "25/388, train_loss: 0.1368, step time: 0.5507\n",
      "26/388, train_loss: 0.0803, step time: 0.5105\n",
      "27/388, train_loss: 0.1574, step time: 0.4994\n",
      "28/388, train_loss: 0.0386, step time: 0.4985\n",
      "29/388, train_loss: 0.0925, step time: 0.8878\n",
      "30/388, train_loss: 0.1000, step time: 0.5523\n",
      "31/388, train_loss: 0.0767, step time: 0.5228\n",
      "32/388, train_loss: 0.2709, step time: 0.4879\n",
      "33/388, train_loss: 0.1673, step time: 0.5020\n",
      "34/388, train_loss: 0.2747, step time: 0.5652\n",
      "35/388, train_loss: 0.1992, step time: 0.5315\n",
      "36/388, train_loss: 0.0616, step time: 0.5096\n",
      "37/388, train_loss: 0.1024, step time: 0.5041\n",
      "38/388, train_loss: 0.3474, step time: 0.5782\n",
      "39/388, train_loss: 0.1755, step time: 0.5461\n",
      "40/388, train_loss: 0.1801, step time: 0.5134\n",
      "41/388, train_loss: 0.2106, step time: 0.5124\n",
      "42/388, train_loss: 0.1245, step time: 0.5746\n",
      "43/388, train_loss: 0.2267, step time: 0.5651\n",
      "44/388, train_loss: 0.1251, step time: 0.5259\n",
      "45/388, train_loss: 0.1206, step time: 0.5186\n",
      "46/388, train_loss: 0.0663, step time: 0.5011\n",
      "47/388, train_loss: 0.5432, step time: 0.5032\n",
      "48/388, train_loss: 0.0832, step time: 0.8373\n",
      "49/388, train_loss: 0.2095, step time: 0.5511\n",
      "50/388, train_loss: 0.0651, step time: 0.5068\n",
      "51/388, train_loss: 0.0961, step time: 0.4914\n",
      "52/388, train_loss: 0.1157, step time: 0.5004\n",
      "53/388, train_loss: 0.1870, step time: 0.5007\n",
      "54/388, train_loss: 0.2279, step time: 0.7318\n",
      "55/388, train_loss: 0.2303, step time: 0.5669\n",
      "56/388, train_loss: 0.1866, step time: 0.5277\n",
      "57/388, train_loss: 0.0995, step time: 0.4926\n",
      "58/388, train_loss: 0.1876, step time: 0.5245\n",
      "59/388, train_loss: 0.1725, step time: 0.5860\n",
      "60/388, train_loss: 0.2996, step time: 0.5478\n",
      "61/388, train_loss: 0.4475, step time: 0.5154\n",
      "62/388, train_loss: 0.0719, step time: 0.5043\n",
      "63/388, train_loss: 0.5003, step time: 0.5114\n",
      "64/388, train_loss: 0.0961, step time: 0.6548\n",
      "65/388, train_loss: 0.2535, step time: 0.5608\n",
      "66/388, train_loss: 0.0971, step time: 0.5209\n",
      "67/388, train_loss: 0.1672, step time: 0.5077\n",
      "68/388, train_loss: 0.0887, step time: 0.4995\n",
      "69/388, train_loss: 0.2032, step time: 0.5639\n",
      "70/388, train_loss: 0.0385, step time: 0.7554\n",
      "71/388, train_loss: 0.0928, step time: 0.5392\n",
      "72/388, train_loss: 0.4099, step time: 0.5268\n",
      "73/388, train_loss: 0.2617, step time: 0.5057\n",
      "74/388, train_loss: 0.0755, step time: 0.5659\n",
      "75/388, train_loss: 0.2527, step time: 0.5225\n",
      "76/388, train_loss: 0.0908, step time: 0.4975\n",
      "77/388, train_loss: 0.1635, step time: 0.4826\n",
      "78/388, train_loss: 0.2030, step time: 0.5039\n",
      "79/388, train_loss: 0.5076, step time: 0.4925\n",
      "80/388, train_loss: 0.1094, step time: 1.0173\n",
      "81/388, train_loss: 0.4560, step time: 0.5612\n",
      "82/388, train_loss: 0.1389, step time: 0.5254\n",
      "83/388, train_loss: 0.1197, step time: 0.5038\n",
      "84/388, train_loss: 0.2574, step time: 0.5035\n",
      "85/388, train_loss: 0.0949, step time: 0.4897\n",
      "86/388, train_loss: 0.0983, step time: 0.5058\n",
      "87/388, train_loss: 0.1251, step time: 0.4998\n",
      "88/388, train_loss: 0.1525, step time: 0.5157\n",
      "89/388, train_loss: 0.1665, step time: 0.4994\n",
      "90/388, train_loss: 0.2056, step time: 0.5476\n",
      "91/388, train_loss: 0.2547, step time: 0.5282\n",
      "92/388, train_loss: 0.0664, step time: 0.5076\n",
      "93/388, train_loss: 0.1181, step time: 0.5022\n",
      "94/388, train_loss: 0.1446, step time: 0.5271\n",
      "95/388, train_loss: 0.0698, step time: 0.5077\n",
      "96/388, train_loss: 0.1584, step time: 0.5037\n",
      "97/388, train_loss: 0.2640, step time: 0.4967\n",
      "98/388, train_loss: 0.3514, step time: 0.4787\n",
      "99/388, train_loss: 0.2594, step time: 0.5324\n",
      "100/388, train_loss: 0.2031, step time: 0.5029\n",
      "101/388, train_loss: 0.0970, step time: 1.0168\n",
      "102/388, train_loss: 0.3739, step time: 0.5534\n",
      "103/388, train_loss: 0.2958, step time: 0.5321\n",
      "104/388, train_loss: 0.2401, step time: 0.5197\n",
      "105/388, train_loss: 0.1933, step time: 0.4973\n",
      "106/388, train_loss: 0.1530, step time: 0.5933\n",
      "107/388, train_loss: 0.2076, step time: 0.5639\n",
      "108/388, train_loss: 0.2722, step time: 0.5243\n",
      "109/388, train_loss: 0.0978, step time: 0.4859\n",
      "110/388, train_loss: 0.1535, step time: 0.8371\n",
      "111/388, train_loss: 0.3206, step time: 0.5625\n",
      "112/388, train_loss: 0.2796, step time: 0.5274\n",
      "113/388, train_loss: 0.1766, step time: 0.5027\n",
      "114/388, train_loss: 0.4852, step time: 0.4855\n",
      "115/388, train_loss: 0.1502, step time: 0.5005\n",
      "116/388, train_loss: 0.3705, step time: 0.5309\n",
      "117/388, train_loss: 0.0408, step time: 0.4865\n",
      "118/388, train_loss: 0.2605, step time: 0.4926\n",
      "119/388, train_loss: 0.3269, step time: 0.4987\n",
      "120/388, train_loss: 0.2152, step time: 0.9735\n",
      "121/388, train_loss: 0.1822, step time: 0.5516\n",
      "122/388, train_loss: 0.0991, step time: 0.5274\n",
      "123/388, train_loss: 0.1128, step time: 0.5030\n",
      "124/388, train_loss: 0.1817, step time: 0.4962\n",
      "125/388, train_loss: 0.2575, step time: 0.4970\n",
      "126/388, train_loss: 0.1828, step time: 0.4946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/388, train_loss: 0.1340, step time: 0.5234\n",
      "128/388, train_loss: 0.0938, step time: 0.4972\n",
      "129/388, train_loss: 0.2651, step time: 0.5786\n",
      "130/388, train_loss: 0.0786, step time: 0.5825\n",
      "131/388, train_loss: 0.0964, step time: 0.5234\n",
      "132/388, train_loss: 0.0904, step time: 0.5055\n",
      "133/388, train_loss: 0.6071, step time: 0.5049\n",
      "134/388, train_loss: 0.1257, step time: 0.4929\n",
      "135/388, train_loss: 0.3633, step time: 0.7037\n",
      "136/388, train_loss: 0.1062, step time: 0.5660\n",
      "137/388, train_loss: 0.1721, step time: 0.5395\n",
      "138/388, train_loss: 0.1274, step time: 0.5133\n",
      "139/388, train_loss: 0.2205, step time: 0.5108\n",
      "140/388, train_loss: 0.1069, step time: 0.4965\n",
      "141/388, train_loss: 0.1390, step time: 0.4979\n",
      "142/388, train_loss: 0.6330, step time: 1.2275\n",
      "143/388, train_loss: 0.2923, step time: 0.5408\n",
      "144/388, train_loss: 0.2645, step time: 0.5227\n",
      "145/388, train_loss: 0.3954, step time: 0.4997\n",
      "146/388, train_loss: 0.1615, step time: 0.4946\n",
      "147/388, train_loss: 0.0816, step time: 0.4856\n",
      "148/388, train_loss: 0.1425, step time: 0.4733\n",
      "149/388, train_loss: 0.1449, step time: 1.0133\n",
      "150/388, train_loss: 0.0408, step time: 0.5247\n",
      "151/388, train_loss: 0.1893, step time: 0.5124\n",
      "152/388, train_loss: 0.3503, step time: 0.5437\n",
      "153/388, train_loss: 0.2140, step time: 0.5362\n",
      "154/388, train_loss: 0.4680, step time: 0.5084\n",
      "155/388, train_loss: 0.2415, step time: 0.4999\n",
      "156/388, train_loss: 0.1007, step time: 0.5240\n",
      "157/388, train_loss: 0.1339, step time: 0.5131\n",
      "158/388, train_loss: 0.1826, step time: 0.4971\n",
      "159/388, train_loss: 0.2693, step time: 0.5034\n",
      "160/388, train_loss: 0.1653, step time: 1.1028\n",
      "161/388, train_loss: 0.4664, step time: 0.5325\n",
      "162/388, train_loss: 0.2782, step time: 0.5077\n",
      "163/388, train_loss: 0.2086, step time: 0.4900\n",
      "164/388, train_loss: 0.0831, step time: 0.4955\n",
      "165/388, train_loss: 0.0951, step time: 0.4786\n",
      "166/388, train_loss: 0.0790, step time: 0.4789\n",
      "167/388, train_loss: 0.1043, step time: 0.4786\n",
      "168/388, train_loss: 0.3712, step time: 0.4829\n",
      "169/388, train_loss: 0.0723, step time: 0.4924\n",
      "170/388, train_loss: 0.1778, step time: 0.9497\n",
      "171/388, train_loss: 0.1251, step time: 0.5483\n",
      "172/388, train_loss: 0.0782, step time: 0.5212\n",
      "173/388, train_loss: 0.1827, step time: 0.4953\n",
      "174/388, train_loss: 0.2279, step time: 0.4993\n",
      "175/388, train_loss: 0.1399, step time: 0.4792\n",
      "176/388, train_loss: 0.1428, step time: 0.4804\n",
      "177/388, train_loss: 0.3432, step time: 0.5132\n",
      "178/388, train_loss: 0.0892, step time: 0.4963\n",
      "179/388, train_loss: 0.0676, step time: 0.4984\n",
      "180/388, train_loss: 0.0952, step time: 0.4815\n",
      "181/388, train_loss: 0.1565, step time: 1.0315\n",
      "182/388, train_loss: 0.1613, step time: 0.5560\n",
      "183/388, train_loss: 0.3085, step time: 0.5307\n",
      "184/388, train_loss: 0.1331, step time: 0.5068\n",
      "185/388, train_loss: 0.0949, step time: 0.4959\n",
      "186/388, train_loss: 0.1005, step time: 0.5002\n",
      "187/388, train_loss: 0.1182, step time: 0.4849\n",
      "188/388, train_loss: 0.2123, step time: 0.5086\n",
      "189/388, train_loss: 0.1696, step time: 0.4881\n",
      "190/388, train_loss: 0.2487, step time: 0.5233\n",
      "191/388, train_loss: 0.1897, step time: 0.5110\n",
      "192/388, train_loss: 0.1842, step time: 0.5818\n",
      "193/388, train_loss: 0.2059, step time: 0.5460\n",
      "194/388, train_loss: 0.4542, step time: 0.5152\n",
      "195/388, train_loss: 0.2097, step time: 0.5007\n",
      "196/388, train_loss: 0.1250, step time: 0.4938\n",
      "197/388, train_loss: 0.3502, step time: 0.5074\n",
      "198/388, train_loss: 0.4321, step time: 0.5230\n",
      "199/388, train_loss: 0.2312, step time: 0.5794\n",
      "200/388, train_loss: 0.2364, step time: 0.5310\n",
      "201/388, train_loss: 0.1040, step time: 0.5030\n",
      "202/388, train_loss: 0.3610, step time: 0.5017\n",
      "203/388, train_loss: 0.1279, step time: 1.1897\n",
      "204/388, train_loss: 0.2088, step time: 0.5227\n",
      "205/388, train_loss: 0.1557, step time: 0.5011\n",
      "206/388, train_loss: 0.1378, step time: 0.4995\n",
      "207/388, train_loss: 0.1821, step time: 0.4883\n",
      "208/388, train_loss: 0.2699, step time: 0.4941\n",
      "209/388, train_loss: 0.1343, step time: 0.5181\n",
      "210/388, train_loss: 0.3689, step time: 0.5322\n",
      "211/388, train_loss: 0.1855, step time: 0.5200\n",
      "212/388, train_loss: 0.4210, step time: 0.5585\n",
      "213/388, train_loss: 0.1774, step time: 0.5264\n",
      "214/388, train_loss: 0.2840, step time: 0.5172\n",
      "215/388, train_loss: 0.1007, step time: 0.4874\n",
      "216/388, train_loss: 0.1066, step time: 0.5447\n",
      "217/388, train_loss: 0.1293, step time: 0.5184\n",
      "218/388, train_loss: 0.0510, step time: 0.5115\n",
      "219/388, train_loss: 0.1465, step time: 0.4929\n",
      "220/388, train_loss: 0.2379, step time: 0.4988\n",
      "221/388, train_loss: 0.0960, step time: 0.5041\n",
      "222/388, train_loss: 0.2011, step time: 0.5553\n",
      "223/388, train_loss: 0.3901, step time: 0.5137\n",
      "224/388, train_loss: 0.1709, step time: 0.4901\n",
      "225/388, train_loss: 0.4005, step time: 0.4956\n",
      "226/388, train_loss: 0.5203, step time: 0.5082\n",
      "227/388, train_loss: 0.1145, step time: 0.5025\n",
      "228/388, train_loss: 0.1500, step time: 0.4857\n",
      "229/388, train_loss: 0.2367, step time: 0.5016\n",
      "230/388, train_loss: 0.3053, step time: 0.4945\n",
      "231/388, train_loss: 0.2488, step time: 0.4981\n",
      "232/388, train_loss: 0.1239, step time: 0.5320\n",
      "233/388, train_loss: 0.0981, step time: 0.5218\n",
      "234/388, train_loss: 0.3672, step time: 0.6722\n",
      "235/388, train_loss: 0.1510, step time: 0.5630\n",
      "236/388, train_loss: 0.1095, step time: 0.5514\n",
      "237/388, train_loss: 0.2749, step time: 0.5173\n",
      "238/388, train_loss: 0.1929, step time: 0.4979\n",
      "239/388, train_loss: 0.1587, step time: 0.4961\n",
      "240/388, train_loss: 0.4657, step time: 0.4960\n",
      "241/388, train_loss: 0.1283, step time: 1.1453\n",
      "242/388, train_loss: 0.0678, step time: 0.5353\n",
      "243/388, train_loss: 0.1316, step time: 0.4983\n",
      "244/388, train_loss: 0.4635, step time: 0.4952\n",
      "245/388, train_loss: 0.1363, step time: 0.4803\n",
      "246/388, train_loss: 0.1794, step time: 0.4822\n",
      "247/388, train_loss: 0.1043, step time: 0.4923\n",
      "248/388, train_loss: 0.1383, step time: 0.4802\n",
      "249/388, train_loss: 0.1752, step time: 0.4844\n",
      "250/388, train_loss: 0.4748, step time: 0.4878\n",
      "251/388, train_loss: 0.1689, step time: 0.4808\n",
      "252/388, train_loss: 0.1083, step time: 1.1394\n",
      "253/388, train_loss: 0.1289, step time: 0.5556\n",
      "254/388, train_loss: 0.2082, step time: 0.5169\n",
      "255/388, train_loss: 0.1686, step time: 0.4919\n",
      "256/388, train_loss: 0.2222, step time: 0.4904\n",
      "257/388, train_loss: 0.2808, step time: 0.5008\n",
      "258/388, train_loss: 0.0944, step time: 0.5004\n",
      "259/388, train_loss: 0.2880, step time: 0.4986\n",
      "260/388, train_loss: 0.0753, step time: 0.4796\n",
      "261/388, train_loss: 0.0972, step time: 0.5009\n",
      "262/388, train_loss: 0.3147, step time: 0.5155\n",
      "263/388, train_loss: 0.3737, step time: 0.5119\n",
      "264/388, train_loss: 0.1901, step time: 0.4925\n",
      "265/388, train_loss: 0.1830, step time: 0.5091\n",
      "266/388, train_loss: 0.0909, step time: 0.5328\n",
      "267/388, train_loss: 0.3295, step time: 0.5270\n",
      "268/388, train_loss: 0.1001, step time: 0.5077\n",
      "269/388, train_loss: 0.1080, step time: 0.4933\n",
      "270/388, train_loss: 0.4145, step time: 0.4947\n",
      "271/388, train_loss: 0.1049, step time: 1.1434\n",
      "272/388, train_loss: 0.1479, step time: 0.5632\n",
      "273/388, train_loss: 0.1649, step time: 0.5246\n",
      "274/388, train_loss: 0.2292, step time: 0.5085\n",
      "275/388, train_loss: 0.2636, step time: 0.4892\n",
      "276/388, train_loss: 0.2042, step time: 0.4993\n",
      "277/388, train_loss: 0.1530, step time: 0.9119\n",
      "278/388, train_loss: 0.2917, step time: 0.5423\n",
      "279/388, train_loss: 0.2052, step time: 0.5174\n",
      "280/388, train_loss: 0.0943, step time: 0.4876\n",
      "281/388, train_loss: 0.1467, step time: 0.5340\n",
      "282/388, train_loss: 0.1158, step time: 0.5140\n",
      "283/388, train_loss: 0.3164, step time: 0.5085\n",
      "284/388, train_loss: 0.2272, step time: 0.4895\n",
      "285/388, train_loss: 0.1675, step time: 0.4921\n",
      "286/388, train_loss: 0.1729, step time: 0.4836\n",
      "287/388, train_loss: 0.0487, step time: 0.4786\n",
      "288/388, train_loss: 0.1282, step time: 0.4873\n",
      "289/388, train_loss: 0.1358, step time: 0.4748\n",
      "290/388, train_loss: 0.0884, step time: 1.0109\n",
      "291/388, train_loss: 0.1674, step time: 0.5516\n",
      "292/388, train_loss: 0.1512, step time: 0.5257\n",
      "293/388, train_loss: 0.1637, step time: 0.5061\n",
      "294/388, train_loss: 0.6311, step time: 0.4928\n",
      "295/388, train_loss: 0.1668, step time: 0.4948\n",
      "296/388, train_loss: 0.0953, step time: 0.5436\n",
      "297/388, train_loss: 0.1419, step time: 0.5268\n",
      "298/388, train_loss: 0.2142, step time: 0.5138\n",
      "299/388, train_loss: 0.4495, step time: 0.4891\n",
      "300/388, train_loss: 0.3221, step time: 0.4952\n",
      "301/388, train_loss: 0.1098, step time: 0.4814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/388, train_loss: 0.3117, step time: 0.4804\n",
      "303/388, train_loss: 0.1119, step time: 1.2109\n",
      "304/388, train_loss: 0.2002, step time: 0.5218\n",
      "305/388, train_loss: 0.5464, step time: 0.5083\n",
      "306/388, train_loss: 0.1609, step time: 0.4884\n",
      "307/388, train_loss: 0.3207, step time: 0.4972\n",
      "308/388, train_loss: 0.0862, step time: 0.4784\n",
      "309/388, train_loss: 0.0682, step time: 0.4796\n",
      "310/388, train_loss: 0.2532, step time: 0.4835\n",
      "311/388, train_loss: 0.0989, step time: 0.5175\n",
      "312/388, train_loss: 0.1785, step time: 0.5124\n",
      "313/388, train_loss: 0.2819, step time: 0.4876\n",
      "314/388, train_loss: 0.7689, step time: 0.4911\n",
      "315/388, train_loss: 0.4461, step time: 0.4777\n",
      "316/388, train_loss: 0.1037, step time: 0.4804\n",
      "317/388, train_loss: 0.1484, step time: 0.7389\n",
      "318/388, train_loss: 0.2923, step time: 0.5614\n",
      "319/388, train_loss: 0.3057, step time: 0.5176\n",
      "320/388, train_loss: 0.2123, step time: 0.4902\n",
      "321/388, train_loss: 0.2467, step time: 0.4851\n",
      "322/388, train_loss: 0.1951, step time: 0.4871\n",
      "323/388, train_loss: 0.0355, step time: 0.4792\n",
      "324/388, train_loss: 0.1276, step time: 0.5117\n",
      "325/388, train_loss: 0.2773, step time: 0.4934\n",
      "326/388, train_loss: 0.2918, step time: 0.5250\n",
      "327/388, train_loss: 0.2945, step time: 0.5118\n",
      "328/388, train_loss: 0.1047, step time: 0.5005\n",
      "329/388, train_loss: 0.1100, step time: 0.4801\n",
      "330/388, train_loss: 0.2560, step time: 0.4866\n",
      "331/388, train_loss: 0.2770, step time: 0.6438\n",
      "332/388, train_loss: 0.0957, step time: 0.5588\n",
      "333/388, train_loss: 0.3037, step time: 0.5327\n",
      "334/388, train_loss: 0.1988, step time: 0.5104\n",
      "335/388, train_loss: 0.3085, step time: 0.4930\n",
      "336/388, train_loss: 0.0473, step time: 0.4891\n",
      "337/388, train_loss: 0.2821, step time: 0.4780\n",
      "338/388, train_loss: 0.2226, step time: 0.4819\n",
      "339/388, train_loss: 0.0818, step time: 0.4951\n",
      "340/388, train_loss: 0.1094, step time: 0.8734\n",
      "341/388, train_loss: 0.2621, step time: 0.5500\n",
      "342/388, train_loss: 0.1501, step time: 0.5106\n",
      "343/388, train_loss: 0.0622, step time: 0.5037\n",
      "344/388, train_loss: 0.1590, step time: 0.4832\n",
      "345/388, train_loss: 0.0679, step time: 0.4971\n",
      "346/388, train_loss: 0.1422, step time: 0.4863\n",
      "347/388, train_loss: 0.0897, step time: 0.4936\n",
      "348/388, train_loss: 0.2530, step time: 0.4905\n",
      "349/388, train_loss: 0.2706, step time: 0.4787\n",
      "350/388, train_loss: 0.2328, step time: 0.4934\n",
      "351/388, train_loss: 0.1111, step time: 0.5182\n",
      "352/388, train_loss: 0.2064, step time: 0.6451\n",
      "353/388, train_loss: 0.0969, step time: 0.5739\n",
      "354/388, train_loss: 0.0704, step time: 0.5317\n",
      "355/388, train_loss: 0.1108, step time: 0.5081\n",
      "356/388, train_loss: 0.3082, step time: 0.4983\n",
      "357/388, train_loss: 0.2766, step time: 0.4860\n",
      "358/388, train_loss: 0.1767, step time: 0.4922\n",
      "359/388, train_loss: 0.3905, step time: 1.1711\n",
      "360/388, train_loss: 0.5701, step time: 0.5247\n",
      "361/388, train_loss: 0.0685, step time: 0.5051\n",
      "362/388, train_loss: 0.4791, step time: 0.4900\n",
      "363/388, train_loss: 0.0902, step time: 0.4907\n",
      "364/388, train_loss: 0.0554, step time: 0.4800\n",
      "365/388, train_loss: 0.1763, step time: 0.4774\n",
      "366/388, train_loss: 0.1736, step time: 0.4839\n",
      "367/388, train_loss: 0.1229, step time: 0.4751\n",
      "368/388, train_loss: 0.0873, step time: 0.4743\n",
      "369/388, train_loss: 0.2159, step time: 0.4870\n",
      "370/388, train_loss: 0.1961, step time: 0.4833\n",
      "371/388, train_loss: 0.2889, step time: 0.4986\n",
      "372/388, train_loss: 0.0512, step time: 0.4992\n",
      "373/388, train_loss: 0.2534, step time: 0.4929\n",
      "374/388, train_loss: 0.0353, step time: 0.4788\n",
      "375/388, train_loss: 0.1136, step time: 0.4800\n",
      "376/388, train_loss: 0.3029, step time: 0.4874\n",
      "377/388, train_loss: 0.0795, step time: 0.4815\n",
      "378/388, train_loss: 0.1537, step time: 0.4784\n",
      "379/388, train_loss: 0.2517, step time: 0.5056\n",
      "380/388, train_loss: 0.3377, step time: 0.4938\n",
      "381/388, train_loss: 0.1539, step time: 1.1809\n",
      "382/388, train_loss: 0.1062, step time: 0.5470\n",
      "383/388, train_loss: 0.2006, step time: 0.5256\n",
      "384/388, train_loss: 0.2337, step time: 0.4997\n",
      "385/388, train_loss: 0.1444, step time: 0.4930\n",
      "386/388, train_loss: 0.2292, step time: 0.4832\n",
      "387/388, train_loss: 0.1390, step time: 0.4734\n",
      "388/388, train_loss: 0.1320, step time: 0.5000\n",
      "epoch 41 average loss: 0.1994\n",
      "current epoch: 41 current mean dice: 0.7314 tc: 0.7741 wt: 0.8652 et: 0.5550\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 41 is: 299.8578\n",
      "----------\n",
      "epoch 42/300\n",
      "1/388, train_loss: 0.2526, step time: 0.4744\n",
      "2/388, train_loss: 0.1375, step time: 0.4877\n",
      "3/388, train_loss: 0.2631, step time: 0.5140\n",
      "4/388, train_loss: 0.0810, step time: 0.5034\n",
      "5/388, train_loss: 0.1305, step time: 0.4872\n",
      "6/388, train_loss: 0.2401, step time: 1.1724\n",
      "7/388, train_loss: 0.3000, step time: 0.5297\n",
      "8/388, train_loss: 0.1998, step time: 0.5097\n",
      "9/388, train_loss: 0.1310, step time: 0.4944\n",
      "10/388, train_loss: 0.1912, step time: 0.4986\n",
      "11/388, train_loss: 0.2338, step time: 0.5059\n",
      "12/388, train_loss: 0.0963, step time: 0.5617\n",
      "13/388, train_loss: 0.3825, step time: 0.5327\n",
      "14/388, train_loss: 0.0828, step time: 0.4985\n",
      "15/388, train_loss: 0.0855, step time: 0.4807\n",
      "16/388, train_loss: 0.2667, step time: 1.1381\n",
      "17/388, train_loss: 0.1035, step time: 0.5366\n",
      "18/388, train_loss: 0.1779, step time: 0.5116\n",
      "19/388, train_loss: 0.0914, step time: 0.4874\n",
      "20/388, train_loss: 0.1965, step time: 0.4802\n",
      "21/388, train_loss: 0.3517, step time: 0.4853\n",
      "22/388, train_loss: 0.1534, step time: 0.5523\n",
      "23/388, train_loss: 0.1510, step time: 0.5402\n",
      "24/388, train_loss: 0.1219, step time: 0.5095\n",
      "25/388, train_loss: 0.2735, step time: 0.4937\n",
      "26/388, train_loss: 0.0994, step time: 0.9592\n",
      "27/388, train_loss: 0.5120, step time: 0.5631\n",
      "28/388, train_loss: 0.1951, step time: 0.5161\n",
      "29/388, train_loss: 0.2023, step time: 0.5027\n",
      "30/388, train_loss: 0.1641, step time: 0.4885\n",
      "31/388, train_loss: 0.1957, step time: 0.4797\n",
      "32/388, train_loss: 0.1184, step time: 0.5334\n",
      "33/388, train_loss: 0.0703, step time: 0.5095\n",
      "34/388, train_loss: 0.2358, step time: 0.5208\n",
      "35/388, train_loss: 0.5803, step time: 0.5165\n",
      "36/388, train_loss: 0.1573, step time: 0.4997\n",
      "37/388, train_loss: 0.1911, step time: 0.4872\n",
      "38/388, train_loss: 0.2944, step time: 0.4824\n",
      "39/388, train_loss: 0.1694, step time: 0.4859\n",
      "40/388, train_loss: 0.2010, step time: 1.2216\n",
      "41/388, train_loss: 0.1021, step time: 0.5291\n",
      "42/388, train_loss: 0.0874, step time: 0.5010\n",
      "43/388, train_loss: 0.1770, step time: 0.4974\n",
      "44/388, train_loss: 0.5413, step time: 0.4881\n",
      "45/388, train_loss: 0.0640, step time: 0.4913\n",
      "46/388, train_loss: 0.1221, step time: 0.4771\n",
      "47/388, train_loss: 0.1457, step time: 0.4753\n",
      "48/388, train_loss: 0.3168, step time: 0.4886\n",
      "49/388, train_loss: 0.1223, step time: 0.5282\n",
      "50/388, train_loss: 0.0875, step time: 0.5215\n",
      "51/388, train_loss: 0.3170, step time: 0.5040\n",
      "52/388, train_loss: 0.1025, step time: 0.4957\n",
      "53/388, train_loss: 0.1279, step time: 0.4946\n",
      "54/388, train_loss: 0.1889, step time: 0.5051\n",
      "55/388, train_loss: 0.0714, step time: 0.4995\n",
      "56/388, train_loss: 0.1306, step time: 0.4849\n",
      "57/388, train_loss: 0.1831, step time: 0.4944\n",
      "58/388, train_loss: 0.0902, step time: 0.5703\n",
      "59/388, train_loss: 0.2288, step time: 0.5357\n",
      "60/388, train_loss: 0.4776, step time: 0.5026\n",
      "61/388, train_loss: 0.5992, step time: 0.4937\n",
      "62/388, train_loss: 0.2304, step time: 0.9418\n",
      "63/388, train_loss: 0.3300, step time: 0.5483\n",
      "64/388, train_loss: 0.0711, step time: 0.5270\n",
      "65/388, train_loss: 0.0780, step time: 0.5018\n",
      "66/388, train_loss: 0.1296, step time: 0.5477\n",
      "67/388, train_loss: 0.1136, step time: 0.5143\n",
      "68/388, train_loss: 0.1015, step time: 0.4985\n",
      "69/388, train_loss: 0.0301, step time: 0.4920\n",
      "70/388, train_loss: 0.4619, step time: 0.4984\n",
      "71/388, train_loss: 0.0843, step time: 0.4962\n",
      "72/388, train_loss: 0.0904, step time: 0.5138\n",
      "73/388, train_loss: 0.2127, step time: 0.5031\n",
      "74/388, train_loss: 0.2396, step time: 0.5002\n",
      "75/388, train_loss: 0.2713, step time: 0.4867\n",
      "76/388, train_loss: 0.2187, step time: 0.4874\n",
      "77/388, train_loss: 0.2743, step time: 0.4743\n",
      "78/388, train_loss: 0.1417, step time: 0.6579\n",
      "79/388, train_loss: 0.1036, step time: 0.5406\n",
      "80/388, train_loss: 0.1083, step time: 0.5165\n",
      "81/388, train_loss: 0.3973, step time: 0.4939\n",
      "82/388, train_loss: 0.1439, step time: 0.4951\n",
      "83/388, train_loss: 0.1079, step time: 0.4818\n",
      "84/388, train_loss: 0.1235, step time: 0.4870\n",
      "85/388, train_loss: 0.2170, step time: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/388, train_loss: 0.0992, step time: 0.5520\n",
      "87/388, train_loss: 0.3564, step time: 0.5138\n",
      "88/388, train_loss: 0.0686, step time: 0.4928\n",
      "89/388, train_loss: 0.0982, step time: 0.4869\n",
      "90/388, train_loss: 0.1974, step time: 0.4918\n",
      "91/388, train_loss: 0.2769, step time: 0.4787\n",
      "92/388, train_loss: 0.1071, step time: 0.6431\n",
      "93/388, train_loss: 0.0823, step time: 0.5588\n",
      "94/388, train_loss: 0.2109, step time: 0.5255\n",
      "95/388, train_loss: 0.0873, step time: 0.5070\n",
      "96/388, train_loss: 0.2186, step time: 0.5007\n",
      "97/388, train_loss: 0.1945, step time: 0.4887\n",
      "98/388, train_loss: 0.0897, step time: 1.1681\n",
      "99/388, train_loss: 0.1117, step time: 0.5373\n",
      "100/388, train_loss: 0.1200, step time: 0.5147\n",
      "101/388, train_loss: 0.2595, step time: 0.4952\n",
      "102/388, train_loss: 0.1809, step time: 0.4993\n",
      "103/388, train_loss: 0.1791, step time: 0.4804\n",
      "104/388, train_loss: 0.1240, step time: 0.4984\n",
      "105/388, train_loss: 0.2170, step time: 0.4894\n",
      "106/388, train_loss: 0.3961, step time: 0.4938\n",
      "107/388, train_loss: 0.1557, step time: 0.4838\n",
      "108/388, train_loss: 0.2287, step time: 1.0479\n",
      "109/388, train_loss: 0.4236, step time: 0.5316\n",
      "110/388, train_loss: 0.1482, step time: 0.5115\n",
      "111/388, train_loss: 0.2809, step time: 0.4831\n",
      "112/388, train_loss: 0.0607, step time: 0.4862\n",
      "113/388, train_loss: 0.0964, step time: 0.5310\n",
      "114/388, train_loss: 0.3507, step time: 0.5083\n",
      "115/388, train_loss: 0.1448, step time: 0.4885\n",
      "116/388, train_loss: 0.1522, step time: 0.4882\n",
      "117/388, train_loss: 0.1583, step time: 0.4797\n",
      "118/388, train_loss: 0.2109, step time: 0.5208\n",
      "119/388, train_loss: 0.0448, step time: 0.5376\n",
      "120/388, train_loss: 0.2008, step time: 0.5196\n",
      "121/388, train_loss: 0.3566, step time: 0.5103\n",
      "122/388, train_loss: 0.1592, step time: 0.5029\n",
      "123/388, train_loss: 0.4055, step time: 0.5226\n",
      "124/388, train_loss: 0.1661, step time: 0.5097\n",
      "125/388, train_loss: 0.1369, step time: 0.5013\n",
      "126/388, train_loss: 0.4147, step time: 0.5013\n",
      "127/388, train_loss: 0.4770, step time: 0.4833\n",
      "128/388, train_loss: 0.0844, step time: 0.4895\n",
      "129/388, train_loss: 0.1156, step time: 0.4924\n",
      "130/388, train_loss: 0.1906, step time: 0.5067\n",
      "131/388, train_loss: 0.4346, step time: 0.5000\n",
      "132/388, train_loss: 0.1506, step time: 0.4835\n",
      "133/388, train_loss: 0.3338, step time: 0.4844\n",
      "134/388, train_loss: 0.1317, step time: 0.5056\n",
      "135/388, train_loss: 0.2890, step time: 0.4895\n",
      "136/388, train_loss: 0.0917, step time: 0.4989\n",
      "137/388, train_loss: 0.1409, step time: 0.4847\n",
      "138/388, train_loss: 0.2777, step time: 0.4931\n",
      "139/388, train_loss: 0.2871, step time: 0.5027\n",
      "140/388, train_loss: 0.2487, step time: 0.4838\n",
      "141/388, train_loss: 0.2342, step time: 0.4956\n",
      "142/388, train_loss: 0.1966, step time: 0.4964\n",
      "143/388, train_loss: 0.0667, step time: 0.9015\n",
      "144/388, train_loss: 0.2251, step time: 0.5543\n",
      "145/388, train_loss: 0.0993, step time: 0.5214\n",
      "146/388, train_loss: 0.0930, step time: 0.5097\n",
      "147/388, train_loss: 0.0494, step time: 0.5885\n",
      "148/388, train_loss: 0.1647, step time: 0.5366\n",
      "149/388, train_loss: 0.0599, step time: 0.5157\n",
      "150/388, train_loss: 0.1610, step time: 0.4929\n",
      "151/388, train_loss: 0.1008, step time: 0.5047\n",
      "152/388, train_loss: 0.1345, step time: 0.4877\n",
      "153/388, train_loss: 0.3258, step time: 0.5092\n",
      "154/388, train_loss: 0.1858, step time: 0.5966\n",
      "155/388, train_loss: 0.1072, step time: 0.5354\n",
      "156/388, train_loss: 0.1708, step time: 0.5222\n",
      "157/388, train_loss: 0.2205, step time: 0.5018\n",
      "158/388, train_loss: 0.1863, step time: 0.4976\n",
      "159/388, train_loss: 0.2874, step time: 0.4830\n",
      "160/388, train_loss: 0.0939, step time: 0.4944\n",
      "161/388, train_loss: 0.1504, step time: 0.4840\n",
      "162/388, train_loss: 0.2234, step time: 1.0475\n",
      "163/388, train_loss: 0.2775, step time: 0.5383\n",
      "164/388, train_loss: 0.1638, step time: 0.5081\n",
      "165/388, train_loss: 0.1382, step time: 0.4985\n",
      "166/388, train_loss: 0.3907, step time: 0.4973\n",
      "167/388, train_loss: 0.0904, step time: 0.5142\n",
      "168/388, train_loss: 0.3802, step time: 0.6005\n",
      "169/388, train_loss: 0.2007, step time: 0.5410\n",
      "170/388, train_loss: 0.2069, step time: 0.5253\n",
      "171/388, train_loss: 0.3899, step time: 0.5011\n",
      "172/388, train_loss: 0.3944, step time: 0.5008\n",
      "173/388, train_loss: 0.1411, step time: 0.4841\n",
      "174/388, train_loss: 0.1512, step time: 0.5085\n",
      "175/388, train_loss: 0.1952, step time: 0.4849\n",
      "176/388, train_loss: 0.2999, step time: 0.5229\n",
      "177/388, train_loss: 0.1052, step time: 0.5032\n",
      "178/388, train_loss: 0.2337, step time: 0.5037\n",
      "179/388, train_loss: 0.2959, step time: 0.4901\n",
      "180/388, train_loss: 0.2152, step time: 0.4971\n",
      "181/388, train_loss: 0.1577, step time: 1.1472\n",
      "182/388, train_loss: 0.0829, step time: 0.5387\n",
      "183/388, train_loss: 0.1697, step time: 0.5070\n",
      "184/388, train_loss: 0.4271, step time: 0.4889\n",
      "185/388, train_loss: 0.1303, step time: 1.1175\n",
      "186/388, train_loss: 0.5383, step time: 0.5387\n",
      "187/388, train_loss: 0.2556, step time: 0.4987\n",
      "188/388, train_loss: 0.1356, step time: 0.4931\n",
      "189/388, train_loss: 0.1739, step time: 0.4791\n",
      "190/388, train_loss: 0.4315, step time: 0.4767\n",
      "191/388, train_loss: 0.0969, step time: 0.4880\n",
      "192/388, train_loss: 0.1173, step time: 0.5028\n",
      "193/388, train_loss: 0.1273, step time: 0.4985\n",
      "194/388, train_loss: 0.0527, step time: 1.1134\n",
      "195/388, train_loss: 0.1253, step time: 0.5169\n",
      "196/388, train_loss: 0.1764, step time: 0.5019\n",
      "197/388, train_loss: 0.5203, step time: 0.4845\n",
      "198/388, train_loss: 0.2418, step time: 0.5035\n",
      "199/388, train_loss: 0.1714, step time: 0.4912\n",
      "200/388, train_loss: 0.1519, step time: 0.4835\n",
      "201/388, train_loss: 0.2093, step time: 0.4868\n",
      "202/388, train_loss: 0.1705, step time: 0.7980\n",
      "203/388, train_loss: 0.2181, step time: 0.5383\n",
      "204/388, train_loss: 0.0614, step time: 0.5219\n",
      "205/388, train_loss: 0.1082, step time: 0.5024\n",
      "206/388, train_loss: 0.1415, step time: 0.4977\n",
      "207/388, train_loss: 0.3683, step time: 0.4848\n",
      "208/388, train_loss: 0.2498, step time: 0.5114\n",
      "209/388, train_loss: 0.1939, step time: 0.4922\n",
      "210/388, train_loss: 0.0867, step time: 0.4970\n",
      "211/388, train_loss: 0.0535, step time: 0.4895\n",
      "212/388, train_loss: 0.1053, step time: 0.4841\n",
      "213/388, train_loss: 0.1413, step time: 0.5003\n",
      "214/388, train_loss: 0.3952, step time: 0.5729\n",
      "215/388, train_loss: 0.2441, step time: 0.5395\n",
      "216/388, train_loss: 0.0928, step time: 0.5152\n",
      "217/388, train_loss: 0.1461, step time: 0.4901\n",
      "218/388, train_loss: 0.2741, step time: 0.4930\n",
      "219/388, train_loss: 0.2219, step time: 0.4804\n",
      "220/388, train_loss: 0.1287, step time: 0.5205\n",
      "221/388, train_loss: 0.2293, step time: 0.5238\n",
      "222/388, train_loss: 0.2885, step time: 0.5086\n",
      "223/388, train_loss: 0.2199, step time: 0.4865\n",
      "224/388, train_loss: 0.1505, step time: 0.4974\n",
      "225/388, train_loss: 0.3051, step time: 0.4909\n",
      "226/388, train_loss: 0.0930, step time: 0.5082\n",
      "227/388, train_loss: 0.6752, step time: 0.4944\n",
      "228/388, train_loss: 0.2643, step time: 0.5676\n",
      "229/388, train_loss: 0.0740, step time: 0.5418\n",
      "230/388, train_loss: 0.3047, step time: 0.5266\n",
      "231/388, train_loss: 0.1897, step time: 0.4893\n",
      "232/388, train_loss: 0.0529, step time: 0.5063\n",
      "233/388, train_loss: 0.0721, step time: 0.5018\n",
      "234/388, train_loss: 0.0701, step time: 0.4812\n",
      "235/388, train_loss: 0.2389, step time: 0.4852\n",
      "236/388, train_loss: 0.1898, step time: 0.4769\n",
      "237/388, train_loss: 0.1341, step time: 1.0507\n",
      "238/388, train_loss: 0.2084, step time: 0.5429\n",
      "239/388, train_loss: 0.2268, step time: 0.5281\n",
      "240/388, train_loss: 0.1290, step time: 0.4971\n",
      "241/388, train_loss: 0.1862, step time: 0.4952\n",
      "242/388, train_loss: 0.1877, step time: 0.4932\n",
      "243/388, train_loss: 0.0895, step time: 0.4745\n",
      "244/388, train_loss: 0.2197, step time: 0.4815\n",
      "245/388, train_loss: 0.1935, step time: 0.9513\n",
      "246/388, train_loss: 0.2291, step time: 0.5390\n",
      "247/388, train_loss: 0.0701, step time: 0.5098\n",
      "248/388, train_loss: 0.2043, step time: 0.5013\n",
      "249/388, train_loss: 0.3079, step time: 0.4914\n",
      "250/388, train_loss: 0.2336, step time: 0.4949\n",
      "251/388, train_loss: 0.1146, step time: 0.4775\n",
      "252/388, train_loss: 0.0691, step time: 0.4936\n",
      "253/388, train_loss: 0.3914, step time: 0.4963\n",
      "254/388, train_loss: 0.1442, step time: 1.0290\n",
      "255/388, train_loss: 0.2076, step time: 0.5375\n",
      "256/388, train_loss: 0.2230, step time: 0.5087\n",
      "257/388, train_loss: 0.1356, step time: 0.5005\n",
      "258/388, train_loss: 0.2734, step time: 0.4846\n",
      "259/388, train_loss: 0.1229, step time: 0.4963\n",
      "260/388, train_loss: 0.4966, step time: 0.4891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/388, train_loss: 0.3157, step time: 0.4874\n",
      "262/388, train_loss: 0.2038, step time: 1.1375\n",
      "263/388, train_loss: 0.1094, step time: 0.5317\n",
      "264/388, train_loss: 0.1054, step time: 0.5150\n",
      "265/388, train_loss: 0.0981, step time: 0.4914\n",
      "266/388, train_loss: 0.2671, step time: 0.4992\n",
      "267/388, train_loss: 0.2929, step time: 0.4861\n",
      "268/388, train_loss: 0.2054, step time: 0.4792\n",
      "269/388, train_loss: 0.3312, step time: 0.4921\n",
      "270/388, train_loss: 0.1067, step time: 0.4916\n",
      "271/388, train_loss: 0.2422, step time: 0.4789\n",
      "272/388, train_loss: 0.1732, step time: 0.5294\n",
      "273/388, train_loss: 0.2797, step time: 0.4934\n",
      "274/388, train_loss: 0.0637, step time: 0.4951\n",
      "275/388, train_loss: 0.4360, step time: 0.4938\n",
      "276/388, train_loss: 0.2300, step time: 0.4844\n",
      "277/388, train_loss: 0.2448, step time: 0.5181\n",
      "278/388, train_loss: 0.1878, step time: 0.5016\n",
      "279/388, train_loss: 0.1173, step time: 0.4927\n",
      "280/388, train_loss: 0.1348, step time: 1.0062\n",
      "281/388, train_loss: 0.2762, step time: 0.5395\n",
      "282/388, train_loss: 0.0362, step time: 0.4988\n",
      "283/388, train_loss: 0.1983, step time: 0.4970\n",
      "284/388, train_loss: 0.2015, step time: 0.4878\n",
      "285/388, train_loss: 0.0538, step time: 0.4784\n",
      "286/388, train_loss: 0.2553, step time: 1.0963\n",
      "287/388, train_loss: 0.3093, step time: 0.5345\n",
      "288/388, train_loss: 0.1734, step time: 0.5141\n",
      "289/388, train_loss: 0.2313, step time: 0.4977\n",
      "290/388, train_loss: 0.1094, step time: 0.4853\n",
      "291/388, train_loss: 0.0927, step time: 0.4865\n",
      "292/388, train_loss: 0.1690, step time: 0.4799\n",
      "293/388, train_loss: 0.1157, step time: 0.6845\n",
      "294/388, train_loss: 0.2536, step time: 0.5476\n",
      "295/388, train_loss: 0.1365, step time: 0.5267\n",
      "296/388, train_loss: 0.1715, step time: 0.5098\n",
      "297/388, train_loss: 0.2110, step time: 0.4872\n",
      "298/388, train_loss: 0.1721, step time: 0.4828\n",
      "299/388, train_loss: 0.6396, step time: 0.4862\n",
      "300/388, train_loss: 0.1103, step time: 0.5013\n",
      "301/388, train_loss: 0.1003, step time: 0.5133\n",
      "302/388, train_loss: 0.0960, step time: 0.5003\n",
      "303/388, train_loss: 0.0679, step time: 0.4793\n",
      "304/388, train_loss: 0.4479, step time: 0.5143\n",
      "305/388, train_loss: 0.0612, step time: 0.5014\n",
      "306/388, train_loss: 0.2272, step time: 0.5480\n",
      "307/388, train_loss: 0.1415, step time: 0.5213\n",
      "308/388, train_loss: 0.3046, step time: 0.5046\n",
      "309/388, train_loss: 0.3020, step time: 0.5035\n",
      "310/388, train_loss: 0.3799, step time: 0.5093\n",
      "311/388, train_loss: 0.1038, step time: 0.5093\n",
      "312/388, train_loss: 0.2881, step time: 0.5354\n",
      "313/388, train_loss: 0.2837, step time: 0.5567\n",
      "314/388, train_loss: 0.2531, step time: 0.5399\n",
      "315/388, train_loss: 0.2407, step time: 0.5093\n",
      "316/388, train_loss: 0.0737, step time: 0.4999\n",
      "317/388, train_loss: 0.2400, step time: 0.4843\n",
      "318/388, train_loss: 0.1062, step time: 1.0185\n",
      "319/388, train_loss: 0.1785, step time: 0.5407\n",
      "320/388, train_loss: 0.1549, step time: 0.5159\n",
      "321/388, train_loss: 0.1195, step time: 0.4973\n",
      "322/388, train_loss: 0.2216, step time: 0.4960\n",
      "323/388, train_loss: 0.1708, step time: 0.4837\n",
      "324/388, train_loss: 0.2171, step time: 0.4915\n",
      "325/388, train_loss: 0.0789, step time: 1.0262\n",
      "326/388, train_loss: 0.1662, step time: 0.5454\n",
      "327/388, train_loss: 0.2779, step time: 0.5126\n",
      "328/388, train_loss: 0.1822, step time: 0.4926\n",
      "329/388, train_loss: 0.2007, step time: 0.4989\n",
      "330/388, train_loss: 0.2622, step time: 0.4956\n",
      "331/388, train_loss: 0.1533, step time: 0.4973\n",
      "332/388, train_loss: 0.3551, step time: 0.4831\n",
      "333/388, train_loss: 0.0929, step time: 0.4965\n",
      "334/388, train_loss: 0.1223, step time: 0.4865\n",
      "335/388, train_loss: 0.1321, step time: 0.4862\n",
      "336/388, train_loss: 0.2092, step time: 0.4928\n",
      "337/388, train_loss: 0.4073, step time: 0.5174\n",
      "338/388, train_loss: 0.2656, step time: 0.5602\n",
      "339/388, train_loss: 0.1518, step time: 0.5317\n",
      "340/388, train_loss: 0.0700, step time: 0.5086\n",
      "341/388, train_loss: 0.3567, step time: 0.4896\n",
      "342/388, train_loss: 0.0835, step time: 0.4831\n",
      "343/388, train_loss: 0.1188, step time: 0.9849\n",
      "344/388, train_loss: 0.2509, step time: 0.5474\n",
      "345/388, train_loss: 0.2887, step time: 0.5123\n",
      "346/388, train_loss: 0.1126, step time: 0.4978\n",
      "347/388, train_loss: 0.0996, step time: 0.4952\n",
      "348/388, train_loss: 0.4400, step time: 0.4936\n",
      "349/388, train_loss: 0.2716, step time: 1.1614\n",
      "350/388, train_loss: 0.1036, step time: 0.5387\n",
      "351/388, train_loss: 0.1397, step time: 0.5138\n",
      "352/388, train_loss: 0.2759, step time: 0.4985\n",
      "353/388, train_loss: 0.1448, step time: 0.4855\n",
      "354/388, train_loss: 0.3191, step time: 1.2201\n",
      "355/388, train_loss: 0.1961, step time: 0.5328\n",
      "356/388, train_loss: 0.1440, step time: 0.5027\n",
      "357/388, train_loss: 0.0296, step time: 0.4837\n",
      "358/388, train_loss: 0.0457, step time: 0.4895\n",
      "359/388, train_loss: 0.1058, step time: 0.4771\n",
      "360/388, train_loss: 0.2356, step time: 0.4752\n",
      "361/388, train_loss: 0.0949, step time: 0.4752\n",
      "362/388, train_loss: 0.0876, step time: 0.4774\n",
      "363/388, train_loss: 0.1060, step time: 1.0414\n",
      "364/388, train_loss: 0.1448, step time: 0.5397\n",
      "365/388, train_loss: 0.4172, step time: 0.5199\n",
      "366/388, train_loss: 0.2820, step time: 0.5010\n",
      "367/388, train_loss: 0.1080, step time: 0.4948\n",
      "368/388, train_loss: 0.0798, step time: 0.4928\n",
      "369/388, train_loss: 0.1273, step time: 0.4845\n",
      "370/388, train_loss: 0.2307, step time: 0.4858\n",
      "371/388, train_loss: 0.0982, step time: 0.4718\n",
      "372/388, train_loss: 0.1484, step time: 0.4770\n",
      "373/388, train_loss: 0.3575, step time: 0.5020\n",
      "374/388, train_loss: 0.1173, step time: 0.5014\n",
      "375/388, train_loss: 0.5878, step time: 0.4897\n",
      "376/388, train_loss: 0.1019, step time: 0.5191\n",
      "377/388, train_loss: 0.2535, step time: 0.4999\n",
      "378/388, train_loss: 0.2000, step time: 0.4920\n",
      "379/388, train_loss: 0.1021, step time: 0.4756\n",
      "380/388, train_loss: 0.2020, step time: 0.4911\n",
      "381/388, train_loss: 0.2087, step time: 0.9343\n",
      "382/388, train_loss: 0.1974, step time: 0.5582\n",
      "383/388, train_loss: 0.1048, step time: 0.5170\n",
      "384/388, train_loss: 0.0733, step time: 0.4965\n",
      "385/388, train_loss: 0.0964, step time: 0.4959\n",
      "386/388, train_loss: 0.1365, step time: 0.4807\n",
      "387/388, train_loss: 0.1273, step time: 0.4783\n",
      "388/388, train_loss: 0.0912, step time: 0.4837\n",
      "epoch 42 average loss: 0.1969\n",
      "current epoch: 42 current mean dice: 0.7551 tc: 0.8094 wt: 0.8922 et: 0.5638\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 42 is: 300.8088\n",
      "----------\n",
      "epoch 43/300\n",
      "1/388, train_loss: 0.4428, step time: 0.4760\n",
      "2/388, train_loss: 0.1209, step time: 0.4849\n",
      "3/388, train_loss: 0.0927, step time: 0.5230\n",
      "4/388, train_loss: 0.1609, step time: 0.5008\n",
      "5/388, train_loss: 0.2461, step time: 1.0886\n",
      "6/388, train_loss: 0.2337, step time: 0.5557\n",
      "7/388, train_loss: 0.0792, step time: 0.5268\n",
      "8/388, train_loss: 0.1439, step time: 0.5092\n",
      "9/388, train_loss: 0.1221, step time: 0.5127\n",
      "10/388, train_loss: 0.3682, step time: 0.4991\n",
      "11/388, train_loss: 0.4434, step time: 0.5631\n",
      "12/388, train_loss: 0.1449, step time: 0.5221\n",
      "13/388, train_loss: 0.0915, step time: 0.5005\n",
      "14/388, train_loss: 0.0801, step time: 0.5171\n",
      "15/388, train_loss: 0.0931, step time: 0.5317\n",
      "16/388, train_loss: 0.2244, step time: 0.5027\n",
      "17/388, train_loss: 0.1103, step time: 0.5165\n",
      "18/388, train_loss: 0.2433, step time: 0.5406\n",
      "19/388, train_loss: 0.4821, step time: 0.5780\n",
      "20/388, train_loss: 0.2037, step time: 0.5321\n",
      "21/388, train_loss: 0.4119, step time: 0.5011\n",
      "22/388, train_loss: 0.3894, step time: 0.6239\n",
      "23/388, train_loss: 0.3244, step time: 0.6043\n",
      "24/388, train_loss: 0.2483, step time: 0.5575\n",
      "25/388, train_loss: 0.1464, step time: 0.5366\n",
      "26/388, train_loss: 0.1279, step time: 0.6102\n",
      "27/388, train_loss: 0.2576, step time: 0.6042\n",
      "28/388, train_loss: 0.1108, step time: 0.5546\n",
      "29/388, train_loss: 0.2004, step time: 0.5219\n",
      "30/388, train_loss: 0.1205, step time: 0.4979\n",
      "31/388, train_loss: 0.2287, step time: 0.4894\n",
      "32/388, train_loss: 0.0945, step time: 1.0646\n",
      "33/388, train_loss: 0.1582, step time: 0.5433\n",
      "34/388, train_loss: 0.0867, step time: 0.5184\n",
      "35/388, train_loss: 0.2030, step time: 0.6034\n",
      "36/388, train_loss: 0.1798, step time: 0.5525\n",
      "37/388, train_loss: 0.1816, step time: 0.5232\n",
      "38/388, train_loss: 0.2659, step time: 0.5060\n",
      "39/388, train_loss: 0.1659, step time: 0.5018\n",
      "40/388, train_loss: 0.1361, step time: 0.5200\n",
      "41/388, train_loss: 0.3697, step time: 0.5185\n",
      "42/388, train_loss: 0.0510, step time: 0.5321\n",
      "43/388, train_loss: 0.4302, step time: 0.5305\n",
      "44/388, train_loss: 0.1475, step time: 0.5488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/388, train_loss: 0.0762, step time: 0.5218\n",
      "46/388, train_loss: 0.0870, step time: 0.5005\n",
      "47/388, train_loss: 0.0570, step time: 1.0599\n",
      "48/388, train_loss: 0.1304, step time: 0.5478\n",
      "49/388, train_loss: 0.0408, step time: 0.5266\n",
      "50/388, train_loss: 0.1808, step time: 0.5194\n",
      "51/388, train_loss: 0.0897, step time: 0.5784\n",
      "52/388, train_loss: 0.1268, step time: 0.5489\n",
      "53/388, train_loss: 0.1298, step time: 0.5306\n",
      "54/388, train_loss: 0.2160, step time: 0.5076\n",
      "55/388, train_loss: 0.1598, step time: 0.4997\n",
      "56/388, train_loss: 0.1873, step time: 0.4836\n",
      "57/388, train_loss: 0.0875, step time: 0.5011\n",
      "58/388, train_loss: 0.3754, step time: 0.9794\n",
      "59/388, train_loss: 0.1321, step time: 0.5270\n",
      "60/388, train_loss: 0.0951, step time: 0.5067\n",
      "61/388, train_loss: 0.2348, step time: 0.4982\n",
      "62/388, train_loss: 0.1113, step time: 0.4992\n",
      "63/388, train_loss: 0.2083, step time: 0.4906\n",
      "64/388, train_loss: 0.1021, step time: 0.4925\n",
      "65/388, train_loss: 0.3142, step time: 0.4863\n",
      "66/388, train_loss: 0.0910, step time: 1.1772\n",
      "67/388, train_loss: 0.1526, step time: 0.5438\n",
      "68/388, train_loss: 0.0725, step time: 0.5193\n",
      "69/388, train_loss: 0.2594, step time: 0.5114\n",
      "70/388, train_loss: 0.1302, step time: 0.5003\n",
      "71/388, train_loss: 0.0909, step time: 0.5008\n",
      "72/388, train_loss: 0.0947, step time: 0.4992\n",
      "73/388, train_loss: 0.2746, step time: 0.4880\n",
      "74/388, train_loss: 0.1568, step time: 1.2045\n",
      "75/388, train_loss: 0.1136, step time: 0.5429\n",
      "76/388, train_loss: 0.2007, step time: 0.5094\n",
      "77/388, train_loss: 0.2745, step time: 0.4997\n",
      "78/388, train_loss: 0.3848, step time: 0.4976\n",
      "79/388, train_loss: 0.1697, step time: 0.4840\n",
      "80/388, train_loss: 0.1387, step time: 0.4998\n",
      "81/388, train_loss: 0.2908, step time: 1.2268\n",
      "82/388, train_loss: 0.1586, step time: 0.5409\n",
      "83/388, train_loss: 0.0765, step time: 0.5267\n",
      "84/388, train_loss: 0.1315, step time: 0.5175\n",
      "85/388, train_loss: 0.2048, step time: 0.5231\n",
      "86/388, train_loss: 0.2461, step time: 0.4917\n",
      "87/388, train_loss: 0.3902, step time: 0.4829\n",
      "88/388, train_loss: 0.2118, step time: 0.5044\n",
      "89/388, train_loss: 0.4849, step time: 0.4981\n",
      "90/388, train_loss: 0.2240, step time: 0.4893\n",
      "91/388, train_loss: 0.2149, step time: 0.7406\n",
      "92/388, train_loss: 0.1320, step time: 0.5578\n",
      "93/388, train_loss: 0.0552, step time: 0.5214\n",
      "94/388, train_loss: 0.0702, step time: 0.5169\n",
      "95/388, train_loss: 0.0898, step time: 0.4882\n",
      "96/388, train_loss: 0.1714, step time: 0.4841\n",
      "97/388, train_loss: 0.1280, step time: 0.4814\n",
      "98/388, train_loss: 0.2517, step time: 0.5044\n",
      "99/388, train_loss: 0.1861, step time: 0.4899\n",
      "100/388, train_loss: 0.0425, step time: 0.4990\n",
      "101/388, train_loss: 0.1145, step time: 0.5448\n",
      "102/388, train_loss: 0.0788, step time: 0.5158\n",
      "103/388, train_loss: 0.5850, step time: 0.4897\n",
      "104/388, train_loss: 0.0280, step time: 0.9646\n",
      "105/388, train_loss: 0.0929, step time: 0.5599\n",
      "106/388, train_loss: 0.1718, step time: 0.5253\n",
      "107/388, train_loss: 0.2100, step time: 0.4909\n",
      "108/388, train_loss: 0.2859, step time: 0.5216\n",
      "109/388, train_loss: 0.3681, step time: 0.5917\n",
      "110/388, train_loss: 0.3599, step time: 0.5425\n",
      "111/388, train_loss: 0.1734, step time: 0.5130\n",
      "112/388, train_loss: 0.1541, step time: 0.4835\n",
      "113/388, train_loss: 0.1448, step time: 0.4914\n",
      "114/388, train_loss: 0.1846, step time: 0.4751\n",
      "115/388, train_loss: 0.1827, step time: 0.9416\n",
      "116/388, train_loss: 0.0839, step time: 0.5321\n",
      "117/388, train_loss: 0.3191, step time: 0.4981\n",
      "118/388, train_loss: 0.4585, step time: 0.5222\n",
      "119/388, train_loss: 0.1027, step time: 0.5743\n",
      "120/388, train_loss: 0.0699, step time: 0.5384\n",
      "121/388, train_loss: 0.1245, step time: 0.5148\n",
      "122/388, train_loss: 0.1479, step time: 0.4956\n",
      "123/388, train_loss: 0.3009, step time: 0.5500\n",
      "124/388, train_loss: 0.1542, step time: 0.6252\n",
      "125/388, train_loss: 0.0819, step time: 0.5421\n",
      "126/388, train_loss: 0.4398, step time: 0.5155\n",
      "127/388, train_loss: 0.0958, step time: 0.4993\n",
      "128/388, train_loss: 0.2978, step time: 0.5868\n",
      "129/388, train_loss: 0.1061, step time: 0.5292\n",
      "130/388, train_loss: 0.2887, step time: 0.5102\n",
      "131/388, train_loss: 0.1003, step time: 0.5023\n",
      "132/388, train_loss: 0.3123, step time: 0.5400\n",
      "133/388, train_loss: 0.4136, step time: 0.5599\n",
      "134/388, train_loss: 0.1143, step time: 0.6584\n",
      "135/388, train_loss: 0.0588, step time: 0.5379\n",
      "136/388, train_loss: 0.2431, step time: 0.5138\n",
      "137/388, train_loss: 0.1201, step time: 0.4959\n",
      "138/388, train_loss: 0.1915, step time: 0.4909\n",
      "139/388, train_loss: 0.1199, step time: 0.5013\n",
      "140/388, train_loss: 0.2565, step time: 0.4995\n",
      "141/388, train_loss: 0.2082, step time: 0.4856\n",
      "142/388, train_loss: 0.1150, step time: 0.8980\n",
      "143/388, train_loss: 0.0966, step time: 0.5581\n",
      "144/388, train_loss: 0.0616, step time: 0.5236\n",
      "145/388, train_loss: 0.6065, step time: 0.5126\n",
      "146/388, train_loss: 0.1324, step time: 0.5000\n",
      "147/388, train_loss: 0.5978, step time: 0.5162\n",
      "148/388, train_loss: 0.2161, step time: 0.5009\n",
      "149/388, train_loss: 0.4288, step time: 0.4842\n",
      "150/388, train_loss: 0.1593, step time: 0.5306\n",
      "151/388, train_loss: 0.2285, step time: 0.4992\n",
      "152/388, train_loss: 0.3496, step time: 0.5121\n",
      "153/388, train_loss: 0.0659, step time: 0.6007\n",
      "154/388, train_loss: 0.1571, step time: 0.5456\n",
      "155/388, train_loss: 0.2050, step time: 0.5103\n",
      "156/388, train_loss: 0.1105, step time: 0.5094\n",
      "157/388, train_loss: 0.2125, step time: 0.4957\n",
      "158/388, train_loss: 0.1401, step time: 0.4952\n",
      "159/388, train_loss: 0.0834, step time: 1.1542\n",
      "160/388, train_loss: 0.0692, step time: 0.5473\n",
      "161/388, train_loss: 0.1527, step time: 0.5087\n",
      "162/388, train_loss: 0.3765, step time: 0.4896\n",
      "163/388, train_loss: 0.1508, step time: 0.4916\n",
      "164/388, train_loss: 0.1560, step time: 0.4764\n",
      "165/388, train_loss: 0.1243, step time: 0.4749\n",
      "166/388, train_loss: 0.1511, step time: 0.4855\n",
      "167/388, train_loss: 0.1125, step time: 1.0651\n",
      "168/388, train_loss: 0.1091, step time: 0.5460\n",
      "169/388, train_loss: 0.1021, step time: 0.5213\n",
      "170/388, train_loss: 0.0754, step time: 0.5084\n",
      "171/388, train_loss: 0.1310, step time: 0.5514\n",
      "172/388, train_loss: 0.1761, step time: 0.5233\n",
      "173/388, train_loss: 0.2369, step time: 0.5591\n",
      "174/388, train_loss: 0.1530, step time: 0.5370\n",
      "175/388, train_loss: 0.1875, step time: 0.5105\n",
      "176/388, train_loss: 0.0480, step time: 0.4918\n",
      "177/388, train_loss: 0.1407, step time: 0.4955\n",
      "178/388, train_loss: 0.2849, step time: 0.4819\n",
      "179/388, train_loss: 0.1769, step time: 0.5068\n",
      "180/388, train_loss: 0.2884, step time: 0.4921\n",
      "181/388, train_loss: 0.1204, step time: 0.8764\n",
      "182/388, train_loss: 0.2468, step time: 0.5594\n",
      "183/388, train_loss: 0.2067, step time: 0.5243\n",
      "184/388, train_loss: 0.2277, step time: 0.4966\n",
      "185/388, train_loss: 0.1001, step time: 0.4913\n",
      "186/388, train_loss: 0.2620, step time: 1.1370\n",
      "187/388, train_loss: 0.1996, step time: 0.5436\n",
      "188/388, train_loss: 0.2592, step time: 0.5155\n",
      "189/388, train_loss: 0.2564, step time: 0.5018\n",
      "190/388, train_loss: 0.1151, step time: 0.5002\n",
      "191/388, train_loss: 0.1881, step time: 0.4844\n",
      "192/388, train_loss: 0.0924, step time: 0.5069\n",
      "193/388, train_loss: 0.2263, step time: 0.4925\n",
      "194/388, train_loss: 0.0964, step time: 0.5007\n",
      "195/388, train_loss: 0.1503, step time: 0.5378\n",
      "196/388, train_loss: 0.2657, step time: 0.5179\n",
      "197/388, train_loss: 0.1432, step time: 0.5072\n",
      "198/388, train_loss: 0.1701, step time: 0.4986\n",
      "199/388, train_loss: 0.1796, step time: 0.4834\n",
      "200/388, train_loss: 0.1848, step time: 0.5035\n",
      "201/388, train_loss: 0.2581, step time: 0.5102\n",
      "202/388, train_loss: 0.2566, step time: 0.5829\n",
      "203/388, train_loss: 0.0896, step time: 0.5411\n",
      "204/388, train_loss: 0.0669, step time: 0.5140\n",
      "205/388, train_loss: 0.4237, step time: 0.4957\n",
      "206/388, train_loss: 0.1074, step time: 0.4860\n",
      "207/388, train_loss: 0.3358, step time: 0.4920\n",
      "208/388, train_loss: 0.2909, step time: 0.5033\n",
      "209/388, train_loss: 0.1093, step time: 0.5140\n",
      "210/388, train_loss: 0.2945, step time: 0.5847\n",
      "211/388, train_loss: 0.0696, step time: 0.5471\n",
      "212/388, train_loss: 0.1230, step time: 0.5212\n",
      "213/388, train_loss: 0.2118, step time: 0.5052\n",
      "214/388, train_loss: 0.1719, step time: 0.4828\n",
      "215/388, train_loss: 0.1734, step time: 1.1290\n",
      "216/388, train_loss: 0.3124, step time: 0.5403\n",
      "217/388, train_loss: 0.0951, step time: 0.5165\n",
      "218/388, train_loss: 0.1445, step time: 0.4952\n",
      "219/388, train_loss: 0.1228, step time: 0.4999\n",
      "220/388, train_loss: 0.2601, step time: 0.4857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/388, train_loss: 0.2771, step time: 1.1553\n",
      "222/388, train_loss: 0.3083, step time: 0.5484\n",
      "223/388, train_loss: 0.1456, step time: 0.5182\n",
      "224/388, train_loss: 0.1553, step time: 0.5055\n",
      "225/388, train_loss: 0.2138, step time: 0.4859\n",
      "226/388, train_loss: 0.2406, step time: 0.4908\n",
      "227/388, train_loss: 0.1960, step time: 0.4837\n",
      "228/388, train_loss: 0.2221, step time: 0.7072\n",
      "229/388, train_loss: 0.3005, step time: 0.5308\n",
      "230/388, train_loss: 0.2971, step time: 0.4958\n",
      "231/388, train_loss: 0.5765, step time: 0.4986\n",
      "232/388, train_loss: 0.4235, step time: 0.5052\n",
      "233/388, train_loss: 0.1496, step time: 0.5151\n",
      "234/388, train_loss: 0.3155, step time: 0.6423\n",
      "235/388, train_loss: 0.0926, step time: 0.5718\n",
      "236/388, train_loss: 0.1817, step time: 0.5306\n",
      "237/388, train_loss: 0.2050, step time: 0.5104\n",
      "238/388, train_loss: 0.1096, step time: 0.4910\n",
      "239/388, train_loss: 0.3748, step time: 0.5012\n",
      "240/388, train_loss: 0.1759, step time: 0.4999\n",
      "241/388, train_loss: 0.2666, step time: 0.5035\n",
      "242/388, train_loss: 0.1070, step time: 0.4917\n",
      "243/388, train_loss: 0.2389, step time: 0.5007\n",
      "244/388, train_loss: 0.2626, step time: 0.5525\n",
      "245/388, train_loss: 0.2370, step time: 0.5346\n",
      "246/388, train_loss: 0.2358, step time: 0.5091\n",
      "247/388, train_loss: 0.0680, step time: 0.4867\n",
      "248/388, train_loss: 0.0894, step time: 0.5262\n",
      "249/388, train_loss: 0.1561, step time: 0.5804\n",
      "250/388, train_loss: 0.4924, step time: 0.5435\n",
      "251/388, train_loss: 0.5641, step time: 0.5173\n",
      "252/388, train_loss: 0.2810, step time: 0.5020\n",
      "253/388, train_loss: 0.2156, step time: 0.5135\n",
      "254/388, train_loss: 0.0724, step time: 0.4930\n",
      "255/388, train_loss: 0.1372, step time: 0.5058\n",
      "256/388, train_loss: 0.4767, step time: 0.5169\n",
      "257/388, train_loss: 0.0977, step time: 0.5740\n",
      "258/388, train_loss: 0.1492, step time: 0.5331\n",
      "259/388, train_loss: 0.0402, step time: 0.5011\n",
      "260/388, train_loss: 0.1096, step time: 0.4970\n",
      "261/388, train_loss: 0.0984, step time: 0.4820\n",
      "262/388, train_loss: 0.0971, step time: 0.4850\n",
      "263/388, train_loss: 0.2292, step time: 0.4807\n",
      "264/388, train_loss: 0.3382, step time: 0.5145\n",
      "265/388, train_loss: 0.3173, step time: 0.5925\n",
      "266/388, train_loss: 0.0950, step time: 0.6006\n",
      "267/388, train_loss: 0.0333, step time: 0.5258\n",
      "268/388, train_loss: 0.1276, step time: 0.5001\n",
      "269/388, train_loss: 0.1231, step time: 0.5042\n",
      "270/388, train_loss: 0.1195, step time: 0.5734\n",
      "271/388, train_loss: 0.2987, step time: 0.5396\n",
      "272/388, train_loss: 0.2928, step time: 0.4948\n",
      "273/388, train_loss: 0.2264, step time: 0.5162\n",
      "274/388, train_loss: 0.0919, step time: 0.5386\n",
      "275/388, train_loss: 0.5000, step time: 0.5937\n",
      "276/388, train_loss: 0.0678, step time: 0.5217\n",
      "277/388, train_loss: 0.1374, step time: 0.5018\n",
      "278/388, train_loss: 0.2877, step time: 0.4976\n",
      "279/388, train_loss: 0.4322, step time: 1.1215\n",
      "280/388, train_loss: 0.2270, step time: 0.5435\n",
      "281/388, train_loss: 0.2437, step time: 0.5040\n",
      "282/388, train_loss: 0.2051, step time: 0.5031\n",
      "283/388, train_loss: 0.2160, step time: 0.4857\n",
      "284/388, train_loss: 0.1559, step time: 0.4899\n",
      "285/388, train_loss: 0.3770, step time: 0.4929\n",
      "286/388, train_loss: 0.0676, step time: 0.4781\n",
      "287/388, train_loss: 0.1428, step time: 0.4850\n",
      "288/388, train_loss: 0.1297, step time: 1.0815\n",
      "289/388, train_loss: 0.2117, step time: 0.5211\n",
      "290/388, train_loss: 0.2362, step time: 0.4997\n",
      "291/388, train_loss: 0.2995, step time: 0.5007\n",
      "292/388, train_loss: 0.1445, step time: 0.4894\n",
      "293/388, train_loss: 0.1912, step time: 0.4857\n",
      "294/388, train_loss: 0.1071, step time: 0.5102\n",
      "295/388, train_loss: 0.1541, step time: 0.4994\n",
      "296/388, train_loss: 0.1104, step time: 0.5006\n",
      "297/388, train_loss: 0.1865, step time: 1.1994\n",
      "298/388, train_loss: 0.3559, step time: 0.5374\n",
      "299/388, train_loss: 0.2091, step time: 0.5135\n",
      "300/388, train_loss: 0.0745, step time: 0.5039\n",
      "301/388, train_loss: 0.0939, step time: 0.4884\n",
      "302/388, train_loss: 0.1661, step time: 0.4935\n",
      "303/388, train_loss: 0.0965, step time: 0.4846\n",
      "304/388, train_loss: 0.2515, step time: 0.4782\n",
      "305/388, train_loss: 0.0869, step time: 0.4757\n",
      "306/388, train_loss: 0.1608, step time: 0.5187\n",
      "307/388, train_loss: 0.5490, step time: 0.5177\n",
      "308/388, train_loss: 0.1785, step time: 0.5671\n",
      "309/388, train_loss: 0.0887, step time: 0.5479\n",
      "310/388, train_loss: 0.2279, step time: 0.5195\n",
      "311/388, train_loss: 0.2977, step time: 0.5080\n",
      "312/388, train_loss: 0.2149, step time: 0.4917\n",
      "313/388, train_loss: 0.1008, step time: 0.4908\n",
      "314/388, train_loss: 0.3324, step time: 0.5135\n",
      "315/388, train_loss: 0.1598, step time: 0.4904\n",
      "316/388, train_loss: 0.1148, step time: 0.4800\n",
      "317/388, train_loss: 0.1870, step time: 1.1588\n",
      "318/388, train_loss: 0.4156, step time: 0.5203\n",
      "319/388, train_loss: 0.1940, step time: 0.5061\n",
      "320/388, train_loss: 0.2319, step time: 0.4856\n",
      "321/388, train_loss: 0.1416, step time: 0.4886\n",
      "322/388, train_loss: 0.1102, step time: 0.4801\n",
      "323/388, train_loss: 0.0995, step time: 0.9389\n",
      "324/388, train_loss: 0.1467, step time: 0.5321\n",
      "325/388, train_loss: 0.0832, step time: 0.5102\n",
      "326/388, train_loss: 0.1895, step time: 0.4980\n",
      "327/388, train_loss: 0.0940, step time: 0.4921\n",
      "328/388, train_loss: 0.1127, step time: 0.4780\n",
      "329/388, train_loss: 0.0857, step time: 0.4889\n",
      "330/388, train_loss: 0.3186, step time: 0.4776\n",
      "331/388, train_loss: 0.2818, step time: 0.4884\n",
      "332/388, train_loss: 0.1254, step time: 0.4910\n",
      "333/388, train_loss: 0.1324, step time: 0.4786\n",
      "334/388, train_loss: 0.2498, step time: 0.4864\n",
      "335/388, train_loss: 0.3486, step time: 0.5197\n",
      "336/388, train_loss: 0.1115, step time: 0.5204\n",
      "337/388, train_loss: 0.2404, step time: 0.5942\n",
      "338/388, train_loss: 0.1304, step time: 0.5514\n",
      "339/388, train_loss: 0.1005, step time: 0.5297\n",
      "340/388, train_loss: 0.2515, step time: 0.5102\n",
      "341/388, train_loss: 0.1142, step time: 0.4945\n",
      "342/388, train_loss: 0.0626, step time: 0.4932\n",
      "343/388, train_loss: 0.1124, step time: 0.4806\n",
      "344/388, train_loss: 0.3701, step time: 0.5094\n",
      "345/388, train_loss: 0.1326, step time: 0.4951\n",
      "346/388, train_loss: 0.2202, step time: 0.5128\n",
      "347/388, train_loss: 0.1945, step time: 0.5027\n",
      "348/388, train_loss: 0.0847, step time: 0.5338\n",
      "349/388, train_loss: 0.4278, step time: 0.5177\n",
      "350/388, train_loss: 0.3707, step time: 0.5040\n",
      "351/388, train_loss: 0.2214, step time: 0.4894\n",
      "352/388, train_loss: 0.1046, step time: 0.5039\n",
      "353/388, train_loss: 0.1593, step time: 0.4835\n",
      "354/388, train_loss: 0.0959, step time: 1.1420\n",
      "355/388, train_loss: 0.1168, step time: 0.5326\n",
      "356/388, train_loss: 0.0698, step time: 0.5060\n",
      "357/388, train_loss: 0.5195, step time: 0.5008\n",
      "358/388, train_loss: 0.5845, step time: 0.4849\n",
      "359/388, train_loss: 0.0791, step time: 0.5253\n",
      "360/388, train_loss: 0.1743, step time: 0.5177\n",
      "361/388, train_loss: 0.2048, step time: 0.4901\n",
      "362/388, train_loss: 0.1120, step time: 0.4864\n",
      "363/388, train_loss: 0.0660, step time: 0.4846\n",
      "364/388, train_loss: 0.5139, step time: 0.4754\n",
      "365/388, train_loss: 0.1866, step time: 0.5317\n",
      "366/388, train_loss: 0.0706, step time: 0.5026\n",
      "367/388, train_loss: 0.3284, step time: 0.5239\n",
      "368/388, train_loss: 0.2074, step time: 0.5195\n",
      "369/388, train_loss: 0.1062, step time: 0.5469\n",
      "370/388, train_loss: 0.5615, step time: 0.5252\n",
      "371/388, train_loss: 0.2768, step time: 0.5025\n",
      "372/388, train_loss: 0.3216, step time: 0.5007\n",
      "373/388, train_loss: 0.2067, step time: 0.5400\n",
      "374/388, train_loss: 0.1080, step time: 0.5106\n",
      "375/388, train_loss: 0.3439, step time: 0.4998\n",
      "376/388, train_loss: 0.1561, step time: 0.4896\n",
      "377/388, train_loss: 0.1738, step time: 0.4805\n",
      "378/388, train_loss: 0.1480, step time: 0.8995\n",
      "379/388, train_loss: 0.1101, step time: 0.5565\n",
      "380/388, train_loss: 0.1641, step time: 0.5222\n",
      "381/388, train_loss: 0.1342, step time: 0.5044\n",
      "382/388, train_loss: 0.1902, step time: 0.4951\n",
      "383/388, train_loss: 0.2680, step time: 0.4930\n",
      "384/388, train_loss: 0.2881, step time: 0.4709\n",
      "385/388, train_loss: 0.1085, step time: 0.7642\n",
      "386/388, train_loss: 0.1969, step time: 0.5410\n",
      "387/388, train_loss: 0.0721, step time: 0.5037\n",
      "388/388, train_loss: 0.2807, step time: 0.4892\n",
      "epoch 43 average loss: 0.1980\n",
      "current epoch: 43 current mean dice: 0.7478 tc: 0.7984 wt: 0.8840 et: 0.5610\n",
      "best mean dice: 0.7585 at epoch: 37\n",
      "time consuming of epoch 43 is: 301.7003\n",
      "----------\n",
      "epoch 44/300\n",
      "1/388, train_loss: 0.0998, step time: 0.4727\n",
      "2/388, train_loss: 0.2725, step time: 0.4901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/388, train_loss: 0.0488, step time: 0.4858\n",
      "4/388, train_loss: 0.3492, step time: 0.5105\n",
      "5/388, train_loss: 0.0652, step time: 0.5175\n",
      "6/388, train_loss: 0.3170, step time: 0.5008\n",
      "7/388, train_loss: 0.1302, step time: 0.4979\n",
      "8/388, train_loss: 0.4109, step time: 1.1566\n",
      "9/388, train_loss: 0.3276, step time: 0.5359\n",
      "10/388, train_loss: 0.0836, step time: 0.5273\n",
      "11/388, train_loss: 0.1893, step time: 0.5123\n",
      "12/388, train_loss: 0.0807, step time: 0.4943\n",
      "13/388, train_loss: 0.1484, step time: 0.4870\n",
      "14/388, train_loss: 0.0861, step time: 0.4838\n",
      "15/388, train_loss: 0.0740, step time: 1.0023\n",
      "16/388, train_loss: 0.0913, step time: 0.5505\n",
      "17/388, train_loss: 0.1136, step time: 0.5116\n",
      "18/388, train_loss: 0.2503, step time: 0.4976\n",
      "19/388, train_loss: 0.1109, step time: 0.4932\n",
      "20/388, train_loss: 0.0754, step time: 1.1593\n",
      "21/388, train_loss: 0.2545, step time: 0.5419\n",
      "22/388, train_loss: 0.2627, step time: 0.5083\n",
      "23/388, train_loss: 0.2210, step time: 0.5009\n",
      "24/388, train_loss: 0.2442, step time: 0.4935\n",
      "25/388, train_loss: 0.4399, step time: 0.4948\n",
      "26/388, train_loss: 0.1730, step time: 0.4822\n",
      "27/388, train_loss: 0.1508, step time: 0.7644\n",
      "28/388, train_loss: 0.6714, step time: 0.5431\n",
      "29/388, train_loss: 0.2371, step time: 0.5187\n",
      "30/388, train_loss: 0.3775, step time: 0.5014\n",
      "31/388, train_loss: 0.2964, step time: 0.4840\n",
      "32/388, train_loss: 0.0783, step time: 0.4875\n",
      "33/388, train_loss: 0.1833, step time: 0.5108\n",
      "34/388, train_loss: 0.0861, step time: 0.6004\n",
      "35/388, train_loss: 0.0970, step time: 0.5370\n",
      "36/388, train_loss: 0.2809, step time: 0.5147\n",
      "37/388, train_loss: 0.1449, step time: 0.5061\n",
      "38/388, train_loss: 0.1079, step time: 0.4876\n",
      "39/388, train_loss: 0.0601, step time: 0.4951\n",
      "40/388, train_loss: 0.4597, step time: 0.4853\n",
      "41/388, train_loss: 0.1097, step time: 0.4789\n",
      "42/388, train_loss: 0.1593, step time: 0.4925\n",
      "43/388, train_loss: 0.2785, step time: 0.5195\n",
      "44/388, train_loss: 0.1178, step time: 0.6200\n",
      "45/388, train_loss: 0.2105, step time: 0.5545\n",
      "46/388, train_loss: 0.6597, step time: 0.5377\n",
      "47/388, train_loss: 0.1072, step time: 0.5018\n",
      "48/388, train_loss: 0.2443, step time: 0.5061\n",
      "49/388, train_loss: 0.6758, step time: 0.4896\n",
      "50/388, train_loss: 0.3593, step time: 0.8465\n",
      "51/388, train_loss: 0.1042, step time: 0.5401\n",
      "52/388, train_loss: 0.0680, step time: 0.5176\n",
      "53/388, train_loss: 0.1710, step time: 0.4974\n",
      "54/388, train_loss: 0.2084, step time: 0.5048\n",
      "55/388, train_loss: 0.0603, step time: 0.4842\n",
      "56/388, train_loss: 0.2303, step time: 0.5091\n",
      "57/388, train_loss: 0.0988, step time: 0.5067\n",
      "58/388, train_loss: 0.5651, step time: 0.5841\n",
      "59/388, train_loss: 0.1137, step time: 0.5419\n",
      "60/388, train_loss: 0.1335, step time: 0.5234\n",
      "61/388, train_loss: 0.1409, step time: 0.5075\n",
      "62/388, train_loss: 0.1030, step time: 0.4896\n",
      "63/388, train_loss: 0.1309, step time: 0.7461\n",
      "64/388, train_loss: 0.2116, step time: 0.5482\n",
      "65/388, train_loss: 0.2504, step time: 0.5222\n",
      "66/388, train_loss: 0.1980, step time: 0.5044\n",
      "67/388, train_loss: 0.0578, step time: 0.4875\n",
      "68/388, train_loss: 0.1039, step time: 0.4913\n",
      "69/388, train_loss: 0.1167, step time: 0.4796\n",
      "70/388, train_loss: 0.0852, step time: 0.4806\n",
      "71/388, train_loss: 0.0323, step time: 0.4822\n",
      "72/388, train_loss: 0.1953, step time: 0.4704\n",
      "73/388, train_loss: 0.4068, step time: 0.7525\n",
      "74/388, train_loss: 0.3082, step time: 0.5443\n",
      "75/388, train_loss: 0.1260, step time: 0.5101\n",
      "76/388, train_loss: 0.1853, step time: 0.5058\n",
      "77/388, train_loss: 0.2680, step time: 0.5632\n",
      "78/388, train_loss: 0.1985, step time: 0.5278\n",
      "79/388, train_loss: 0.1275, step time: 0.5115\n",
      "80/388, train_loss: 0.1705, step time: 0.4876\n",
      "81/388, train_loss: 0.2301, step time: 0.4943\n",
      "82/388, train_loss: 0.1393, step time: 0.9611\n",
      "83/388, train_loss: 0.1683, step time: 0.5356\n",
      "84/388, train_loss: 0.1819, step time: 0.5084\n",
      "85/388, train_loss: 0.2498, step time: 0.4950\n",
      "86/388, train_loss: 0.3405, step time: 0.5048\n",
      "87/388, train_loss: 0.1497, step time: 0.4786\n",
      "88/388, train_loss: 0.1755, step time: 0.4745\n",
      "89/388, train_loss: 0.1634, step time: 0.5850\n",
      "90/388, train_loss: 0.1899, step time: 0.5382\n",
      "91/388, train_loss: 0.0885, step time: 0.5170\n",
      "92/388, train_loss: 0.0567, step time: 0.4951\n",
      "93/388, train_loss: 0.0334, step time: 0.4850\n",
      "94/388, train_loss: 0.2394, step time: 0.4912\n",
      "95/388, train_loss: 0.2318, step time: 0.5118\n",
      "96/388, train_loss: 0.2291, step time: 0.4952\n",
      "97/388, train_loss: 0.1277, step time: 0.4994\n",
      "98/388, train_loss: 0.1188, step time: 0.4940\n",
      "99/388, train_loss: 0.1606, step time: 0.4957\n",
      "100/388, train_loss: 0.2125, step time: 0.4969\n",
      "101/388, train_loss: 0.3694, step time: 0.4839\n",
      "102/388, train_loss: 0.2653, step time: 1.1800\n",
      "103/388, train_loss: 0.1200, step time: 0.5297\n",
      "104/388, train_loss: 0.1236, step time: 0.5180\n",
      "105/388, train_loss: 0.4459, step time: 0.4988\n",
      "106/388, train_loss: 0.0648, step time: 0.4969\n",
      "107/388, train_loss: 0.3114, step time: 0.4842\n",
      "108/388, train_loss: 0.1281, step time: 1.1363\n",
      "109/388, train_loss: 0.2876, step time: 0.5491\n",
      "110/388, train_loss: 0.1488, step time: 0.5072\n",
      "111/388, train_loss: 0.1053, step time: 0.4949\n",
      "112/388, train_loss: 0.0785, step time: 0.4832\n",
      "113/388, train_loss: 0.1037, step time: 0.4899\n",
      "114/388, train_loss: 0.1750, step time: 0.4889\n",
      "115/388, train_loss: 0.1509, step time: 0.4781\n",
      "116/388, train_loss: 0.1463, step time: 0.5914\n",
      "117/388, train_loss: 0.2306, step time: 0.5512\n",
      "118/388, train_loss: 0.1361, step time: 0.5223\n",
      "119/388, train_loss: 0.1962, step time: 0.5027\n",
      "120/388, train_loss: 0.1875, step time: 0.4849\n",
      "121/388, train_loss: 0.2682, step time: 0.4924\n",
      "122/388, train_loss: 0.1512, step time: 0.4763\n",
      "123/388, train_loss: 0.4163, step time: 0.4788\n",
      "124/388, train_loss: 0.2577, step time: 0.4816\n",
      "125/388, train_loss: 0.1089, step time: 1.0893\n",
      "126/388, train_loss: 0.2501, step time: 0.5423\n",
      "127/388, train_loss: 0.2632, step time: 0.5085\n",
      "128/388, train_loss: 0.1307, step time: 0.4884\n",
      "129/388, train_loss: 0.2195, step time: 0.5076\n",
      "130/388, train_loss: 0.0526, step time: 0.4981\n",
      "131/388, train_loss: 0.1300, step time: 0.4961\n",
      "132/388, train_loss: 0.2952, step time: 0.4845\n",
      "133/388, train_loss: 0.1004, step time: 0.4912\n",
      "134/388, train_loss: 0.1129, step time: 0.4822\n",
      "135/388, train_loss: 0.1129, step time: 1.0201\n",
      "136/388, train_loss: 0.1896, step time: 0.5494\n",
      "137/388, train_loss: 0.3390, step time: 0.5121\n",
      "138/388, train_loss: 0.1712, step time: 0.4973\n",
      "139/388, train_loss: 0.2154, step time: 0.4883\n",
      "140/388, train_loss: 0.2616, step time: 0.4952\n",
      "141/388, train_loss: 0.1822, step time: 0.4874\n",
      "142/388, train_loss: 0.1099, step time: 0.4747\n",
      "143/388, train_loss: 0.1244, step time: 0.4941\n",
      "144/388, train_loss: 0.1355, step time: 0.4898\n",
      "145/388, train_loss: 0.0983, step time: 1.0436\n",
      "146/388, train_loss: 0.1352, step time: 0.5383\n",
      "147/388, train_loss: 0.1716, step time: 0.5156\n",
      "148/388, train_loss: 0.1015, step time: 0.5006\n",
      "149/388, train_loss: 0.3969, step time: 0.4902\n",
      "150/388, train_loss: 0.1565, step time: 0.4807\n",
      "151/388, train_loss: 0.4890, step time: 1.1410\n",
      "152/388, train_loss: 0.1798, step time: 0.5379\n",
      "153/388, train_loss: 0.2106, step time: 0.5011\n",
      "154/388, train_loss: 0.2652, step time: 0.4795\n",
      "155/388, train_loss: 0.1892, step time: 0.4822\n",
      "156/388, train_loss: 0.6014, step time: 0.4862\n",
      "157/388, train_loss: 0.1014, step time: 0.4732\n",
      "158/388, train_loss: 0.3865, step time: 0.5351\n",
      "159/388, train_loss: 0.1949, step time: 0.5167\n",
      "160/388, train_loss: 0.2100, step time: 0.5062\n",
      "161/388, train_loss: 0.2263, step time: 0.4960\n",
      "162/388, train_loss: 0.0503, step time: 0.4853\n",
      "163/388, train_loss: 0.2746, step time: 0.4828\n",
      "164/388, train_loss: 0.0574, step time: 0.7771\n",
      "165/388, train_loss: 0.0999, step time: 0.5409\n",
      "166/388, train_loss: 0.1895, step time: 0.5204\n",
      "167/388, train_loss: 0.1073, step time: 0.4980\n",
      "168/388, train_loss: 0.4129, step time: 0.4931\n",
      "169/388, train_loss: 0.1159, step time: 0.4978\n",
      "170/388, train_loss: 0.0889, step time: 0.4871\n",
      "171/388, train_loss: 0.2056, step time: 0.5064\n",
      "172/388, train_loss: 0.3537, step time: 0.5031\n",
      "173/388, train_loss: 0.2118, step time: 0.4963\n",
      "174/388, train_loss: 0.2925, step time: 0.4870\n",
      "175/388, train_loss: 0.1017, step time: 0.5103\n",
      "176/388, train_loss: 0.0860, step time: 0.5751\n",
      "177/388, train_loss: 0.1773, step time: 0.5282\n",
      "178/388, train_loss: 0.2254, step time: 0.5049\n",
      "179/388, train_loss: 0.2571, step time: 0.4886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/388, train_loss: 0.1402, step time: 0.4898\n",
      "181/388, train_loss: 0.5399, step time: 0.6292\n",
      "182/388, train_loss: 0.1421, step time: 0.5461\n",
      "183/388, train_loss: 0.3805, step time: 0.5167\n",
      "184/388, train_loss: 0.1890, step time: 0.5035\n",
      "185/388, train_loss: 0.1884, step time: 0.4862\n",
      "186/388, train_loss: 0.3195, step time: 0.4917\n",
      "187/388, train_loss: 0.0682, step time: 1.0958\n",
      "188/388, train_loss: 0.1060, step time: 0.5300\n",
      "189/388, train_loss: 0.1398, step time: 0.5068\n",
      "190/388, train_loss: 0.5043, step time: 0.4891\n",
      "191/388, train_loss: 0.2368, step time: 0.5239\n",
      "192/388, train_loss: 0.4808, step time: 0.4974\n",
      "193/388, train_loss: 0.0710, step time: 0.4987\n",
      "194/388, train_loss: 0.1895, step time: 0.5276\n",
      "195/388, train_loss: 0.2118, step time: 0.5072\n",
      "196/388, train_loss: 0.2675, step time: 0.4897\n",
      "197/388, train_loss: 0.1912, step time: 0.5313\n",
      "198/388, train_loss: 0.1711, step time: 0.5013\n",
      "199/388, train_loss: 0.1901, step time: 0.5023\n",
      "200/388, train_loss: 0.1255, step time: 0.5239\n",
      "201/388, train_loss: 0.1386, step time: 0.5094\n",
      "202/388, train_loss: 0.1094, step time: 0.5014\n",
      "203/388, train_loss: 0.1047, step time: 0.5258\n",
      "204/388, train_loss: 0.1969, step time: 0.5476\n",
      "205/388, train_loss: 0.1403, step time: 0.5210\n",
      "206/388, train_loss: 0.1485, step time: 0.5164\n",
      "207/388, train_loss: 0.0469, step time: 0.5698\n",
      "208/388, train_loss: 0.0892, step time: 0.5382\n",
      "209/388, train_loss: 0.1895, step time: 0.5152\n",
      "210/388, train_loss: 0.2112, step time: 0.5112\n",
      "211/388, train_loss: 0.3797, step time: 0.5308\n",
      "212/388, train_loss: 0.0940, step time: 0.5053\n",
      "213/388, train_loss: 0.1663, step time: 0.4936\n",
      "214/388, train_loss: 0.1341, step time: 0.4890\n",
      "215/388, train_loss: 0.1074, step time: 0.4918\n",
      "216/388, train_loss: 0.0959, step time: 0.5188\n",
      "217/388, train_loss: 0.1955, step time: 0.4938\n",
      "218/388, train_loss: 0.4246, step time: 0.4975\n",
      "219/388, train_loss: 0.1309, step time: 0.4825\n",
      "220/388, train_loss: 0.3911, step time: 0.8317\n",
      "221/388, train_loss: 0.2143, step time: 0.5860\n",
      "222/388, train_loss: 0.2138, step time: 0.5517\n",
      "223/388, train_loss: 0.0684, step time: 0.5215\n",
      "224/388, train_loss: 0.4219, step time: 0.5042\n",
      "225/388, train_loss: 0.1467, step time: 0.5079\n",
      "226/388, train_loss: 0.1589, step time: 0.4918\n",
      "227/388, train_loss: 0.3310, step time: 1.0114\n",
      "228/388, train_loss: 0.1493, step time: 0.5369\n",
      "229/388, train_loss: 0.5775, step time: 0.5161\n",
      "230/388, train_loss: 0.2424, step time: 0.4977\n",
      "231/388, train_loss: 0.1696, step time: 0.4887\n",
      "232/388, train_loss: 0.1743, step time: 0.4856\n",
      "233/388, train_loss: 0.1654, step time: 0.4783\n",
      "234/388, train_loss: 0.1718, step time: 0.4801\n",
      "235/388, train_loss: 0.1046, step time: 0.5046\n",
      "236/388, train_loss: 0.2843, step time: 0.5157\n",
      "237/388, train_loss: 0.0650, step time: 0.5320\n",
      "238/388, train_loss: 0.3094, step time: 0.5107\n",
      "239/388, train_loss: 0.2894, step time: 0.5046\n",
      "240/388, train_loss: 0.1248, step time: 0.8163\n",
      "241/388, train_loss: 0.1635, step time: 0.5660\n",
      "242/388, train_loss: 0.1875, step time: 0.5252\n",
      "243/388, train_loss: 0.2760, step time: 0.5162\n",
      "244/388, train_loss: 0.1164, step time: 0.5034\n",
      "245/388, train_loss: 0.4557, step time: 0.5083\n",
      "246/388, train_loss: 0.2310, step time: 0.5006\n",
      "247/388, train_loss: 0.2411, step time: 0.4960\n",
      "248/388, train_loss: 0.2982, step time: 0.5383\n",
      "249/388, train_loss: 0.1932, step time: 0.5812\n",
      "250/388, train_loss: 0.2765, step time: 0.5385\n",
      "251/388, train_loss: 0.1647, step time: 0.5250\n",
      "252/388, train_loss: 0.2162, step time: 0.5120\n",
      "253/388, train_loss: 0.1719, step time: 0.4925\n",
      "254/388, train_loss: 0.1409, step time: 0.4969\n",
      "255/388, train_loss: 0.0982, step time: 1.1214\n",
      "256/388, train_loss: 0.0894, step time: 0.5565\n",
      "257/388, train_loss: 0.2028, step time: 0.5379\n",
      "258/388, train_loss: 0.1695, step time: 0.5055\n",
      "259/388, train_loss: 0.1171, step time: 0.4893\n",
      "260/388, train_loss: 0.0835, step time: 0.4880\n",
      "261/388, train_loss: 0.6709, step time: 0.4952\n",
      "262/388, train_loss: 0.1743, step time: 0.5134\n",
      "263/388, train_loss: 0.1364, step time: 0.5093\n",
      "264/388, train_loss: 0.1849, step time: 0.5151\n",
      "265/388, train_loss: 0.1981, step time: 0.6059\n",
      "266/388, train_loss: 0.0676, step time: 0.5705\n",
      "267/388, train_loss: 0.3027, step time: 0.5404\n",
      "268/388, train_loss: 0.5224, step time: 0.5255\n",
      "269/388, train_loss: 0.2325, step time: 0.6126\n",
      "270/388, train_loss: 0.2464, step time: 0.5401\n",
      "271/388, train_loss: 0.1593, step time: 0.5087\n",
      "272/388, train_loss: 0.0548, step time: 0.5055\n",
      "273/388, train_loss: 0.2894, step time: 0.4880\n",
      "274/388, train_loss: 0.0756, step time: 0.4982\n",
      "275/388, train_loss: 0.1734, step time: 0.5264\n",
      "276/388, train_loss: 0.1533, step time: 0.5013\n",
      "277/388, train_loss: 0.2855, step time: 0.4874\n",
      "278/388, train_loss: 0.2353, step time: 0.5026\n",
      "279/388, train_loss: 0.3078, step time: 0.5023\n",
      "280/388, train_loss: 0.1378, step time: 0.5581\n",
      "281/388, train_loss: 0.3653, step time: 0.5211\n",
      "282/388, train_loss: 0.0292, step time: 0.5029\n",
      "283/388, train_loss: 0.0979, step time: 0.4890\n",
      "284/388, train_loss: 0.1290, step time: 0.5150\n",
      "285/388, train_loss: 0.0993, step time: 0.5030\n",
      "286/388, train_loss: 0.1501, step time: 0.5159\n",
      "287/388, train_loss: 0.2895, step time: 0.6063\n",
      "288/388, train_loss: 0.2177, step time: 0.5621\n",
      "289/388, train_loss: 0.3800, step time: 0.5335\n",
      "290/388, train_loss: 0.1496, step time: 0.5709\n",
      "291/388, train_loss: 0.1557, step time: 0.5255\n",
      "292/388, train_loss: 0.2071, step time: 0.5058\n",
      "293/388, train_loss: 0.1141, step time: 0.4874\n",
      "294/388, train_loss: 0.1567, step time: 1.1655\n",
      "295/388, train_loss: 0.4100, step time: 0.5432\n",
      "296/388, train_loss: 0.1646, step time: 0.5135\n",
      "297/388, train_loss: 0.0944, step time: 0.4901\n",
      "298/388, train_loss: 0.3133, step time: 0.4965\n",
      "299/388, train_loss: 0.1300, step time: 1.1133\n",
      "300/388, train_loss: 0.2858, step time: 0.5414\n",
      "301/388, train_loss: 0.1312, step time: 0.5111\n",
      "302/388, train_loss: 0.1533, step time: 0.5021\n",
      "303/388, train_loss: 0.0681, step time: 0.4893\n",
      "304/388, train_loss: 0.1443, step time: 0.4918\n",
      "305/388, train_loss: 0.2808, step time: 0.4781\n",
      "306/388, train_loss: 0.1366, step time: 0.9970\n",
      "307/388, train_loss: 0.1102, step time: 0.5370\n",
      "308/388, train_loss: 0.3387, step time: 0.5083\n",
      "309/388, train_loss: 0.2074, step time: 0.4951\n",
      "310/388, train_loss: 0.3067, step time: 0.4852\n",
      "311/388, train_loss: 0.2341, step time: 0.4874\n",
      "312/388, train_loss: 0.1010, step time: 1.0575\n",
      "313/388, train_loss: 0.0955, step time: 0.5401\n",
      "314/388, train_loss: 0.3159, step time: 0.5233\n",
      "315/388, train_loss: 0.2020, step time: 0.5013\n",
      "316/388, train_loss: 0.1580, step time: 0.4891\n",
      "317/388, train_loss: 0.1717, step time: 0.4857\n",
      "318/388, train_loss: 0.1606, step time: 0.4793\n",
      "319/388, train_loss: 0.1433, step time: 0.4727\n",
      "320/388, train_loss: 0.1052, step time: 0.4819\n",
      "321/388, train_loss: 0.2457, step time: 0.5408\n",
      "322/388, train_loss: 0.1413, step time: 0.5211\n",
      "323/388, train_loss: 0.4540, step time: 0.4942\n",
      "324/388, train_loss: 0.1019, step time: 0.4908\n",
      "325/388, train_loss: 0.2360, step time: 1.2092\n",
      "326/388, train_loss: 0.1555, step time: 0.5322\n",
      "327/388, train_loss: 0.1816, step time: 0.5147\n",
      "328/388, train_loss: 0.0981, step time: 0.4915\n",
      "329/388, train_loss: 0.2772, step time: 0.4868\n",
      "330/388, train_loss: 0.1020, step time: 0.4912\n",
      "331/388, train_loss: 0.2081, step time: 0.4763\n",
      "332/388, train_loss: 0.0689, step time: 0.4777\n",
      "333/388, train_loss: 0.1003, step time: 1.1115\n",
      "334/388, train_loss: 0.2768, step time: 0.5348\n",
      "335/388, train_loss: 0.3044, step time: 0.5064\n",
      "336/388, train_loss: 0.1425, step time: 0.4908\n",
      "337/388, train_loss: 0.1027, step time: 0.4825\n",
      "338/388, train_loss: 0.0905, step time: 0.4866\n",
      "339/388, train_loss: 0.0810, step time: 0.4740\n",
      "340/388, train_loss: 0.0997, step time: 0.5865\n",
      "341/388, train_loss: 0.1195, step time: 0.5479\n",
      "342/388, train_loss: 0.0961, step time: 0.5210\n",
      "343/388, train_loss: 0.1024, step time: 0.5068\n",
      "344/388, train_loss: 0.1317, step time: 0.4890\n",
      "345/388, train_loss: 0.2105, step time: 0.4996\n",
      "346/388, train_loss: 0.1032, step time: 0.5011\n",
      "347/388, train_loss: 0.2805, step time: 0.5418\n",
      "348/388, train_loss: 0.0836, step time: 0.6349\n",
      "349/388, train_loss: 0.3533, step time: 0.5404\n",
      "350/388, train_loss: 0.0876, step time: 0.5206\n",
      "351/388, train_loss: 0.3425, step time: 0.4997\n",
      "352/388, train_loss: 0.3131, step time: 0.4813\n",
      "353/388, train_loss: 0.2950, step time: 0.4961\n",
      "354/388, train_loss: 0.1156, step time: 0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/388, train_loss: 0.0999, step time: 0.5113\n",
      "356/388, train_loss: 0.2047, step time: 0.4897\n",
      "357/388, train_loss: 0.1294, step time: 0.4775\n",
      "358/388, train_loss: 0.0694, step time: 0.9635\n",
      "359/388, train_loss: 0.0766, step time: 0.5493\n",
      "360/388, train_loss: 0.2647, step time: 0.5204\n",
      "361/388, train_loss: 0.0924, step time: 0.4991\n",
      "362/388, train_loss: 0.0863, step time: 0.4911\n",
      "363/388, train_loss: 0.6337, step time: 0.4959\n",
      "364/388, train_loss: 0.1750, step time: 0.4808\n",
      "365/388, train_loss: 0.0839, step time: 1.0240\n",
      "366/388, train_loss: 0.1556, step time: 0.5366\n",
      "367/388, train_loss: 0.2772, step time: 0.5097\n",
      "368/388, train_loss: 0.2275, step time: 0.4832\n",
      "369/388, train_loss: 0.2523, step time: 0.9548\n",
      "370/388, train_loss: 0.3028, step time: 0.5318\n",
      "371/388, train_loss: 0.2993, step time: 0.5140\n",
      "372/388, train_loss: 0.0591, step time: 0.4902\n",
      "373/388, train_loss: 0.0703, step time: 0.4956\n",
      "374/388, train_loss: 0.0947, step time: 0.4796\n",
      "375/388, train_loss: 0.2232, step time: 0.4806\n",
      "376/388, train_loss: 0.0846, step time: 0.4790\n",
      "377/388, train_loss: 0.0804, step time: 0.4780\n",
      "378/388, train_loss: 0.2025, step time: 1.0260\n",
      "379/388, train_loss: 0.2349, step time: 0.5358\n",
      "380/388, train_loss: 0.2104, step time: 0.5036\n",
      "381/388, train_loss: 0.0740, step time: 0.4923\n",
      "382/388, train_loss: 0.1608, step time: 0.4923\n",
      "383/388, train_loss: 0.1108, step time: 0.4763\n",
      "384/388, train_loss: 0.2203, step time: 0.4710\n",
      "385/388, train_loss: 0.5072, step time: 0.4857\n",
      "386/388, train_loss: 0.0696, step time: 0.4803\n",
      "387/388, train_loss: 0.1363, step time: 0.4729\n",
      "388/388, train_loss: 0.2047, step time: 0.4720\n",
      "epoch 44 average loss: 0.1973\n",
      "saved new best metric model\n",
      "current epoch: 44 current mean dice: 0.7616 tc: 0.8177 wt: 0.8958 et: 0.5713\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 44 is: 302.1106\n",
      "----------\n",
      "epoch 45/300\n",
      "1/388, train_loss: 0.1912, step time: 0.4879\n",
      "2/388, train_loss: 0.1786, step time: 0.4920\n",
      "3/388, train_loss: 0.1671, step time: 0.4770\n",
      "4/388, train_loss: 0.1565, step time: 0.5033\n",
      "5/388, train_loss: 0.1037, step time: 0.5037\n",
      "6/388, train_loss: 0.1601, step time: 0.5265\n",
      "7/388, train_loss: 0.0496, step time: 0.5249\n",
      "8/388, train_loss: 0.0766, step time: 0.6366\n",
      "9/388, train_loss: 0.3078, step time: 0.5303\n",
      "10/388, train_loss: 0.0709, step time: 0.5012\n",
      "11/388, train_loss: 0.3889, step time: 0.5211\n",
      "12/388, train_loss: 0.1291, step time: 0.6058\n",
      "13/388, train_loss: 0.1689, step time: 0.5231\n",
      "14/388, train_loss: 0.1683, step time: 0.5133\n",
      "15/388, train_loss: 0.2976, step time: 0.4919\n",
      "16/388, train_loss: 0.2037, step time: 0.7228\n",
      "17/388, train_loss: 0.1986, step time: 0.5413\n",
      "18/388, train_loss: 0.1262, step time: 0.5066\n",
      "19/388, train_loss: 0.1935, step time: 0.7328\n",
      "20/388, train_loss: 0.1237, step time: 0.6116\n",
      "21/388, train_loss: 0.3782, step time: 0.5370\n",
      "22/388, train_loss: 0.1169, step time: 0.5117\n",
      "23/388, train_loss: 0.0792, step time: 0.5024\n",
      "24/388, train_loss: 0.1050, step time: 0.5039\n",
      "25/388, train_loss: 0.0794, step time: 0.5003\n",
      "26/388, train_loss: 0.1424, step time: 0.5080\n",
      "27/388, train_loss: 0.0937, step time: 0.5020\n",
      "28/388, train_loss: 0.1787, step time: 0.4796\n",
      "29/388, train_loss: 0.1191, step time: 0.7463\n",
      "30/388, train_loss: 0.0563, step time: 0.5714\n",
      "31/388, train_loss: 0.2032, step time: 0.5347\n",
      "32/388, train_loss: 0.1553, step time: 0.5113\n",
      "33/388, train_loss: 0.0870, step time: 0.5644\n",
      "34/388, train_loss: 0.2352, step time: 0.5288\n",
      "35/388, train_loss: 0.1090, step time: 0.5024\n",
      "36/388, train_loss: 0.1583, step time: 0.4939\n",
      "37/388, train_loss: 0.2622, step time: 0.4845\n",
      "38/388, train_loss: 0.1656, step time: 0.5032\n",
      "39/388, train_loss: 0.0720, step time: 0.5055\n",
      "40/388, train_loss: 0.2933, step time: 0.5206\n",
      "41/388, train_loss: 0.1213, step time: 0.4972\n",
      "42/388, train_loss: 0.0972, step time: 0.4895\n",
      "43/388, train_loss: 0.0839, step time: 0.4891\n",
      "44/388, train_loss: 0.0333, step time: 0.4890\n",
      "45/388, train_loss: 0.3784, step time: 0.5219\n",
      "46/388, train_loss: 0.0866, step time: 0.5022\n",
      "47/388, train_loss: 0.5156, step time: 0.5175\n",
      "48/388, train_loss: 0.1285, step time: 0.5640\n",
      "49/388, train_loss: 0.1019, step time: 0.5447\n",
      "50/388, train_loss: 0.0755, step time: 0.5049\n",
      "51/388, train_loss: 0.0900, step time: 0.5703\n",
      "52/388, train_loss: 0.1907, step time: 0.5422\n",
      "53/388, train_loss: 0.3212, step time: 0.5075\n",
      "54/388, train_loss: 0.1077, step time: 0.4950\n",
      "55/388, train_loss: 0.0974, step time: 0.4790\n",
      "56/388, train_loss: 0.0992, step time: 0.4806\n",
      "57/388, train_loss: 0.1151, step time: 0.5528\n",
      "58/388, train_loss: 0.1274, step time: 0.5095\n",
      "59/388, train_loss: 0.6474, step time: 0.5576\n",
      "60/388, train_loss: 0.0660, step time: 0.5374\n",
      "61/388, train_loss: 0.1344, step time: 0.5167\n",
      "62/388, train_loss: 0.2065, step time: 0.5058\n",
      "63/388, train_loss: 0.1222, step time: 0.4938\n",
      "64/388, train_loss: 0.1735, step time: 0.5028\n",
      "65/388, train_loss: 0.0778, step time: 0.5569\n",
      "66/388, train_loss: 0.3568, step time: 0.5348\n",
      "67/388, train_loss: 0.1947, step time: 0.5189\n",
      "68/388, train_loss: 0.0990, step time: 0.9656\n",
      "69/388, train_loss: 0.0661, step time: 0.5386\n",
      "70/388, train_loss: 0.0680, step time: 0.5099\n",
      "71/388, train_loss: 0.2361, step time: 0.4960\n",
      "72/388, train_loss: 0.2837, step time: 0.4979\n",
      "73/388, train_loss: 0.0677, step time: 0.4821\n",
      "74/388, train_loss: 0.2309, step time: 0.4918\n",
      "75/388, train_loss: 0.4335, step time: 0.5069\n",
      "76/388, train_loss: 0.2162, step time: 0.4861\n",
      "77/388, train_loss: 0.2741, step time: 1.1326\n",
      "78/388, train_loss: 0.1096, step time: 0.5375\n",
      "79/388, train_loss: 0.2785, step time: 0.5067\n",
      "80/388, train_loss: 0.2641, step time: 0.4910\n",
      "81/388, train_loss: 0.1029, step time: 0.4823\n",
      "82/388, train_loss: 0.1170, step time: 0.4881\n",
      "83/388, train_loss: 0.1047, step time: 0.4737\n",
      "84/388, train_loss: 0.1466, step time: 0.4855\n",
      "85/388, train_loss: 0.0975, step time: 0.4920\n",
      "86/388, train_loss: 0.3764, step time: 0.4795\n",
      "87/388, train_loss: 0.2543, step time: 1.1379\n",
      "88/388, train_loss: 0.1084, step time: 0.5291\n",
      "89/388, train_loss: 0.1039, step time: 0.5027\n",
      "90/388, train_loss: 0.1296, step time: 0.4962\n",
      "91/388, train_loss: 0.2343, step time: 0.4806\n",
      "92/388, train_loss: 0.3827, step time: 0.5082\n",
      "93/388, train_loss: 0.2290, step time: 0.5001\n",
      "94/388, train_loss: 0.1384, step time: 0.4831\n",
      "95/388, train_loss: 0.0993, step time: 0.7288\n",
      "96/388, train_loss: 0.0835, step time: 0.5453\n",
      "97/388, train_loss: 0.1023, step time: 0.5217\n",
      "98/388, train_loss: 0.1007, step time: 0.5051\n",
      "99/388, train_loss: 0.1520, step time: 0.4860\n",
      "100/388, train_loss: 0.1279, step time: 0.4870\n",
      "101/388, train_loss: 0.1071, step time: 0.9512\n",
      "102/388, train_loss: 0.1426, step time: 0.5401\n",
      "103/388, train_loss: 0.0960, step time: 0.5047\n",
      "104/388, train_loss: 0.3941, step time: 0.4978\n",
      "105/388, train_loss: 0.0666, step time: 0.4906\n",
      "106/388, train_loss: 0.2614, step time: 0.4937\n",
      "107/388, train_loss: 0.1819, step time: 1.1332\n",
      "108/388, train_loss: 0.1018, step time: 0.5173\n",
      "109/388, train_loss: 0.2055, step time: 0.5004\n",
      "110/388, train_loss: 0.0288, step time: 0.4855\n",
      "111/388, train_loss: 0.1369, step time: 0.4997\n",
      "112/388, train_loss: 0.0842, step time: 0.4818\n",
      "113/388, train_loss: 0.3607, step time: 0.4906\n",
      "114/388, train_loss: 0.3042, step time: 0.4908\n",
      "115/388, train_loss: 0.3832, step time: 0.5068\n",
      "116/388, train_loss: 0.0815, step time: 0.4933\n",
      "117/388, train_loss: 0.1881, step time: 0.4908\n",
      "118/388, train_loss: 0.2478, step time: 0.5145\n",
      "119/388, train_loss: 0.2183, step time: 0.5060\n",
      "120/388, train_loss: 0.1604, step time: 0.4869\n",
      "121/388, train_loss: 0.1525, step time: 1.0096\n",
      "122/388, train_loss: 0.0678, step time: 0.5376\n",
      "123/388, train_loss: 0.1118, step time: 0.5066\n",
      "124/388, train_loss: 0.2610, step time: 0.4833\n",
      "125/388, train_loss: 0.1498, step time: 0.4783\n",
      "126/388, train_loss: 0.1938, step time: 0.5050\n",
      "127/388, train_loss: 0.2154, step time: 0.4829\n",
      "128/388, train_loss: 0.2209, step time: 0.4930\n",
      "129/388, train_loss: 0.1393, step time: 0.4983\n",
      "130/388, train_loss: 0.1634, step time: 0.4781\n",
      "131/388, train_loss: 0.1983, step time: 0.5100\n",
      "132/388, train_loss: 0.4130, step time: 0.4905\n",
      "133/388, train_loss: 0.2307, step time: 0.4934\n",
      "134/388, train_loss: 0.3061, step time: 0.4781\n",
      "135/388, train_loss: 0.2377, step time: 0.4796\n",
      "136/388, train_loss: 0.1624, step time: 0.4994\n",
      "137/388, train_loss: 0.0989, step time: 0.5746\n",
      "138/388, train_loss: 0.2346, step time: 0.5204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/388, train_loss: 0.0489, step time: 0.5081\n",
      "140/388, train_loss: 0.3536, step time: 0.4939\n",
      "141/388, train_loss: 0.1601, step time: 0.4957\n",
      "142/388, train_loss: 0.3693, step time: 0.5171\n",
      "143/388, train_loss: 0.1813, step time: 0.5029\n",
      "144/388, train_loss: 0.1990, step time: 0.5119\n",
      "145/388, train_loss: 0.2795, step time: 0.5936\n",
      "146/388, train_loss: 0.0653, step time: 0.5450\n",
      "147/388, train_loss: 0.1883, step time: 0.5099\n",
      "148/388, train_loss: 0.2239, step time: 0.5044\n",
      "149/388, train_loss: 0.0967, step time: 0.4988\n",
      "150/388, train_loss: 0.1575, step time: 0.5058\n",
      "151/388, train_loss: 0.2070, step time: 0.4802\n",
      "152/388, train_loss: 0.1832, step time: 0.4895\n",
      "153/388, train_loss: 0.3094, step time: 1.1948\n",
      "154/388, train_loss: 0.3772, step time: 0.5429\n",
      "155/388, train_loss: 0.0513, step time: 0.5127\n",
      "156/388, train_loss: 0.1844, step time: 0.5002\n",
      "157/388, train_loss: 0.3421, step time: 0.4924\n",
      "158/388, train_loss: 0.1005, step time: 0.9819\n",
      "159/388, train_loss: 0.1050, step time: 0.5397\n",
      "160/388, train_loss: 0.1433, step time: 0.5045\n",
      "161/388, train_loss: 0.2240, step time: 0.5066\n",
      "162/388, train_loss: 0.1308, step time: 0.5161\n",
      "163/388, train_loss: 0.1467, step time: 0.5477\n",
      "164/388, train_loss: 0.0907, step time: 0.5144\n",
      "165/388, train_loss: 0.0899, step time: 0.5017\n",
      "166/388, train_loss: 0.0550, step time: 0.4861\n",
      "167/388, train_loss: 0.4637, step time: 0.9983\n",
      "168/388, train_loss: 0.1198, step time: 0.5328\n",
      "169/388, train_loss: 0.1423, step time: 0.5002\n",
      "170/388, train_loss: 0.2673, step time: 0.4939\n",
      "171/388, train_loss: 0.2599, step time: 0.4855\n",
      "172/388, train_loss: 0.2173, step time: 0.4902\n",
      "173/388, train_loss: 0.1702, step time: 1.0543\n",
      "174/388, train_loss: 0.1764, step time: 0.5253\n",
      "175/388, train_loss: 0.0701, step time: 0.5056\n",
      "176/388, train_loss: 0.2918, step time: 0.4877\n",
      "177/388, train_loss: 0.0933, step time: 0.4900\n",
      "178/388, train_loss: 0.2165, step time: 0.4909\n",
      "179/388, train_loss: 0.1271, step time: 0.5033\n",
      "180/388, train_loss: 0.1884, step time: 0.4985\n",
      "181/388, train_loss: 0.0972, step time: 0.4857\n",
      "182/388, train_loss: 0.4307, step time: 0.5334\n",
      "183/388, train_loss: 0.1423, step time: 0.6080\n",
      "184/388, train_loss: 0.1551, step time: 0.5471\n",
      "185/388, train_loss: 0.1360, step time: 0.5180\n",
      "186/388, train_loss: 0.1106, step time: 0.5062\n",
      "187/388, train_loss: 0.1506, step time: 0.4914\n",
      "188/388, train_loss: 0.0923, step time: 0.4838\n",
      "189/388, train_loss: 0.1922, step time: 0.5538\n",
      "190/388, train_loss: 0.2395, step time: 0.5281\n",
      "191/388, train_loss: 0.2022, step time: 0.4967\n",
      "192/388, train_loss: 0.4606, step time: 0.4917\n",
      "193/388, train_loss: 0.0857, step time: 0.4782\n",
      "194/388, train_loss: 0.0679, step time: 0.4699\n",
      "195/388, train_loss: 0.1915, step time: 0.4811\n",
      "196/388, train_loss: 0.2421, step time: 0.4905\n",
      "197/388, train_loss: 0.1123, step time: 0.4985\n",
      "198/388, train_loss: 0.4216, step time: 0.5688\n",
      "199/388, train_loss: 0.2155, step time: 0.5401\n",
      "200/388, train_loss: 0.1063, step time: 0.5155\n",
      "201/388, train_loss: 0.4150, step time: 0.5782\n",
      "202/388, train_loss: 0.4777, step time: 0.5541\n",
      "203/388, train_loss: 0.1290, step time: 0.5187\n",
      "204/388, train_loss: 0.1188, step time: 0.4955\n",
      "205/388, train_loss: 0.1991, step time: 0.9661\n",
      "206/388, train_loss: 0.2821, step time: 0.5489\n",
      "207/388, train_loss: 0.3867, step time: 0.5072\n",
      "208/388, train_loss: 0.3135, step time: 0.4909\n",
      "209/388, train_loss: 0.1998, step time: 0.4853\n",
      "210/388, train_loss: 0.0777, step time: 0.5002\n",
      "211/388, train_loss: 0.2424, step time: 0.6715\n",
      "212/388, train_loss: 0.1544, step time: 0.5611\n",
      "213/388, train_loss: 0.0351, step time: 0.5230\n",
      "214/388, train_loss: 0.0667, step time: 0.5167\n",
      "215/388, train_loss: 0.2028, step time: 0.4915\n",
      "216/388, train_loss: 0.1758, step time: 0.5477\n",
      "217/388, train_loss: 0.3145, step time: 0.5354\n",
      "218/388, train_loss: 0.2629, step time: 0.5132\n",
      "219/388, train_loss: 0.2634, step time: 0.5059\n",
      "220/388, train_loss: 0.1537, step time: 0.4873\n",
      "221/388, train_loss: 0.1714, step time: 0.5007\n",
      "222/388, train_loss: 0.1878, step time: 0.5840\n",
      "223/388, train_loss: 0.3543, step time: 0.5384\n",
      "224/388, train_loss: 0.1511, step time: 0.5079\n",
      "225/388, train_loss: 0.2047, step time: 0.4905\n",
      "226/388, train_loss: 0.1278, step time: 0.4904\n",
      "227/388, train_loss: 0.1666, step time: 0.4778\n",
      "228/388, train_loss: 0.1732, step time: 1.1144\n",
      "229/388, train_loss: 0.4549, step time: 0.5427\n",
      "230/388, train_loss: 0.1191, step time: 0.5169\n",
      "231/388, train_loss: 0.2624, step time: 0.5014\n",
      "232/388, train_loss: 0.0752, step time: 0.5025\n",
      "233/388, train_loss: 0.2552, step time: 0.4855\n",
      "234/388, train_loss: 0.0668, step time: 0.5045\n",
      "235/388, train_loss: 0.2806, step time: 0.4800\n",
      "236/388, train_loss: 0.1594, step time: 0.9734\n",
      "237/388, train_loss: 0.1690, step time: 0.5532\n",
      "238/388, train_loss: 0.2239, step time: 0.5344\n",
      "239/388, train_loss: 0.1716, step time: 0.5062\n",
      "240/388, train_loss: 0.1766, step time: 0.5010\n",
      "241/388, train_loss: 0.2684, step time: 0.5108\n",
      "242/388, train_loss: 0.0521, step time: 0.5098\n",
      "243/388, train_loss: 0.2271, step time: 0.4901\n",
      "244/388, train_loss: 0.0813, step time: 0.5029\n",
      "245/388, train_loss: 0.1886, step time: 0.4991\n",
      "246/388, train_loss: 0.1690, step time: 0.5677\n",
      "247/388, train_loss: 0.2202, step time: 0.5392\n",
      "248/388, train_loss: 0.2352, step time: 0.5152\n",
      "249/388, train_loss: 0.3878, step time: 0.5702\n",
      "250/388, train_loss: 0.1449, step time: 0.5521\n",
      "251/388, train_loss: 0.0900, step time: 0.5220\n",
      "252/388, train_loss: 0.2797, step time: 0.4997\n",
      "253/388, train_loss: 0.2013, step time: 0.5062\n",
      "254/388, train_loss: 0.2466, step time: 0.5960\n",
      "255/388, train_loss: 0.0763, step time: 0.5472\n",
      "256/388, train_loss: 0.2002, step time: 0.5400\n",
      "257/388, train_loss: 0.1311, step time: 0.5394\n",
      "258/388, train_loss: 0.1379, step time: 0.5691\n",
      "259/388, train_loss: 0.1648, step time: 0.5380\n",
      "260/388, train_loss: 0.5640, step time: 0.5144\n",
      "261/388, train_loss: 0.2944, step time: 0.5013\n",
      "262/388, train_loss: 0.1904, step time: 0.5123\n",
      "263/388, train_loss: 0.1953, step time: 0.5598\n",
      "264/388, train_loss: 0.1601, step time: 0.5415\n",
      "265/388, train_loss: 0.3440, step time: 0.5215\n",
      "266/388, train_loss: 0.2300, step time: 0.5006\n",
      "267/388, train_loss: 0.1562, step time: 0.4949\n",
      "268/388, train_loss: 0.2571, step time: 0.5645\n",
      "269/388, train_loss: 0.2404, step time: 0.5268\n",
      "270/388, train_loss: 0.2611, step time: 0.5013\n",
      "271/388, train_loss: 0.4017, step time: 0.9940\n",
      "272/388, train_loss: 0.1242, step time: 0.5513\n",
      "273/388, train_loss: 0.1678, step time: 0.5193\n",
      "274/388, train_loss: 0.0904, step time: 0.5045\n",
      "275/388, train_loss: 0.1641, step time: 0.5233\n",
      "276/388, train_loss: 0.2603, step time: 0.5005\n",
      "277/388, train_loss: 0.3540, step time: 0.5017\n",
      "278/388, train_loss: 0.2410, step time: 0.5165\n",
      "279/388, train_loss: 0.2168, step time: 0.4995\n",
      "280/388, train_loss: 0.0492, step time: 0.5439\n",
      "281/388, train_loss: 0.1355, step time: 0.5350\n",
      "282/388, train_loss: 0.4094, step time: 0.5621\n",
      "283/388, train_loss: 0.0904, step time: 0.5247\n",
      "284/388, train_loss: 0.3176, step time: 0.5010\n",
      "285/388, train_loss: 0.0996, step time: 0.4927\n",
      "286/388, train_loss: 0.3187, step time: 0.4795\n",
      "287/388, train_loss: 0.1295, step time: 0.5219\n",
      "288/388, train_loss: 0.1012, step time: 0.5219\n",
      "289/388, train_loss: 0.2321, step time: 0.4987\n",
      "290/388, train_loss: 0.2439, step time: 0.5057\n",
      "291/388, train_loss: 0.0853, step time: 0.5355\n",
      "292/388, train_loss: 0.2733, step time: 0.5121\n",
      "293/388, train_loss: 0.1350, step time: 0.5316\n",
      "294/388, train_loss: 0.0909, step time: 0.4941\n",
      "295/388, train_loss: 0.3334, step time: 1.0749\n",
      "296/388, train_loss: 0.1543, step time: 0.5683\n",
      "297/388, train_loss: 0.5114, step time: 0.5386\n",
      "298/388, train_loss: 0.2496, step time: 0.5258\n",
      "299/388, train_loss: 0.0747, step time: 0.5110\n",
      "300/388, train_loss: 0.4392, step time: 0.5097\n",
      "301/388, train_loss: 0.1089, step time: 0.5695\n",
      "302/388, train_loss: 0.0407, step time: 0.5743\n",
      "303/388, train_loss: 0.2205, step time: 0.5498\n",
      "304/388, train_loss: 0.0751, step time: 0.5190\n",
      "305/388, train_loss: 0.2659, step time: 0.5449\n",
      "306/388, train_loss: 0.0857, step time: 0.7544\n",
      "307/388, train_loss: 0.0953, step time: 0.5459\n",
      "308/388, train_loss: 0.1843, step time: 0.5117\n",
      "309/388, train_loss: 0.0945, step time: 0.5016\n",
      "310/388, train_loss: 0.0628, step time: 0.4902\n",
      "311/388, train_loss: 0.3898, step time: 0.5011\n",
      "312/388, train_loss: 0.2389, step time: 0.5625\n",
      "313/388, train_loss: 0.1474, step time: 0.5734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/388, train_loss: 0.3261, step time: 0.5395\n",
      "315/388, train_loss: 0.1755, step time: 0.5383\n",
      "316/388, train_loss: 0.2665, step time: 0.5354\n",
      "317/388, train_loss: 0.2811, step time: 0.5130\n",
      "318/388, train_loss: 0.2551, step time: 0.5465\n",
      "319/388, train_loss: 0.1448, step time: 0.6010\n",
      "320/388, train_loss: 0.1323, step time: 0.5735\n",
      "321/388, train_loss: 0.5854, step time: 0.5334\n",
      "322/388, train_loss: 0.0583, step time: 0.5044\n",
      "323/388, train_loss: 0.1158, step time: 1.1324\n",
      "324/388, train_loss: 0.2204, step time: 0.5436\n",
      "325/388, train_loss: 0.5440, step time: 0.5112\n",
      "326/388, train_loss: 0.0977, step time: 0.5045\n",
      "327/388, train_loss: 0.1294, step time: 0.4888\n",
      "328/388, train_loss: 0.2947, step time: 0.4891\n",
      "329/388, train_loss: 0.4484, step time: 1.2306\n",
      "330/388, train_loss: 0.3029, step time: 0.5346\n",
      "331/388, train_loss: 0.9650, step time: 0.4987\n",
      "332/388, train_loss: 0.1914, step time: 0.4906\n",
      "333/388, train_loss: 0.1854, step time: 0.4835\n",
      "334/388, train_loss: 0.1309, step time: 0.4780\n",
      "335/388, train_loss: 0.0923, step time: 1.0446\n",
      "336/388, train_loss: 0.2327, step time: 0.5434\n",
      "337/388, train_loss: 0.2442, step time: 0.5068\n",
      "338/388, train_loss: 0.1126, step time: 0.5276\n",
      "339/388, train_loss: 0.2646, step time: 0.5055\n",
      "340/388, train_loss: 0.1574, step time: 0.4969\n",
      "341/388, train_loss: 0.3245, step time: 0.6074\n",
      "342/388, train_loss: 0.2314, step time: 0.5544\n",
      "343/388, train_loss: 0.1964, step time: 0.5299\n",
      "344/388, train_loss: 0.2260, step time: 0.5208\n",
      "345/388, train_loss: 0.2593, step time: 0.4958\n",
      "346/388, train_loss: 0.2024, step time: 0.4942\n",
      "347/388, train_loss: 0.0638, step time: 0.4765\n",
      "348/388, train_loss: 0.2344, step time: 0.4958\n",
      "349/388, train_loss: 0.2677, step time: 0.4953\n",
      "350/388, train_loss: 0.1739, step time: 0.5018\n",
      "351/388, train_loss: 0.1106, step time: 0.4973\n",
      "352/388, train_loss: 0.1488, step time: 0.5194\n",
      "353/388, train_loss: 0.0792, step time: 0.5089\n",
      "354/388, train_loss: 0.5296, step time: 0.5023\n",
      "355/388, train_loss: 0.1912, step time: 0.5025\n",
      "356/388, train_loss: 0.1306, step time: 0.5384\n",
      "357/388, train_loss: 0.1669, step time: 0.6580\n",
      "358/388, train_loss: 0.1594, step time: 0.5518\n",
      "359/388, train_loss: 0.1124, step time: 0.5260\n",
      "360/388, train_loss: 0.2324, step time: 0.4982\n",
      "361/388, train_loss: 0.1650, step time: 0.4942\n",
      "362/388, train_loss: 0.1297, step time: 0.4816\n",
      "363/388, train_loss: 0.2862, step time: 0.4971\n",
      "364/388, train_loss: 0.2784, step time: 0.5646\n",
      "365/388, train_loss: 0.2219, step time: 0.5279\n",
      "366/388, train_loss: 0.2771, step time: 0.5022\n",
      "367/388, train_loss: 0.0985, step time: 0.9276\n",
      "368/388, train_loss: 0.2892, step time: 0.5606\n",
      "369/388, train_loss: 0.2299, step time: 0.5276\n",
      "370/388, train_loss: 0.1830, step time: 0.5081\n",
      "371/388, train_loss: 0.1303, step time: 0.5732\n",
      "372/388, train_loss: 0.3760, step time: 0.5381\n",
      "373/388, train_loss: 0.1073, step time: 0.5138\n",
      "374/388, train_loss: 0.1443, step time: 0.5023\n",
      "375/388, train_loss: 0.2984, step time: 0.5091\n",
      "376/388, train_loss: 0.1036, step time: 0.5033\n",
      "377/388, train_loss: 0.1523, step time: 0.4810\n",
      "378/388, train_loss: 0.0447, step time: 0.5008\n",
      "379/388, train_loss: 0.2012, step time: 0.5245\n",
      "380/388, train_loss: 0.1535, step time: 0.5018\n",
      "381/388, train_loss: 0.1255, step time: 0.5687\n",
      "382/388, train_loss: 0.2597, step time: 0.5112\n",
      "383/388, train_loss: 0.2230, step time: 0.4944\n",
      "384/388, train_loss: 0.0861, step time: 0.4811\n",
      "385/388, train_loss: 0.1357, step time: 0.5624\n",
      "386/388, train_loss: 0.0683, step time: 0.5586\n",
      "387/388, train_loss: 0.2094, step time: 0.5430\n",
      "388/388, train_loss: 0.5587, step time: 0.5014\n",
      "epoch 45 average loss: 0.1959\n",
      "current epoch: 45 current mean dice: 0.7552 tc: 0.8017 wt: 0.8960 et: 0.5679\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 45 is: 300.1072\n",
      "----------\n",
      "epoch 46/300\n",
      "1/388, train_loss: 0.2150, step time: 0.4766\n",
      "2/388, train_loss: 0.2445, step time: 0.4927\n",
      "3/388, train_loss: 0.2928, step time: 0.5065\n",
      "4/388, train_loss: 0.1539, step time: 0.6372\n",
      "5/388, train_loss: 0.0680, step time: 0.5292\n",
      "6/388, train_loss: 0.2218, step time: 0.5265\n",
      "7/388, train_loss: 0.0969, step time: 0.5248\n",
      "8/388, train_loss: 0.1127, step time: 0.6026\n",
      "9/388, train_loss: 0.1382, step time: 0.5234\n",
      "10/388, train_loss: 0.1903, step time: 0.5493\n",
      "11/388, train_loss: 0.0961, step time: 0.6734\n",
      "12/388, train_loss: 0.2910, step time: 0.5632\n",
      "13/388, train_loss: 0.1084, step time: 0.5237\n",
      "14/388, train_loss: 0.2812, step time: 0.5069\n",
      "15/388, train_loss: 0.1103, step time: 0.4957\n",
      "16/388, train_loss: 0.2717, step time: 0.5678\n",
      "17/388, train_loss: 0.2885, step time: 0.5235\n",
      "18/388, train_loss: 0.2250, step time: 0.4963\n",
      "19/388, train_loss: 0.4585, step time: 0.6168\n",
      "20/388, train_loss: 0.1769, step time: 0.5588\n",
      "21/388, train_loss: 0.2356, step time: 0.5334\n",
      "22/388, train_loss: 0.1352, step time: 0.5194\n",
      "23/388, train_loss: 0.1880, step time: 0.5131\n",
      "24/388, train_loss: 0.0754, step time: 0.5035\n",
      "25/388, train_loss: 0.2816, step time: 0.5038\n",
      "26/388, train_loss: 0.1477, step time: 0.4871\n",
      "27/388, train_loss: 0.3111, step time: 1.1010\n",
      "28/388, train_loss: 0.1063, step time: 0.5686\n",
      "29/388, train_loss: 0.1428, step time: 0.5161\n",
      "30/388, train_loss: 0.2241, step time: 0.5006\n",
      "31/388, train_loss: 0.3620, step time: 0.4901\n",
      "32/388, train_loss: 0.1159, step time: 0.4980\n",
      "33/388, train_loss: 0.2059, step time: 0.4918\n",
      "34/388, train_loss: 0.2888, step time: 0.4956\n",
      "35/388, train_loss: 0.1347, step time: 0.9719\n",
      "36/388, train_loss: 0.1494, step time: 0.5583\n",
      "37/388, train_loss: 0.1074, step time: 0.5096\n",
      "38/388, train_loss: 0.2894, step time: 0.4882\n",
      "39/388, train_loss: 0.0943, step time: 0.5085\n",
      "40/388, train_loss: 0.1096, step time: 0.5137\n",
      "41/388, train_loss: 0.0430, step time: 0.4920\n",
      "42/388, train_loss: 0.1841, step time: 0.4948\n",
      "43/388, train_loss: 0.0963, step time: 0.4853\n",
      "44/388, train_loss: 0.0997, step time: 0.4757\n",
      "45/388, train_loss: 0.0972, step time: 0.9220\n",
      "46/388, train_loss: 0.1342, step time: 0.5334\n",
      "47/388, train_loss: 0.3462, step time: 0.5157\n",
      "48/388, train_loss: 0.1096, step time: 0.4993\n",
      "49/388, train_loss: 0.0458, step time: 0.4830\n",
      "50/388, train_loss: 0.2114, step time: 0.4823\n",
      "51/388, train_loss: 0.0898, step time: 0.4848\n",
      "52/388, train_loss: 0.0773, step time: 0.4835\n",
      "53/388, train_loss: 0.1101, step time: 0.5167\n",
      "54/388, train_loss: 0.2136, step time: 0.5052\n",
      "55/388, train_loss: 0.0614, step time: 0.4894\n",
      "56/388, train_loss: 0.1039, step time: 0.5092\n",
      "57/388, train_loss: 0.1332, step time: 0.4841\n",
      "58/388, train_loss: 0.1679, step time: 0.5097\n",
      "59/388, train_loss: 0.4591, step time: 0.5664\n",
      "60/388, train_loss: 0.5275, step time: 0.5358\n",
      "61/388, train_loss: 0.3412, step time: 0.5103\n",
      "62/388, train_loss: 0.4462, step time: 0.5161\n",
      "63/388, train_loss: 0.3016, step time: 0.5308\n",
      "64/388, train_loss: 0.2581, step time: 0.5515\n",
      "65/388, train_loss: 0.3044, step time: 0.5324\n",
      "66/388, train_loss: 0.3708, step time: 0.5103\n",
      "67/388, train_loss: 0.1211, step time: 0.4889\n",
      "68/388, train_loss: 0.3567, step time: 0.6382\n",
      "69/388, train_loss: 0.2662, step time: 0.5502\n",
      "70/388, train_loss: 0.1552, step time: 0.5230\n",
      "71/388, train_loss: 0.1090, step time: 0.4898\n",
      "72/388, train_loss: 0.3115, step time: 0.4947\n",
      "73/388, train_loss: 0.0734, step time: 0.4981\n",
      "74/388, train_loss: 0.0934, step time: 0.4867\n",
      "75/388, train_loss: 0.1590, step time: 0.5066\n",
      "76/388, train_loss: 0.2073, step time: 0.4969\n",
      "77/388, train_loss: 0.1309, step time: 0.4958\n",
      "78/388, train_loss: 0.1033, step time: 0.4794\n",
      "79/388, train_loss: 0.3828, step time: 0.4784\n",
      "80/388, train_loss: 0.1468, step time: 0.9718\n",
      "81/388, train_loss: 0.1827, step time: 0.5466\n",
      "82/388, train_loss: 0.0836, step time: 0.5134\n",
      "83/388, train_loss: 0.2232, step time: 0.4936\n",
      "84/388, train_loss: 0.3914, step time: 0.4965\n",
      "85/388, train_loss: 0.1564, step time: 0.4964\n",
      "86/388, train_loss: 0.0932, step time: 0.5178\n",
      "87/388, train_loss: 0.1945, step time: 0.4993\n",
      "88/388, train_loss: 0.1553, step time: 0.4972\n",
      "89/388, train_loss: 0.5900, step time: 0.4844\n",
      "90/388, train_loss: 0.1660, step time: 0.5117\n",
      "91/388, train_loss: 0.2654, step time: 0.4887\n",
      "92/388, train_loss: 0.6472, step time: 0.4867\n",
      "93/388, train_loss: 0.0775, step time: 0.5165\n",
      "94/388, train_loss: 0.4585, step time: 0.5089\n",
      "95/388, train_loss: 0.0345, step time: 0.5029\n",
      "96/388, train_loss: 0.2123, step time: 0.4857\n",
      "97/388, train_loss: 0.0752, step time: 0.4748\n",
      "98/388, train_loss: 0.2698, step time: 1.1422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/388, train_loss: 0.1322, step time: 0.5361\n",
      "100/388, train_loss: 0.1517, step time: 0.4979\n",
      "101/388, train_loss: 0.1214, step time: 0.4938\n",
      "102/388, train_loss: 0.2793, step time: 0.4879\n",
      "103/388, train_loss: 0.4147, step time: 0.4974\n",
      "104/388, train_loss: 0.0856, step time: 0.4841\n",
      "105/388, train_loss: 0.1597, step time: 0.4896\n",
      "106/388, train_loss: 0.1363, step time: 0.4745\n",
      "107/388, train_loss: 0.1030, step time: 0.4847\n",
      "108/388, train_loss: 0.1909, step time: 0.5037\n",
      "109/388, train_loss: 0.1392, step time: 0.4923\n",
      "110/388, train_loss: 0.1304, step time: 1.2018\n",
      "111/388, train_loss: 0.1026, step time: 0.5361\n",
      "112/388, train_loss: 0.3039, step time: 0.5187\n",
      "113/388, train_loss: 0.1810, step time: 0.5067\n",
      "114/388, train_loss: 0.1627, step time: 0.4897\n",
      "115/388, train_loss: 0.0834, step time: 0.4962\n",
      "116/388, train_loss: 0.1818, step time: 0.4919\n",
      "117/388, train_loss: 0.2282, step time: 0.4838\n",
      "118/388, train_loss: 0.1918, step time: 0.4878\n",
      "119/388, train_loss: 0.2416, step time: 0.4854\n",
      "120/388, train_loss: 0.0821, step time: 0.4893\n",
      "121/388, train_loss: 0.3480, step time: 1.1960\n",
      "122/388, train_loss: 0.0985, step time: 0.5368\n",
      "123/388, train_loss: 0.1275, step time: 0.5025\n",
      "124/388, train_loss: 0.2661, step time: 0.5233\n",
      "125/388, train_loss: 0.2514, step time: 0.5070\n",
      "126/388, train_loss: 0.3897, step time: 0.5045\n",
      "127/388, train_loss: 0.3898, step time: 0.5384\n",
      "128/388, train_loss: 0.1304, step time: 0.5270\n",
      "129/388, train_loss: 0.1328, step time: 0.5091\n",
      "130/388, train_loss: 0.2125, step time: 0.4918\n",
      "131/388, train_loss: 0.1952, step time: 0.4958\n",
      "132/388, train_loss: 0.1592, step time: 0.4834\n",
      "133/388, train_loss: 0.1461, step time: 0.6064\n",
      "134/388, train_loss: 0.3640, step time: 0.5389\n",
      "135/388, train_loss: 0.2919, step time: 0.5095\n",
      "136/388, train_loss: 0.2994, step time: 0.5305\n",
      "137/388, train_loss: 0.0900, step time: 0.5057\n",
      "138/388, train_loss: 0.0315, step time: 0.5015\n",
      "139/388, train_loss: 0.5073, step time: 0.4878\n",
      "140/388, train_loss: 0.3001, step time: 0.4960\n",
      "141/388, train_loss: 0.1672, step time: 1.0680\n",
      "142/388, train_loss: 0.2305, step time: 0.5290\n",
      "143/388, train_loss: 0.0763, step time: 0.5059\n",
      "144/388, train_loss: 0.1826, step time: 0.4875\n",
      "145/388, train_loss: 0.3241, step time: 0.4823\n",
      "146/388, train_loss: 0.4034, step time: 0.4886\n",
      "147/388, train_loss: 0.2290, step time: 0.4766\n",
      "148/388, train_loss: 0.1849, step time: 0.4845\n",
      "149/388, train_loss: 0.1076, step time: 0.4931\n",
      "150/388, train_loss: 0.1518, step time: 0.4897\n",
      "151/388, train_loss: 0.2534, step time: 0.4801\n",
      "152/388, train_loss: 0.3602, step time: 0.4957\n",
      "153/388, train_loss: 0.3531, step time: 0.4883\n",
      "154/388, train_loss: 0.3661, step time: 0.4863\n",
      "155/388, train_loss: 0.3316, step time: 1.2017\n",
      "156/388, train_loss: 0.1140, step time: 0.5317\n",
      "157/388, train_loss: 0.2230, step time: 0.5060\n",
      "158/388, train_loss: 0.0771, step time: 0.4905\n",
      "159/388, train_loss: 0.1711, step time: 0.4954\n",
      "160/388, train_loss: 0.1400, step time: 0.4827\n",
      "161/388, train_loss: 0.2294, step time: 0.4809\n",
      "162/388, train_loss: 0.2467, step time: 0.4846\n",
      "163/388, train_loss: 0.2653, step time: 0.4777\n",
      "164/388, train_loss: 0.1514, step time: 0.9389\n",
      "165/388, train_loss: 0.1069, step time: 0.5381\n",
      "166/388, train_loss: 0.2660, step time: 0.5203\n",
      "167/388, train_loss: 0.2472, step time: 0.4958\n",
      "168/388, train_loss: 0.1117, step time: 0.4998\n",
      "169/388, train_loss: 0.3476, step time: 0.4866\n",
      "170/388, train_loss: 0.1711, step time: 0.5081\n",
      "171/388, train_loss: 0.0991, step time: 0.4911\n",
      "172/388, train_loss: 0.0908, step time: 0.4835\n",
      "173/388, train_loss: 0.2300, step time: 0.4877\n",
      "174/388, train_loss: 0.1182, step time: 0.4775\n",
      "175/388, train_loss: 0.1050, step time: 0.4853\n",
      "176/388, train_loss: 0.3696, step time: 1.0916\n",
      "177/388, train_loss: 0.2915, step time: 0.5361\n",
      "178/388, train_loss: 0.7000, step time: 0.5013\n",
      "179/388, train_loss: 0.1576, step time: 0.8533\n",
      "180/388, train_loss: 0.0758, step time: 0.5472\n",
      "181/388, train_loss: 0.1012, step time: 0.5123\n",
      "182/388, train_loss: 0.1560, step time: 0.4932\n",
      "183/388, train_loss: 0.3737, step time: 0.4946\n",
      "184/388, train_loss: 0.2881, step time: 0.4917\n",
      "185/388, train_loss: 0.2140, step time: 0.4977\n",
      "186/388, train_loss: 0.2190, step time: 0.5589\n",
      "187/388, train_loss: 0.2239, step time: 0.5344\n",
      "188/388, train_loss: 0.1030, step time: 0.5145\n",
      "189/388, train_loss: 0.2584, step time: 0.5019\n",
      "190/388, train_loss: 0.1076, step time: 0.4889\n",
      "191/388, train_loss: 0.2605, step time: 0.4980\n",
      "192/388, train_loss: 0.1566, step time: 0.4790\n",
      "193/388, train_loss: 0.2640, step time: 0.4778\n",
      "194/388, train_loss: 0.1238, step time: 0.5963\n",
      "195/388, train_loss: 0.1874, step time: 0.5595\n",
      "196/388, train_loss: 0.0583, step time: 0.5262\n",
      "197/388, train_loss: 0.1824, step time: 0.5007\n",
      "198/388, train_loss: 0.1810, step time: 0.4923\n",
      "199/388, train_loss: 0.1592, step time: 0.4814\n",
      "200/388, train_loss: 0.0891, step time: 0.4838\n",
      "201/388, train_loss: 0.0841, step time: 0.4809\n",
      "202/388, train_loss: 0.2550, step time: 0.4932\n",
      "203/388, train_loss: 0.4651, step time: 0.4953\n",
      "204/388, train_loss: 0.5004, step time: 0.6998\n",
      "205/388, train_loss: 0.1485, step time: 0.5556\n",
      "206/388, train_loss: 0.1283, step time: 0.5323\n",
      "207/388, train_loss: 0.0865, step time: 0.5055\n",
      "208/388, train_loss: 0.0924, step time: 0.5485\n",
      "209/388, train_loss: 0.0971, step time: 0.5257\n",
      "210/388, train_loss: 0.1907, step time: 0.5096\n",
      "211/388, train_loss: 0.4884, step time: 0.4927\n",
      "212/388, train_loss: 0.1016, step time: 0.4923\n",
      "213/388, train_loss: 0.1116, step time: 0.4793\n",
      "214/388, train_loss: 0.2837, step time: 0.5134\n",
      "215/388, train_loss: 0.0693, step time: 0.6130\n",
      "216/388, train_loss: 0.2070, step time: 0.5630\n",
      "217/388, train_loss: 0.1583, step time: 0.5191\n",
      "218/388, train_loss: 0.0601, step time: 0.5014\n",
      "219/388, train_loss: 0.0831, step time: 0.4811\n",
      "220/388, train_loss: 0.1533, step time: 0.4799\n",
      "221/388, train_loss: 0.0801, step time: 0.9480\n",
      "222/388, train_loss: 0.1551, step time: 0.5383\n",
      "223/388, train_loss: 0.3008, step time: 0.5193\n",
      "224/388, train_loss: 0.1184, step time: 0.4917\n",
      "225/388, train_loss: 0.2396, step time: 0.5145\n",
      "226/388, train_loss: 0.2017, step time: 0.5649\n",
      "227/388, train_loss: 0.3709, step time: 0.5409\n",
      "228/388, train_loss: 0.1618, step time: 0.5139\n",
      "229/388, train_loss: 0.0699, step time: 0.4999\n",
      "230/388, train_loss: 0.0765, step time: 1.0085\n",
      "231/388, train_loss: 0.1100, step time: 0.5424\n",
      "232/388, train_loss: 0.2349, step time: 0.5278\n",
      "233/388, train_loss: 0.0856, step time: 0.4998\n",
      "234/388, train_loss: 0.1566, step time: 0.4867\n",
      "235/388, train_loss: 0.1246, step time: 0.4930\n",
      "236/388, train_loss: 0.2957, step time: 0.4797\n",
      "237/388, train_loss: 0.0786, step time: 0.4740\n",
      "238/388, train_loss: 0.1851, step time: 0.4809\n",
      "239/388, train_loss: 0.1961, step time: 0.4781\n",
      "240/388, train_loss: 0.1110, step time: 1.0426\n",
      "241/388, train_loss: 0.3330, step time: 0.5217\n",
      "242/388, train_loss: 0.1102, step time: 0.5030\n",
      "243/388, train_loss: 0.1805, step time: 0.4812\n",
      "244/388, train_loss: 0.1863, step time: 1.1269\n",
      "245/388, train_loss: 0.1701, step time: 0.5271\n",
      "246/388, train_loss: 0.1086, step time: 0.5012\n",
      "247/388, train_loss: 0.1458, step time: 0.4958\n",
      "248/388, train_loss: 0.1132, step time: 0.5352\n",
      "249/388, train_loss: 0.1437, step time: 0.5072\n",
      "250/388, train_loss: 0.1451, step time: 0.5069\n",
      "251/388, train_loss: 0.0959, step time: 0.4876\n",
      "252/388, train_loss: 0.1852, step time: 0.4937\n",
      "253/388, train_loss: 0.0633, step time: 0.5036\n",
      "254/388, train_loss: 0.3542, step time: 0.5009\n",
      "255/388, train_loss: 0.0961, step time: 0.5069\n",
      "256/388, train_loss: 0.1544, step time: 0.4942\n",
      "257/388, train_loss: 0.1297, step time: 0.5170\n",
      "258/388, train_loss: 0.1181, step time: 0.6567\n",
      "259/388, train_loss: 0.6238, step time: 0.5406\n",
      "260/388, train_loss: 0.2062, step time: 0.5186\n",
      "261/388, train_loss: 0.1356, step time: 0.5032\n",
      "262/388, train_loss: 0.1776, step time: 0.5482\n",
      "263/388, train_loss: 0.2618, step time: 0.5306\n",
      "264/388, train_loss: 0.1443, step time: 0.5025\n",
      "265/388, train_loss: 0.2313, step time: 0.4927\n",
      "266/388, train_loss: 0.2131, step time: 0.4956\n",
      "267/388, train_loss: 0.0588, step time: 1.1392\n",
      "268/388, train_loss: 0.2009, step time: 0.5385\n",
      "269/388, train_loss: 0.2516, step time: 0.5088\n",
      "270/388, train_loss: 0.2766, step time: 0.4932\n",
      "271/388, train_loss: 0.0660, step time: 0.5004\n",
      "272/388, train_loss: 0.1853, step time: 0.4841\n",
      "273/388, train_loss: 0.0634, step time: 0.4834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/388, train_loss: 0.1035, step time: 0.4782\n",
      "275/388, train_loss: 0.0553, step time: 0.7706\n",
      "276/388, train_loss: 0.1361, step time: 0.5459\n",
      "277/388, train_loss: 0.0971, step time: 0.5173\n",
      "278/388, train_loss: 0.0969, step time: 0.5000\n",
      "279/388, train_loss: 0.1559, step time: 0.4936\n",
      "280/388, train_loss: 0.2862, step time: 0.4840\n",
      "281/388, train_loss: 0.1253, step time: 0.4939\n",
      "282/388, train_loss: 0.2736, step time: 1.1695\n",
      "283/388, train_loss: 0.2950, step time: 0.5260\n",
      "284/388, train_loss: 0.0951, step time: 0.5009\n",
      "285/388, train_loss: 0.2612, step time: 0.4871\n",
      "286/388, train_loss: 0.1160, step time: 0.4949\n",
      "287/388, train_loss: 0.1724, step time: 0.5168\n",
      "288/388, train_loss: 0.2148, step time: 0.5102\n",
      "289/388, train_loss: 0.0453, step time: 0.4947\n",
      "290/388, train_loss: 0.2090, step time: 0.4811\n",
      "291/388, train_loss: 0.2115, step time: 0.5038\n",
      "292/388, train_loss: 0.2203, step time: 0.5113\n",
      "293/388, train_loss: 0.2253, step time: 0.5635\n",
      "294/388, train_loss: 0.0789, step time: 0.5373\n",
      "295/388, train_loss: 0.1224, step time: 0.5152\n",
      "296/388, train_loss: 0.1808, step time: 0.4989\n",
      "297/388, train_loss: 0.1847, step time: 1.1909\n",
      "298/388, train_loss: 0.2922, step time: 0.5362\n",
      "299/388, train_loss: 0.1304, step time: 0.5229\n",
      "300/388, train_loss: 0.5118, step time: 0.4982\n",
      "301/388, train_loss: 0.1025, step time: 0.4849\n",
      "302/388, train_loss: 0.3622, step time: 0.4924\n",
      "303/388, train_loss: 0.3145, step time: 0.4771\n",
      "304/388, train_loss: 0.0576, step time: 0.4818\n",
      "305/388, train_loss: 0.1965, step time: 0.4884\n",
      "306/388, train_loss: 0.0658, step time: 0.5062\n",
      "307/388, train_loss: 0.2827, step time: 0.5035\n",
      "308/388, train_loss: 0.1271, step time: 0.4842\n",
      "309/388, train_loss: 0.1312, step time: 0.4821\n",
      "310/388, train_loss: 0.2034, step time: 0.9647\n",
      "311/388, train_loss: 0.0773, step time: 0.5489\n",
      "312/388, train_loss: 0.1473, step time: 0.5156\n",
      "313/388, train_loss: 0.0971, step time: 0.4921\n",
      "314/388, train_loss: 0.2169, step time: 0.5001\n",
      "315/388, train_loss: 0.1233, step time: 0.4871\n",
      "316/388, train_loss: 0.3047, step time: 0.4945\n",
      "317/388, train_loss: 0.0700, step time: 1.2277\n",
      "318/388, train_loss: 0.1448, step time: 0.5383\n",
      "319/388, train_loss: 0.4071, step time: 0.5128\n",
      "320/388, train_loss: 0.2720, step time: 0.5038\n",
      "321/388, train_loss: 0.4144, step time: 0.5339\n",
      "322/388, train_loss: 0.1458, step time: 0.5062\n",
      "323/388, train_loss: 0.1305, step time: 0.4959\n",
      "324/388, train_loss: 0.1181, step time: 0.4933\n",
      "325/388, train_loss: 0.1807, step time: 1.0861\n",
      "326/388, train_loss: 0.0544, step time: 0.5367\n",
      "327/388, train_loss: 0.2286, step time: 0.5085\n",
      "328/388, train_loss: 0.5458, step time: 0.4887\n",
      "329/388, train_loss: 0.1602, step time: 0.4886\n",
      "330/388, train_loss: 0.2016, step time: 0.4962\n",
      "331/388, train_loss: 0.3689, step time: 0.4778\n",
      "332/388, train_loss: 0.2224, step time: 0.4906\n",
      "333/388, train_loss: 0.3026, step time: 0.7637\n",
      "334/388, train_loss: 0.1369, step time: 0.5509\n",
      "335/388, train_loss: 0.2174, step time: 0.5173\n",
      "336/388, train_loss: 0.1010, step time: 0.5001\n",
      "337/388, train_loss: 0.1516, step time: 0.4880\n",
      "338/388, train_loss: 0.1316, step time: 0.5003\n",
      "339/388, train_loss: 0.2568, step time: 0.8657\n",
      "340/388, train_loss: 0.3216, step time: 0.5443\n",
      "341/388, train_loss: 0.3594, step time: 0.5043\n",
      "342/388, train_loss: 0.0324, step time: 0.4986\n",
      "343/388, train_loss: 0.1765, step time: 0.4877\n",
      "344/388, train_loss: 0.2868, step time: 0.4906\n",
      "345/388, train_loss: 0.0506, step time: 0.5031\n",
      "346/388, train_loss: 0.1062, step time: 0.5001\n",
      "347/388, train_loss: 0.0998, step time: 1.0904\n",
      "348/388, train_loss: 0.0854, step time: 0.5388\n",
      "349/388, train_loss: 0.1696, step time: 0.5167\n",
      "350/388, train_loss: 0.0592, step time: 0.4884\n",
      "351/388, train_loss: 0.1635, step time: 0.5217\n",
      "352/388, train_loss: 0.1024, step time: 0.5001\n",
      "353/388, train_loss: 0.1608, step time: 0.4831\n",
      "354/388, train_loss: 0.0734, step time: 0.4847\n",
      "355/388, train_loss: 0.0824, step time: 0.5320\n",
      "356/388, train_loss: 0.0871, step time: 0.5796\n",
      "357/388, train_loss: 0.3046, step time: 0.5530\n",
      "358/388, train_loss: 0.0662, step time: 0.5224\n",
      "359/388, train_loss: 0.1189, step time: 0.5079\n",
      "360/388, train_loss: 0.1201, step time: 0.5352\n",
      "361/388, train_loss: 0.2328, step time: 0.5136\n",
      "362/388, train_loss: 0.1184, step time: 0.4899\n",
      "363/388, train_loss: 0.2602, step time: 1.1739\n",
      "364/388, train_loss: 0.1648, step time: 0.5318\n",
      "365/388, train_loss: 0.2854, step time: 0.5115\n",
      "366/388, train_loss: 0.1102, step time: 0.4988\n",
      "367/388, train_loss: 0.3592, step time: 0.4900\n",
      "368/388, train_loss: 0.1030, step time: 0.9078\n",
      "369/388, train_loss: 0.1996, step time: 0.5347\n",
      "370/388, train_loss: 0.1973, step time: 0.5172\n",
      "371/388, train_loss: 0.2110, step time: 0.4920\n",
      "372/388, train_loss: 0.1177, step time: 0.5819\n",
      "373/388, train_loss: 0.5055, step time: 0.5589\n",
      "374/388, train_loss: 0.1464, step time: 0.5264\n",
      "375/388, train_loss: 0.2731, step time: 0.5126\n",
      "376/388, train_loss: 0.1652, step time: 0.4989\n",
      "377/388, train_loss: 0.0567, step time: 0.5017\n",
      "378/388, train_loss: 0.2323, step time: 0.4879\n",
      "379/388, train_loss: 0.1907, step time: 0.5024\n",
      "380/388, train_loss: 0.1867, step time: 0.5172\n",
      "381/388, train_loss: 0.1875, step time: 0.4939\n",
      "382/388, train_loss: 0.4699, step time: 1.1265\n",
      "383/388, train_loss: 0.1614, step time: 0.5266\n",
      "384/388, train_loss: 0.0899, step time: 0.5188\n",
      "385/388, train_loss: 0.1377, step time: 0.5046\n",
      "386/388, train_loss: 0.1888, step time: 0.4856\n",
      "387/388, train_loss: 0.2475, step time: 0.4867\n",
      "388/388, train_loss: 0.1666, step time: 0.4837\n",
      "epoch 46 average loss: 0.1947\n",
      "current epoch: 46 current mean dice: 0.7530 tc: 0.8007 wt: 0.8859 et: 0.5725\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 46 is: 303.2435\n",
      "----------\n",
      "epoch 47/300\n",
      "1/388, train_loss: 0.0654, step time: 0.4819\n",
      "2/388, train_loss: 0.0744, step time: 0.4767\n",
      "3/388, train_loss: 0.2598, step time: 1.0856\n",
      "4/388, train_loss: 0.1953, step time: 0.5561\n",
      "5/388, train_loss: 0.1585, step time: 0.5086\n",
      "6/388, train_loss: 0.0864, step time: 0.4956\n",
      "7/388, train_loss: 0.1026, step time: 0.5140\n",
      "8/388, train_loss: 0.1413, step time: 0.7830\n",
      "9/388, train_loss: 0.3142, step time: 0.5490\n",
      "10/388, train_loss: 0.1563, step time: 0.5197\n",
      "11/388, train_loss: 0.2892, step time: 0.5274\n",
      "12/388, train_loss: 0.4766, step time: 0.5747\n",
      "13/388, train_loss: 0.2092, step time: 0.5477\n",
      "14/388, train_loss: 0.1420, step time: 0.5200\n",
      "15/388, train_loss: 0.0368, step time: 0.5082\n",
      "16/388, train_loss: 0.2044, step time: 0.4897\n",
      "17/388, train_loss: 0.4375, step time: 0.4977\n",
      "18/388, train_loss: 0.2646, step time: 1.1877\n",
      "19/388, train_loss: 0.0999, step time: 0.5464\n",
      "20/388, train_loss: 0.2923, step time: 0.5140\n",
      "21/388, train_loss: 0.0995, step time: 0.5015\n",
      "22/388, train_loss: 0.1124, step time: 0.4853\n",
      "23/388, train_loss: 0.0789, step time: 0.5091\n",
      "24/388, train_loss: 0.1962, step time: 0.5177\n",
      "25/388, train_loss: 0.2220, step time: 0.5103\n",
      "26/388, train_loss: 0.1021, step time: 0.4917\n",
      "27/388, train_loss: 0.2676, step time: 0.5142\n",
      "28/388, train_loss: 0.0979, step time: 0.5585\n",
      "29/388, train_loss: 0.0688, step time: 0.5356\n",
      "30/388, train_loss: 0.2682, step time: 0.5114\n",
      "31/388, train_loss: 0.2720, step time: 0.5189\n",
      "32/388, train_loss: 0.1813, step time: 0.5540\n",
      "33/388, train_loss: 0.0530, step time: 0.6214\n",
      "34/388, train_loss: 0.1310, step time: 0.5623\n",
      "35/388, train_loss: 0.1283, step time: 0.5199\n",
      "36/388, train_loss: 0.1726, step time: 0.5092\n",
      "37/388, train_loss: 0.1311, step time: 0.5113\n",
      "38/388, train_loss: 0.3358, step time: 0.5585\n",
      "39/388, train_loss: 0.2231, step time: 0.5422\n",
      "40/388, train_loss: 0.1471, step time: 0.5163\n",
      "41/388, train_loss: 0.4213, step time: 0.5148\n",
      "42/388, train_loss: 0.1978, step time: 0.5181\n",
      "43/388, train_loss: 0.1184, step time: 0.5029\n",
      "44/388, train_loss: 0.1833, step time: 0.5150\n",
      "45/388, train_loss: 0.2136, step time: 0.5841\n",
      "46/388, train_loss: 0.3073, step time: 0.5729\n",
      "47/388, train_loss: 0.0513, step time: 0.5224\n",
      "48/388, train_loss: 0.4226, step time: 0.5068\n",
      "49/388, train_loss: 0.1387, step time: 0.5056\n",
      "50/388, train_loss: 0.0922, step time: 0.5326\n",
      "51/388, train_loss: 0.1100, step time: 0.5218\n",
      "52/388, train_loss: 0.4540, step time: 0.5051\n",
      "53/388, train_loss: 0.3143, step time: 0.4833\n",
      "54/388, train_loss: 0.1502, step time: 0.4828\n",
      "55/388, train_loss: 0.0988, step time: 0.4821\n",
      "56/388, train_loss: 0.1964, step time: 0.7170\n",
      "57/388, train_loss: 0.2142, step time: 0.5587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/388, train_loss: 0.0750, step time: 0.5168\n",
      "59/388, train_loss: 0.2414, step time: 0.5006\n",
      "60/388, train_loss: 0.3833, step time: 0.4927\n",
      "61/388, train_loss: 0.3576, step time: 0.4831\n",
      "62/388, train_loss: 0.2856, step time: 0.4789\n",
      "63/388, train_loss: 0.0765, step time: 0.5073\n",
      "64/388, train_loss: 0.1395, step time: 0.5105\n",
      "65/388, train_loss: 0.2548, step time: 0.5214\n",
      "66/388, train_loss: 0.1597, step time: 0.5381\n",
      "67/388, train_loss: 0.1877, step time: 0.6038\n",
      "68/388, train_loss: 0.1952, step time: 0.5305\n",
      "69/388, train_loss: 0.1715, step time: 0.4997\n",
      "70/388, train_loss: 0.1101, step time: 0.4963\n",
      "71/388, train_loss: 0.3529, step time: 0.5837\n",
      "72/388, train_loss: 0.1289, step time: 0.5598\n",
      "73/388, train_loss: 0.0916, step time: 0.5265\n",
      "74/388, train_loss: 0.1902, step time: 0.5092\n",
      "75/388, train_loss: 0.0986, step time: 0.4914\n",
      "76/388, train_loss: 0.0712, step time: 0.4902\n",
      "77/388, train_loss: 0.1601, step time: 1.1980\n",
      "78/388, train_loss: 0.1286, step time: 0.5499\n",
      "79/388, train_loss: 0.1117, step time: 0.5122\n",
      "80/388, train_loss: 0.0668, step time: 0.5018\n",
      "81/388, train_loss: 0.1967, step time: 0.4982\n",
      "82/388, train_loss: 0.0803, step time: 0.4944\n",
      "83/388, train_loss: 0.2857, step time: 0.4946\n",
      "84/388, train_loss: 0.1082, step time: 0.4800\n",
      "85/388, train_loss: 0.1751, step time: 0.5413\n",
      "86/388, train_loss: 0.2946, step time: 0.5299\n",
      "87/388, train_loss: 0.1664, step time: 0.5103\n",
      "88/388, train_loss: 0.1969, step time: 0.5879\n",
      "89/388, train_loss: 0.3259, step time: 0.6027\n",
      "90/388, train_loss: 0.2533, step time: 0.5340\n",
      "91/388, train_loss: 0.2152, step time: 0.4951\n",
      "92/388, train_loss: 0.3390, step time: 0.4963\n",
      "93/388, train_loss: 0.2732, step time: 0.5455\n",
      "94/388, train_loss: 0.1765, step time: 0.7221\n",
      "95/388, train_loss: 0.0930, step time: 0.5490\n",
      "96/388, train_loss: 0.1696, step time: 0.5084\n",
      "97/388, train_loss: 0.2098, step time: 0.5020\n",
      "98/388, train_loss: 0.0990, step time: 0.4863\n",
      "99/388, train_loss: 0.2100, step time: 0.4853\n",
      "100/388, train_loss: 0.0688, step time: 1.1544\n",
      "101/388, train_loss: 0.1469, step time: 0.5410\n",
      "102/388, train_loss: 0.1006, step time: 0.5106\n",
      "103/388, train_loss: 0.1923, step time: 0.4871\n",
      "104/388, train_loss: 0.1779, step time: 0.4874\n",
      "105/388, train_loss: 0.1486, step time: 0.4932\n",
      "106/388, train_loss: 0.3755, step time: 0.4829\n",
      "107/388, train_loss: 0.1443, step time: 0.4931\n",
      "108/388, train_loss: 0.3005, step time: 0.4878\n",
      "109/388, train_loss: 0.0878, step time: 0.5000\n",
      "110/388, train_loss: 0.0755, step time: 0.4931\n",
      "111/388, train_loss: 0.1439, step time: 0.4836\n",
      "112/388, train_loss: 0.1830, step time: 0.6860\n",
      "113/388, train_loss: 0.1609, step time: 0.5433\n",
      "114/388, train_loss: 0.1620, step time: 0.5207\n",
      "115/388, train_loss: 0.3225, step time: 0.5177\n",
      "116/388, train_loss: 0.0888, step time: 0.4889\n",
      "117/388, train_loss: 0.3001, step time: 0.4843\n",
      "118/388, train_loss: 0.4712, step time: 0.5001\n",
      "119/388, train_loss: 0.0895, step time: 0.5253\n",
      "120/388, train_loss: 0.0657, step time: 0.5018\n",
      "121/388, train_loss: 0.1972, step time: 0.4893\n",
      "122/388, train_loss: 0.1791, step time: 0.4951\n",
      "123/388, train_loss: 0.1337, step time: 0.4855\n",
      "124/388, train_loss: 0.4323, step time: 0.4835\n",
      "125/388, train_loss: 0.5416, step time: 0.4812\n",
      "126/388, train_loss: 0.2449, step time: 1.2019\n",
      "127/388, train_loss: 0.2490, step time: 0.5432\n",
      "128/388, train_loss: 0.2014, step time: 0.5094\n",
      "129/388, train_loss: 0.2289, step time: 0.4995\n",
      "130/388, train_loss: 0.2499, step time: 0.4986\n",
      "131/388, train_loss: 0.0997, step time: 0.4820\n",
      "132/388, train_loss: 0.1863, step time: 0.4821\n",
      "133/388, train_loss: 0.3682, step time: 0.4885\n",
      "134/388, train_loss: 0.2038, step time: 0.4842\n",
      "135/388, train_loss: 0.0916, step time: 0.9445\n",
      "136/388, train_loss: 0.3513, step time: 0.5212\n",
      "137/388, train_loss: 0.5174, step time: 0.5122\n",
      "138/388, train_loss: 0.0869, step time: 0.4849\n",
      "139/388, train_loss: 0.1417, step time: 0.4883\n",
      "140/388, train_loss: 0.3464, step time: 0.4896\n",
      "141/388, train_loss: 0.1771, step time: 0.4776\n",
      "142/388, train_loss: 0.0301, step time: 0.9746\n",
      "143/388, train_loss: 0.0626, step time: 0.5324\n",
      "144/388, train_loss: 0.3750, step time: 0.5008\n",
      "145/388, train_loss: 0.2821, step time: 0.4942\n",
      "146/388, train_loss: 0.1367, step time: 0.4811\n",
      "147/388, train_loss: 0.2592, step time: 0.4880\n",
      "148/388, train_loss: 0.1385, step time: 0.4789\n",
      "149/388, train_loss: 0.1158, step time: 0.4742\n",
      "150/388, train_loss: 0.1695, step time: 0.4816\n",
      "151/388, train_loss: 0.1200, step time: 0.5000\n",
      "152/388, train_loss: 0.2396, step time: 0.5605\n",
      "153/388, train_loss: 0.1704, step time: 0.5204\n",
      "154/388, train_loss: 0.2146, step time: 0.5088\n",
      "155/388, train_loss: 0.0993, step time: 0.4986\n",
      "156/388, train_loss: 0.1320, step time: 0.4966\n",
      "157/388, train_loss: 0.2590, step time: 0.4836\n",
      "158/388, train_loss: 0.1431, step time: 0.5078\n",
      "159/388, train_loss: 0.1076, step time: 0.4911\n",
      "160/388, train_loss: 0.5069, step time: 0.5010\n",
      "161/388, train_loss: 0.0432, step time: 0.4839\n",
      "162/388, train_loss: 0.2233, step time: 0.5054\n",
      "163/388, train_loss: 0.0687, step time: 0.5621\n",
      "164/388, train_loss: 0.0644, step time: 0.5308\n",
      "165/388, train_loss: 0.0670, step time: 0.5143\n",
      "166/388, train_loss: 0.1144, step time: 0.4881\n",
      "167/388, train_loss: 0.1134, step time: 0.4980\n",
      "168/388, train_loss: 0.1400, step time: 0.4942\n",
      "169/388, train_loss: 0.0745, step time: 0.4787\n",
      "170/388, train_loss: 0.1535, step time: 0.4795\n",
      "171/388, train_loss: 0.1981, step time: 0.4850\n",
      "172/388, train_loss: 0.2396, step time: 0.5033\n",
      "173/388, train_loss: 0.2625, step time: 0.5077\n",
      "174/388, train_loss: 0.2047, step time: 0.5021\n",
      "175/388, train_loss: 0.0844, step time: 0.5022\n",
      "176/388, train_loss: 0.0724, step time: 0.4826\n",
      "177/388, train_loss: 0.0699, step time: 0.4945\n",
      "178/388, train_loss: 0.3715, step time: 0.4941\n",
      "179/388, train_loss: 0.2296, step time: 0.5815\n",
      "180/388, train_loss: 0.1030, step time: 0.5828\n",
      "181/388, train_loss: 0.2513, step time: 0.5564\n",
      "182/388, train_loss: 0.2865, step time: 0.5105\n",
      "183/388, train_loss: 0.1605, step time: 0.4968\n",
      "184/388, train_loss: 0.0634, step time: 1.0996\n",
      "185/388, train_loss: 0.1354, step time: 0.5215\n",
      "186/388, train_loss: 0.2878, step time: 0.5038\n",
      "187/388, train_loss: 0.2155, step time: 0.4863\n",
      "188/388, train_loss: 0.1150, step time: 0.4946\n",
      "189/388, train_loss: 0.3019, step time: 0.4795\n",
      "190/388, train_loss: 0.1414, step time: 0.4825\n",
      "191/388, train_loss: 0.1286, step time: 0.4737\n",
      "192/388, train_loss: 0.1126, step time: 0.4758\n",
      "193/388, train_loss: 0.1463, step time: 0.4725\n",
      "194/388, train_loss: 0.1809, step time: 0.5749\n",
      "195/388, train_loss: 0.2189, step time: 0.5805\n",
      "196/388, train_loss: 0.1084, step time: 0.5366\n",
      "197/388, train_loss: 0.1410, step time: 0.5076\n",
      "198/388, train_loss: 0.2485, step time: 0.4973\n",
      "199/388, train_loss: 0.2786, step time: 0.5044\n",
      "200/388, train_loss: 0.0911, step time: 0.5496\n",
      "201/388, train_loss: 0.1121, step time: 0.5291\n",
      "202/388, train_loss: 0.3865, step time: 0.5101\n",
      "203/388, train_loss: 0.4825, step time: 0.5003\n",
      "204/388, train_loss: 0.0972, step time: 0.4874\n",
      "205/388, train_loss: 0.2905, step time: 0.4862\n",
      "206/388, train_loss: 0.6170, step time: 0.4890\n",
      "207/388, train_loss: 0.1494, step time: 0.4798\n",
      "208/388, train_loss: 0.0982, step time: 0.4826\n",
      "209/388, train_loss: 0.1305, step time: 0.5257\n",
      "210/388, train_loss: 0.2357, step time: 0.5254\n",
      "211/388, train_loss: 0.0648, step time: 0.5051\n",
      "212/388, train_loss: 0.2239, step time: 0.5422\n",
      "213/388, train_loss: 0.1339, step time: 0.5100\n",
      "214/388, train_loss: 0.1223, step time: 0.5095\n",
      "215/388, train_loss: 0.1547, step time: 0.4887\n",
      "216/388, train_loss: 0.0842, step time: 0.4885\n",
      "217/388, train_loss: 0.0440, step time: 0.4970\n",
      "218/388, train_loss: 0.3105, step time: 0.5344\n",
      "219/388, train_loss: 0.1746, step time: 0.5379\n",
      "220/388, train_loss: 0.5061, step time: 0.5201\n",
      "221/388, train_loss: 0.0803, step time: 0.5107\n",
      "222/388, train_loss: 0.1302, step time: 0.5002\n",
      "223/388, train_loss: 0.1778, step time: 0.5049\n",
      "224/388, train_loss: 0.1880, step time: 0.5569\n",
      "225/388, train_loss: 0.4359, step time: 0.5344\n",
      "226/388, train_loss: 0.2048, step time: 0.5202\n",
      "227/388, train_loss: 0.2044, step time: 0.5058\n",
      "228/388, train_loss: 0.2586, step time: 0.5283\n",
      "229/388, train_loss: 0.2877, step time: 0.5220\n",
      "230/388, train_loss: 0.6098, step time: 0.7120\n",
      "231/388, train_loss: 0.1258, step time: 0.5544\n",
      "232/388, train_loss: 0.2661, step time: 0.5290\n",
      "233/388, train_loss: 0.0328, step time: 0.5084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/388, train_loss: 0.2418, step time: 0.4897\n",
      "235/388, train_loss: 0.2687, step time: 0.4995\n",
      "236/388, train_loss: 0.0884, step time: 0.4814\n",
      "237/388, train_loss: 0.3440, step time: 1.1218\n",
      "238/388, train_loss: 0.2879, step time: 0.5284\n",
      "239/388, train_loss: 0.0874, step time: 0.4975\n",
      "240/388, train_loss: 0.1899, step time: 0.4981\n",
      "241/388, train_loss: 0.2209, step time: 0.4869\n",
      "242/388, train_loss: 0.1054, step time: 0.5088\n",
      "243/388, train_loss: 0.2922, step time: 0.4984\n",
      "244/388, train_loss: 0.2399, step time: 0.4844\n",
      "245/388, train_loss: 0.1213, step time: 0.4898\n",
      "246/388, train_loss: 0.4260, step time: 0.4923\n",
      "247/388, train_loss: 0.4301, step time: 0.5542\n",
      "248/388, train_loss: 0.3001, step time: 0.5332\n",
      "249/388, train_loss: 0.0609, step time: 0.5118\n",
      "250/388, train_loss: 0.0615, step time: 0.5077\n",
      "251/388, train_loss: 0.1194, step time: 0.4896\n",
      "252/388, train_loss: 0.0959, step time: 0.4917\n",
      "253/388, train_loss: 0.1501, step time: 0.4941\n",
      "254/388, train_loss: 0.0963, step time: 0.4929\n",
      "255/388, train_loss: 0.1128, step time: 0.7006\n",
      "256/388, train_loss: 0.2915, step time: 0.5573\n",
      "257/388, train_loss: 0.1705, step time: 0.5174\n",
      "258/388, train_loss: 0.1166, step time: 0.4902\n",
      "259/388, train_loss: 0.1004, step time: 0.4935\n",
      "260/388, train_loss: 0.1332, step time: 0.4821\n",
      "261/388, train_loss: 0.0831, step time: 0.4785\n",
      "262/388, train_loss: 0.5309, step time: 0.4991\n",
      "263/388, train_loss: 0.1338, step time: 0.5019\n",
      "264/388, train_loss: 0.0620, step time: 0.4942\n",
      "265/388, train_loss: 0.2297, step time: 0.5040\n",
      "266/388, train_loss: 0.3015, step time: 1.1654\n",
      "267/388, train_loss: 0.1490, step time: 0.5375\n",
      "268/388, train_loss: 0.1022, step time: 0.5215\n",
      "269/388, train_loss: 0.1341, step time: 0.4990\n",
      "270/388, train_loss: 0.2122, step time: 0.4971\n",
      "271/388, train_loss: 0.1050, step time: 0.4799\n",
      "272/388, train_loss: 0.1065, step time: 0.4885\n",
      "273/388, train_loss: 0.4226, step time: 0.5500\n",
      "274/388, train_loss: 0.1545, step time: 0.5251\n",
      "275/388, train_loss: 0.1031, step time: 0.5127\n",
      "276/388, train_loss: 0.1023, step time: 0.4944\n",
      "277/388, train_loss: 0.1208, step time: 0.5026\n",
      "278/388, train_loss: 0.0563, step time: 0.4867\n",
      "279/388, train_loss: 0.1808, step time: 0.4852\n",
      "280/388, train_loss: 0.0504, step time: 1.0213\n",
      "281/388, train_loss: 0.2006, step time: 0.5413\n",
      "282/388, train_loss: 0.2190, step time: 0.5108\n",
      "283/388, train_loss: 0.2058, step time: 0.4989\n",
      "284/388, train_loss: 0.3176, step time: 0.4843\n",
      "285/388, train_loss: 0.1758, step time: 0.5160\n",
      "286/388, train_loss: 0.1145, step time: 0.5064\n",
      "287/388, train_loss: 0.1837, step time: 0.5001\n",
      "288/388, train_loss: 0.2299, step time: 0.5013\n",
      "289/388, train_loss: 0.2947, step time: 0.4965\n",
      "290/388, train_loss: 0.0863, step time: 0.5007\n",
      "291/388, train_loss: 0.1024, step time: 0.4785\n",
      "292/388, train_loss: 0.0977, step time: 0.4742\n",
      "293/388, train_loss: 0.3293, step time: 0.4818\n",
      "294/388, train_loss: 0.1622, step time: 0.5950\n",
      "295/388, train_loss: 0.2493, step time: 0.5757\n",
      "296/388, train_loss: 0.1273, step time: 0.5244\n",
      "297/388, train_loss: 0.0910, step time: 0.5048\n",
      "298/388, train_loss: 0.1478, step time: 0.5062\n",
      "299/388, train_loss: 0.0785, step time: 0.5411\n",
      "300/388, train_loss: 0.0998, step time: 0.5428\n",
      "301/388, train_loss: 0.2778, step time: 0.6401\n",
      "302/388, train_loss: 0.2344, step time: 0.5353\n",
      "303/388, train_loss: 0.2948, step time: 0.5173\n",
      "304/388, train_loss: 0.6195, step time: 0.5043\n",
      "305/388, train_loss: 0.1935, step time: 0.5099\n",
      "306/388, train_loss: 0.0697, step time: 0.5599\n",
      "307/388, train_loss: 0.2001, step time: 0.5279\n",
      "308/388, train_loss: 0.3112, step time: 0.5088\n",
      "309/388, train_loss: 0.1781, step time: 0.4894\n",
      "310/388, train_loss: 0.2103, step time: 0.5101\n",
      "311/388, train_loss: 0.2020, step time: 0.4998\n",
      "312/388, train_loss: 0.2064, step time: 0.4989\n",
      "313/388, train_loss: 0.0910, step time: 1.0336\n",
      "314/388, train_loss: 0.1349, step time: 0.5357\n",
      "315/388, train_loss: 0.1865, step time: 0.5086\n",
      "316/388, train_loss: 0.1342, step time: 0.5013\n",
      "317/388, train_loss: 0.5338, step time: 0.4892\n",
      "318/388, train_loss: 0.1711, step time: 0.4814\n",
      "319/388, train_loss: 0.1615, step time: 0.4909\n",
      "320/388, train_loss: 0.0749, step time: 0.4868\n",
      "321/388, train_loss: 0.5279, step time: 0.4919\n",
      "322/388, train_loss: 0.1890, step time: 0.8919\n",
      "323/388, train_loss: 0.0932, step time: 0.5322\n",
      "324/388, train_loss: 0.1656, step time: 0.5151\n",
      "325/388, train_loss: 0.0959, step time: 0.4951\n",
      "326/388, train_loss: 0.1411, step time: 0.4943\n",
      "327/388, train_loss: 0.2322, step time: 0.4887\n",
      "328/388, train_loss: 0.0626, step time: 0.5244\n",
      "329/388, train_loss: 0.0862, step time: 0.5017\n",
      "330/388, train_loss: 0.0614, step time: 0.4967\n",
      "331/388, train_loss: 0.1311, step time: 0.4847\n",
      "332/388, train_loss: 0.2364, step time: 0.4762\n",
      "333/388, train_loss: 0.0557, step time: 0.4839\n",
      "334/388, train_loss: 0.1175, step time: 0.7560\n",
      "335/388, train_loss: 0.3238, step time: 0.5595\n",
      "336/388, train_loss: 0.1952, step time: 0.5051\n",
      "337/388, train_loss: 0.1278, step time: 0.4928\n",
      "338/388, train_loss: 0.1032, step time: 0.5101\n",
      "339/388, train_loss: 0.1935, step time: 0.5240\n",
      "340/388, train_loss: 0.0873, step time: 0.5693\n",
      "341/388, train_loss: 0.1462, step time: 0.5296\n",
      "342/388, train_loss: 0.0808, step time: 0.5057\n",
      "343/388, train_loss: 0.2068, step time: 0.4975\n",
      "344/388, train_loss: 0.1313, step time: 0.5338\n",
      "345/388, train_loss: 0.1620, step time: 0.4961\n",
      "346/388, train_loss: 0.2933, step time: 0.5116\n",
      "347/388, train_loss: 0.1002, step time: 0.4949\n",
      "348/388, train_loss: 0.2486, step time: 0.4830\n",
      "349/388, train_loss: 0.1041, step time: 1.0978\n",
      "350/388, train_loss: 0.0749, step time: 0.5435\n",
      "351/388, train_loss: 0.4634, step time: 0.5203\n",
      "352/388, train_loss: 0.2411, step time: 0.4983\n",
      "353/388, train_loss: 0.1972, step time: 0.4949\n",
      "354/388, train_loss: 0.2892, step time: 0.4937\n",
      "355/388, train_loss: 0.2179, step time: 0.4836\n",
      "356/388, train_loss: 0.1264, step time: 1.0324\n",
      "357/388, train_loss: 0.1267, step time: 0.5343\n",
      "358/388, train_loss: 0.2263, step time: 0.5101\n",
      "359/388, train_loss: 0.0982, step time: 0.4987\n",
      "360/388, train_loss: 0.3132, step time: 0.4843\n",
      "361/388, train_loss: 0.1456, step time: 0.4984\n",
      "362/388, train_loss: 0.0997, step time: 0.5385\n",
      "363/388, train_loss: 0.1462, step time: 0.5080\n",
      "364/388, train_loss: 0.1951, step time: 0.4867\n",
      "365/388, train_loss: 0.1854, step time: 0.4905\n",
      "366/388, train_loss: 0.1474, step time: 0.4858\n",
      "367/388, train_loss: 0.1888, step time: 1.0422\n",
      "368/388, train_loss: 0.2629, step time: 0.5288\n",
      "369/388, train_loss: 0.0777, step time: 0.5051\n",
      "370/388, train_loss: 0.1161, step time: 0.4900\n",
      "371/388, train_loss: 0.1147, step time: 0.4948\n",
      "372/388, train_loss: 0.2350, step time: 0.4910\n",
      "373/388, train_loss: 0.2486, step time: 0.4908\n",
      "374/388, train_loss: 0.1395, step time: 0.9946\n",
      "375/388, train_loss: 0.1195, step time: 0.5315\n",
      "376/388, train_loss: 0.3130, step time: 0.5089\n",
      "377/388, train_loss: 0.1888, step time: 0.5150\n",
      "378/388, train_loss: 0.2943, step time: 0.5027\n",
      "379/388, train_loss: 0.2104, step time: 0.5004\n",
      "380/388, train_loss: 0.3469, step time: 0.4878\n",
      "381/388, train_loss: 0.0962, step time: 0.4824\n",
      "382/388, train_loss: 0.3056, step time: 0.4885\n",
      "383/388, train_loss: 0.1198, step time: 0.4702\n",
      "384/388, train_loss: 0.2307, step time: 0.6117\n",
      "385/388, train_loss: 0.0886, step time: 0.5275\n",
      "386/388, train_loss: 0.3395, step time: 0.5022\n",
      "387/388, train_loss: 0.0659, step time: 0.4931\n",
      "388/388, train_loss: 0.1620, step time: 0.4814\n",
      "epoch 47 average loss: 0.1912\n",
      "current epoch: 47 current mean dice: 0.7586 tc: 0.8072 wt: 0.8954 et: 0.5733\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 47 is: 299.4912\n",
      "----------\n",
      "epoch 48/300\n",
      "1/388, train_loss: 0.2313, step time: 0.4709\n",
      "2/388, train_loss: 0.1130, step time: 0.4825\n",
      "3/388, train_loss: 0.2173, step time: 0.6047\n",
      "4/388, train_loss: 0.1065, step time: 0.6569\n",
      "5/388, train_loss: 0.1920, step time: 0.6129\n",
      "6/388, train_loss: 0.1083, step time: 0.5404\n",
      "7/388, train_loss: 0.1362, step time: 0.5136\n",
      "8/388, train_loss: 0.2296, step time: 0.5014\n",
      "9/388, train_loss: 0.1257, step time: 0.7080\n",
      "10/388, train_loss: 0.5178, step time: 0.5359\n",
      "11/388, train_loss: 0.2072, step time: 0.4930\n",
      "12/388, train_loss: 0.1892, step time: 0.6247\n",
      "13/388, train_loss: 0.1142, step time: 0.5616\n",
      "14/388, train_loss: 0.1779, step time: 0.5183\n",
      "15/388, train_loss: 0.1970, step time: 0.4918\n",
      "16/388, train_loss: 0.3320, step time: 1.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/388, train_loss: 0.0902, step time: 0.5262\n",
      "18/388, train_loss: 0.1993, step time: 0.5039\n",
      "19/388, train_loss: 0.1526, step time: 0.5005\n",
      "20/388, train_loss: 0.1841, step time: 0.5550\n",
      "21/388, train_loss: 0.1137, step time: 0.5301\n",
      "22/388, train_loss: 0.2834, step time: 0.5377\n",
      "23/388, train_loss: 0.1348, step time: 0.6179\n",
      "24/388, train_loss: 0.1312, step time: 0.5741\n",
      "25/388, train_loss: 0.1821, step time: 0.5384\n",
      "26/388, train_loss: 0.5058, step time: 0.5037\n",
      "27/388, train_loss: 0.0876, step time: 1.0000\n",
      "28/388, train_loss: 0.2788, step time: 0.5658\n",
      "29/388, train_loss: 0.1366, step time: 0.5432\n",
      "30/388, train_loss: 0.1644, step time: 0.5203\n",
      "31/388, train_loss: 0.2415, step time: 0.5024\n",
      "32/388, train_loss: 0.2453, step time: 0.4919\n",
      "33/388, train_loss: 0.2072, step time: 0.4906\n",
      "34/388, train_loss: 0.0698, step time: 0.4780\n",
      "35/388, train_loss: 0.1315, step time: 1.0378\n",
      "36/388, train_loss: 0.2551, step time: 0.5664\n",
      "37/388, train_loss: 0.1105, step time: 0.5227\n",
      "38/388, train_loss: 0.0975, step time: 0.4992\n",
      "39/388, train_loss: 0.2177, step time: 0.5306\n",
      "40/388, train_loss: 0.4428, step time: 0.6606\n",
      "41/388, train_loss: 0.1537, step time: 0.5608\n",
      "42/388, train_loss: 0.0943, step time: 0.5167\n",
      "43/388, train_loss: 0.3680, step time: 0.5024\n",
      "44/388, train_loss: 0.1377, step time: 0.5096\n",
      "45/388, train_loss: 0.1699, step time: 0.5151\n",
      "46/388, train_loss: 0.1612, step time: 0.5044\n",
      "47/388, train_loss: 0.1854, step time: 0.4962\n",
      "48/388, train_loss: 0.1756, step time: 0.5061\n",
      "49/388, train_loss: 0.2749, step time: 0.5136\n",
      "50/388, train_loss: 0.3786, step time: 0.5043\n",
      "51/388, train_loss: 0.0934, step time: 0.5174\n",
      "52/388, train_loss: 0.1183, step time: 0.5063\n",
      "53/388, train_loss: 0.2910, step time: 0.5163\n",
      "54/388, train_loss: 0.2624, step time: 0.5920\n",
      "55/388, train_loss: 0.0991, step time: 0.5641\n",
      "56/388, train_loss: 0.2768, step time: 0.5540\n",
      "57/388, train_loss: 0.2298, step time: 0.5234\n",
      "58/388, train_loss: 0.0965, step time: 0.5077\n",
      "59/388, train_loss: 0.3806, step time: 0.5242\n",
      "60/388, train_loss: 0.0866, step time: 0.5003\n",
      "61/388, train_loss: 0.0511, step time: 0.5003\n",
      "62/388, train_loss: 0.2682, step time: 0.5083\n",
      "63/388, train_loss: 0.1831, step time: 0.5829\n",
      "64/388, train_loss: 0.1019, step time: 0.5448\n",
      "65/388, train_loss: 0.1815, step time: 0.5350\n",
      "66/388, train_loss: 0.1728, step time: 0.5085\n",
      "67/388, train_loss: 0.1651, step time: 0.5683\n",
      "68/388, train_loss: 0.0687, step time: 0.5842\n",
      "69/388, train_loss: 0.1627, step time: 0.5330\n",
      "70/388, train_loss: 0.1932, step time: 0.5147\n",
      "71/388, train_loss: 0.1435, step time: 0.4935\n",
      "72/388, train_loss: 0.1174, step time: 0.4916\n",
      "73/388, train_loss: 0.3861, step time: 0.6955\n",
      "74/388, train_loss: 0.0734, step time: 0.5485\n",
      "75/388, train_loss: 0.1396, step time: 0.5254\n",
      "76/388, train_loss: 0.2817, step time: 0.5032\n",
      "77/388, train_loss: 0.1040, step time: 0.4965\n",
      "78/388, train_loss: 0.2343, step time: 0.4911\n",
      "79/388, train_loss: 0.2756, step time: 0.5516\n",
      "80/388, train_loss: 0.2601, step time: 0.5355\n",
      "81/388, train_loss: 0.1200, step time: 0.5047\n",
      "82/388, train_loss: 0.1200, step time: 0.4969\n",
      "83/388, train_loss: 0.1013, step time: 0.4943\n",
      "84/388, train_loss: 0.1260, step time: 0.9987\n",
      "85/388, train_loss: 0.1313, step time: 0.5526\n",
      "86/388, train_loss: 0.0678, step time: 0.5037\n",
      "87/388, train_loss: 0.0795, step time: 0.4975\n",
      "88/388, train_loss: 0.2556, step time: 0.4877\n",
      "89/388, train_loss: 0.1196, step time: 0.5025\n",
      "90/388, train_loss: 0.2397, step time: 0.6628\n",
      "91/388, train_loss: 0.2386, step time: 0.5570\n",
      "92/388, train_loss: 0.0968, step time: 0.5345\n",
      "93/388, train_loss: 0.1782, step time: 0.4990\n",
      "94/388, train_loss: 0.1439, step time: 1.1520\n",
      "95/388, train_loss: 0.0518, step time: 0.5424\n",
      "96/388, train_loss: 0.2373, step time: 0.5096\n",
      "97/388, train_loss: 0.4855, step time: 0.4988\n",
      "98/388, train_loss: 0.2855, step time: 0.4905\n",
      "99/388, train_loss: 0.0959, step time: 0.4918\n",
      "100/388, train_loss: 0.0478, step time: 0.4779\n",
      "101/388, train_loss: 0.0916, step time: 0.9387\n",
      "102/388, train_loss: 0.1960, step time: 0.5294\n",
      "103/388, train_loss: 0.1766, step time: 0.5066\n",
      "104/388, train_loss: 0.0488, step time: 0.4943\n",
      "105/388, train_loss: 0.0707, step time: 0.4991\n",
      "106/388, train_loss: 0.0996, step time: 0.4825\n",
      "107/388, train_loss: 0.3195, step time: 0.5014\n",
      "108/388, train_loss: 0.1172, step time: 0.5406\n",
      "109/388, train_loss: 0.4204, step time: 0.5190\n",
      "110/388, train_loss: 0.1702, step time: 0.5106\n",
      "111/388, train_loss: 0.1930, step time: 0.4985\n",
      "112/388, train_loss: 0.1212, step time: 0.4909\n",
      "113/388, train_loss: 0.3004, step time: 0.9188\n",
      "114/388, train_loss: 0.4821, step time: 0.5552\n",
      "115/388, train_loss: 0.2448, step time: 0.5173\n",
      "116/388, train_loss: 0.2069, step time: 0.4947\n",
      "117/388, train_loss: 0.1715, step time: 0.4905\n",
      "118/388, train_loss: 0.0729, step time: 0.4907\n",
      "119/388, train_loss: 0.1791, step time: 0.4898\n",
      "120/388, train_loss: 0.1035, step time: 0.4987\n",
      "121/388, train_loss: 0.1251, step time: 0.4821\n",
      "122/388, train_loss: 0.2261, step time: 0.4933\n",
      "123/388, train_loss: 0.5519, step time: 0.8362\n",
      "124/388, train_loss: 0.1287, step time: 0.5492\n",
      "125/388, train_loss: 0.1502, step time: 0.5263\n",
      "126/388, train_loss: 0.0622, step time: 0.4980\n",
      "127/388, train_loss: 0.1393, step time: 0.4992\n",
      "128/388, train_loss: 0.1057, step time: 0.5161\n",
      "129/388, train_loss: 0.1593, step time: 0.4894\n",
      "130/388, train_loss: 0.1713, step time: 0.4917\n",
      "131/388, train_loss: 0.0689, step time: 0.4906\n",
      "132/388, train_loss: 0.2497, step time: 0.4952\n",
      "133/388, train_loss: 0.1261, step time: 0.4810\n",
      "134/388, train_loss: 0.1277, step time: 0.5309\n",
      "135/388, train_loss: 0.1379, step time: 0.5087\n",
      "136/388, train_loss: 0.0787, step time: 0.5488\n",
      "137/388, train_loss: 0.5546, step time: 0.5313\n",
      "138/388, train_loss: 0.1641, step time: 0.5030\n",
      "139/388, train_loss: 0.1639, step time: 0.5444\n",
      "140/388, train_loss: 0.1580, step time: 0.5325\n",
      "141/388, train_loss: 0.0912, step time: 0.5205\n",
      "142/388, train_loss: 0.1990, step time: 0.5020\n",
      "143/388, train_loss: 0.1674, step time: 0.4886\n",
      "144/388, train_loss: 0.1271, step time: 0.5219\n",
      "145/388, train_loss: 0.1032, step time: 0.5736\n",
      "146/388, train_loss: 0.3088, step time: 0.5290\n",
      "147/388, train_loss: 0.2785, step time: 0.5111\n",
      "148/388, train_loss: 0.1687, step time: 0.4894\n",
      "149/388, train_loss: 0.1084, step time: 0.4973\n",
      "150/388, train_loss: 0.1507, step time: 0.5844\n",
      "151/388, train_loss: 0.0770, step time: 0.5458\n",
      "152/388, train_loss: 0.0554, step time: 0.5194\n",
      "153/388, train_loss: 0.4398, step time: 0.4985\n",
      "154/388, train_loss: 0.4801, step time: 0.5033\n",
      "155/388, train_loss: 0.1298, step time: 1.0097\n",
      "156/388, train_loss: 0.2006, step time: 0.5332\n",
      "157/388, train_loss: 0.1012, step time: 0.5013\n",
      "158/388, train_loss: 0.2091, step time: 0.4919\n",
      "159/388, train_loss: 0.1342, step time: 0.4925\n",
      "160/388, train_loss: 0.1043, step time: 0.4822\n",
      "161/388, train_loss: 0.6823, step time: 0.4895\n",
      "162/388, train_loss: 0.1279, step time: 0.4991\n",
      "163/388, train_loss: 0.2437, step time: 0.4931\n",
      "164/388, train_loss: 0.0490, step time: 0.4797\n",
      "165/388, train_loss: 0.3725, step time: 0.5346\n",
      "166/388, train_loss: 0.1232, step time: 0.5098\n",
      "167/388, train_loss: 0.2432, step time: 0.5061\n",
      "168/388, train_loss: 0.1465, step time: 0.5205\n",
      "169/388, train_loss: 0.2642, step time: 0.5438\n",
      "170/388, train_loss: 0.6548, step time: 0.5286\n",
      "171/388, train_loss: 0.0933, step time: 0.5377\n",
      "172/388, train_loss: 0.3037, step time: 0.5179\n",
      "173/388, train_loss: 0.0405, step time: 0.5081\n",
      "174/388, train_loss: 0.2295, step time: 0.4885\n",
      "175/388, train_loss: 0.0870, step time: 0.6918\n",
      "176/388, train_loss: 0.0861, step time: 0.5584\n",
      "177/388, train_loss: 0.1634, step time: 0.5286\n",
      "178/388, train_loss: 0.1347, step time: 0.5033\n",
      "179/388, train_loss: 0.0872, step time: 0.4998\n",
      "180/388, train_loss: 0.0592, step time: 0.4949\n",
      "181/388, train_loss: 0.1423, step time: 0.4845\n",
      "182/388, train_loss: 0.3724, step time: 0.4842\n",
      "183/388, train_loss: 0.3422, step time: 0.4974\n",
      "184/388, train_loss: 0.1802, step time: 0.5169\n",
      "185/388, train_loss: 0.0312, step time: 0.4990\n",
      "186/388, train_loss: 0.1532, step time: 0.5013\n",
      "187/388, train_loss: 0.3640, step time: 0.4992\n",
      "188/388, train_loss: 0.0535, step time: 0.5180\n",
      "189/388, train_loss: 0.1070, step time: 0.5000\n",
      "190/388, train_loss: 0.0973, step time: 0.5142\n",
      "191/388, train_loss: 0.1114, step time: 0.6381\n",
      "192/388, train_loss: 0.0714, step time: 0.5449\n",
      "193/388, train_loss: 0.0929, step time: 0.5141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/388, train_loss: 0.2281, step time: 0.4950\n",
      "195/388, train_loss: 0.2575, step time: 0.5013\n",
      "196/388, train_loss: 0.1360, step time: 0.5681\n",
      "197/388, train_loss: 0.5431, step time: 0.5493\n",
      "198/388, train_loss: 0.0347, step time: 0.5122\n",
      "199/388, train_loss: 0.1902, step time: 0.4956\n",
      "200/388, train_loss: 0.2171, step time: 0.4971\n",
      "201/388, train_loss: 0.1321, step time: 0.4880\n",
      "202/388, train_loss: 0.2261, step time: 0.4912\n",
      "203/388, train_loss: 0.1897, step time: 0.4973\n",
      "204/388, train_loss: 0.0724, step time: 1.2007\n",
      "205/388, train_loss: 0.0780, step time: 0.5202\n",
      "206/388, train_loss: 0.1482, step time: 0.4976\n",
      "207/388, train_loss: 0.3734, step time: 0.4954\n",
      "208/388, train_loss: 0.6006, step time: 0.4809\n",
      "209/388, train_loss: 0.3002, step time: 0.4785\n",
      "210/388, train_loss: 0.1435, step time: 0.4745\n",
      "211/388, train_loss: 0.0640, step time: 1.0419\n",
      "212/388, train_loss: 0.1498, step time: 0.5377\n",
      "213/388, train_loss: 0.3251, step time: 0.5037\n",
      "214/388, train_loss: 0.3448, step time: 0.5056\n",
      "215/388, train_loss: 0.3629, step time: 0.4885\n",
      "216/388, train_loss: 0.2514, step time: 0.4879\n",
      "217/388, train_loss: 0.6074, step time: 0.4813\n",
      "218/388, train_loss: 0.1103, step time: 0.8749\n",
      "219/388, train_loss: 0.1989, step time: 0.5473\n",
      "220/388, train_loss: 0.0843, step time: 0.5133\n",
      "221/388, train_loss: 0.3600, step time: 0.4907\n",
      "222/388, train_loss: 0.1817, step time: 0.4979\n",
      "223/388, train_loss: 0.1460, step time: 0.4831\n",
      "224/388, train_loss: 0.0829, step time: 0.4776\n",
      "225/388, train_loss: 0.1587, step time: 0.4767\n",
      "226/388, train_loss: 0.1144, step time: 0.4799\n",
      "227/388, train_loss: 0.2574, step time: 0.4859\n",
      "228/388, train_loss: 0.0889, step time: 0.4703\n",
      "229/388, train_loss: 0.1934, step time: 0.4776\n",
      "230/388, train_loss: 0.4224, step time: 0.5117\n",
      "231/388, train_loss: 0.0830, step time: 0.5028\n",
      "232/388, train_loss: 0.6942, step time: 0.4848\n",
      "233/388, train_loss: 0.1155, step time: 0.6991\n",
      "234/388, train_loss: 0.2728, step time: 0.5795\n",
      "235/388, train_loss: 0.2335, step time: 0.5392\n",
      "236/388, train_loss: 0.2183, step time: 0.5208\n",
      "237/388, train_loss: 0.3811, step time: 0.5109\n",
      "238/388, train_loss: 0.2306, step time: 0.4955\n",
      "239/388, train_loss: 0.1119, step time: 0.4969\n",
      "240/388, train_loss: 0.0974, step time: 0.5020\n",
      "241/388, train_loss: 0.0943, step time: 0.5031\n",
      "242/388, train_loss: 0.0770, step time: 0.4952\n",
      "243/388, train_loss: 0.2429, step time: 0.4988\n",
      "244/388, train_loss: 0.1569, step time: 0.5011\n",
      "245/388, train_loss: 0.2472, step time: 0.4861\n",
      "246/388, train_loss: 0.4675, step time: 0.4951\n",
      "247/388, train_loss: 0.1272, step time: 0.4856\n",
      "248/388, train_loss: 0.1805, step time: 0.4841\n",
      "249/388, train_loss: 0.1305, step time: 0.4980\n",
      "250/388, train_loss: 0.1214, step time: 0.5405\n",
      "251/388, train_loss: 0.1648, step time: 0.5168\n",
      "252/388, train_loss: 0.0806, step time: 0.4993\n",
      "253/388, train_loss: 0.3059, step time: 0.4830\n",
      "254/388, train_loss: 0.0706, step time: 0.5273\n",
      "255/388, train_loss: 0.1646, step time: 0.5178\n",
      "256/388, train_loss: 0.2209, step time: 0.5071\n",
      "257/388, train_loss: 0.0981, step time: 0.4904\n",
      "258/388, train_loss: 0.1937, step time: 0.5077\n",
      "259/388, train_loss: 0.1850, step time: 0.5013\n",
      "260/388, train_loss: 0.1731, step time: 0.5037\n",
      "261/388, train_loss: 0.4116, step time: 0.4833\n",
      "262/388, train_loss: 0.0988, step time: 0.4930\n",
      "263/388, train_loss: 0.1720, step time: 0.4906\n",
      "264/388, train_loss: 0.1394, step time: 0.4891\n",
      "265/388, train_loss: 0.0611, step time: 0.8300\n",
      "266/388, train_loss: 0.1710, step time: 0.5482\n",
      "267/388, train_loss: 0.1855, step time: 0.5186\n",
      "268/388, train_loss: 0.0955, step time: 0.5058\n",
      "269/388, train_loss: 0.1799, step time: 0.4931\n",
      "270/388, train_loss: 0.2533, step time: 0.5183\n",
      "271/388, train_loss: 0.1225, step time: 0.4978\n",
      "272/388, train_loss: 0.1704, step time: 0.4907\n",
      "273/388, train_loss: 0.0714, step time: 0.4809\n",
      "274/388, train_loss: 0.3390, step time: 0.8191\n",
      "275/388, train_loss: 0.0696, step time: 0.5529\n",
      "276/388, train_loss: 0.1546, step time: 0.5196\n",
      "277/388, train_loss: 0.2084, step time: 0.4980\n",
      "278/388, train_loss: 0.1000, step time: 0.5458\n",
      "279/388, train_loss: 0.1677, step time: 0.5236\n",
      "280/388, train_loss: 0.0939, step time: 0.5115\n",
      "281/388, train_loss: 0.2077, step time: 0.4960\n",
      "282/388, train_loss: 0.0884, step time: 0.4942\n",
      "283/388, train_loss: 0.4612, step time: 0.4816\n",
      "284/388, train_loss: 0.0966, step time: 0.5062\n",
      "285/388, train_loss: 0.2087, step time: 0.5131\n",
      "286/388, train_loss: 0.1867, step time: 0.5205\n",
      "287/388, train_loss: 0.1652, step time: 0.5896\n",
      "288/388, train_loss: 0.1151, step time: 0.5317\n",
      "289/388, train_loss: 0.1412, step time: 0.5151\n",
      "290/388, train_loss: 0.5043, step time: 0.4977\n",
      "291/388, train_loss: 0.2975, step time: 0.4978\n",
      "292/388, train_loss: 0.1192, step time: 0.4932\n",
      "293/388, train_loss: 0.1010, step time: 0.4778\n",
      "294/388, train_loss: 0.5525, step time: 0.4992\n",
      "295/388, train_loss: 0.1584, step time: 0.5203\n",
      "296/388, train_loss: 0.2708, step time: 0.5007\n",
      "297/388, train_loss: 0.2931, step time: 0.4913\n",
      "298/388, train_loss: 0.5014, step time: 0.5510\n",
      "299/388, train_loss: 0.2596, step time: 0.5481\n",
      "300/388, train_loss: 0.2126, step time: 0.5152\n",
      "301/388, train_loss: 0.0967, step time: 0.4849\n",
      "302/388, train_loss: 0.1129, step time: 0.5063\n",
      "303/388, train_loss: 0.4370, step time: 0.5796\n",
      "304/388, train_loss: 0.3098, step time: 0.5379\n",
      "305/388, train_loss: 0.1849, step time: 0.5153\n",
      "306/388, train_loss: 0.0860, step time: 0.4900\n",
      "307/388, train_loss: 0.2625, step time: 0.5020\n",
      "308/388, train_loss: 0.4946, step time: 1.1488\n",
      "309/388, train_loss: 0.1750, step time: 0.5307\n",
      "310/388, train_loss: 0.5396, step time: 0.4959\n",
      "311/388, train_loss: 0.1523, step time: 0.4968\n",
      "312/388, train_loss: 0.1176, step time: 0.4846\n",
      "313/388, train_loss: 0.0991, step time: 0.4872\n",
      "314/388, train_loss: 0.1980, step time: 0.4872\n",
      "315/388, train_loss: 0.0804, step time: 0.4752\n",
      "316/388, train_loss: 0.1663, step time: 0.5548\n",
      "317/388, train_loss: 0.1436, step time: 0.5305\n",
      "318/388, train_loss: 0.1806, step time: 0.5021\n",
      "319/388, train_loss: 0.0683, step time: 0.4937\n",
      "320/388, train_loss: 0.1224, step time: 0.4806\n",
      "321/388, train_loss: 0.4781, step time: 0.5111\n",
      "322/388, train_loss: 0.1169, step time: 0.4968\n",
      "323/388, train_loss: 0.2328, step time: 0.4986\n",
      "324/388, train_loss: 0.1815, step time: 0.4813\n",
      "325/388, train_loss: 0.5200, step time: 0.4748\n",
      "326/388, train_loss: 0.1424, step time: 0.4836\n",
      "327/388, train_loss: 0.0622, step time: 0.4831\n",
      "328/388, train_loss: 0.2790, step time: 0.5010\n",
      "329/388, train_loss: 0.0607, step time: 0.5474\n",
      "330/388, train_loss: 0.2223, step time: 0.5162\n",
      "331/388, train_loss: 0.1025, step time: 0.5013\n",
      "332/388, train_loss: 0.1956, step time: 0.5087\n",
      "333/388, train_loss: 0.0974, step time: 0.4850\n",
      "334/388, train_loss: 0.4097, step time: 0.4951\n",
      "335/388, train_loss: 0.2706, step time: 0.4995\n",
      "336/388, train_loss: 0.1790, step time: 0.5454\n",
      "337/388, train_loss: 0.2852, step time: 0.5195\n",
      "338/388, train_loss: 0.0919, step time: 0.5447\n",
      "339/388, train_loss: 0.0522, step time: 0.5215\n",
      "340/388, train_loss: 0.2121, step time: 0.5404\n",
      "341/388, train_loss: 0.1282, step time: 0.5213\n",
      "342/388, train_loss: 0.2216, step time: 0.5102\n",
      "343/388, train_loss: 0.1686, step time: 0.5200\n",
      "344/388, train_loss: 0.1142, step time: 0.5661\n",
      "345/388, train_loss: 0.0870, step time: 0.5251\n",
      "346/388, train_loss: 0.2180, step time: 0.5075\n",
      "347/388, train_loss: 0.2019, step time: 0.5151\n",
      "348/388, train_loss: 0.3467, step time: 0.5068\n",
      "349/388, train_loss: 0.1913, step time: 0.5042\n",
      "350/388, train_loss: 0.3226, step time: 0.5136\n",
      "351/388, train_loss: 0.3113, step time: 0.5494\n",
      "352/388, train_loss: 0.2172, step time: 0.5240\n",
      "353/388, train_loss: 0.2204, step time: 0.5057\n",
      "354/388, train_loss: 0.1218, step time: 0.4921\n",
      "355/388, train_loss: 0.2009, step time: 0.5195\n",
      "356/388, train_loss: 0.2776, step time: 0.5082\n",
      "357/388, train_loss: 0.0884, step time: 0.5106\n",
      "358/388, train_loss: 0.0865, step time: 0.4957\n",
      "359/388, train_loss: 0.0645, step time: 0.4940\n",
      "360/388, train_loss: 0.2312, step time: 0.4909\n",
      "361/388, train_loss: 0.2363, step time: 1.0770\n",
      "362/388, train_loss: 0.2478, step time: 0.5359\n",
      "363/388, train_loss: 0.2074, step time: 0.5032\n",
      "364/388, train_loss: 0.2921, step time: 0.4991\n",
      "365/388, train_loss: 0.1889, step time: 0.5128\n",
      "366/388, train_loss: 0.0996, step time: 0.4975\n",
      "367/388, train_loss: 0.2905, step time: 0.4879\n",
      "368/388, train_loss: 0.2534, step time: 0.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/388, train_loss: 0.1570, step time: 0.4884\n",
      "370/388, train_loss: 0.2475, step time: 0.8113\n",
      "371/388, train_loss: 0.1092, step time: 0.5619\n",
      "372/388, train_loss: 0.1780, step time: 0.5314\n",
      "373/388, train_loss: 0.1553, step time: 0.5067\n",
      "374/388, train_loss: 0.1014, step time: 0.5049\n",
      "375/388, train_loss: 0.1191, step time: 0.4913\n",
      "376/388, train_loss: 0.1829, step time: 0.5164\n",
      "377/388, train_loss: 0.5468, step time: 0.5236\n",
      "378/388, train_loss: 0.2658, step time: 0.5019\n",
      "379/388, train_loss: 0.1524, step time: 0.4966\n",
      "380/388, train_loss: 0.3566, step time: 0.5800\n",
      "381/388, train_loss: 0.0927, step time: 0.5357\n",
      "382/388, train_loss: 0.2175, step time: 0.5134\n",
      "383/388, train_loss: 0.2642, step time: 0.4906\n",
      "384/388, train_loss: 0.3541, step time: 0.4725\n",
      "385/388, train_loss: 0.3289, step time: 0.5043\n",
      "386/388, train_loss: 0.0724, step time: 0.5122\n",
      "387/388, train_loss: 0.1665, step time: 0.5582\n",
      "388/388, train_loss: 0.2224, step time: 0.6198\n",
      "epoch 48 average loss: 0.1978\n",
      "current epoch: 48 current mean dice: 0.7482 tc: 0.8034 wt: 0.8884 et: 0.5528\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 48 is: 299.2822\n",
      "----------\n",
      "epoch 49/300\n",
      "1/388, train_loss: 0.1254, step time: 0.4749\n",
      "2/388, train_loss: 0.4576, step time: 0.4894\n",
      "3/388, train_loss: 0.1587, step time: 0.5055\n",
      "4/388, train_loss: 0.1417, step time: 0.5271\n",
      "5/388, train_loss: 0.1875, step time: 0.6764\n",
      "6/388, train_loss: 0.0730, step time: 0.5398\n",
      "7/388, train_loss: 0.0788, step time: 0.5223\n",
      "8/388, train_loss: 0.0951, step time: 0.5123\n",
      "9/388, train_loss: 0.0913, step time: 0.5105\n",
      "10/388, train_loss: 0.2050, step time: 0.4985\n",
      "11/388, train_loss: 0.1769, step time: 0.4926\n",
      "12/388, train_loss: 0.1059, step time: 0.5099\n",
      "13/388, train_loss: 0.3822, step time: 0.5149\n",
      "14/388, train_loss: 0.3064, step time: 0.5328\n",
      "15/388, train_loss: 0.2140, step time: 0.5980\n",
      "16/388, train_loss: 0.2887, step time: 0.5411\n",
      "17/388, train_loss: 0.1613, step time: 0.5089\n",
      "18/388, train_loss: 0.2987, step time: 0.5083\n",
      "19/388, train_loss: 0.1079, step time: 0.5034\n",
      "20/388, train_loss: 0.1784, step time: 0.5765\n",
      "21/388, train_loss: 0.1004, step time: 0.5212\n",
      "22/388, train_loss: 0.0951, step time: 0.4972\n",
      "23/388, train_loss: 0.2464, step time: 0.5563\n",
      "24/388, train_loss: 0.2068, step time: 0.5428\n",
      "25/388, train_loss: 0.1873, step time: 0.5053\n",
      "26/388, train_loss: 0.1086, step time: 0.5628\n",
      "27/388, train_loss: 0.2035, step time: 0.5267\n",
      "28/388, train_loss: 0.1308, step time: 0.5028\n",
      "29/388, train_loss: 0.3909, step time: 0.4968\n",
      "30/388, train_loss: 0.1568, step time: 0.4853\n",
      "31/388, train_loss: 0.2514, step time: 0.5022\n",
      "32/388, train_loss: 0.1010, step time: 0.5284\n",
      "33/388, train_loss: 0.3854, step time: 0.5008\n",
      "34/388, train_loss: 0.1666, step time: 0.9571\n",
      "35/388, train_loss: 0.1048, step time: 0.5300\n",
      "36/388, train_loss: 0.1800, step time: 0.4958\n",
      "37/388, train_loss: 0.0893, step time: 0.4935\n",
      "38/388, train_loss: 0.2335, step time: 0.4939\n",
      "39/388, train_loss: 0.3300, step time: 0.5411\n",
      "40/388, train_loss: 0.4239, step time: 0.4971\n",
      "41/388, train_loss: 0.1886, step time: 0.9382\n",
      "42/388, train_loss: 0.0911, step time: 0.5424\n",
      "43/388, train_loss: 0.1742, step time: 0.5153\n",
      "44/388, train_loss: 0.1012, step time: 0.5051\n",
      "45/388, train_loss: 0.2311, step time: 0.4872\n",
      "46/388, train_loss: 0.1405, step time: 0.8302\n",
      "47/388, train_loss: 0.2859, step time: 0.5644\n",
      "48/388, train_loss: 0.2550, step time: 0.5198\n",
      "49/388, train_loss: 0.1071, step time: 0.5692\n",
      "50/388, train_loss: 0.1463, step time: 0.6090\n",
      "51/388, train_loss: 0.1857, step time: 0.5450\n",
      "52/388, train_loss: 0.2142, step time: 0.5103\n",
      "53/388, train_loss: 0.6003, step time: 0.5144\n",
      "54/388, train_loss: 0.1758, step time: 0.5195\n",
      "55/388, train_loss: 0.2824, step time: 0.5186\n",
      "56/388, train_loss: 0.1267, step time: 0.5056\n",
      "57/388, train_loss: 0.2554, step time: 0.5101\n",
      "58/388, train_loss: 0.2972, step time: 0.4935\n",
      "59/388, train_loss: 0.3260, step time: 0.9689\n",
      "60/388, train_loss: 0.0893, step time: 0.5473\n",
      "61/388, train_loss: 0.1778, step time: 0.5300\n",
      "62/388, train_loss: 0.1401, step time: 0.4947\n",
      "63/388, train_loss: 0.1840, step time: 0.5085\n",
      "64/388, train_loss: 0.2009, step time: 0.5463\n",
      "65/388, train_loss: 0.2258, step time: 0.6300\n",
      "66/388, train_loss: 0.3020, step time: 0.5788\n",
      "67/388, train_loss: 0.0766, step time: 0.5210\n",
      "68/388, train_loss: 0.1337, step time: 0.4983\n",
      "69/388, train_loss: 0.1874, step time: 1.1425\n",
      "70/388, train_loss: 0.2518, step time: 0.5433\n",
      "71/388, train_loss: 0.1118, step time: 0.5201\n",
      "72/388, train_loss: 0.1137, step time: 0.4996\n",
      "73/388, train_loss: 0.5672, step time: 0.5066\n",
      "74/388, train_loss: 0.1788, step time: 0.4866\n",
      "75/388, train_loss: 0.3267, step time: 0.4985\n",
      "76/388, train_loss: 0.5512, step time: 1.2155\n",
      "77/388, train_loss: 0.5952, step time: 0.5434\n",
      "78/388, train_loss: 0.1771, step time: 0.5025\n",
      "79/388, train_loss: 0.2880, step time: 0.5175\n",
      "80/388, train_loss: 0.2695, step time: 0.5055\n",
      "81/388, train_loss: 0.1126, step time: 0.4994\n",
      "82/388, train_loss: 0.1090, step time: 0.4880\n",
      "83/388, train_loss: 0.1140, step time: 0.4965\n",
      "84/388, train_loss: 0.2779, step time: 0.4876\n",
      "85/388, train_loss: 0.1790, step time: 1.0262\n",
      "86/388, train_loss: 0.2575, step time: 0.5365\n",
      "87/388, train_loss: 0.1207, step time: 0.5051\n",
      "88/388, train_loss: 0.0987, step time: 0.4913\n",
      "89/388, train_loss: 0.2457, step time: 0.4797\n",
      "90/388, train_loss: 0.2212, step time: 0.5060\n",
      "91/388, train_loss: 0.0877, step time: 0.4936\n",
      "92/388, train_loss: 0.1386, step time: 0.7804\n",
      "93/388, train_loss: 0.0921, step time: 0.5456\n",
      "94/388, train_loss: 0.4915, step time: 0.5293\n",
      "95/388, train_loss: 0.1029, step time: 0.4912\n",
      "96/388, train_loss: 0.1778, step time: 0.4846\n",
      "97/388, train_loss: 0.0929, step time: 0.5061\n",
      "98/388, train_loss: 0.3542, step time: 0.5576\n",
      "99/388, train_loss: 0.1399, step time: 0.5310\n",
      "100/388, train_loss: 0.1898, step time: 0.5109\n",
      "101/388, train_loss: 0.0552, step time: 0.4958\n",
      "102/388, train_loss: 0.1642, step time: 0.4916\n",
      "103/388, train_loss: 0.1523, step time: 1.1218\n",
      "104/388, train_loss: 0.1088, step time: 0.5456\n",
      "105/388, train_loss: 0.0913, step time: 0.5060\n",
      "106/388, train_loss: 0.1984, step time: 0.4907\n",
      "107/388, train_loss: 0.1871, step time: 0.9175\n",
      "108/388, train_loss: 0.1974, step time: 0.5472\n",
      "109/388, train_loss: 0.1184, step time: 0.5215\n",
      "110/388, train_loss: 0.0467, step time: 0.4987\n",
      "111/388, train_loss: 0.0894, step time: 0.4878\n",
      "112/388, train_loss: 0.1388, step time: 0.4892\n",
      "113/388, train_loss: 0.1372, step time: 0.4779\n",
      "114/388, train_loss: 0.0462, step time: 0.8087\n",
      "115/388, train_loss: 0.0942, step time: 0.5336\n",
      "116/388, train_loss: 0.0607, step time: 0.5065\n",
      "117/388, train_loss: 0.0995, step time: 0.5499\n",
      "118/388, train_loss: 0.0789, step time: 0.5243\n",
      "119/388, train_loss: 0.1804, step time: 0.5007\n",
      "120/388, train_loss: 0.0985, step time: 0.4963\n",
      "121/388, train_loss: 0.0889, step time: 0.4867\n",
      "122/388, train_loss: 0.1622, step time: 0.5020\n",
      "123/388, train_loss: 0.2312, step time: 1.1690\n",
      "124/388, train_loss: 0.3130, step time: 0.5288\n",
      "125/388, train_loss: 0.3310, step time: 0.5032\n",
      "126/388, train_loss: 0.1172, step time: 0.4841\n",
      "127/388, train_loss: 0.4177, step time: 0.4808\n",
      "128/388, train_loss: 0.0775, step time: 0.4888\n",
      "129/388, train_loss: 0.1951, step time: 0.4844\n",
      "130/388, train_loss: 0.1444, step time: 0.5174\n",
      "131/388, train_loss: 0.1814, step time: 0.4939\n",
      "132/388, train_loss: 0.2849, step time: 0.5141\n",
      "133/388, train_loss: 0.1123, step time: 0.5759\n",
      "134/388, train_loss: 0.1169, step time: 0.5423\n",
      "135/388, train_loss: 0.0864, step time: 0.5126\n",
      "136/388, train_loss: 0.1563, step time: 0.4952\n",
      "137/388, train_loss: 0.2711, step time: 1.1273\n",
      "138/388, train_loss: 0.1595, step time: 0.5360\n",
      "139/388, train_loss: 0.3840, step time: 0.5080\n",
      "140/388, train_loss: 0.1946, step time: 0.4910\n",
      "141/388, train_loss: 0.0777, step time: 0.4856\n",
      "142/388, train_loss: 0.1199, step time: 0.4917\n",
      "143/388, train_loss: 0.1234, step time: 0.4836\n",
      "144/388, train_loss: 0.1229, step time: 0.4964\n",
      "145/388, train_loss: 0.4879, step time: 0.4895\n",
      "146/388, train_loss: 0.1174, step time: 0.5074\n",
      "147/388, train_loss: 0.2122, step time: 0.4864\n",
      "148/388, train_loss: 0.0871, step time: 1.1315\n",
      "149/388, train_loss: 0.0970, step time: 0.5304\n",
      "150/388, train_loss: 0.2034, step time: 0.5164\n",
      "151/388, train_loss: 0.1236, step time: 0.5000\n",
      "152/388, train_loss: 0.2268, step time: 0.4804\n",
      "153/388, train_loss: 0.1092, step time: 0.4828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/388, train_loss: 0.1766, step time: 0.5003\n",
      "155/388, train_loss: 0.2103, step time: 0.4857\n",
      "156/388, train_loss: 0.1423, step time: 0.5191\n",
      "157/388, train_loss: 0.1401, step time: 0.5527\n",
      "158/388, train_loss: 0.0695, step time: 0.5257\n",
      "159/388, train_loss: 0.1041, step time: 0.5039\n",
      "160/388, train_loss: 0.1670, step time: 0.4853\n",
      "161/388, train_loss: 0.0394, step time: 0.4936\n",
      "162/388, train_loss: 0.1310, step time: 0.4749\n",
      "163/388, train_loss: 0.2212, step time: 0.5189\n",
      "164/388, train_loss: 0.2274, step time: 0.5014\n",
      "165/388, train_loss: 0.0654, step time: 1.0815\n",
      "166/388, train_loss: 0.0803, step time: 0.5354\n",
      "167/388, train_loss: 0.1073, step time: 0.5032\n",
      "168/388, train_loss: 0.3016, step time: 0.4947\n",
      "169/388, train_loss: 0.2873, step time: 0.4973\n",
      "170/388, train_loss: 0.1897, step time: 0.5023\n",
      "171/388, train_loss: 0.2571, step time: 0.5608\n",
      "172/388, train_loss: 0.0452, step time: 0.5256\n",
      "173/388, train_loss: 0.1513, step time: 0.5120\n",
      "174/388, train_loss: 0.1188, step time: 0.4904\n",
      "175/388, train_loss: 0.3993, step time: 0.4929\n",
      "176/388, train_loss: 0.1209, step time: 0.4858\n",
      "177/388, train_loss: 0.1420, step time: 0.5010\n",
      "178/388, train_loss: 0.1263, step time: 0.5454\n",
      "179/388, train_loss: 0.1785, step time: 0.5669\n",
      "180/388, train_loss: 0.0944, step time: 0.5419\n",
      "181/388, train_loss: 0.1817, step time: 0.5762\n",
      "182/388, train_loss: 0.1788, step time: 0.6252\n",
      "183/388, train_loss: 0.0672, step time: 0.5379\n",
      "184/388, train_loss: 0.0731, step time: 0.5127\n",
      "185/388, train_loss: 0.2157, step time: 0.5502\n",
      "186/388, train_loss: 0.3247, step time: 0.6245\n",
      "187/388, train_loss: 0.0946, step time: 0.5593\n",
      "188/388, train_loss: 0.2162, step time: 0.5396\n",
      "189/388, train_loss: 0.2809, step time: 0.5022\n",
      "190/388, train_loss: 0.2241, step time: 0.4941\n",
      "191/388, train_loss: 0.3324, step time: 0.5059\n",
      "192/388, train_loss: 0.1021, step time: 0.5067\n",
      "193/388, train_loss: 0.1976, step time: 0.4951\n",
      "194/388, train_loss: 0.3815, step time: 0.4946\n",
      "195/388, train_loss: 0.0964, step time: 0.4827\n",
      "196/388, train_loss: 0.5252, step time: 1.1598\n",
      "197/388, train_loss: 0.1570, step time: 0.5251\n",
      "198/388, train_loss: 0.5260, step time: 0.5021\n",
      "199/388, train_loss: 0.0460, step time: 0.4951\n",
      "200/388, train_loss: 0.3256, step time: 0.4963\n",
      "201/388, train_loss: 0.0631, step time: 0.4841\n",
      "202/388, train_loss: 0.2284, step time: 0.4854\n",
      "203/388, train_loss: 0.2440, step time: 0.4864\n",
      "204/388, train_loss: 0.1837, step time: 0.5430\n",
      "205/388, train_loss: 0.2545, step time: 0.5359\n",
      "206/388, train_loss: 0.5577, step time: 0.5255\n",
      "207/388, train_loss: 0.2852, step time: 0.5911\n",
      "208/388, train_loss: 0.1278, step time: 0.5251\n",
      "209/388, train_loss: 0.2791, step time: 0.4906\n",
      "210/388, train_loss: 0.0985, step time: 0.5185\n",
      "211/388, train_loss: 0.4115, step time: 0.5777\n",
      "212/388, train_loss: 0.2516, step time: 0.5238\n",
      "213/388, train_loss: 0.3008, step time: 0.5067\n",
      "214/388, train_loss: 0.2311, step time: 0.4899\n",
      "215/388, train_loss: 0.3617, step time: 0.4930\n",
      "216/388, train_loss: 0.0656, step time: 0.7814\n",
      "217/388, train_loss: 0.0697, step time: 0.5477\n",
      "218/388, train_loss: 0.1161, step time: 0.5173\n",
      "219/388, train_loss: 0.3318, step time: 0.4914\n",
      "220/388, train_loss: 0.2583, step time: 0.4897\n",
      "221/388, train_loss: 0.2004, step time: 0.4891\n",
      "222/388, train_loss: 0.2057, step time: 0.5104\n",
      "223/388, train_loss: 0.0669, step time: 0.5018\n",
      "224/388, train_loss: 0.1622, step time: 0.5036\n",
      "225/388, train_loss: 0.2033, step time: 0.4900\n",
      "226/388, train_loss: 0.1225, step time: 0.5019\n",
      "227/388, train_loss: 0.1504, step time: 0.5093\n",
      "228/388, train_loss: 0.0842, step time: 0.5043\n",
      "229/388, train_loss: 0.1309, step time: 0.5287\n",
      "230/388, train_loss: 0.2308, step time: 0.5174\n",
      "231/388, train_loss: 0.2338, step time: 0.5075\n",
      "232/388, train_loss: 0.1095, step time: 0.5126\n",
      "233/388, train_loss: 0.0882, step time: 0.5035\n",
      "234/388, train_loss: 0.2270, step time: 0.4857\n",
      "235/388, train_loss: 0.1294, step time: 0.5172\n",
      "236/388, train_loss: 0.2073, step time: 0.4905\n",
      "237/388, train_loss: 0.5258, step time: 0.4926\n",
      "238/388, train_loss: 0.3857, step time: 1.2344\n",
      "239/388, train_loss: 0.1468, step time: 0.5384\n",
      "240/388, train_loss: 0.2178, step time: 0.5116\n",
      "241/388, train_loss: 0.0943, step time: 0.4979\n",
      "242/388, train_loss: 0.0773, step time: 0.4847\n",
      "243/388, train_loss: 0.4187, step time: 0.4890\n",
      "244/388, train_loss: 0.0996, step time: 0.8225\n",
      "245/388, train_loss: 0.3036, step time: 0.5331\n",
      "246/388, train_loss: 0.2694, step time: 0.5109\n",
      "247/388, train_loss: 0.2703, step time: 0.5018\n",
      "248/388, train_loss: 0.1330, step time: 0.4882\n",
      "249/388, train_loss: 0.4401, step time: 0.4874\n",
      "250/388, train_loss: 0.2069, step time: 0.4853\n",
      "251/388, train_loss: 0.2283, step time: 0.4890\n",
      "252/388, train_loss: 0.1726, step time: 0.4955\n",
      "253/388, train_loss: 0.2662, step time: 1.2295\n",
      "254/388, train_loss: 0.2069, step time: 0.5245\n",
      "255/388, train_loss: 0.4721, step time: 0.5054\n",
      "256/388, train_loss: 0.0995, step time: 0.4959\n",
      "257/388, train_loss: 0.0688, step time: 0.4861\n",
      "258/388, train_loss: 0.2330, step time: 0.4947\n",
      "259/388, train_loss: 0.1971, step time: 1.1096\n",
      "260/388, train_loss: 0.0847, step time: 0.5137\n",
      "261/388, train_loss: 0.4628, step time: 0.5060\n",
      "262/388, train_loss: 0.1520, step time: 0.4826\n",
      "263/388, train_loss: 0.2828, step time: 0.4794\n",
      "264/388, train_loss: 0.4986, step time: 0.4860\n",
      "265/388, train_loss: 0.0588, step time: 0.4830\n",
      "266/388, train_loss: 0.0762, step time: 0.4812\n",
      "267/388, train_loss: 0.1514, step time: 0.5018\n",
      "268/388, train_loss: 0.2459, step time: 0.4846\n",
      "269/388, train_loss: 0.3134, step time: 0.4831\n",
      "270/388, train_loss: 0.1183, step time: 0.4893\n",
      "271/388, train_loss: 0.1532, step time: 0.7312\n",
      "272/388, train_loss: 0.1740, step time: 0.5537\n",
      "273/388, train_loss: 0.1811, step time: 0.5313\n",
      "274/388, train_loss: 0.2345, step time: 0.5013\n",
      "275/388, train_loss: 0.2563, step time: 0.5026\n",
      "276/388, train_loss: 0.1187, step time: 0.4988\n",
      "277/388, train_loss: 0.1956, step time: 0.4860\n",
      "278/388, train_loss: 0.0651, step time: 0.4876\n",
      "279/388, train_loss: 0.0707, step time: 0.4869\n",
      "280/388, train_loss: 0.3572, step time: 0.4752\n",
      "281/388, train_loss: 0.1405, step time: 0.4848\n",
      "282/388, train_loss: 0.2220, step time: 0.4758\n",
      "283/388, train_loss: 0.1929, step time: 0.4774\n",
      "284/388, train_loss: 0.3172, step time: 1.0126\n",
      "285/388, train_loss: 0.1818, step time: 0.5634\n",
      "286/388, train_loss: 0.3050, step time: 0.5157\n",
      "287/388, train_loss: 0.3897, step time: 0.5040\n",
      "288/388, train_loss: 0.0922, step time: 0.4972\n",
      "289/388, train_loss: 0.2191, step time: 0.4867\n",
      "290/388, train_loss: 0.0852, step time: 0.4913\n",
      "291/388, train_loss: 0.0828, step time: 0.5348\n",
      "292/388, train_loss: 0.1103, step time: 0.5195\n",
      "293/388, train_loss: 0.4913, step time: 0.5083\n",
      "294/388, train_loss: 0.2566, step time: 0.4890\n",
      "295/388, train_loss: 0.0838, step time: 0.4940\n",
      "296/388, train_loss: 0.2040, step time: 0.4940\n",
      "297/388, train_loss: 0.1397, step time: 0.5534\n",
      "298/388, train_loss: 0.2504, step time: 0.5611\n",
      "299/388, train_loss: 0.1603, step time: 0.5247\n",
      "300/388, train_loss: 0.1604, step time: 0.5111\n",
      "301/388, train_loss: 0.1819, step time: 0.5003\n",
      "302/388, train_loss: 0.0825, step time: 0.4974\n",
      "303/388, train_loss: 0.1562, step time: 0.4820\n",
      "304/388, train_loss: 0.0881, step time: 0.4816\n",
      "305/388, train_loss: 0.1410, step time: 0.4859\n",
      "306/388, train_loss: 0.1093, step time: 1.1034\n",
      "307/388, train_loss: 0.2142, step time: 0.5257\n",
      "308/388, train_loss: 0.0936, step time: 0.5085\n",
      "309/388, train_loss: 0.2745, step time: 0.4922\n",
      "310/388, train_loss: 0.1725, step time: 0.4945\n",
      "311/388, train_loss: 0.1970, step time: 0.4808\n",
      "312/388, train_loss: 0.1179, step time: 0.4803\n",
      "313/388, train_loss: 0.2706, step time: 0.4913\n",
      "314/388, train_loss: 0.1148, step time: 0.4974\n",
      "315/388, train_loss: 0.1327, step time: 0.5139\n",
      "316/388, train_loss: 0.2499, step time: 0.4938\n",
      "317/388, train_loss: 0.2371, step time: 0.5021\n",
      "318/388, train_loss: 0.0753, step time: 0.4786\n",
      "319/388, train_loss: 0.1900, step time: 1.0708\n",
      "320/388, train_loss: 0.1640, step time: 0.5239\n",
      "321/388, train_loss: 0.1364, step time: 0.5089\n",
      "322/388, train_loss: 0.0989, step time: 0.4943\n",
      "323/388, train_loss: 0.1101, step time: 0.4999\n",
      "324/388, train_loss: 0.3471, step time: 0.4979\n",
      "325/388, train_loss: 0.1253, step time: 0.4961\n",
      "326/388, train_loss: 0.5259, step time: 0.5011\n",
      "327/388, train_loss: 0.1552, step time: 0.4854\n",
      "328/388, train_loss: 0.2576, step time: 0.4850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/388, train_loss: 0.1316, step time: 0.4962\n",
      "330/388, train_loss: 0.0931, step time: 0.4802\n",
      "331/388, train_loss: 0.2662, step time: 0.4960\n",
      "332/388, train_loss: 0.1067, step time: 0.5203\n",
      "333/388, train_loss: 0.0864, step time: 0.5146\n",
      "334/388, train_loss: 0.2259, step time: 0.4943\n",
      "335/388, train_loss: 0.2816, step time: 0.5234\n",
      "336/388, train_loss: 0.1396, step time: 0.5027\n",
      "337/388, train_loss: 0.0629, step time: 0.4829\n",
      "338/388, train_loss: 0.2818, step time: 1.1261\n",
      "339/388, train_loss: 0.1042, step time: 0.5386\n",
      "340/388, train_loss: 0.2602, step time: 0.5149\n",
      "341/388, train_loss: 0.1417, step time: 0.4971\n",
      "342/388, train_loss: 0.1056, step time: 0.4994\n",
      "343/388, train_loss: 0.0752, step time: 0.4889\n",
      "344/388, train_loss: 0.3748, step time: 0.4942\n",
      "345/388, train_loss: 0.3304, step time: 0.4794\n",
      "346/388, train_loss: 0.0593, step time: 0.4801\n",
      "347/388, train_loss: 0.2023, step time: 1.1575\n",
      "348/388, train_loss: 0.1537, step time: 0.5341\n",
      "349/388, train_loss: 0.0868, step time: 0.5026\n",
      "350/388, train_loss: 0.3189, step time: 0.4865\n",
      "351/388, train_loss: 0.0548, step time: 0.4939\n",
      "352/388, train_loss: 0.2478, step time: 0.4731\n",
      "353/388, train_loss: 0.6501, step time: 0.4728\n",
      "354/388, train_loss: 0.2587, step time: 0.4781\n",
      "355/388, train_loss: 0.1692, step time: 0.6440\n",
      "356/388, train_loss: 0.2872, step time: 0.5549\n",
      "357/388, train_loss: 0.1269, step time: 0.5307\n",
      "358/388, train_loss: 0.0318, step time: 0.4981\n",
      "359/388, train_loss: 0.1105, step time: 0.4963\n",
      "360/388, train_loss: 0.1516, step time: 0.4799\n",
      "361/388, train_loss: 0.1666, step time: 0.4769\n",
      "362/388, train_loss: 0.1854, step time: 0.4773\n",
      "363/388, train_loss: 0.2193, step time: 0.4749\n",
      "364/388, train_loss: 0.0366, step time: 0.8232\n",
      "365/388, train_loss: 0.0848, step time: 0.5438\n",
      "366/388, train_loss: 0.1803, step time: 0.5147\n",
      "367/388, train_loss: 0.3547, step time: 0.5110\n",
      "368/388, train_loss: 0.2563, step time: 0.4959\n",
      "369/388, train_loss: 0.1034, step time: 0.4930\n",
      "370/388, train_loss: 0.2085, step time: 0.4962\n",
      "371/388, train_loss: 0.1559, step time: 0.4912\n",
      "372/388, train_loss: 0.3258, step time: 0.5105\n",
      "373/388, train_loss: 0.1390, step time: 0.5008\n",
      "374/388, train_loss: 0.0678, step time: 0.4866\n",
      "375/388, train_loss: 0.1139, step time: 0.4954\n",
      "376/388, train_loss: 0.1678, step time: 0.4814\n",
      "377/388, train_loss: 0.1569, step time: 0.4915\n",
      "378/388, train_loss: 0.1258, step time: 0.4827\n",
      "379/388, train_loss: 0.1866, step time: 1.2009\n",
      "380/388, train_loss: 0.1617, step time: 0.5259\n",
      "381/388, train_loss: 0.2222, step time: 0.4946\n",
      "382/388, train_loss: 0.0658, step time: 0.4911\n",
      "383/388, train_loss: 0.1798, step time: 0.4863\n",
      "384/388, train_loss: 0.1776, step time: 0.4747\n",
      "385/388, train_loss: 0.1048, step time: 0.4867\n",
      "386/388, train_loss: 0.1264, step time: 0.4683\n",
      "387/388, train_loss: 0.1765, step time: 0.4705\n",
      "388/388, train_loss: 0.2248, step time: 0.4902\n",
      "epoch 49 average loss: 0.1933\n",
      "current epoch: 49 current mean dice: 0.7609 tc: 0.8086 wt: 0.8909 et: 0.5832\n",
      "best mean dice: 0.7616 at epoch: 44\n",
      "time consuming of epoch 49 is: 301.0795\n",
      "----------\n",
      "epoch 50/300\n",
      "1/388, train_loss: 0.2952, step time: 0.4787\n",
      "2/388, train_loss: 0.1398, step time: 0.4924\n",
      "3/388, train_loss: 0.2783, step time: 0.5230\n",
      "4/388, train_loss: 0.1617, step time: 0.5093\n",
      "5/388, train_loss: 0.3761, step time: 0.4909\n",
      "6/388, train_loss: 0.2225, step time: 0.5164\n",
      "7/388, train_loss: 0.2567, step time: 0.5613\n",
      "8/388, train_loss: 0.0967, step time: 0.6858\n",
      "9/388, train_loss: 0.0832, step time: 0.5125\n",
      "10/388, train_loss: 0.0746, step time: 0.5225\n",
      "11/388, train_loss: 0.1958, step time: 0.5742\n",
      "12/388, train_loss: 0.0722, step time: 0.5309\n",
      "13/388, train_loss: 0.0803, step time: 0.4997\n",
      "14/388, train_loss: 0.0307, step time: 0.4892\n",
      "15/388, train_loss: 0.1257, step time: 0.8000\n",
      "16/388, train_loss: 0.3105, step time: 0.6220\n",
      "17/388, train_loss: 0.0826, step time: 0.5326\n",
      "18/388, train_loss: 0.2585, step time: 0.5066\n",
      "19/388, train_loss: 0.3482, step time: 0.4974\n",
      "20/388, train_loss: 0.1344, step time: 0.5017\n",
      "21/388, train_loss: 0.2859, step time: 0.4955\n",
      "22/388, train_loss: 0.2071, step time: 0.4970\n",
      "23/388, train_loss: 0.3534, step time: 0.5868\n",
      "24/388, train_loss: 0.1598, step time: 0.6027\n",
      "25/388, train_loss: 0.1416, step time: 0.5751\n",
      "26/388, train_loss: 0.1514, step time: 0.5362\n",
      "27/388, train_loss: 0.0867, step time: 0.5113\n",
      "28/388, train_loss: 0.1288, step time: 0.4932\n",
      "29/388, train_loss: 0.1061, step time: 0.5252\n",
      "30/388, train_loss: 0.1674, step time: 0.5014\n",
      "31/388, train_loss: 0.1753, step time: 0.5302\n",
      "32/388, train_loss: 0.0802, step time: 0.5212\n",
      "33/388, train_loss: 0.4832, step time: 0.5309\n",
      "34/388, train_loss: 0.2574, step time: 0.6257\n",
      "35/388, train_loss: 0.1246, step time: 0.5504\n",
      "36/388, train_loss: 0.1307, step time: 0.5256\n",
      "37/388, train_loss: 0.0896, step time: 0.5189\n",
      "38/388, train_loss: 0.1928, step time: 0.5583\n",
      "39/388, train_loss: 0.1127, step time: 0.5263\n",
      "40/388, train_loss: 0.1297, step time: 0.5033\n",
      "41/388, train_loss: 0.1285, step time: 0.4985\n",
      "42/388, train_loss: 0.0750, step time: 1.0465\n",
      "43/388, train_loss: 0.2320, step time: 0.5677\n",
      "44/388, train_loss: 0.1084, step time: 0.5219\n",
      "45/388, train_loss: 0.0699, step time: 0.4998\n",
      "46/388, train_loss: 0.0969, step time: 0.5402\n",
      "47/388, train_loss: 0.0504, step time: 0.5312\n",
      "48/388, train_loss: 0.0795, step time: 0.5042\n",
      "49/388, train_loss: 0.1460, step time: 0.4920\n",
      "50/388, train_loss: 0.2113, step time: 0.4922\n",
      "51/388, train_loss: 0.2237, step time: 0.4895\n",
      "52/388, train_loss: 0.0899, step time: 0.5356\n",
      "53/388, train_loss: 0.3272, step time: 0.6226\n",
      "54/388, train_loss: 0.2880, step time: 0.5469\n",
      "55/388, train_loss: 0.5559, step time: 0.5354\n",
      "56/388, train_loss: 0.1500, step time: 0.5800\n",
      "57/388, train_loss: 0.1315, step time: 0.5395\n",
      "58/388, train_loss: 0.2414, step time: 0.5059\n",
      "59/388, train_loss: 0.2409, step time: 0.4862\n",
      "60/388, train_loss: 0.0743, step time: 1.1439\n",
      "61/388, train_loss: 0.2759, step time: 0.5478\n",
      "62/388, train_loss: 0.2483, step time: 0.5031\n",
      "63/388, train_loss: 0.1811, step time: 0.5017\n",
      "64/388, train_loss: 0.1357, step time: 0.4892\n",
      "65/388, train_loss: 0.1204, step time: 0.4801\n",
      "66/388, train_loss: 0.3773, step time: 0.5013\n",
      "67/388, train_loss: 0.0937, step time: 0.4825\n",
      "68/388, train_loss: 0.1553, step time: 0.4865\n",
      "69/388, train_loss: 0.1348, step time: 0.4774\n",
      "70/388, train_loss: 0.2801, step time: 0.6876\n",
      "71/388, train_loss: 0.1230, step time: 0.5512\n",
      "72/388, train_loss: 0.1531, step time: 0.5266\n",
      "73/388, train_loss: 0.3354, step time: 0.5065\n",
      "74/388, train_loss: 0.0765, step time: 0.5043\n",
      "75/388, train_loss: 0.1986, step time: 0.5071\n",
      "76/388, train_loss: 0.1758, step time: 0.5925\n",
      "77/388, train_loss: 0.1564, step time: 0.5409\n",
      "78/388, train_loss: 0.1247, step time: 0.5238\n",
      "79/388, train_loss: 0.1452, step time: 0.5091\n",
      "80/388, train_loss: 0.1662, step time: 0.4973\n",
      "81/388, train_loss: 0.3764, step time: 0.4889\n",
      "82/388, train_loss: 0.1984, step time: 0.4997\n",
      "83/388, train_loss: 0.2143, step time: 0.5422\n",
      "84/388, train_loss: 0.1028, step time: 0.5547\n",
      "85/388, train_loss: 0.0972, step time: 0.5288\n",
      "86/388, train_loss: 0.0850, step time: 0.5065\n",
      "87/388, train_loss: 0.3627, step time: 0.5042\n",
      "88/388, train_loss: 0.3214, step time: 0.4963\n",
      "89/388, train_loss: 0.1098, step time: 1.2058\n",
      "90/388, train_loss: 0.1854, step time: 0.5386\n",
      "91/388, train_loss: 0.1691, step time: 0.5051\n",
      "92/388, train_loss: 0.1295, step time: 0.4884\n",
      "93/388, train_loss: 0.2988, step time: 0.4963\n",
      "94/388, train_loss: 0.3667, step time: 0.4788\n",
      "95/388, train_loss: 0.1070, step time: 0.4841\n",
      "96/388, train_loss: 0.1289, step time: 0.4833\n",
      "97/388, train_loss: 0.2222, step time: 0.5011\n",
      "98/388, train_loss: 0.0796, step time: 0.4959\n",
      "99/388, train_loss: 0.2316, step time: 0.4904\n",
      "100/388, train_loss: 0.1011, step time: 0.4968\n",
      "101/388, train_loss: 0.2055, step time: 0.4866\n",
      "102/388, train_loss: 0.1652, step time: 0.4828\n",
      "103/388, train_loss: 0.0602, step time: 1.0341\n",
      "104/388, train_loss: 0.2204, step time: 0.5433\n",
      "105/388, train_loss: 0.2049, step time: 0.5215\n",
      "106/388, train_loss: 0.1808, step time: 0.5112\n",
      "107/388, train_loss: 0.1142, step time: 0.4993\n",
      "108/388, train_loss: 0.1338, step time: 0.4803\n",
      "109/388, train_loss: 0.2998, step time: 0.4871\n",
      "110/388, train_loss: 0.2501, step time: 1.1273\n",
      "111/388, train_loss: 0.2454, step time: 0.5452\n",
      "112/388, train_loss: 0.2000, step time: 0.5061\n",
      "113/388, train_loss: 0.1718, step time: 0.5074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/388, train_loss: 0.1485, step time: 0.4826\n",
      "115/388, train_loss: 0.1546, step time: 1.1782\n",
      "116/388, train_loss: 0.1437, step time: 0.5361\n",
      "117/388, train_loss: 0.1059, step time: 0.5052\n",
      "118/388, train_loss: 0.2308, step time: 0.4866\n",
      "119/388, train_loss: 0.1506, step time: 0.4928\n",
      "120/388, train_loss: 0.4750, step time: 0.4800\n",
      "121/388, train_loss: 0.0767, step time: 1.0069\n",
      "122/388, train_loss: 0.0971, step time: 0.5427\n",
      "123/388, train_loss: 0.1587, step time: 0.5123\n",
      "124/388, train_loss: 0.1733, step time: 0.4940\n",
      "125/388, train_loss: 0.1582, step time: 0.4909\n",
      "126/388, train_loss: 0.2687, step time: 0.4942\n",
      "127/388, train_loss: 0.1550, step time: 0.4813\n",
      "128/388, train_loss: 0.2327, step time: 0.4945\n",
      "129/388, train_loss: 0.0566, step time: 0.4819\n",
      "130/388, train_loss: 0.3372, step time: 0.4914\n",
      "131/388, train_loss: 0.3655, step time: 0.5003\n",
      "132/388, train_loss: 0.0668, step time: 0.4781\n",
      "133/388, train_loss: 0.3100, step time: 0.4873\n",
      "134/388, train_loss: 0.1196, step time: 0.5208\n",
      "135/388, train_loss: 0.1071, step time: 0.5044\n",
      "136/388, train_loss: 0.1797, step time: 0.4868\n",
      "137/388, train_loss: 0.1751, step time: 0.5038\n",
      "138/388, train_loss: 0.2803, step time: 0.4857\n",
      "139/388, train_loss: 0.1336, step time: 0.4789\n",
      "140/388, train_loss: 0.1765, step time: 0.5717\n",
      "141/388, train_loss: 0.1233, step time: 0.5207\n",
      "142/388, train_loss: 0.1479, step time: 0.4857\n",
      "143/388, train_loss: 0.1362, step time: 0.7368\n",
      "144/388, train_loss: 0.4640, step time: 0.5369\n",
      "145/388, train_loss: 0.2396, step time: 0.5087\n",
      "146/388, train_loss: 0.3407, step time: 0.5430\n",
      "147/388, train_loss: 0.1128, step time: 0.5711\n",
      "148/388, train_loss: 0.1418, step time: 0.5274\n",
      "149/388, train_loss: 0.6382, step time: 0.5037\n",
      "150/388, train_loss: 0.1309, step time: 0.5007\n",
      "151/388, train_loss: 0.1441, step time: 0.5458\n",
      "152/388, train_loss: 0.2661, step time: 0.6842\n",
      "153/388, train_loss: 0.0908, step time: 0.5425\n",
      "154/388, train_loss: 0.1829, step time: 0.5175\n",
      "155/388, train_loss: 0.1470, step time: 0.4986\n",
      "156/388, train_loss: 0.0989, step time: 0.4980\n",
      "157/388, train_loss: 0.1132, step time: 0.5159\n",
      "158/388, train_loss: 0.1241, step time: 0.5009\n",
      "159/388, train_loss: 0.1605, step time: 0.4895\n",
      "160/388, train_loss: 0.1130, step time: 0.7771\n",
      "161/388, train_loss: 0.2344, step time: 0.5595\n",
      "162/388, train_loss: 0.0576, step time: 0.5352\n",
      "163/388, train_loss: 0.2581, step time: 0.5091\n",
      "164/388, train_loss: 0.0798, step time: 0.4992\n",
      "165/388, train_loss: 0.0445, step time: 0.4888\n",
      "166/388, train_loss: 0.2602, step time: 0.4956\n",
      "167/388, train_loss: 0.5164, step time: 0.5169\n",
      "168/388, train_loss: 0.0493, step time: 0.5079\n",
      "169/388, train_loss: 0.1033, step time: 0.4921\n",
      "170/388, train_loss: 0.1734, step time: 0.4863\n",
      "171/388, train_loss: 0.2744, step time: 0.4937\n",
      "172/388, train_loss: 0.4437, step time: 1.0950\n",
      "173/388, train_loss: 0.2239, step time: 0.5408\n",
      "174/388, train_loss: 0.2059, step time: 0.5305\n",
      "175/388, train_loss: 0.2943, step time: 0.5066\n",
      "176/388, train_loss: 0.4400, step time: 0.4978\n",
      "177/388, train_loss: 0.2199, step time: 0.5166\n",
      "178/388, train_loss: 0.0727, step time: 0.5759\n",
      "179/388, train_loss: 0.0670, step time: 0.5405\n",
      "180/388, train_loss: 0.1730, step time: 0.5113\n",
      "181/388, train_loss: 0.2909, step time: 0.5120\n",
      "182/388, train_loss: 0.3844, step time: 0.5022\n",
      "183/388, train_loss: 0.2850, step time: 0.4846\n",
      "184/388, train_loss: 0.1381, step time: 0.4902\n",
      "185/388, train_loss: 0.2171, step time: 0.5014\n",
      "186/388, train_loss: 0.1085, step time: 0.4933\n",
      "187/388, train_loss: 0.3056, step time: 0.4810\n",
      "188/388, train_loss: 0.0896, step time: 0.4793\n",
      "189/388, train_loss: 0.2958, step time: 0.9806\n",
      "190/388, train_loss: 0.1780, step time: 0.5299\n",
      "191/388, train_loss: 0.1449, step time: 0.5086\n",
      "192/388, train_loss: 0.4284, step time: 0.4893\n",
      "193/388, train_loss: 0.1151, step time: 0.4931\n",
      "194/388, train_loss: 0.2198, step time: 0.4812\n",
      "195/388, train_loss: 0.2273, step time: 0.5391\n",
      "196/388, train_loss: 0.1922, step time: 0.5045\n",
      "197/388, train_loss: 0.2413, step time: 0.5059\n",
      "198/388, train_loss: 0.0885, step time: 0.4984\n",
      "199/388, train_loss: 0.3186, step time: 0.9615\n",
      "200/388, train_loss: 0.1906, step time: 0.5302\n",
      "201/388, train_loss: 0.0925, step time: 0.4917\n",
      "202/388, train_loss: 0.1222, step time: 0.4877\n",
      "203/388, train_loss: 0.2490, step time: 0.4825\n",
      "204/388, train_loss: 0.1193, step time: 0.5079\n",
      "205/388, train_loss: 0.0784, step time: 0.5467\n",
      "206/388, train_loss: 0.1343, step time: 0.5237\n",
      "207/388, train_loss: 0.0905, step time: 0.4992\n",
      "208/388, train_loss: 0.1404, step time: 0.4957\n",
      "209/388, train_loss: 0.5113, step time: 1.0310\n",
      "210/388, train_loss: 0.1729, step time: 0.5515\n",
      "211/388, train_loss: 0.3527, step time: 0.5264\n",
      "212/388, train_loss: 0.1172, step time: 0.5051\n",
      "213/388, train_loss: 0.2862, step time: 0.4969\n",
      "214/388, train_loss: 0.3298, step time: 0.4990\n",
      "215/388, train_loss: 0.1607, step time: 0.5571\n",
      "216/388, train_loss: 0.0870, step time: 0.5423\n",
      "217/388, train_loss: 0.0618, step time: 0.5173\n",
      "218/388, train_loss: 0.2843, step time: 0.6012\n",
      "219/388, train_loss: 0.0981, step time: 0.5409\n",
      "220/388, train_loss: 0.2049, step time: 0.5136\n",
      "221/388, train_loss: 0.0912, step time: 0.5052\n",
      "222/388, train_loss: 0.2926, step time: 0.4874\n",
      "223/388, train_loss: 0.2064, step time: 0.5468\n",
      "224/388, train_loss: 0.0580, step time: 0.5105\n",
      "225/388, train_loss: 0.1996, step time: 0.4981\n",
      "226/388, train_loss: 0.0865, step time: 1.1436\n",
      "227/388, train_loss: 0.0632, step time: 0.5260\n",
      "228/388, train_loss: 0.3188, step time: 0.5200\n",
      "229/388, train_loss: 0.0945, step time: 0.4893\n",
      "230/388, train_loss: 0.0949, step time: 0.4920\n",
      "231/388, train_loss: 0.1714, step time: 0.5062\n",
      "232/388, train_loss: 0.1682, step time: 0.5012\n",
      "233/388, train_loss: 0.0838, step time: 0.7766\n",
      "234/388, train_loss: 0.1890, step time: 0.5562\n",
      "235/388, train_loss: 0.0618, step time: 0.5354\n",
      "236/388, train_loss: 0.2060, step time: 0.5135\n",
      "237/388, train_loss: 0.1203, step time: 0.4931\n",
      "238/388, train_loss: 0.1318, step time: 0.5276\n",
      "239/388, train_loss: 0.3582, step time: 0.5433\n",
      "240/388, train_loss: 0.1205, step time: 0.5326\n",
      "241/388, train_loss: 0.0837, step time: 0.5077\n",
      "242/388, train_loss: 0.1891, step time: 0.4832\n",
      "243/388, train_loss: 0.1341, step time: 0.4768\n",
      "244/388, train_loss: 0.0644, step time: 0.5107\n",
      "245/388, train_loss: 0.1283, step time: 0.5083\n",
      "246/388, train_loss: 0.4214, step time: 0.5057\n",
      "247/388, train_loss: 0.0801, step time: 0.4870\n",
      "248/388, train_loss: 0.2499, step time: 1.0243\n",
      "249/388, train_loss: 0.1998, step time: 0.5343\n",
      "250/388, train_loss: 0.0907, step time: 0.5058\n",
      "251/388, train_loss: 0.1871, step time: 0.4903\n",
      "252/388, train_loss: 0.1164, step time: 0.4957\n",
      "253/388, train_loss: 0.1757, step time: 0.4754\n",
      "254/388, train_loss: 0.2537, step time: 0.5288\n",
      "255/388, train_loss: 0.1301, step time: 0.5268\n",
      "256/388, train_loss: 0.0408, step time: 0.5094\n",
      "257/388, train_loss: 0.0797, step time: 0.4878\n",
      "258/388, train_loss: 0.2507, step time: 0.4914\n",
      "259/388, train_loss: 0.1451, step time: 0.6511\n",
      "260/388, train_loss: 0.2099, step time: 0.5592\n",
      "261/388, train_loss: 0.0862, step time: 0.5285\n",
      "262/388, train_loss: 0.0983, step time: 0.5228\n",
      "263/388, train_loss: 0.0877, step time: 0.4931\n",
      "264/388, train_loss: 0.1374, step time: 0.4975\n",
      "265/388, train_loss: 0.2916, step time: 0.5313\n",
      "266/388, train_loss: 0.5147, step time: 0.5119\n",
      "267/388, train_loss: 0.1012, step time: 0.4924\n",
      "268/388, train_loss: 0.2905, step time: 0.5259\n",
      "269/388, train_loss: 0.1258, step time: 0.5146\n",
      "270/388, train_loss: 0.0652, step time: 0.5685\n",
      "271/388, train_loss: 0.1115, step time: 0.5396\n",
      "272/388, train_loss: 0.0936, step time: 0.5222\n",
      "273/388, train_loss: 0.0527, step time: 0.4992\n",
      "274/388, train_loss: 0.2871, step time: 0.4941\n",
      "275/388, train_loss: 0.1509, step time: 1.1753\n",
      "276/388, train_loss: 0.1832, step time: 0.5372\n",
      "277/388, train_loss: 0.0744, step time: 0.5230\n",
      "278/388, train_loss: 0.2284, step time: 0.4982\n",
      "279/388, train_loss: 0.2870, step time: 0.4967\n",
      "280/388, train_loss: 0.2464, step time: 0.9077\n",
      "281/388, train_loss: 0.1705, step time: 0.5667\n",
      "282/388, train_loss: 0.1431, step time: 0.5281\n",
      "283/388, train_loss: 0.3259, step time: 0.5159\n",
      "284/388, train_loss: 0.2246, step time: 0.5624\n",
      "285/388, train_loss: 0.2869, step time: 0.5200\n",
      "286/388, train_loss: 0.2202, step time: 0.5013\n",
      "287/388, train_loss: 0.1437, step time: 0.5018\n",
      "288/388, train_loss: 0.2433, step time: 0.4861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/388, train_loss: 0.1568, step time: 0.4835\n",
      "290/388, train_loss: 0.3584, step time: 0.5033\n",
      "291/388, train_loss: 0.1726, step time: 0.5014\n",
      "292/388, train_loss: 0.2092, step time: 0.4950\n",
      "293/388, train_loss: 0.2136, step time: 0.5034\n",
      "294/388, train_loss: 0.1879, step time: 0.5404\n",
      "295/388, train_loss: 0.1292, step time: 0.5068\n",
      "296/388, train_loss: 0.2534, step time: 0.4869\n",
      "297/388, train_loss: 0.2317, step time: 0.4984\n",
      "298/388, train_loss: 0.1743, step time: 0.5211\n",
      "299/388, train_loss: 0.1124, step time: 0.5288\n",
      "300/388, train_loss: 0.2482, step time: 0.5544\n",
      "301/388, train_loss: 0.0679, step time: 0.5359\n",
      "302/388, train_loss: 0.3513, step time: 0.5111\n",
      "303/388, train_loss: 0.5721, step time: 0.5360\n",
      "304/388, train_loss: 0.2221, step time: 0.5255\n",
      "305/388, train_loss: 0.1452, step time: 0.5020\n",
      "306/388, train_loss: 0.3461, step time: 0.4916\n",
      "307/388, train_loss: 0.2797, step time: 0.5064\n",
      "308/388, train_loss: 0.1553, step time: 0.4942\n",
      "309/388, train_loss: 0.4410, step time: 0.5276\n",
      "310/388, train_loss: 0.1238, step time: 0.5947\n",
      "311/388, train_loss: 0.1595, step time: 0.5241\n",
      "312/388, train_loss: 0.3775, step time: 0.5021\n",
      "313/388, train_loss: 0.0631, step time: 0.4999\n",
      "314/388, train_loss: 0.1038, step time: 0.4815\n",
      "315/388, train_loss: 0.1309, step time: 0.5048\n",
      "316/388, train_loss: 0.5889, step time: 0.5215\n",
      "317/388, train_loss: 0.1023, step time: 0.5267\n",
      "318/388, train_loss: 0.0867, step time: 0.5246\n",
      "319/388, train_loss: 0.0773, step time: 0.5789\n",
      "320/388, train_loss: 0.1650, step time: 0.5455\n",
      "321/388, train_loss: 0.0986, step time: 0.5198\n",
      "322/388, train_loss: 0.2036, step time: 0.5073\n",
      "323/388, train_loss: 0.2995, step time: 0.5021\n",
      "324/388, train_loss: 0.1985, step time: 0.5335\n",
      "325/388, train_loss: 0.2497, step time: 0.5797\n",
      "326/388, train_loss: 0.3632, step time: 0.5408\n",
      "327/388, train_loss: 0.0326, step time: 0.4980\n",
      "328/388, train_loss: 0.3473, step time: 0.5268\n",
      "329/388, train_loss: 0.2184, step time: 0.6103\n",
      "330/388, train_loss: 0.5678, step time: 0.5326\n",
      "331/388, train_loss: 0.0981, step time: 0.5158\n",
      "332/388, train_loss: 0.3401, step time: 0.5583\n",
      "333/388, train_loss: 0.2091, step time: 0.5399\n",
      "334/388, train_loss: 0.3318, step time: 0.5061\n",
      "335/388, train_loss: 0.1781, step time: 0.5233\n",
      "336/388, train_loss: 0.1582, step time: 0.4994\n",
      "337/388, train_loss: 0.3179, step time: 0.4901\n",
      "338/388, train_loss: 0.1166, step time: 1.1368\n",
      "339/388, train_loss: 0.1721, step time: 0.5441\n",
      "340/388, train_loss: 0.1176, step time: 0.5161\n",
      "341/388, train_loss: 0.1308, step time: 0.5046\n",
      "342/388, train_loss: 0.1519, step time: 0.5018\n",
      "343/388, train_loss: 0.2968, step time: 0.4996\n",
      "344/388, train_loss: 0.2933, step time: 1.1187\n",
      "345/388, train_loss: 0.1429, step time: 0.5323\n",
      "346/388, train_loss: 0.0788, step time: 0.5093\n",
      "347/388, train_loss: 0.1320, step time: 0.4898\n",
      "348/388, train_loss: 0.1351, step time: 0.5586\n",
      "349/388, train_loss: 0.0981, step time: 0.5155\n",
      "350/388, train_loss: 0.1097, step time: 0.4912\n",
      "351/388, train_loss: 0.2208, step time: 0.4891\n",
      "352/388, train_loss: 0.3218, step time: 0.4961\n",
      "353/388, train_loss: 0.3171, step time: 0.4790\n",
      "354/388, train_loss: 0.1002, step time: 1.1367\n",
      "355/388, train_loss: 0.3089, step time: 0.5440\n",
      "356/388, train_loss: 0.3569, step time: 0.5063\n",
      "357/388, train_loss: 0.3171, step time: 0.4977\n",
      "358/388, train_loss: 0.0740, step time: 0.4870\n",
      "359/388, train_loss: 0.1056, step time: 0.4935\n",
      "360/388, train_loss: 0.2652, step time: 0.4792\n",
      "361/388, train_loss: 0.1496, step time: 0.4872\n",
      "362/388, train_loss: 0.2900, step time: 0.4797\n",
      "363/388, train_loss: 0.0741, step time: 0.6821\n",
      "364/388, train_loss: 0.3160, step time: 0.5518\n",
      "365/388, train_loss: 0.2066, step time: 0.5332\n",
      "366/388, train_loss: 0.1963, step time: 0.5122\n",
      "367/388, train_loss: 0.0985, step time: 0.5001\n",
      "368/388, train_loss: 0.2140, step time: 0.4880\n",
      "369/388, train_loss: 0.1422, step time: 0.5068\n",
      "370/388, train_loss: 0.2251, step time: 0.4939\n",
      "371/388, train_loss: 0.2829, step time: 0.4975\n",
      "372/388, train_loss: 0.1199, step time: 0.5367\n",
      "373/388, train_loss: 0.0876, step time: 0.5334\n",
      "374/388, train_loss: 0.0617, step time: 0.5097\n",
      "375/388, train_loss: 0.0765, step time: 0.4934\n",
      "376/388, train_loss: 0.1954, step time: 0.4953\n",
      "377/388, train_loss: 0.2894, step time: 1.2215\n",
      "378/388, train_loss: 0.0954, step time: 0.5298\n",
      "379/388, train_loss: 0.1190, step time: 0.5025\n",
      "380/388, train_loss: 0.1646, step time: 0.4990\n",
      "381/388, train_loss: 0.4927, step time: 0.4816\n",
      "382/388, train_loss: 0.1129, step time: 0.4730\n",
      "383/388, train_loss: 0.2197, step time: 0.4733\n",
      "384/388, train_loss: 0.6075, step time: 1.0144\n",
      "385/388, train_loss: 0.5378, step time: 0.5272\n",
      "386/388, train_loss: 0.0782, step time: 0.5101\n",
      "387/388, train_loss: 0.5111, step time: 0.4875\n",
      "388/388, train_loss: 0.0742, step time: 0.4810\n",
      "epoch 50 average loss: 0.1939\n",
      "saved new best metric model\n",
      "current epoch: 50 current mean dice: 0.7635 tc: 0.8119 wt: 0.8957 et: 0.5828\n",
      "best mean dice: 0.7635 at epoch: 50\n",
      "time consuming of epoch 50 is: 300.4171\n",
      "----------\n",
      "epoch 51/300\n",
      "1/388, train_loss: 0.1283, step time: 0.4750\n",
      "2/388, train_loss: 0.1440, step time: 0.4921\n",
      "3/388, train_loss: 0.2852, step time: 0.9594\n",
      "4/388, train_loss: 0.1808, step time: 0.5883\n",
      "5/388, train_loss: 0.0980, step time: 0.5630\n",
      "6/388, train_loss: 0.1296, step time: 0.5157\n",
      "7/388, train_loss: 0.1577, step time: 0.4855\n",
      "8/388, train_loss: 0.5733, step time: 1.1547\n",
      "9/388, train_loss: 0.2048, step time: 0.5484\n",
      "10/388, train_loss: 0.2635, step time: 0.5070\n",
      "11/388, train_loss: 0.3676, step time: 0.4886\n",
      "12/388, train_loss: 0.0866, step time: 0.5899\n",
      "13/388, train_loss: 0.1928, step time: 0.5563\n",
      "14/388, train_loss: 0.1034, step time: 0.5223\n",
      "15/388, train_loss: 0.1089, step time: 0.5075\n",
      "16/388, train_loss: 0.0992, step time: 0.5041\n",
      "17/388, train_loss: 0.1066, step time: 0.4879\n",
      "18/388, train_loss: 0.3086, step time: 0.5035\n",
      "19/388, train_loss: 0.0852, step time: 0.5406\n",
      "20/388, train_loss: 0.2199, step time: 0.6107\n",
      "21/388, train_loss: 0.4576, step time: 0.5386\n",
      "22/388, train_loss: 0.1345, step time: 0.5156\n",
      "23/388, train_loss: 0.1951, step time: 0.5028\n",
      "24/388, train_loss: 0.0878, step time: 0.5152\n",
      "25/388, train_loss: 0.0998, step time: 0.4975\n",
      "26/388, train_loss: 0.4647, step time: 0.4911\n",
      "27/388, train_loss: 0.1404, step time: 0.4844\n",
      "28/388, train_loss: 0.1050, step time: 0.6271\n",
      "29/388, train_loss: 0.1021, step time: 0.5505\n",
      "30/388, train_loss: 0.4652, step time: 0.5329\n",
      "31/388, train_loss: 0.1041, step time: 0.5073\n",
      "32/388, train_loss: 0.2507, step time: 0.5088\n",
      "33/388, train_loss: 0.0951, step time: 0.4868\n",
      "34/388, train_loss: 0.0994, step time: 0.5080\n",
      "35/388, train_loss: 0.0888, step time: 0.5040\n",
      "36/388, train_loss: 0.1011, step time: 0.4957\n",
      "37/388, train_loss: 0.4386, step time: 0.9408\n",
      "38/388, train_loss: 0.1871, step time: 0.5438\n",
      "39/388, train_loss: 0.0968, step time: 0.5165\n",
      "40/388, train_loss: 0.0677, step time: 0.5002\n",
      "41/388, train_loss: 0.1799, step time: 0.4957\n",
      "42/388, train_loss: 0.1500, step time: 0.4902\n",
      "43/388, train_loss: 0.2475, step time: 1.1172\n",
      "44/388, train_loss: 0.2466, step time: 0.5355\n",
      "45/388, train_loss: 0.2995, step time: 0.5097\n",
      "46/388, train_loss: 0.1687, step time: 0.4922\n",
      "47/388, train_loss: 0.0504, step time: 0.4971\n",
      "48/388, train_loss: 0.2010, step time: 0.4827\n",
      "49/388, train_loss: 0.2525, step time: 0.5071\n",
      "50/388, train_loss: 0.2223, step time: 0.4965\n",
      "51/388, train_loss: 0.1483, step time: 0.4877\n",
      "52/388, train_loss: 0.1305, step time: 0.4977\n",
      "53/388, train_loss: 0.3313, step time: 0.5008\n",
      "54/388, train_loss: 0.0496, step time: 0.5027\n",
      "55/388, train_loss: 0.1118, step time: 0.4848\n",
      "56/388, train_loss: 0.2102, step time: 0.5264\n",
      "57/388, train_loss: 0.2719, step time: 0.4906\n",
      "58/388, train_loss: 0.5481, step time: 0.5673\n",
      "59/388, train_loss: 0.1888, step time: 0.5881\n",
      "60/388, train_loss: 0.2287, step time: 0.5471\n",
      "61/388, train_loss: 0.2093, step time: 0.5165\n",
      "62/388, train_loss: 0.0861, step time: 0.5134\n",
      "63/388, train_loss: 0.1767, step time: 0.5502\n",
      "64/388, train_loss: 0.1312, step time: 0.5235\n",
      "65/388, train_loss: 0.2600, step time: 0.5107\n",
      "66/388, train_loss: 0.1255, step time: 0.4898\n",
      "67/388, train_loss: 0.6299, step time: 0.5091\n",
      "68/388, train_loss: 0.1629, step time: 0.4987\n",
      "69/388, train_loss: 0.0671, step time: 1.2424\n",
      "70/388, train_loss: 0.1895, step time: 0.5460\n",
      "71/388, train_loss: 0.1746, step time: 0.5315\n",
      "72/388, train_loss: 0.3894, step time: 0.5042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/388, train_loss: 0.1397, step time: 0.4865\n",
      "74/388, train_loss: 0.2309, step time: 0.5209\n",
      "75/388, train_loss: 0.1310, step time: 0.5350\n",
      "76/388, train_loss: 0.1889, step time: 0.5195\n",
      "77/388, train_loss: 0.3572, step time: 0.4972\n",
      "78/388, train_loss: 0.0697, step time: 0.5014\n",
      "79/388, train_loss: 0.2427, step time: 1.0732\n",
      "80/388, train_loss: 0.0952, step time: 0.5257\n",
      "81/388, train_loss: 0.2229, step time: 0.5035\n",
      "82/388, train_loss: 0.1824, step time: 0.4919\n",
      "83/388, train_loss: 0.3086, step time: 0.4847\n",
      "84/388, train_loss: 0.2118, step time: 0.5312\n",
      "85/388, train_loss: 0.1841, step time: 0.5039\n",
      "86/388, train_loss: 0.1102, step time: 0.4949\n",
      "87/388, train_loss: 0.2979, step time: 0.4839\n",
      "88/388, train_loss: 0.1279, step time: 0.4955\n",
      "89/388, train_loss: 0.0483, step time: 0.4905\n",
      "90/388, train_loss: 0.0916, step time: 0.4832\n",
      "91/388, train_loss: 0.2641, step time: 0.5033\n",
      "92/388, train_loss: 0.1451, step time: 0.5178\n",
      "93/388, train_loss: 0.1086, step time: 0.5017\n",
      "94/388, train_loss: 0.3311, step time: 0.5068\n",
      "95/388, train_loss: 0.2137, step time: 0.4905\n",
      "96/388, train_loss: 0.1159, step time: 1.1239\n",
      "97/388, train_loss: 0.1063, step time: 0.5351\n",
      "98/388, train_loss: 0.0362, step time: 0.5023\n",
      "99/388, train_loss: 0.1948, step time: 0.5017\n",
      "100/388, train_loss: 0.1239, step time: 0.4948\n",
      "101/388, train_loss: 0.2057, step time: 0.4916\n",
      "102/388, train_loss: 0.0809, step time: 0.4806\n",
      "103/388, train_loss: 0.3025, step time: 0.4910\n",
      "104/388, train_loss: 0.1098, step time: 1.1218\n",
      "105/388, train_loss: 0.1086, step time: 0.5350\n",
      "106/388, train_loss: 0.1080, step time: 0.5045\n",
      "107/388, train_loss: 0.1101, step time: 0.4923\n",
      "108/388, train_loss: 0.1755, step time: 0.4933\n",
      "109/388, train_loss: 0.2734, step time: 0.9003\n",
      "110/388, train_loss: 0.1386, step time: 0.5405\n",
      "111/388, train_loss: 0.0637, step time: 0.5205\n",
      "112/388, train_loss: 0.1190, step time: 0.4929\n",
      "113/388, train_loss: 0.0883, step time: 0.4912\n",
      "114/388, train_loss: 0.1456, step time: 0.4828\n",
      "115/388, train_loss: 0.0956, step time: 0.4776\n",
      "116/388, train_loss: 0.1088, step time: 0.4967\n",
      "117/388, train_loss: 0.0969, step time: 0.4912\n",
      "118/388, train_loss: 0.2107, step time: 0.4848\n",
      "119/388, train_loss: 0.1210, step time: 1.0196\n",
      "120/388, train_loss: 0.1352, step time: 0.5260\n",
      "121/388, train_loss: 0.0665, step time: 0.5008\n",
      "122/388, train_loss: 0.1308, step time: 0.4977\n",
      "123/388, train_loss: 0.1149, step time: 0.4873\n",
      "124/388, train_loss: 0.2701, step time: 0.4897\n",
      "125/388, train_loss: 0.1208, step time: 0.4931\n",
      "126/388, train_loss: 0.1217, step time: 0.5016\n",
      "127/388, train_loss: 0.0443, step time: 0.5042\n",
      "128/388, train_loss: 0.0531, step time: 0.4821\n",
      "129/388, train_loss: 0.2694, step time: 0.4791\n",
      "130/388, train_loss: 0.1369, step time: 0.6911\n",
      "131/388, train_loss: 0.1572, step time: 0.5454\n",
      "132/388, train_loss: 0.0792, step time: 0.5120\n",
      "133/388, train_loss: 0.1180, step time: 0.5007\n",
      "134/388, train_loss: 0.1641, step time: 0.4923\n",
      "135/388, train_loss: 0.1767, step time: 0.5151\n",
      "136/388, train_loss: 0.0837, step time: 0.4893\n",
      "137/388, train_loss: 0.2307, step time: 0.4974\n",
      "138/388, train_loss: 0.1547, step time: 0.4835\n",
      "139/388, train_loss: 0.1341, step time: 0.4992\n",
      "140/388, train_loss: 0.1476, step time: 0.5812\n",
      "141/388, train_loss: 0.0902, step time: 0.5344\n",
      "142/388, train_loss: 0.1353, step time: 0.5113\n",
      "143/388, train_loss: 0.0370, step time: 0.4923\n",
      "144/388, train_loss: 0.0676, step time: 0.4970\n",
      "145/388, train_loss: 0.1049, step time: 0.4850\n",
      "146/388, train_loss: 0.0847, step time: 1.0125\n",
      "147/388, train_loss: 0.0698, step time: 0.5401\n",
      "148/388, train_loss: 0.2440, step time: 0.5181\n",
      "149/388, train_loss: 0.2776, step time: 0.5022\n",
      "150/388, train_loss: 0.1958, step time: 0.4930\n",
      "151/388, train_loss: 0.2104, step time: 0.4941\n",
      "152/388, train_loss: 0.2741, step time: 0.5093\n",
      "153/388, train_loss: 0.2586, step time: 0.5032\n",
      "154/388, train_loss: 0.1951, step time: 0.4838\n",
      "155/388, train_loss: 0.1555, step time: 1.1758\n",
      "156/388, train_loss: 0.1557, step time: 0.5590\n",
      "157/388, train_loss: 0.2625, step time: 0.5353\n",
      "158/388, train_loss: 0.2286, step time: 0.5256\n",
      "159/388, train_loss: 0.1358, step time: 0.5867\n",
      "160/388, train_loss: 0.1226, step time: 0.5295\n",
      "161/388, train_loss: 0.3027, step time: 0.5065\n",
      "162/388, train_loss: 0.1960, step time: 0.4983\n",
      "163/388, train_loss: 0.1043, step time: 0.4913\n",
      "164/388, train_loss: 0.1632, step time: 0.4956\n",
      "165/388, train_loss: 0.0531, step time: 0.9482\n",
      "166/388, train_loss: 0.1662, step time: 0.5324\n",
      "167/388, train_loss: 0.2867, step time: 0.5091\n",
      "168/388, train_loss: 0.4357, step time: 0.4977\n",
      "169/388, train_loss: 0.2078, step time: 1.1602\n",
      "170/388, train_loss: 0.0664, step time: 0.5171\n",
      "171/388, train_loss: 0.1407, step time: 0.5027\n",
      "172/388, train_loss: 0.1244, step time: 0.4853\n",
      "173/388, train_loss: 0.2237, step time: 0.4964\n",
      "174/388, train_loss: 0.2768, step time: 0.4898\n",
      "175/388, train_loss: 0.1017, step time: 0.4846\n",
      "176/388, train_loss: 0.1141, step time: 0.5035\n",
      "177/388, train_loss: 0.3345, step time: 0.4978\n",
      "178/388, train_loss: 0.1845, step time: 0.4820\n",
      "179/388, train_loss: 0.0909, step time: 0.4785\n",
      "180/388, train_loss: 0.1253, step time: 0.4962\n",
      "181/388, train_loss: 0.2730, step time: 0.7008\n",
      "182/388, train_loss: 0.0934, step time: 0.5758\n",
      "183/388, train_loss: 0.0907, step time: 0.5357\n",
      "184/388, train_loss: 0.2232, step time: 0.4981\n",
      "185/388, train_loss: 0.0750, step time: 0.4961\n",
      "186/388, train_loss: 0.0575, step time: 0.5127\n",
      "187/388, train_loss: 0.1733, step time: 0.5464\n",
      "188/388, train_loss: 0.1857, step time: 0.5199\n",
      "189/388, train_loss: 0.1364, step time: 0.4991\n",
      "190/388, train_loss: 0.2589, step time: 0.4966\n",
      "191/388, train_loss: 0.0937, step time: 0.4817\n",
      "192/388, train_loss: 0.3414, step time: 0.7492\n",
      "193/388, train_loss: 0.0416, step time: 0.5426\n",
      "194/388, train_loss: 0.0827, step time: 0.5197\n",
      "195/388, train_loss: 0.2371, step time: 0.5031\n",
      "196/388, train_loss: 0.0676, step time: 0.5009\n",
      "197/388, train_loss: 0.2166, step time: 0.4811\n",
      "198/388, train_loss: 0.2107, step time: 0.4909\n",
      "199/388, train_loss: 0.2030, step time: 0.4940\n",
      "200/388, train_loss: 0.3566, step time: 1.1609\n",
      "201/388, train_loss: 0.3772, step time: 0.5474\n",
      "202/388, train_loss: 0.1497, step time: 0.5149\n",
      "203/388, train_loss: 0.1991, step time: 0.4999\n",
      "204/388, train_loss: 0.2894, step time: 0.4901\n",
      "205/388, train_loss: 0.1764, step time: 0.8937\n",
      "206/388, train_loss: 0.1005, step time: 0.5497\n",
      "207/388, train_loss: 0.1712, step time: 0.5199\n",
      "208/388, train_loss: 0.0828, step time: 0.4963\n",
      "209/388, train_loss: 0.1248, step time: 0.4836\n",
      "210/388, train_loss: 0.3674, step time: 0.7902\n",
      "211/388, train_loss: 0.1126, step time: 0.5612\n",
      "212/388, train_loss: 0.0910, step time: 0.5125\n",
      "213/388, train_loss: 0.1741, step time: 0.5021\n",
      "214/388, train_loss: 0.1584, step time: 0.4892\n",
      "215/388, train_loss: 0.1486, step time: 0.9653\n",
      "216/388, train_loss: 0.4131, step time: 0.5294\n",
      "217/388, train_loss: 0.1748, step time: 0.5055\n",
      "218/388, train_loss: 0.5046, step time: 0.4945\n",
      "219/388, train_loss: 0.2982, step time: 0.5329\n",
      "220/388, train_loss: 0.0879, step time: 0.5308\n",
      "221/388, train_loss: 0.1573, step time: 0.5207\n",
      "222/388, train_loss: 0.2028, step time: 0.5017\n",
      "223/388, train_loss: 0.2895, step time: 0.4992\n",
      "224/388, train_loss: 0.1004, step time: 0.4782\n",
      "225/388, train_loss: 0.1128, step time: 0.4814\n",
      "226/388, train_loss: 0.0715, step time: 0.4882\n",
      "227/388, train_loss: 0.1262, step time: 0.5076\n",
      "228/388, train_loss: 0.2354, step time: 0.5031\n",
      "229/388, train_loss: 0.1199, step time: 0.5170\n",
      "230/388, train_loss: 0.2980, step time: 0.4947\n",
      "231/388, train_loss: 0.0644, step time: 1.1300\n",
      "232/388, train_loss: 0.3779, step time: 0.5657\n",
      "233/388, train_loss: 0.4753, step time: 0.5180\n",
      "234/388, train_loss: 0.2783, step time: 0.5040\n",
      "235/388, train_loss: 0.4137, step time: 0.5577\n",
      "236/388, train_loss: 0.0985, step time: 0.5181\n",
      "237/388, train_loss: 0.1895, step time: 0.5088\n",
      "238/388, train_loss: 0.1038, step time: 0.4841\n",
      "239/388, train_loss: 0.1596, step time: 0.4879\n",
      "240/388, train_loss: 0.1066, step time: 1.1431\n",
      "241/388, train_loss: 0.2069, step time: 0.5360\n",
      "242/388, train_loss: 0.0743, step time: 0.5080\n",
      "243/388, train_loss: 0.1302, step time: 0.4854\n",
      "244/388, train_loss: 0.1131, step time: 0.4915\n",
      "245/388, train_loss: 0.4677, step time: 0.4802\n",
      "246/388, train_loss: 0.2736, step time: 0.5208\n",
      "247/388, train_loss: 0.2067, step time: 0.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/388, train_loss: 0.1275, step time: 0.4944\n",
      "249/388, train_loss: 0.2064, step time: 0.4848\n",
      "250/388, train_loss: 0.0919, step time: 0.6449\n",
      "251/388, train_loss: 0.1875, step time: 0.5477\n",
      "252/388, train_loss: 0.2078, step time: 0.5107\n",
      "253/388, train_loss: 0.1787, step time: 0.4887\n",
      "254/388, train_loss: 0.2765, step time: 0.5114\n",
      "255/388, train_loss: 0.2222, step time: 0.5485\n",
      "256/388, train_loss: 0.1023, step time: 0.5242\n",
      "257/388, train_loss: 0.0849, step time: 0.5045\n",
      "258/388, train_loss: 0.1650, step time: 0.4959\n",
      "259/388, train_loss: 0.1476, step time: 0.4825\n",
      "260/388, train_loss: 0.3648, step time: 1.0081\n",
      "261/388, train_loss: 0.1410, step time: 0.5546\n",
      "262/388, train_loss: 0.0661, step time: 0.5239\n",
      "263/388, train_loss: 0.2767, step time: 0.5180\n",
      "264/388, train_loss: 0.1857, step time: 0.5668\n",
      "265/388, train_loss: 0.1169, step time: 0.5212\n",
      "266/388, train_loss: 0.1488, step time: 0.5130\n",
      "267/388, train_loss: 0.0872, step time: 0.4983\n",
      "268/388, train_loss: 0.3849, step time: 0.5012\n",
      "269/388, train_loss: 0.0894, step time: 0.5126\n",
      "270/388, train_loss: 0.1486, step time: 0.5033\n",
      "271/388, train_loss: 0.0634, step time: 0.4831\n",
      "272/388, train_loss: 0.1549, step time: 0.5217\n",
      "273/388, train_loss: 0.0954, step time: 0.4966\n",
      "274/388, train_loss: 0.2703, step time: 0.5027\n",
      "275/388, train_loss: 0.2143, step time: 0.4888\n",
      "276/388, train_loss: 0.7190, step time: 0.5873\n",
      "277/388, train_loss: 0.0987, step time: 0.5450\n",
      "278/388, train_loss: 0.4858, step time: 0.5327\n",
      "279/388, train_loss: 0.0873, step time: 0.5051\n",
      "280/388, train_loss: 0.2785, step time: 0.5447\n",
      "281/388, train_loss: 0.2617, step time: 0.6308\n",
      "282/388, train_loss: 0.1568, step time: 0.5653\n",
      "283/388, train_loss: 0.1859, step time: 0.5374\n",
      "284/388, train_loss: 0.2719, step time: 0.5081\n",
      "285/388, train_loss: 0.1197, step time: 1.0398\n",
      "286/388, train_loss: 0.1166, step time: 0.5727\n",
      "287/388, train_loss: 0.2759, step time: 0.5262\n",
      "288/388, train_loss: 0.1323, step time: 0.5009\n",
      "289/388, train_loss: 0.1225, step time: 0.5033\n",
      "290/388, train_loss: 0.1752, step time: 0.5203\n",
      "291/388, train_loss: 0.2502, step time: 0.5052\n",
      "292/388, train_loss: 0.1560, step time: 0.5306\n",
      "293/388, train_loss: 0.1780, step time: 0.5073\n",
      "294/388, train_loss: 0.1628, step time: 0.5003\n",
      "295/388, train_loss: 0.1567, step time: 0.5088\n",
      "296/388, train_loss: 0.2915, step time: 0.6049\n",
      "297/388, train_loss: 0.1909, step time: 0.5459\n",
      "298/388, train_loss: 0.1642, step time: 0.5375\n",
      "299/388, train_loss: 0.2200, step time: 0.5179\n",
      "300/388, train_loss: 0.2428, step time: 0.5067\n",
      "301/388, train_loss: 0.5028, step time: 0.4928\n",
      "302/388, train_loss: 0.4231, step time: 1.0234\n",
      "303/388, train_loss: 0.4737, step time: 0.5562\n",
      "304/388, train_loss: 0.1073, step time: 0.5192\n",
      "305/388, train_loss: 0.0920, step time: 0.5001\n",
      "306/388, train_loss: 0.1304, step time: 0.4981\n",
      "307/388, train_loss: 0.1929, step time: 0.9283\n",
      "308/388, train_loss: 0.1261, step time: 0.5347\n",
      "309/388, train_loss: 0.1465, step time: 0.5148\n",
      "310/388, train_loss: 0.2633, step time: 0.4909\n",
      "311/388, train_loss: 0.1984, step time: 0.4945\n",
      "312/388, train_loss: 0.1576, step time: 0.4881\n",
      "313/388, train_loss: 0.0810, step time: 0.5082\n",
      "314/388, train_loss: 0.2221, step time: 0.5104\n",
      "315/388, train_loss: 0.0297, step time: 0.4965\n",
      "316/388, train_loss: 0.1033, step time: 0.4822\n",
      "317/388, train_loss: 0.1287, step time: 0.5411\n",
      "318/388, train_loss: 0.0766, step time: 0.5521\n",
      "319/388, train_loss: 0.2158, step time: 0.5406\n",
      "320/388, train_loss: 0.2168, step time: 0.5118\n",
      "321/388, train_loss: 0.1467, step time: 0.5102\n",
      "322/388, train_loss: 0.0695, step time: 0.4983\n",
      "323/388, train_loss: 0.1565, step time: 0.4988\n",
      "324/388, train_loss: 0.1703, step time: 0.5281\n",
      "325/388, train_loss: 0.1020, step time: 0.5122\n",
      "326/388, train_loss: 0.1112, step time: 0.5066\n",
      "327/388, train_loss: 0.0814, step time: 0.4930\n",
      "328/388, train_loss: 0.2901, step time: 0.5136\n",
      "329/388, train_loss: 0.2865, step time: 0.6266\n",
      "330/388, train_loss: 0.0657, step time: 0.6218\n",
      "331/388, train_loss: 0.1876, step time: 0.5537\n",
      "332/388, train_loss: 0.3104, step time: 0.5315\n",
      "333/388, train_loss: 0.1003, step time: 0.5633\n",
      "334/388, train_loss: 0.3424, step time: 0.5436\n",
      "335/388, train_loss: 0.1622, step time: 0.5250\n",
      "336/388, train_loss: 0.1743, step time: 0.5057\n",
      "337/388, train_loss: 0.2315, step time: 0.4909\n",
      "338/388, train_loss: 0.0681, step time: 0.5027\n",
      "339/388, train_loss: 0.2993, step time: 0.4846\n",
      "340/388, train_loss: 0.0871, step time: 0.4779\n",
      "341/388, train_loss: 0.1020, step time: 0.5041\n",
      "342/388, train_loss: 0.2320, step time: 0.5039\n",
      "343/388, train_loss: 0.0932, step time: 0.4985\n",
      "344/388, train_loss: 0.3143, step time: 0.4793\n",
      "345/388, train_loss: 0.1716, step time: 0.4945\n",
      "346/388, train_loss: 0.1569, step time: 1.1119\n",
      "347/388, train_loss: 0.2198, step time: 0.5610\n",
      "348/388, train_loss: 0.0721, step time: 0.5338\n",
      "349/388, train_loss: 0.2833, step time: 0.5192\n",
      "350/388, train_loss: 0.2703, step time: 0.5166\n",
      "351/388, train_loss: 0.2855, step time: 0.5144\n",
      "352/388, train_loss: 0.2117, step time: 0.4968\n",
      "353/388, train_loss: 0.4511, step time: 0.9273\n",
      "354/388, train_loss: 0.1523, step time: 0.5470\n",
      "355/388, train_loss: 0.2148, step time: 0.5218\n",
      "356/388, train_loss: 0.4799, step time: 0.5005\n",
      "357/388, train_loss: 0.2528, step time: 0.4908\n",
      "358/388, train_loss: 0.1755, step time: 0.4926\n",
      "359/388, train_loss: 0.0690, step time: 0.4858\n",
      "360/388, train_loss: 0.1683, step time: 0.4942\n",
      "361/388, train_loss: 0.2576, step time: 1.1299\n",
      "362/388, train_loss: 0.1013, step time: 0.5422\n",
      "363/388, train_loss: 0.1387, step time: 0.5006\n",
      "364/388, train_loss: 0.5167, step time: 0.4938\n",
      "365/388, train_loss: 0.2304, step time: 0.5287\n",
      "366/388, train_loss: 0.1200, step time: 0.5095\n",
      "367/388, train_loss: 0.0937, step time: 0.4875\n",
      "368/388, train_loss: 0.1053, step time: 0.4831\n",
      "369/388, train_loss: 0.3371, step time: 1.0837\n",
      "370/388, train_loss: 0.2860, step time: 0.5422\n",
      "371/388, train_loss: 0.2914, step time: 0.5186\n",
      "372/388, train_loss: 0.1660, step time: 0.4896\n",
      "373/388, train_loss: 0.0468, step time: 0.4989\n",
      "374/388, train_loss: 0.4744, step time: 0.4844\n",
      "375/388, train_loss: 0.1335, step time: 0.6832\n",
      "376/388, train_loss: 0.2201, step time: 0.5692\n",
      "377/388, train_loss: 0.0711, step time: 0.5379\n",
      "378/388, train_loss: 0.1553, step time: 0.5081\n",
      "379/388, train_loss: 0.1963, step time: 0.5028\n",
      "380/388, train_loss: 0.2405, step time: 0.4932\n",
      "381/388, train_loss: 0.1998, step time: 0.4901\n",
      "382/388, train_loss: 0.1803, step time: 1.1645\n",
      "383/388, train_loss: 0.2544, step time: 0.5129\n",
      "384/388, train_loss: 0.1306, step time: 0.4954\n",
      "385/388, train_loss: 0.1277, step time: 0.4797\n",
      "386/388, train_loss: 0.1049, step time: 0.4859\n",
      "387/388, train_loss: 0.4729, step time: 0.5063\n",
      "388/388, train_loss: 0.1990, step time: 0.5557\n",
      "epoch 51 average loss: 0.1878\n",
      "current epoch: 51 current mean dice: 0.7627 tc: 0.8132 wt: 0.8946 et: 0.5803\n",
      "best mean dice: 0.7635 at epoch: 50\n",
      "time consuming of epoch 51 is: 304.0042\n",
      "----------\n",
      "epoch 52/300\n",
      "1/388, train_loss: 0.1524, step time: 0.4719\n",
      "2/388, train_loss: 0.3347, step time: 0.4769\n",
      "3/388, train_loss: 0.1637, step time: 0.8772\n",
      "4/388, train_loss: 0.2147, step time: 0.5733\n",
      "5/388, train_loss: 0.1784, step time: 0.5464\n",
      "6/388, train_loss: 0.0892, step time: 0.5178\n",
      "7/388, train_loss: 0.0992, step time: 0.4908\n",
      "8/388, train_loss: 0.3924, step time: 0.5285\n",
      "9/388, train_loss: 0.3286, step time: 0.5881\n",
      "10/388, train_loss: 0.1040, step time: 0.5573\n",
      "11/388, train_loss: 0.1918, step time: 0.5325\n",
      "12/388, train_loss: 0.2973, step time: 0.5350\n",
      "13/388, train_loss: 0.1144, step time: 0.4981\n",
      "14/388, train_loss: 0.0986, step time: 0.9770\n",
      "15/388, train_loss: 0.2614, step time: 0.5635\n",
      "16/388, train_loss: 0.0777, step time: 0.5388\n",
      "17/388, train_loss: 0.2026, step time: 0.5075\n",
      "18/388, train_loss: 0.1084, step time: 0.5018\n",
      "19/388, train_loss: 0.2483, step time: 0.4959\n",
      "20/388, train_loss: 0.0587, step time: 0.5045\n",
      "21/388, train_loss: 0.1537, step time: 0.5756\n",
      "22/388, train_loss: 0.1969, step time: 0.7377\n",
      "23/388, train_loss: 0.4208, step time: 0.5601\n",
      "24/388, train_loss: 0.0446, step time: 0.5257\n",
      "25/388, train_loss: 0.2236, step time: 0.5079\n",
      "26/388, train_loss: 0.1665, step time: 0.5051\n",
      "27/388, train_loss: 0.1887, step time: 0.9714\n",
      "28/388, train_loss: 0.3030, step time: 0.5363\n",
      "29/388, train_loss: 0.0672, step time: 0.5057\n",
      "30/388, train_loss: 0.0897, step time: 0.5064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/388, train_loss: 0.1971, step time: 0.5135\n",
      "32/388, train_loss: 0.2596, step time: 0.4877\n",
      "33/388, train_loss: 0.0736, step time: 0.4836\n",
      "34/388, train_loss: 0.1496, step time: 0.7814\n",
      "35/388, train_loss: 0.1030, step time: 0.5544\n",
      "36/388, train_loss: 0.0678, step time: 0.5165\n",
      "37/388, train_loss: 0.0731, step time: 0.5096\n",
      "38/388, train_loss: 0.1813, step time: 0.4935\n",
      "39/388, train_loss: 0.1401, step time: 0.4950\n",
      "40/388, train_loss: 0.1731, step time: 0.4906\n",
      "41/388, train_loss: 0.0594, step time: 0.5369\n",
      "42/388, train_loss: 0.2653, step time: 0.4998\n",
      "43/388, train_loss: 0.2251, step time: 0.5099\n",
      "44/388, train_loss: 0.1113, step time: 0.4935\n",
      "45/388, train_loss: 0.1558, step time: 0.5152\n",
      "46/388, train_loss: 0.1209, step time: 0.5209\n",
      "47/388, train_loss: 0.0804, step time: 0.6596\n",
      "48/388, train_loss: 0.1095, step time: 0.5318\n",
      "49/388, train_loss: 0.1773, step time: 0.5100\n",
      "50/388, train_loss: 0.1408, step time: 0.4934\n",
      "51/388, train_loss: 0.3351, step time: 0.4947\n",
      "52/388, train_loss: 0.0871, step time: 0.5181\n",
      "53/388, train_loss: 0.1144, step time: 0.5034\n",
      "54/388, train_loss: 0.2742, step time: 0.5353\n",
      "55/388, train_loss: 0.2446, step time: 0.5166\n",
      "56/388, train_loss: 0.2380, step time: 0.5029\n",
      "57/388, train_loss: 0.4586, step time: 0.4966\n",
      "58/388, train_loss: 0.2237, step time: 0.9476\n",
      "59/388, train_loss: 0.1031, step time: 0.5446\n",
      "60/388, train_loss: 0.0705, step time: 0.5294\n",
      "61/388, train_loss: 0.1419, step time: 0.5041\n",
      "62/388, train_loss: 0.3088, step time: 0.4993\n",
      "63/388, train_loss: 0.1137, step time: 0.4839\n",
      "64/388, train_loss: 0.3254, step time: 0.5075\n",
      "65/388, train_loss: 0.1450, step time: 0.4940\n",
      "66/388, train_loss: 0.2553, step time: 0.4966\n",
      "67/388, train_loss: 0.1945, step time: 0.4899\n",
      "68/388, train_loss: 0.0658, step time: 0.4916\n",
      "69/388, train_loss: 0.0335, step time: 0.4974\n",
      "70/388, train_loss: 0.2961, step time: 0.4832\n",
      "71/388, train_loss: 0.1610, step time: 1.1934\n",
      "72/388, train_loss: 0.1814, step time: 0.5329\n",
      "73/388, train_loss: 0.2942, step time: 0.5071\n",
      "74/388, train_loss: 0.2687, step time: 0.4975\n",
      "75/388, train_loss: 0.2348, step time: 0.4843\n",
      "76/388, train_loss: 0.1115, step time: 0.4865\n",
      "77/388, train_loss: 0.3351, step time: 0.4989\n",
      "78/388, train_loss: 0.1608, step time: 0.5024\n",
      "79/388, train_loss: 0.1566, step time: 0.5019\n",
      "80/388, train_loss: 0.4108, step time: 0.4927\n",
      "81/388, train_loss: 0.1712, step time: 0.4921\n",
      "82/388, train_loss: 0.1713, step time: 0.9125\n",
      "83/388, train_loss: 0.2019, step time: 0.5480\n",
      "84/388, train_loss: 0.0972, step time: 0.5187\n",
      "85/388, train_loss: 0.3883, step time: 0.5086\n",
      "86/388, train_loss: 0.1740, step time: 0.4906\n",
      "87/388, train_loss: 0.1466, step time: 0.4864\n",
      "88/388, train_loss: 0.1667, step time: 0.4934\n",
      "89/388, train_loss: 0.1296, step time: 0.4807\n",
      "90/388, train_loss: 0.0649, step time: 0.4810\n",
      "91/388, train_loss: 0.1902, step time: 0.4774\n",
      "92/388, train_loss: 0.2639, step time: 0.5477\n",
      "93/388, train_loss: 0.2290, step time: 0.5260\n",
      "94/388, train_loss: 0.1407, step time: 0.5178\n",
      "95/388, train_loss: 0.1734, step time: 0.5006\n",
      "96/388, train_loss: 0.2605, step time: 0.4866\n",
      "97/388, train_loss: 0.1321, step time: 0.4961\n",
      "98/388, train_loss: 0.0934, step time: 0.5134\n",
      "99/388, train_loss: 0.2006, step time: 0.5077\n",
      "100/388, train_loss: 0.2240, step time: 0.4909\n",
      "101/388, train_loss: 0.2131, step time: 1.0542\n",
      "102/388, train_loss: 0.1921, step time: 0.5298\n",
      "103/388, train_loss: 0.1654, step time: 0.5165\n",
      "104/388, train_loss: 0.2000, step time: 0.4921\n",
      "105/388, train_loss: 0.0765, step time: 0.5025\n",
      "106/388, train_loss: 0.2410, step time: 0.4865\n",
      "107/388, train_loss: 0.3599, step time: 1.1738\n",
      "108/388, train_loss: 0.1070, step time: 0.5315\n",
      "109/388, train_loss: 0.0653, step time: 0.5114\n",
      "110/388, train_loss: 0.1058, step time: 0.4865\n",
      "111/388, train_loss: 0.2979, step time: 0.4842\n",
      "112/388, train_loss: 0.1801, step time: 0.5028\n",
      "113/388, train_loss: 0.2257, step time: 0.5117\n",
      "114/388, train_loss: 0.2056, step time: 0.4872\n",
      "115/388, train_loss: 0.0909, step time: 0.5462\n",
      "116/388, train_loss: 0.2508, step time: 0.5243\n",
      "117/388, train_loss: 0.2674, step time: 0.5023\n",
      "118/388, train_loss: 0.2022, step time: 0.4886\n",
      "119/388, train_loss: 0.3439, step time: 0.4936\n",
      "120/388, train_loss: 0.1476, step time: 0.4994\n",
      "121/388, train_loss: 0.0902, step time: 0.5007\n",
      "122/388, train_loss: 0.2199, step time: 0.5157\n",
      "123/388, train_loss: 0.0719, step time: 0.5212\n",
      "124/388, train_loss: 0.1967, step time: 0.5184\n",
      "125/388, train_loss: 0.3071, step time: 0.4992\n",
      "126/388, train_loss: 0.1063, step time: 0.5124\n",
      "127/388, train_loss: 0.1446, step time: 0.4952\n",
      "128/388, train_loss: 0.0624, step time: 0.4983\n",
      "129/388, train_loss: 0.0818, step time: 0.4833\n",
      "130/388, train_loss: 0.5550, step time: 0.4783\n",
      "131/388, train_loss: 0.2725, step time: 0.5027\n",
      "132/388, train_loss: 0.1617, step time: 0.5236\n",
      "133/388, train_loss: 0.1440, step time: 0.5219\n",
      "134/388, train_loss: 0.0958, step time: 0.4957\n",
      "135/388, train_loss: 0.1263, step time: 0.4935\n",
      "136/388, train_loss: 0.1385, step time: 0.4826\n",
      "137/388, train_loss: 0.2708, step time: 1.0731\n",
      "138/388, train_loss: 0.2575, step time: 0.5439\n",
      "139/388, train_loss: 0.2209, step time: 0.5008\n",
      "140/388, train_loss: 0.1290, step time: 0.4894\n",
      "141/388, train_loss: 0.1682, step time: 0.5114\n",
      "142/388, train_loss: 0.1433, step time: 0.4970\n",
      "143/388, train_loss: 0.2826, step time: 0.4964\n",
      "144/388, train_loss: 0.2446, step time: 0.4863\n",
      "145/388, train_loss: 0.3857, step time: 0.5150\n",
      "146/388, train_loss: 0.2277, step time: 0.4925\n",
      "147/388, train_loss: 0.1116, step time: 0.5108\n",
      "148/388, train_loss: 0.2222, step time: 0.5477\n",
      "149/388, train_loss: 0.4229, step time: 0.5205\n",
      "150/388, train_loss: 0.1036, step time: 0.5063\n",
      "151/388, train_loss: 0.1391, step time: 0.4948\n",
      "152/388, train_loss: 0.2572, step time: 0.4940\n",
      "153/388, train_loss: 0.1691, step time: 0.6908\n",
      "154/388, train_loss: 0.1575, step time: 0.5447\n",
      "155/388, train_loss: 0.3776, step time: 0.5219\n",
      "156/388, train_loss: 0.2161, step time: 0.4959\n",
      "157/388, train_loss: 0.4069, step time: 0.4998\n",
      "158/388, train_loss: 0.1319, step time: 0.4961\n",
      "159/388, train_loss: 0.1016, step time: 0.5027\n",
      "160/388, train_loss: 0.1490, step time: 0.4967\n",
      "161/388, train_loss: 0.1471, step time: 0.7870\n",
      "162/388, train_loss: 0.3334, step time: 0.5381\n",
      "163/388, train_loss: 0.2131, step time: 0.5078\n",
      "164/388, train_loss: 0.1404, step time: 0.5035\n",
      "165/388, train_loss: 0.4139, step time: 0.4891\n",
      "166/388, train_loss: 0.3554, step time: 0.4873\n",
      "167/388, train_loss: 0.3118, step time: 0.4944\n",
      "168/388, train_loss: 0.1392, step time: 0.5171\n",
      "169/388, train_loss: 0.1053, step time: 0.4989\n",
      "170/388, train_loss: 0.3076, step time: 0.8541\n",
      "171/388, train_loss: 0.5101, step time: 0.5629\n",
      "172/388, train_loss: 0.1559, step time: 0.5068\n",
      "173/388, train_loss: 0.1098, step time: 0.4813\n",
      "174/388, train_loss: 0.1395, step time: 0.4806\n",
      "175/388, train_loss: 0.1531, step time: 0.6371\n",
      "176/388, train_loss: 0.0369, step time: 0.5511\n",
      "177/388, train_loss: 0.4042, step time: 0.5104\n",
      "178/388, train_loss: 0.2666, step time: 0.4857\n",
      "179/388, train_loss: 0.2331, step time: 0.4736\n",
      "180/388, train_loss: 0.3025, step time: 0.4853\n",
      "181/388, train_loss: 0.1123, step time: 0.4835\n",
      "182/388, train_loss: 0.1978, step time: 0.5179\n",
      "183/388, train_loss: 0.0842, step time: 0.5029\n",
      "184/388, train_loss: 0.2209, step time: 0.4813\n",
      "185/388, train_loss: 0.2115, step time: 1.0210\n",
      "186/388, train_loss: 0.1802, step time: 0.5227\n",
      "187/388, train_loss: 0.0719, step time: 0.4931\n",
      "188/388, train_loss: 0.2362, step time: 0.4761\n",
      "189/388, train_loss: 0.3773, step time: 0.4769\n",
      "190/388, train_loss: 0.5144, step time: 0.4775\n",
      "191/388, train_loss: 0.0950, step time: 1.0271\n",
      "192/388, train_loss: 0.3095, step time: 0.5438\n",
      "193/388, train_loss: 0.0315, step time: 0.5059\n",
      "194/388, train_loss: 0.2068, step time: 0.4965\n",
      "195/388, train_loss: 0.2092, step time: 0.4822\n",
      "196/388, train_loss: 0.1727, step time: 0.4879\n",
      "197/388, train_loss: 0.1758, step time: 0.4991\n",
      "198/388, train_loss: 0.1324, step time: 0.4884\n",
      "199/388, train_loss: 0.0960, step time: 0.9171\n",
      "200/388, train_loss: 0.0838, step time: 0.5608\n",
      "201/388, train_loss: 0.1931, step time: 0.5349\n",
      "202/388, train_loss: 0.1341, step time: 0.5018\n",
      "203/388, train_loss: 0.2261, step time: 0.5013\n",
      "204/388, train_loss: 0.1143, step time: 0.4878\n",
      "205/388, train_loss: 0.2421, step time: 0.4794\n",
      "206/388, train_loss: 0.1512, step time: 0.4787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/388, train_loss: 0.0622, step time: 0.5872\n",
      "208/388, train_loss: 0.2296, step time: 0.5589\n",
      "209/388, train_loss: 0.1151, step time: 0.5206\n",
      "210/388, train_loss: 0.2384, step time: 0.5128\n",
      "211/388, train_loss: 0.1128, step time: 0.4887\n",
      "212/388, train_loss: 0.3068, step time: 0.7894\n",
      "213/388, train_loss: 0.2976, step time: 0.5632\n",
      "214/388, train_loss: 0.1890, step time: 0.5227\n",
      "215/388, train_loss: 0.1971, step time: 0.5086\n",
      "216/388, train_loss: 0.1666, step time: 0.5521\n",
      "217/388, train_loss: 0.1892, step time: 0.5386\n",
      "218/388, train_loss: 0.1948, step time: 0.5156\n",
      "219/388, train_loss: 0.0883, step time: 0.5027\n",
      "220/388, train_loss: 0.2231, step time: 0.4862\n",
      "221/388, train_loss: 0.0848, step time: 0.4803\n",
      "222/388, train_loss: 0.0668, step time: 1.1634\n",
      "223/388, train_loss: 0.1324, step time: 0.5223\n",
      "224/388, train_loss: 0.6022, step time: 0.5083\n",
      "225/388, train_loss: 0.4355, step time: 0.4950\n",
      "226/388, train_loss: 0.0813, step time: 0.4859\n",
      "227/388, train_loss: 0.1031, step time: 0.4805\n",
      "228/388, train_loss: 0.0485, step time: 0.9506\n",
      "229/388, train_loss: 0.0972, step time: 0.5334\n",
      "230/388, train_loss: 0.0771, step time: 0.5105\n",
      "231/388, train_loss: 0.0970, step time: 0.4862\n",
      "232/388, train_loss: 0.1556, step time: 0.4969\n",
      "233/388, train_loss: 0.3074, step time: 0.4812\n",
      "234/388, train_loss: 0.0860, step time: 0.4812\n",
      "235/388, train_loss: 0.1379, step time: 0.4869\n",
      "236/388, train_loss: 0.1774, step time: 0.4844\n",
      "237/388, train_loss: 0.1620, step time: 0.8233\n",
      "238/388, train_loss: 0.1113, step time: 0.5559\n",
      "239/388, train_loss: 0.2142, step time: 0.5337\n",
      "240/388, train_loss: 0.2241, step time: 0.5132\n",
      "241/388, train_loss: 0.2980, step time: 0.4916\n",
      "242/388, train_loss: 0.1571, step time: 0.4868\n",
      "243/388, train_loss: 0.4719, step time: 0.4945\n",
      "244/388, train_loss: 0.2334, step time: 1.1724\n",
      "245/388, train_loss: 0.2497, step time: 0.5447\n",
      "246/388, train_loss: 0.2439, step time: 0.5161\n",
      "247/388, train_loss: 0.1356, step time: 0.4862\n",
      "248/388, train_loss: 0.2925, step time: 0.4862\n",
      "249/388, train_loss: 0.1643, step time: 0.4893\n",
      "250/388, train_loss: 0.1434, step time: 1.1160\n",
      "251/388, train_loss: 0.1974, step time: 0.5193\n",
      "252/388, train_loss: 0.1041, step time: 0.4984\n",
      "253/388, train_loss: 0.0539, step time: 0.4822\n",
      "254/388, train_loss: 0.0569, step time: 0.5135\n",
      "255/388, train_loss: 0.4105, step time: 0.4963\n",
      "256/388, train_loss: 0.1521, step time: 0.5014\n",
      "257/388, train_loss: 0.2052, step time: 0.4807\n",
      "258/388, train_loss: 0.1011, step time: 0.4759\n",
      "259/388, train_loss: 0.3043, step time: 0.4768\n",
      "260/388, train_loss: 0.0512, step time: 0.7401\n",
      "261/388, train_loss: 0.3388, step time: 0.5610\n",
      "262/388, train_loss: 0.1251, step time: 0.5157\n",
      "263/388, train_loss: 0.1441, step time: 0.4844\n",
      "264/388, train_loss: 0.2875, step time: 0.5220\n",
      "265/388, train_loss: 0.2166, step time: 0.4962\n",
      "266/388, train_loss: 0.1185, step time: 0.4907\n",
      "267/388, train_loss: 0.5151, step time: 0.5340\n",
      "268/388, train_loss: 0.1191, step time: 0.5246\n",
      "269/388, train_loss: 0.1881, step time: 0.4967\n",
      "270/388, train_loss: 0.4081, step time: 1.2003\n",
      "271/388, train_loss: 0.2060, step time: 0.5384\n",
      "272/388, train_loss: 0.1076, step time: 0.5082\n",
      "273/388, train_loss: 0.1821, step time: 0.4930\n",
      "274/388, train_loss: 0.4804, step time: 0.4996\n",
      "275/388, train_loss: 0.1696, step time: 0.5048\n",
      "276/388, train_loss: 0.3513, step time: 0.5795\n",
      "277/388, train_loss: 0.0855, step time: 0.5414\n",
      "278/388, train_loss: 0.2277, step time: 0.5004\n",
      "279/388, train_loss: 0.2519, step time: 0.5030\n",
      "280/388, train_loss: 0.1114, step time: 0.5015\n",
      "281/388, train_loss: 0.0511, step time: 0.4858\n",
      "282/388, train_loss: 0.1604, step time: 0.4922\n",
      "283/388, train_loss: 0.3610, step time: 1.2199\n",
      "284/388, train_loss: 0.1620, step time: 0.5356\n",
      "285/388, train_loss: 0.1722, step time: 0.4996\n",
      "286/388, train_loss: 0.2429, step time: 0.4910\n",
      "287/388, train_loss: 0.1545, step time: 0.4940\n",
      "288/388, train_loss: 0.1684, step time: 0.4821\n",
      "289/388, train_loss: 0.0938, step time: 0.5897\n",
      "290/388, train_loss: 0.1003, step time: 0.5498\n",
      "291/388, train_loss: 0.1101, step time: 0.5125\n",
      "292/388, train_loss: 0.2322, step time: 0.5126\n",
      "293/388, train_loss: 0.2716, step time: 0.5061\n",
      "294/388, train_loss: 0.1035, step time: 0.4850\n",
      "295/388, train_loss: 0.1713, step time: 1.0924\n",
      "296/388, train_loss: 0.0676, step time: 0.5403\n",
      "297/388, train_loss: 0.1455, step time: 0.5112\n",
      "298/388, train_loss: 0.0915, step time: 0.5053\n",
      "299/388, train_loss: 0.3801, step time: 0.4893\n",
      "300/388, train_loss: 0.1805, step time: 0.4862\n",
      "301/388, train_loss: 0.2078, step time: 1.1071\n",
      "302/388, train_loss: 0.5273, step time: 0.5503\n",
      "303/388, train_loss: 0.1329, step time: 0.5136\n",
      "304/388, train_loss: 0.0800, step time: 0.4901\n",
      "305/388, train_loss: 0.0837, step time: 0.4885\n",
      "306/388, train_loss: 0.0664, step time: 0.4912\n",
      "307/388, train_loss: 0.1032, step time: 0.4772\n",
      "308/388, train_loss: 0.2821, step time: 0.4764\n",
      "309/388, train_loss: 0.0953, step time: 0.4858\n",
      "310/388, train_loss: 0.1296, step time: 0.9808\n",
      "311/388, train_loss: 0.1392, step time: 0.5360\n",
      "312/388, train_loss: 0.3314, step time: 0.5169\n",
      "313/388, train_loss: 0.2933, step time: 0.5001\n",
      "314/388, train_loss: 0.2959, step time: 0.5180\n",
      "315/388, train_loss: 0.2102, step time: 0.5262\n",
      "316/388, train_loss: 0.1369, step time: 0.5747\n",
      "317/388, train_loss: 0.2016, step time: 0.5250\n",
      "318/388, train_loss: 0.1040, step time: 0.4947\n",
      "319/388, train_loss: 0.0797, step time: 0.4837\n",
      "320/388, train_loss: 0.0929, step time: 0.5005\n",
      "321/388, train_loss: 0.2674, step time: 0.4962\n",
      "322/388, train_loss: 0.1044, step time: 0.5082\n",
      "323/388, train_loss: 0.1594, step time: 0.4967\n",
      "324/388, train_loss: 0.1908, step time: 0.4976\n",
      "325/388, train_loss: 0.1266, step time: 0.4783\n",
      "326/388, train_loss: 0.3207, step time: 0.6308\n",
      "327/388, train_loss: 0.3010, step time: 0.5716\n",
      "328/388, train_loss: 0.1061, step time: 0.5351\n",
      "329/388, train_loss: 0.0661, step time: 0.4970\n",
      "330/388, train_loss: 0.3054, step time: 0.4906\n",
      "331/388, train_loss: 0.1204, step time: 1.1267\n",
      "332/388, train_loss: 0.1269, step time: 0.5343\n",
      "333/388, train_loss: 0.1332, step time: 0.5067\n",
      "334/388, train_loss: 0.0627, step time: 0.4898\n",
      "335/388, train_loss: 0.0899, step time: 0.4991\n",
      "336/388, train_loss: 0.4224, step time: 0.4823\n",
      "337/388, train_loss: 0.2052, step time: 0.6197\n",
      "338/388, train_loss: 0.3913, step time: 0.5527\n",
      "339/388, train_loss: 0.1026, step time: 0.5106\n",
      "340/388, train_loss: 0.0860, step time: 0.5021\n",
      "341/388, train_loss: 0.1273, step time: 0.5498\n",
      "342/388, train_loss: 0.1352, step time: 0.5977\n",
      "343/388, train_loss: 0.2281, step time: 0.5422\n",
      "344/388, train_loss: 0.1508, step time: 0.5174\n",
      "345/388, train_loss: 0.0880, step time: 0.5079\n",
      "346/388, train_loss: 0.2731, step time: 0.4933\n",
      "347/388, train_loss: 0.5133, step time: 0.4951\n",
      "348/388, train_loss: 0.0749, step time: 0.4987\n",
      "349/388, train_loss: 0.1384, step time: 0.9815\n",
      "350/388, train_loss: 0.1137, step time: 0.5344\n",
      "351/388, train_loss: 0.0617, step time: 0.5015\n",
      "352/388, train_loss: 0.1050, step time: 0.4893\n",
      "353/388, train_loss: 0.0628, step time: 0.5473\n",
      "354/388, train_loss: 0.0884, step time: 0.5262\n",
      "355/388, train_loss: 0.2056, step time: 0.4962\n",
      "356/388, train_loss: 0.0482, step time: 0.4945\n",
      "357/388, train_loss: 0.0430, step time: 0.4921\n",
      "358/388, train_loss: 0.1037, step time: 0.5027\n",
      "359/388, train_loss: 0.3338, step time: 0.5028\n",
      "360/388, train_loss: 0.2098, step time: 0.4966\n",
      "361/388, train_loss: 0.1710, step time: 1.1569\n",
      "362/388, train_loss: 0.6236, step time: 0.5531\n",
      "363/388, train_loss: 0.1210, step time: 0.5173\n",
      "364/388, train_loss: 0.2400, step time: 0.5006\n",
      "365/388, train_loss: 0.3313, step time: 0.4879\n",
      "366/388, train_loss: 0.3968, step time: 1.0417\n",
      "367/388, train_loss: 0.2882, step time: 0.5194\n",
      "368/388, train_loss: 0.1171, step time: 0.5000\n",
      "369/388, train_loss: 0.2757, step time: 0.4930\n",
      "370/388, train_loss: 0.1817, step time: 0.4780\n",
      "371/388, train_loss: 0.2041, step time: 0.4803\n",
      "372/388, train_loss: 0.2325, step time: 0.4845\n",
      "373/388, train_loss: 0.1810, step time: 0.5168\n",
      "374/388, train_loss: 0.1117, step time: 0.5140\n",
      "375/388, train_loss: 0.1748, step time: 0.5064\n",
      "376/388, train_loss: 0.1486, step time: 0.6462\n",
      "377/388, train_loss: 0.0969, step time: 0.5596\n",
      "378/388, train_loss: 0.1978, step time: 0.5100\n",
      "379/388, train_loss: 0.1277, step time: 0.4945\n",
      "380/388, train_loss: 0.0855, step time: 0.5100\n",
      "381/388, train_loss: 0.0511, step time: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/388, train_loss: 0.0875, step time: 0.5337\n",
      "383/388, train_loss: 0.2900, step time: 0.5110\n",
      "384/388, train_loss: 0.2086, step time: 0.4976\n",
      "385/388, train_loss: 0.1534, step time: 0.5040\n",
      "386/388, train_loss: 0.1431, step time: 0.5197\n",
      "387/388, train_loss: 0.1161, step time: 0.5074\n",
      "388/388, train_loss: 0.2219, step time: 0.5032\n",
      "epoch 52 average loss: 0.1908\n",
      "current epoch: 52 current mean dice: 0.7431 tc: 0.7879 wt: 0.8917 et: 0.5497\n",
      "best mean dice: 0.7635 at epoch: 50\n",
      "time consuming of epoch 52 is: 304.0809\n",
      "----------\n",
      "epoch 53/300\n",
      "1/388, train_loss: 0.2066, step time: 0.4715\n",
      "2/388, train_loss: 0.2435, step time: 0.5960\n",
      "3/388, train_loss: 0.2521, step time: 0.5745\n",
      "4/388, train_loss: 0.0915, step time: 0.5526\n",
      "5/388, train_loss: 0.0684, step time: 0.5114\n",
      "6/388, train_loss: 0.2244, step time: 0.5122\n",
      "7/388, train_loss: 0.2514, step time: 0.6046\n",
      "8/388, train_loss: 0.1902, step time: 0.5704\n",
      "9/388, train_loss: 0.1304, step time: 0.5213\n",
      "10/388, train_loss: 0.3017, step time: 0.5095\n",
      "11/388, train_loss: 0.1613, step time: 0.4921\n",
      "12/388, train_loss: 0.1823, step time: 0.5461\n",
      "13/388, train_loss: 0.1355, step time: 0.4898\n",
      "14/388, train_loss: 0.0899, step time: 0.5066\n",
      "15/388, train_loss: 0.0829, step time: 0.6557\n",
      "16/388, train_loss: 0.0735, step time: 0.5831\n",
      "17/388, train_loss: 0.1923, step time: 0.5546\n",
      "18/388, train_loss: 0.5370, step time: 0.5450\n",
      "19/388, train_loss: 0.0738, step time: 0.5398\n",
      "20/388, train_loss: 0.0846, step time: 0.5117\n",
      "21/388, train_loss: 0.1023, step time: 0.5126\n",
      "22/388, train_loss: 0.1222, step time: 0.5908\n",
      "23/388, train_loss: 0.1693, step time: 0.5554\n",
      "24/388, train_loss: 0.0920, step time: 0.5374\n",
      "25/388, train_loss: 0.4323, step time: 0.5392\n",
      "26/388, train_loss: 0.2380, step time: 0.5671\n",
      "27/388, train_loss: 0.3069, step time: 0.5369\n",
      "28/388, train_loss: 0.1899, step time: 0.5627\n",
      "29/388, train_loss: 0.1441, step time: 0.5342\n",
      "30/388, train_loss: 0.0938, step time: 0.5180\n",
      "31/388, train_loss: 0.1301, step time: 0.5093\n",
      "32/388, train_loss: 0.1198, step time: 1.2489\n",
      "33/388, train_loss: 0.6716, step time: 0.5429\n",
      "34/388, train_loss: 0.1409, step time: 0.5103\n",
      "35/388, train_loss: 0.1852, step time: 0.4949\n",
      "36/388, train_loss: 0.0348, step time: 0.4949\n",
      "37/388, train_loss: 0.1266, step time: 0.4922\n",
      "38/388, train_loss: 0.1327, step time: 0.4989\n",
      "39/388, train_loss: 0.2315, step time: 0.4939\n",
      "40/388, train_loss: 0.3530, step time: 0.4872\n",
      "41/388, train_loss: 0.2586, step time: 0.5328\n",
      "42/388, train_loss: 0.1655, step time: 0.5062\n",
      "43/388, train_loss: 0.5026, step time: 0.5876\n",
      "44/388, train_loss: 0.3091, step time: 0.5697\n",
      "45/388, train_loss: 0.1149, step time: 0.5264\n",
      "46/388, train_loss: 0.2308, step time: 0.5019\n",
      "47/388, train_loss: 0.1412, step time: 0.5227\n",
      "48/388, train_loss: 0.1653, step time: 0.5005\n",
      "49/388, train_loss: 0.4554, step time: 0.5051\n",
      "50/388, train_loss: 0.1903, step time: 0.4820\n",
      "51/388, train_loss: 0.1113, step time: 0.5129\n",
      "52/388, train_loss: 0.4700, step time: 0.4981\n",
      "53/388, train_loss: 0.2685, step time: 0.7372\n",
      "54/388, train_loss: 0.1195, step time: 0.5896\n",
      "55/388, train_loss: 0.1158, step time: 0.5294\n",
      "56/388, train_loss: 0.1559, step time: 0.5115\n",
      "57/388, train_loss: 0.2323, step time: 0.5179\n",
      "58/388, train_loss: 0.4420, step time: 0.5077\n",
      "59/388, train_loss: 0.1769, step time: 0.4886\n",
      "60/388, train_loss: 0.2500, step time: 0.4896\n",
      "61/388, train_loss: 0.1198, step time: 1.1832\n",
      "62/388, train_loss: 0.3841, step time: 0.5405\n",
      "63/388, train_loss: 0.5774, step time: 0.5058\n",
      "64/388, train_loss: 0.2075, step time: 0.5053\n",
      "65/388, train_loss: 0.2151, step time: 0.5045\n",
      "66/388, train_loss: 0.2714, step time: 0.5024\n",
      "67/388, train_loss: 0.1128, step time: 0.4854\n",
      "68/388, train_loss: 0.1224, step time: 0.8161\n",
      "69/388, train_loss: 0.2014, step time: 0.5565\n",
      "70/388, train_loss: 0.1019, step time: 0.5204\n",
      "71/388, train_loss: 0.1500, step time: 0.4920\n",
      "72/388, train_loss: 0.2630, step time: 0.4901\n",
      "73/388, train_loss: 0.1080, step time: 0.4913\n",
      "74/388, train_loss: 0.5093, step time: 0.4906\n",
      "75/388, train_loss: 0.0906, step time: 0.5133\n",
      "76/388, train_loss: 0.1061, step time: 0.4903\n",
      "77/388, train_loss: 0.1965, step time: 0.4962\n",
      "78/388, train_loss: 0.1690, step time: 0.5002\n",
      "79/388, train_loss: 0.1035, step time: 1.1828\n",
      "80/388, train_loss: 0.5251, step time: 0.5362\n",
      "81/388, train_loss: 0.1028, step time: 0.5101\n",
      "82/388, train_loss: 0.1925, step time: 0.4880\n",
      "83/388, train_loss: 0.2647, step time: 0.4973\n",
      "84/388, train_loss: 0.0640, step time: 0.4815\n",
      "85/388, train_loss: 0.1964, step time: 0.6973\n",
      "86/388, train_loss: 0.4152, step time: 0.5361\n",
      "87/388, train_loss: 0.2911, step time: 0.5183\n",
      "88/388, train_loss: 0.0926, step time: 0.5040\n",
      "89/388, train_loss: 0.1929, step time: 0.5004\n",
      "90/388, train_loss: 0.3669, step time: 0.4812\n",
      "91/388, train_loss: 0.1123, step time: 0.4922\n",
      "92/388, train_loss: 0.1576, step time: 0.4797\n",
      "93/388, train_loss: 0.3406, step time: 0.5032\n",
      "94/388, train_loss: 0.1767, step time: 0.4848\n",
      "95/388, train_loss: 0.1081, step time: 0.4897\n",
      "96/388, train_loss: 0.0857, step time: 0.9985\n",
      "97/388, train_loss: 0.1038, step time: 0.5392\n",
      "98/388, train_loss: 0.1376, step time: 0.5113\n",
      "99/388, train_loss: 0.3346, step time: 0.4915\n",
      "100/388, train_loss: 0.1635, step time: 0.4872\n",
      "101/388, train_loss: 0.2003, step time: 0.4898\n",
      "102/388, train_loss: 0.1955, step time: 0.4742\n",
      "103/388, train_loss: 0.0789, step time: 0.4856\n",
      "104/388, train_loss: 0.1798, step time: 0.4882\n",
      "105/388, train_loss: 0.3228, step time: 0.9993\n",
      "106/388, train_loss: 0.1642, step time: 0.5416\n",
      "107/388, train_loss: 0.0986, step time: 0.5148\n",
      "108/388, train_loss: 0.1163, step time: 0.4981\n",
      "109/388, train_loss: 0.3295, step time: 0.4823\n",
      "110/388, train_loss: 0.2895, step time: 0.4845\n",
      "111/388, train_loss: 0.4091, step time: 0.4749\n",
      "112/388, train_loss: 0.4450, step time: 0.7093\n",
      "113/388, train_loss: 0.1982, step time: 0.5622\n",
      "114/388, train_loss: 0.1430, step time: 0.5206\n",
      "115/388, train_loss: 0.1422, step time: 0.4997\n",
      "116/388, train_loss: 0.2983, step time: 0.4810\n",
      "117/388, train_loss: 0.1526, step time: 0.4854\n",
      "118/388, train_loss: 0.1103, step time: 0.4753\n",
      "119/388, train_loss: 0.1576, step time: 0.4714\n",
      "120/388, train_loss: 0.1127, step time: 0.4749\n",
      "121/388, train_loss: 0.1393, step time: 0.4785\n",
      "122/388, train_loss: 0.2036, step time: 0.8551\n",
      "123/388, train_loss: 0.4104, step time: 0.5562\n",
      "124/388, train_loss: 0.0810, step time: 0.5099\n",
      "125/388, train_loss: 0.1605, step time: 0.5005\n",
      "126/388, train_loss: 0.1504, step time: 0.4818\n",
      "127/388, train_loss: 0.1241, step time: 0.5157\n",
      "128/388, train_loss: 0.2115, step time: 0.5061\n",
      "129/388, train_loss: 0.1122, step time: 0.4902\n",
      "130/388, train_loss: 0.3593, step time: 0.4917\n",
      "131/388, train_loss: 0.1996, step time: 0.4770\n",
      "132/388, train_loss: 0.2133, step time: 0.4790\n",
      "133/388, train_loss: 0.2008, step time: 0.9336\n",
      "134/388, train_loss: 0.2776, step time: 0.5492\n",
      "135/388, train_loss: 0.4672, step time: 0.5113\n",
      "136/388, train_loss: 0.1737, step time: 0.4857\n",
      "137/388, train_loss: 0.5881, step time: 0.4865\n",
      "138/388, train_loss: 0.0631, step time: 0.4906\n",
      "139/388, train_loss: 0.0844, step time: 0.4766\n",
      "140/388, train_loss: 0.1600, step time: 0.9530\n",
      "141/388, train_loss: 0.1559, step time: 0.5355\n",
      "142/388, train_loss: 0.1931, step time: 0.5004\n",
      "143/388, train_loss: 0.1044, step time: 0.4970\n",
      "144/388, train_loss: 0.1441, step time: 0.4846\n",
      "145/388, train_loss: 0.2248, step time: 1.0141\n",
      "146/388, train_loss: 0.1818, step time: 0.5346\n",
      "147/388, train_loss: 0.1290, step time: 0.5106\n",
      "148/388, train_loss: 0.0903, step time: 0.4889\n",
      "149/388, train_loss: 0.2494, step time: 0.4864\n",
      "150/388, train_loss: 0.1206, step time: 0.5022\n",
      "151/388, train_loss: 0.2944, step time: 0.4840\n",
      "152/388, train_loss: 0.0753, step time: 0.4853\n",
      "153/388, train_loss: 0.2640, step time: 0.4891\n",
      "154/388, train_loss: 0.1378, step time: 0.4931\n",
      "155/388, train_loss: 0.1579, step time: 0.4917\n",
      "156/388, train_loss: 0.3380, step time: 0.5454\n",
      "157/388, train_loss: 0.2893, step time: 0.5136\n",
      "158/388, train_loss: 0.1380, step time: 0.4978\n",
      "159/388, train_loss: 0.1137, step time: 0.4868\n",
      "160/388, train_loss: 0.3361, step time: 0.4919\n",
      "161/388, train_loss: 0.1406, step time: 0.4962\n",
      "162/388, train_loss: 0.4579, step time: 0.4976\n",
      "163/388, train_loss: 0.2333, step time: 0.4931\n",
      "164/388, train_loss: 0.4989, step time: 0.4940\n",
      "165/388, train_loss: 0.1231, step time: 0.5164\n",
      "166/388, train_loss: 0.1710, step time: 0.5360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/388, train_loss: 0.1497, step time: 0.5139\n",
      "168/388, train_loss: 0.1484, step time: 0.4905\n",
      "169/388, train_loss: 0.2293, step time: 0.4857\n",
      "170/388, train_loss: 0.2768, step time: 0.5075\n",
      "171/388, train_loss: 0.1254, step time: 0.5862\n",
      "172/388, train_loss: 0.0775, step time: 0.5585\n",
      "173/388, train_loss: 0.0968, step time: 0.5150\n",
      "174/388, train_loss: 0.2136, step time: 0.5034\n",
      "175/388, train_loss: 0.1493, step time: 0.4875\n",
      "176/388, train_loss: 0.2448, step time: 0.4826\n",
      "177/388, train_loss: 0.1601, step time: 0.4845\n",
      "178/388, train_loss: 0.1724, step time: 0.6040\n",
      "179/388, train_loss: 0.0748, step time: 0.5644\n",
      "180/388, train_loss: 0.0875, step time: 0.5265\n",
      "181/388, train_loss: 0.1048, step time: 0.5134\n",
      "182/388, train_loss: 0.1206, step time: 0.5083\n",
      "183/388, train_loss: 0.2658, step time: 0.5520\n",
      "184/388, train_loss: 0.0537, step time: 0.5199\n",
      "185/388, train_loss: 0.3363, step time: 0.5000\n",
      "186/388, train_loss: 0.1160, step time: 0.4981\n",
      "187/388, train_loss: 0.0838, step time: 0.4936\n",
      "188/388, train_loss: 0.1617, step time: 0.4869\n",
      "189/388, train_loss: 0.1275, step time: 0.5321\n",
      "190/388, train_loss: 0.1249, step time: 0.5166\n",
      "191/388, train_loss: 0.1964, step time: 0.5023\n",
      "192/388, train_loss: 0.1589, step time: 0.5427\n",
      "193/388, train_loss: 0.0492, step time: 0.5133\n",
      "194/388, train_loss: 0.1417, step time: 0.4995\n",
      "195/388, train_loss: 0.0940, step time: 0.4931\n",
      "196/388, train_loss: 0.1166, step time: 0.5272\n",
      "197/388, train_loss: 0.0827, step time: 0.5317\n",
      "198/388, train_loss: 0.0979, step time: 0.5061\n",
      "199/388, train_loss: 0.1971, step time: 0.5039\n",
      "200/388, train_loss: 0.0630, step time: 0.5254\n",
      "201/388, train_loss: 0.2654, step time: 0.4997\n",
      "202/388, train_loss: 0.0954, step time: 0.4941\n",
      "203/388, train_loss: 0.2909, step time: 0.4821\n",
      "204/388, train_loss: 0.1758, step time: 0.5019\n",
      "205/388, train_loss: 0.1022, step time: 0.5077\n",
      "206/388, train_loss: 0.4714, step time: 0.5599\n",
      "207/388, train_loss: 0.0612, step time: 0.5465\n",
      "208/388, train_loss: 0.0714, step time: 0.5061\n",
      "209/388, train_loss: 0.1030, step time: 0.4964\n",
      "210/388, train_loss: 0.2755, step time: 0.4916\n",
      "211/388, train_loss: 0.0797, step time: 0.4894\n",
      "212/388, train_loss: 0.3046, step time: 0.4808\n",
      "213/388, train_loss: 0.1640, step time: 0.4988\n",
      "214/388, train_loss: 0.2575, step time: 0.5212\n",
      "215/388, train_loss: 0.0394, step time: 0.6521\n",
      "216/388, train_loss: 0.2744, step time: 0.5615\n",
      "217/388, train_loss: 0.2047, step time: 0.5336\n",
      "218/388, train_loss: 0.0284, step time: 0.5070\n",
      "219/388, train_loss: 0.0814, step time: 0.5016\n",
      "220/388, train_loss: 0.1626, step time: 0.4883\n",
      "221/388, train_loss: 0.1055, step time: 0.4976\n",
      "222/388, train_loss: 0.1866, step time: 0.4825\n",
      "223/388, train_loss: 0.2486, step time: 1.0629\n",
      "224/388, train_loss: 0.0983, step time: 0.5403\n",
      "225/388, train_loss: 0.1929, step time: 0.5138\n",
      "226/388, train_loss: 0.2248, step time: 0.5474\n",
      "227/388, train_loss: 0.2265, step time: 0.5481\n",
      "228/388, train_loss: 0.2397, step time: 0.5233\n",
      "229/388, train_loss: 0.4433, step time: 0.4890\n",
      "230/388, train_loss: 0.1155, step time: 0.4882\n",
      "231/388, train_loss: 0.4187, step time: 0.4884\n",
      "232/388, train_loss: 0.0735, step time: 0.8834\n",
      "233/388, train_loss: 0.3097, step time: 0.5481\n",
      "234/388, train_loss: 0.2596, step time: 0.5214\n",
      "235/388, train_loss: 0.1049, step time: 0.4962\n",
      "236/388, train_loss: 0.2832, step time: 0.5006\n",
      "237/388, train_loss: 0.1369, step time: 0.4916\n",
      "238/388, train_loss: 0.1477, step time: 1.0707\n",
      "239/388, train_loss: 0.1857, step time: 0.5247\n",
      "240/388, train_loss: 0.2385, step time: 0.5031\n",
      "241/388, train_loss: 0.1836, step time: 0.4835\n",
      "242/388, train_loss: 0.1705, step time: 0.4837\n",
      "243/388, train_loss: 0.1235, step time: 0.4982\n",
      "244/388, train_loss: 0.3884, step time: 0.4857\n",
      "245/388, train_loss: 0.1639, step time: 0.9433\n",
      "246/388, train_loss: 0.1343, step time: 0.5355\n",
      "247/388, train_loss: 0.0808, step time: 0.5041\n",
      "248/388, train_loss: 0.1388, step time: 0.4941\n",
      "249/388, train_loss: 0.1116, step time: 0.4861\n",
      "250/388, train_loss: 0.0680, step time: 0.4957\n",
      "251/388, train_loss: 0.4345, step time: 0.4770\n",
      "252/388, train_loss: 0.0974, step time: 0.8880\n",
      "253/388, train_loss: 0.0662, step time: 0.5194\n",
      "254/388, train_loss: 0.2946, step time: 0.5085\n",
      "255/388, train_loss: 0.3244, step time: 0.4892\n",
      "256/388, train_loss: 0.2810, step time: 0.4910\n",
      "257/388, train_loss: 0.1232, step time: 0.4796\n",
      "258/388, train_loss: 0.2482, step time: 0.4906\n",
      "259/388, train_loss: 0.1643, step time: 1.1493\n",
      "260/388, train_loss: 0.0949, step time: 0.5301\n",
      "261/388, train_loss: 0.1099, step time: 0.5134\n",
      "262/388, train_loss: 0.2315, step time: 0.4862\n",
      "263/388, train_loss: 0.0789, step time: 0.4921\n",
      "264/388, train_loss: 0.5307, step time: 0.4818\n",
      "265/388, train_loss: 0.1529, step time: 0.4763\n",
      "266/388, train_loss: 0.1094, step time: 0.4760\n",
      "267/388, train_loss: 0.1978, step time: 0.8730\n",
      "268/388, train_loss: 0.0898, step time: 0.5457\n",
      "269/388, train_loss: 0.0941, step time: 0.5102\n",
      "270/388, train_loss: 0.0906, step time: 0.4956\n",
      "271/388, train_loss: 0.1769, step time: 0.4793\n",
      "272/388, train_loss: 0.0607, step time: 0.5169\n",
      "273/388, train_loss: 0.2759, step time: 0.4961\n",
      "274/388, train_loss: 0.0861, step time: 0.4970\n",
      "275/388, train_loss: 0.2811, step time: 0.4800\n",
      "276/388, train_loss: 0.1101, step time: 0.4742\n",
      "277/388, train_loss: 0.0665, step time: 0.4765\n",
      "278/388, train_loss: 0.4513, step time: 0.8990\n",
      "279/388, train_loss: 0.1081, step time: 0.5378\n",
      "280/388, train_loss: 0.0713, step time: 0.5182\n",
      "281/388, train_loss: 0.2314, step time: 0.4952\n",
      "282/388, train_loss: 0.2992, step time: 0.5472\n",
      "283/388, train_loss: 0.1025, step time: 0.5103\n",
      "284/388, train_loss: 0.0632, step time: 0.4988\n",
      "285/388, train_loss: 0.0527, step time: 0.4955\n",
      "286/388, train_loss: 0.0967, step time: 0.4827\n",
      "287/388, train_loss: 0.1031, step time: 0.4880\n",
      "288/388, train_loss: 0.1305, step time: 0.5165\n",
      "289/388, train_loss: 0.1543, step time: 0.5006\n",
      "290/388, train_loss: 0.0479, step time: 0.5039\n",
      "291/388, train_loss: 0.0469, step time: 0.5031\n",
      "292/388, train_loss: 0.1989, step time: 0.5222\n",
      "293/388, train_loss: 0.1071, step time: 0.5091\n",
      "294/388, train_loss: 0.0681, step time: 0.4973\n",
      "295/388, train_loss: 0.1650, step time: 0.4967\n",
      "296/388, train_loss: 0.1470, step time: 0.4754\n",
      "297/388, train_loss: 0.3090, step time: 0.4933\n",
      "298/388, train_loss: 0.0508, step time: 0.5017\n",
      "299/388, train_loss: 0.0811, step time: 0.5161\n",
      "300/388, train_loss: 0.2373, step time: 0.5034\n",
      "301/388, train_loss: 0.2380, step time: 0.4867\n",
      "302/388, train_loss: 0.1541, step time: 1.1907\n",
      "303/388, train_loss: 0.1927, step time: 0.5379\n",
      "304/388, train_loss: 0.1235, step time: 0.5090\n",
      "305/388, train_loss: 0.2995, step time: 0.4906\n",
      "306/388, train_loss: 0.1486, step time: 0.4964\n",
      "307/388, train_loss: 0.1884, step time: 0.4810\n",
      "308/388, train_loss: 0.4509, step time: 0.4751\n",
      "309/388, train_loss: 0.2682, step time: 0.7242\n",
      "310/388, train_loss: 0.2965, step time: 0.5638\n",
      "311/388, train_loss: 0.1949, step time: 0.5336\n",
      "312/388, train_loss: 0.2823, step time: 0.5005\n",
      "313/388, train_loss: 0.1909, step time: 0.4835\n",
      "314/388, train_loss: 0.2261, step time: 1.1580\n",
      "315/388, train_loss: 0.0750, step time: 0.5267\n",
      "316/388, train_loss: 0.1626, step time: 0.5028\n",
      "317/388, train_loss: 0.1459, step time: 0.4893\n",
      "318/388, train_loss: 0.1731, step time: 0.4872\n",
      "319/388, train_loss: 0.1108, step time: 0.4856\n",
      "320/388, train_loss: 0.1684, step time: 0.4774\n",
      "321/388, train_loss: 0.2175, step time: 0.4784\n",
      "322/388, train_loss: 0.1923, step time: 0.7232\n",
      "323/388, train_loss: 0.3480, step time: 0.5435\n",
      "324/388, train_loss: 0.2248, step time: 0.5227\n",
      "325/388, train_loss: 0.1807, step time: 0.5049\n",
      "326/388, train_loss: 0.5862, step time: 0.4922\n",
      "327/388, train_loss: 0.2161, step time: 0.4981\n",
      "328/388, train_loss: 0.0799, step time: 1.0815\n",
      "329/388, train_loss: 0.1535, step time: 0.5263\n",
      "330/388, train_loss: 0.1579, step time: 0.5096\n",
      "331/388, train_loss: 0.1014, step time: 0.5034\n",
      "332/388, train_loss: 0.2121, step time: 0.5357\n",
      "333/388, train_loss: 0.3032, step time: 0.5125\n",
      "334/388, train_loss: 0.2604, step time: 0.4901\n",
      "335/388, train_loss: 0.2814, step time: 0.4912\n",
      "336/388, train_loss: 0.1636, step time: 0.4937\n",
      "337/388, train_loss: 0.2529, step time: 0.4909\n",
      "338/388, train_loss: 0.1392, step time: 1.1219\n",
      "339/388, train_loss: 0.0922, step time: 0.5347\n",
      "340/388, train_loss: 0.4035, step time: 0.5052\n",
      "341/388, train_loss: 0.1164, step time: 0.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/388, train_loss: 0.2831, step time: 0.4899\n",
      "343/388, train_loss: 0.1682, step time: 0.4943\n",
      "344/388, train_loss: 0.0935, step time: 0.5149\n",
      "345/388, train_loss: 0.0946, step time: 0.4981\n",
      "346/388, train_loss: 0.1056, step time: 0.4845\n",
      "347/388, train_loss: 0.3019, step time: 0.5301\n",
      "348/388, train_loss: 0.2053, step time: 0.5103\n",
      "349/388, train_loss: 0.0879, step time: 0.5006\n",
      "350/388, train_loss: 0.1927, step time: 0.4845\n",
      "351/388, train_loss: 0.1451, step time: 0.4983\n",
      "352/388, train_loss: 0.1030, step time: 0.5117\n",
      "353/388, train_loss: 0.0598, step time: 0.5004\n",
      "354/388, train_loss: 0.0979, step time: 0.5061\n",
      "355/388, train_loss: 0.4078, step time: 0.5701\n",
      "356/388, train_loss: 0.0680, step time: 0.5409\n",
      "357/388, train_loss: 0.1194, step time: 0.5077\n",
      "358/388, train_loss: 0.2213, step time: 0.5039\n",
      "359/388, train_loss: 0.0463, step time: 1.2117\n",
      "360/388, train_loss: 0.3684, step time: 0.5330\n",
      "361/388, train_loss: 0.1928, step time: 0.5077\n",
      "362/388, train_loss: 0.3801, step time: 0.4894\n",
      "363/388, train_loss: 0.1310, step time: 0.4888\n",
      "364/388, train_loss: 0.0965, step time: 0.4923\n",
      "365/388, train_loss: 0.3414, step time: 1.1179\n",
      "366/388, train_loss: 0.1705, step time: 0.5404\n",
      "367/388, train_loss: 0.1469, step time: 0.4981\n",
      "368/388, train_loss: 0.0945, step time: 0.4906\n",
      "369/388, train_loss: 0.2486, step time: 0.4888\n",
      "370/388, train_loss: 0.0741, step time: 0.4760\n",
      "371/388, train_loss: 0.2160, step time: 0.4801\n",
      "372/388, train_loss: 0.0533, step time: 0.4816\n",
      "373/388, train_loss: 0.0889, step time: 0.5089\n",
      "374/388, train_loss: 0.2130, step time: 0.5045\n",
      "375/388, train_loss: 0.0846, step time: 0.4835\n",
      "376/388, train_loss: 0.1858, step time: 0.5231\n",
      "377/388, train_loss: 0.1015, step time: 0.5029\n",
      "378/388, train_loss: 0.2128, step time: 0.5442\n",
      "379/388, train_loss: 0.1391, step time: 0.6049\n",
      "380/388, train_loss: 0.1624, step time: 0.5461\n",
      "381/388, train_loss: 0.0811, step time: 0.5254\n",
      "382/388, train_loss: 0.1861, step time: 0.5170\n",
      "383/388, train_loss: 0.2573, step time: 0.5308\n",
      "384/388, train_loss: 0.1777, step time: 0.4984\n",
      "385/388, train_loss: 0.1039, step time: 0.4930\n",
      "386/388, train_loss: 0.1460, step time: 0.5711\n",
      "387/388, train_loss: 0.0565, step time: 0.7082\n",
      "388/388, train_loss: 0.1279, step time: 0.5383\n",
      "epoch 53 average loss: 0.1897\n",
      "current epoch: 53 current mean dice: 0.7445 tc: 0.7775 wt: 0.8996 et: 0.5565\n",
      "best mean dice: 0.7635 at epoch: 50\n",
      "time consuming of epoch 53 is: 301.7534\n",
      "----------\n",
      "epoch 54/300\n",
      "1/388, train_loss: 0.2044, step time: 0.4809\n",
      "2/388, train_loss: 0.2502, step time: 0.4954\n",
      "3/388, train_loss: 0.2524, step time: 0.5360\n",
      "4/388, train_loss: 0.0957, step time: 0.6326\n",
      "5/388, train_loss: 0.1951, step time: 0.5928\n",
      "6/388, train_loss: 0.1398, step time: 0.6995\n",
      "7/388, train_loss: 0.0787, step time: 0.5455\n",
      "8/388, train_loss: 0.0842, step time: 0.5289\n",
      "9/388, train_loss: 0.1923, step time: 0.6197\n",
      "10/388, train_loss: 0.2079, step time: 0.5420\n",
      "11/388, train_loss: 0.0874, step time: 0.5243\n",
      "12/388, train_loss: 0.1437, step time: 0.5133\n",
      "13/388, train_loss: 0.1188, step time: 0.5010\n",
      "14/388, train_loss: 0.2963, step time: 0.5275\n",
      "15/388, train_loss: 0.1546, step time: 0.6219\n",
      "16/388, train_loss: 0.0669, step time: 0.5510\n",
      "17/388, train_loss: 0.1632, step time: 0.5194\n",
      "18/388, train_loss: 0.1139, step time: 0.5327\n",
      "19/388, train_loss: 0.0968, step time: 0.6334\n",
      "20/388, train_loss: 0.2661, step time: 0.5784\n",
      "21/388, train_loss: 0.5964, step time: 0.5233\n",
      "22/388, train_loss: 0.0968, step time: 0.5114\n",
      "23/388, train_loss: 0.1967, step time: 0.5077\n",
      "24/388, train_loss: 0.1322, step time: 0.5157\n",
      "25/388, train_loss: 0.1089, step time: 0.6006\n",
      "26/388, train_loss: 0.2016, step time: 0.5755\n",
      "27/388, train_loss: 0.1382, step time: 0.5324\n",
      "28/388, train_loss: 0.1630, step time: 0.5065\n",
      "29/388, train_loss: 0.0726, step time: 1.0425\n",
      "30/388, train_loss: 0.1193, step time: 0.5568\n",
      "31/388, train_loss: 0.0959, step time: 0.5322\n",
      "32/388, train_loss: 0.2278, step time: 0.5213\n",
      "33/388, train_loss: 0.1323, step time: 0.5012\n",
      "34/388, train_loss: 0.3138, step time: 0.4810\n",
      "35/388, train_loss: 0.1014, step time: 0.5067\n",
      "36/388, train_loss: 0.3394, step time: 1.1075\n",
      "37/388, train_loss: 0.2164, step time: 0.5566\n",
      "38/388, train_loss: 0.1228, step time: 0.5184\n",
      "39/388, train_loss: 0.0861, step time: 0.5014\n",
      "40/388, train_loss: 0.1172, step time: 0.4977\n",
      "41/388, train_loss: 0.2035, step time: 0.4840\n",
      "42/388, train_loss: 0.1115, step time: 0.4912\n",
      "43/388, train_loss: 0.0743, step time: 0.4851\n",
      "44/388, train_loss: 0.0963, step time: 0.8679\n",
      "45/388, train_loss: 0.3941, step time: 0.5525\n",
      "46/388, train_loss: 0.1140, step time: 0.5228\n",
      "47/388, train_loss: 0.4504, step time: 0.4935\n",
      "48/388, train_loss: 0.1317, step time: 0.5089\n",
      "49/388, train_loss: 0.0811, step time: 0.4983\n",
      "50/388, train_loss: 0.2415, step time: 0.4938\n",
      "51/388, train_loss: 0.2344, step time: 0.9785\n",
      "52/388, train_loss: 0.4533, step time: 0.5388\n",
      "53/388, train_loss: 0.1501, step time: 0.5160\n",
      "54/388, train_loss: 0.1746, step time: 0.5073\n",
      "55/388, train_loss: 0.1517, step time: 0.4976\n",
      "56/388, train_loss: 0.1697, step time: 0.4824\n",
      "57/388, train_loss: 0.2703, step time: 0.5114\n",
      "58/388, train_loss: 0.0486, step time: 0.5473\n",
      "59/388, train_loss: 0.1943, step time: 0.5341\n",
      "60/388, train_loss: 0.2662, step time: 0.5399\n",
      "61/388, train_loss: 0.3460, step time: 0.5210\n",
      "62/388, train_loss: 0.2285, step time: 0.4927\n",
      "63/388, train_loss: 0.2066, step time: 0.4877\n",
      "64/388, train_loss: 0.0620, step time: 0.4985\n",
      "65/388, train_loss: 0.2445, step time: 0.4896\n",
      "66/388, train_loss: 0.4429, step time: 0.6500\n",
      "67/388, train_loss: 0.2263, step time: 0.5646\n",
      "68/388, train_loss: 0.2114, step time: 0.5456\n",
      "69/388, train_loss: 0.0977, step time: 0.5130\n",
      "70/388, train_loss: 0.4973, step time: 0.5167\n",
      "71/388, train_loss: 0.3608, step time: 0.4962\n",
      "72/388, train_loss: 0.2903, step time: 0.7533\n",
      "73/388, train_loss: 0.1604, step time: 0.5506\n",
      "74/388, train_loss: 0.3271, step time: 0.5134\n",
      "75/388, train_loss: 0.0945, step time: 0.5002\n",
      "76/388, train_loss: 0.3956, step time: 0.4857\n",
      "77/388, train_loss: 0.0556, step time: 0.4991\n",
      "78/388, train_loss: 0.2230, step time: 0.4935\n",
      "79/388, train_loss: 0.1038, step time: 0.4978\n",
      "80/388, train_loss: 0.1626, step time: 0.5341\n",
      "81/388, train_loss: 0.1524, step time: 0.5020\n",
      "82/388, train_loss: 0.2716, step time: 0.4813\n",
      "83/388, train_loss: 0.0607, step time: 0.4800\n",
      "84/388, train_loss: 0.1400, step time: 0.6248\n",
      "85/388, train_loss: 0.1216, step time: 0.5460\n",
      "86/388, train_loss: 0.2051, step time: 0.5145\n",
      "87/388, train_loss: 0.2470, step time: 0.4988\n",
      "88/388, train_loss: 0.2832, step time: 0.5583\n",
      "89/388, train_loss: 0.1509, step time: 0.6192\n",
      "90/388, train_loss: 0.1539, step time: 0.5501\n",
      "91/388, train_loss: 0.2913, step time: 0.5236\n",
      "92/388, train_loss: 0.4084, step time: 0.5124\n",
      "93/388, train_loss: 0.0518, step time: 0.5450\n",
      "94/388, train_loss: 0.2445, step time: 0.5306\n",
      "95/388, train_loss: 0.1028, step time: 0.5072\n",
      "96/388, train_loss: 0.1893, step time: 0.4932\n",
      "97/388, train_loss: 0.1413, step time: 0.4898\n",
      "98/388, train_loss: 0.2205, step time: 1.1147\n",
      "99/388, train_loss: 0.1500, step time: 0.5369\n",
      "100/388, train_loss: 0.1177, step time: 0.5061\n",
      "101/388, train_loss: 0.0968, step time: 0.4914\n",
      "102/388, train_loss: 0.0956, step time: 0.4968\n",
      "103/388, train_loss: 0.1593, step time: 0.4832\n",
      "104/388, train_loss: 0.1037, step time: 0.4807\n",
      "105/388, train_loss: 0.0968, step time: 0.4963\n",
      "106/388, train_loss: 0.2001, step time: 0.4817\n",
      "107/388, train_loss: 0.0754, step time: 0.5026\n",
      "108/388, train_loss: 0.0989, step time: 0.4889\n",
      "109/388, train_loss: 0.1270, step time: 0.4990\n",
      "110/388, train_loss: 0.2882, step time: 0.5122\n",
      "111/388, train_loss: 0.1632, step time: 0.5241\n",
      "112/388, train_loss: 0.0874, step time: 0.5097\n",
      "113/388, train_loss: 0.1265, step time: 0.4919\n",
      "114/388, train_loss: 0.3819, step time: 1.0176\n",
      "115/388, train_loss: 0.1917, step time: 0.5552\n",
      "116/388, train_loss: 0.1614, step time: 0.5312\n",
      "117/388, train_loss: 0.3058, step time: 0.5074\n",
      "118/388, train_loss: 0.2236, step time: 0.4915\n",
      "119/388, train_loss: 0.1647, step time: 0.5196\n",
      "120/388, train_loss: 0.2702, step time: 0.4914\n",
      "121/388, train_loss: 0.0950, step time: 0.4829\n",
      "122/388, train_loss: 0.0807, step time: 0.4913\n",
      "123/388, train_loss: 0.1882, step time: 0.8811\n",
      "124/388, train_loss: 0.1111, step time: 0.5533\n",
      "125/388, train_loss: 0.0992, step time: 0.5114\n",
      "126/388, train_loss: 0.2279, step time: 0.5024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/388, train_loss: 0.0898, step time: 0.4858\n",
      "128/388, train_loss: 0.1536, step time: 0.4905\n",
      "129/388, train_loss: 0.0917, step time: 1.1634\n",
      "130/388, train_loss: 0.1685, step time: 0.5408\n",
      "131/388, train_loss: 0.1130, step time: 0.5038\n",
      "132/388, train_loss: 0.3283, step time: 0.4891\n",
      "133/388, train_loss: 0.1575, step time: 0.4945\n",
      "134/388, train_loss: 0.1184, step time: 0.4775\n",
      "135/388, train_loss: 0.1724, step time: 0.5006\n",
      "136/388, train_loss: 0.0485, step time: 0.4911\n",
      "137/388, train_loss: 0.2568, step time: 0.5115\n",
      "138/388, train_loss: 0.1804, step time: 0.4985\n",
      "139/388, train_loss: 0.0924, step time: 0.4927\n",
      "140/388, train_loss: 0.3365, step time: 0.4903\n",
      "141/388, train_loss: 0.0955, step time: 0.5408\n",
      "142/388, train_loss: 0.2624, step time: 0.5200\n",
      "143/388, train_loss: 0.1172, step time: 0.5025\n",
      "144/388, train_loss: 0.1247, step time: 0.5153\n",
      "145/388, train_loss: 0.5179, step time: 0.5186\n",
      "146/388, train_loss: 0.1240, step time: 0.4924\n",
      "147/388, train_loss: 0.1321, step time: 0.4792\n",
      "148/388, train_loss: 0.3672, step time: 0.4889\n",
      "149/388, train_loss: 0.1431, step time: 0.7218\n",
      "150/388, train_loss: 0.3833, step time: 0.5839\n",
      "151/388, train_loss: 0.0538, step time: 0.5158\n",
      "152/388, train_loss: 0.3060, step time: 0.4990\n",
      "153/388, train_loss: 0.1244, step time: 0.4959\n",
      "154/388, train_loss: 0.2593, step time: 0.4831\n",
      "155/388, train_loss: 0.1044, step time: 0.7375\n",
      "156/388, train_loss: 0.1749, step time: 0.5463\n",
      "157/388, train_loss: 0.0580, step time: 0.5062\n",
      "158/388, train_loss: 0.1295, step time: 0.5029\n",
      "159/388, train_loss: 0.2216, step time: 0.4936\n",
      "160/388, train_loss: 0.2587, step time: 0.4893\n",
      "161/388, train_loss: 0.1977, step time: 0.5136\n",
      "162/388, train_loss: 0.1513, step time: 0.4992\n",
      "163/388, train_loss: 0.0982, step time: 0.5039\n",
      "164/388, train_loss: 0.3831, step time: 0.4876\n",
      "165/388, train_loss: 0.0601, step time: 0.5028\n",
      "166/388, train_loss: 0.1431, step time: 0.4945\n",
      "167/388, train_loss: 0.3284, step time: 0.5084\n",
      "168/388, train_loss: 0.5781, step time: 0.4911\n",
      "169/388, train_loss: 0.0500, step time: 0.4839\n",
      "170/388, train_loss: 0.3272, step time: 0.8059\n",
      "171/388, train_loss: 0.1174, step time: 0.5584\n",
      "172/388, train_loss: 0.1375, step time: 0.5207\n",
      "173/388, train_loss: 0.1453, step time: 0.5069\n",
      "174/388, train_loss: 0.0315, step time: 0.5391\n",
      "175/388, train_loss: 0.2294, step time: 0.5276\n",
      "176/388, train_loss: 0.2030, step time: 0.5398\n",
      "177/388, train_loss: 0.1825, step time: 0.5217\n",
      "178/388, train_loss: 0.1037, step time: 0.5070\n",
      "179/388, train_loss: 0.0982, step time: 0.4943\n",
      "180/388, train_loss: 0.1510, step time: 0.4942\n",
      "181/388, train_loss: 0.3279, step time: 0.4992\n",
      "182/388, train_loss: 0.1716, step time: 0.4806\n",
      "183/388, train_loss: 0.2930, step time: 0.6747\n",
      "184/388, train_loss: 0.0697, step time: 0.5420\n",
      "185/388, train_loss: 0.1518, step time: 0.5140\n",
      "186/388, train_loss: 0.0651, step time: 0.4861\n",
      "187/388, train_loss: 0.2260, step time: 0.4905\n",
      "188/388, train_loss: 0.1099, step time: 0.4867\n",
      "189/388, train_loss: 0.1361, step time: 0.4979\n",
      "190/388, train_loss: 0.1979, step time: 0.4804\n",
      "191/388, train_loss: 0.2752, step time: 0.4789\n",
      "192/388, train_loss: 0.2160, step time: 0.9466\n",
      "193/388, train_loss: 0.2552, step time: 0.5409\n",
      "194/388, train_loss: 0.4879, step time: 0.5114\n",
      "195/388, train_loss: 0.1695, step time: 0.5006\n",
      "196/388, train_loss: 0.2620, step time: 0.4856\n",
      "197/388, train_loss: 0.2207, step time: 0.4841\n",
      "198/388, train_loss: 0.2415, step time: 0.4863\n",
      "199/388, train_loss: 0.1055, step time: 0.4827\n",
      "200/388, train_loss: 0.1354, step time: 0.4816\n",
      "201/388, train_loss: 0.0799, step time: 0.4739\n",
      "202/388, train_loss: 0.1502, step time: 0.4833\n",
      "203/388, train_loss: 0.1036, step time: 0.9638\n",
      "204/388, train_loss: 0.1475, step time: 0.5363\n",
      "205/388, train_loss: 0.1393, step time: 0.5096\n",
      "206/388, train_loss: 0.1334, step time: 0.5134\n",
      "207/388, train_loss: 0.1140, step time: 0.4988\n",
      "208/388, train_loss: 0.2498, step time: 0.5017\n",
      "209/388, train_loss: 0.2414, step time: 0.4849\n",
      "210/388, train_loss: 0.0978, step time: 1.0225\n",
      "211/388, train_loss: 0.1102, step time: 0.5366\n",
      "212/388, train_loss: 0.0339, step time: 0.5143\n",
      "213/388, train_loss: 0.1092, step time: 0.4995\n",
      "214/388, train_loss: 0.2143, step time: 0.4999\n",
      "215/388, train_loss: 0.1153, step time: 0.4909\n",
      "216/388, train_loss: 0.1743, step time: 0.4969\n",
      "217/388, train_loss: 0.1793, step time: 0.4797\n",
      "218/388, train_loss: 0.0643, step time: 0.4952\n",
      "219/388, train_loss: 0.2484, step time: 0.4947\n",
      "220/388, train_loss: 0.2178, step time: 0.4951\n",
      "221/388, train_loss: 0.0628, step time: 0.5458\n",
      "222/388, train_loss: 0.0597, step time: 0.5260\n",
      "223/388, train_loss: 0.3194, step time: 0.5065\n",
      "224/388, train_loss: 0.0427, step time: 0.4999\n",
      "225/388, train_loss: 0.0739, step time: 0.4819\n",
      "226/388, train_loss: 0.1717, step time: 0.4829\n",
      "227/388, train_loss: 0.2119, step time: 0.4850\n",
      "228/388, train_loss: 0.4455, step time: 0.4873\n",
      "229/388, train_loss: 0.1283, step time: 0.5890\n",
      "230/388, train_loss: 0.0982, step time: 0.5475\n",
      "231/388, train_loss: 0.3798, step time: 0.5229\n",
      "232/388, train_loss: 0.2410, step time: 0.5129\n",
      "233/388, train_loss: 0.0911, step time: 0.5267\n",
      "234/388, train_loss: 0.1944, step time: 0.5212\n",
      "235/388, train_loss: 0.2672, step time: 0.5945\n",
      "236/388, train_loss: 0.3007, step time: 0.5372\n",
      "237/388, train_loss: 0.1602, step time: 0.5165\n",
      "238/388, train_loss: 0.2386, step time: 0.5001\n",
      "239/388, train_loss: 0.2133, step time: 0.4819\n",
      "240/388, train_loss: 0.3591, step time: 0.4756\n",
      "241/388, train_loss: 0.3419, step time: 0.9946\n",
      "242/388, train_loss: 0.1185, step time: 0.5368\n",
      "243/388, train_loss: 0.0817, step time: 0.5171\n",
      "244/388, train_loss: 0.2521, step time: 0.4982\n",
      "245/388, train_loss: 0.4415, step time: 0.5019\n",
      "246/388, train_loss: 0.1267, step time: 0.4902\n",
      "247/388, train_loss: 0.2152, step time: 0.4859\n",
      "248/388, train_loss: 0.1917, step time: 0.4784\n",
      "249/388, train_loss: 0.2187, step time: 0.5981\n",
      "250/388, train_loss: 0.0866, step time: 0.5316\n",
      "251/388, train_loss: 0.1121, step time: 0.5184\n",
      "252/388, train_loss: 0.2507, step time: 0.5045\n",
      "253/388, train_loss: 0.1075, step time: 0.4865\n",
      "254/388, train_loss: 0.2954, step time: 0.4927\n",
      "255/388, train_loss: 0.5068, step time: 0.4982\n",
      "256/388, train_loss: 0.2798, step time: 0.4842\n",
      "257/388, train_loss: 0.1747, step time: 1.1780\n",
      "258/388, train_loss: 0.2307, step time: 0.5399\n",
      "259/388, train_loss: 0.0633, step time: 0.5053\n",
      "260/388, train_loss: 0.1810, step time: 0.4857\n",
      "261/388, train_loss: 0.0440, step time: 0.4875\n",
      "262/388, train_loss: 0.0738, step time: 0.4918\n",
      "263/388, train_loss: 0.1910, step time: 0.4807\n",
      "264/388, train_loss: 0.1258, step time: 0.4787\n",
      "265/388, train_loss: 0.1095, step time: 0.4990\n",
      "266/388, train_loss: 0.1147, step time: 0.5472\n",
      "267/388, train_loss: 0.2082, step time: 0.5660\n",
      "268/388, train_loss: 0.1026, step time: 0.5148\n",
      "269/388, train_loss: 0.1785, step time: 0.4871\n",
      "270/388, train_loss: 0.0934, step time: 0.4790\n",
      "271/388, train_loss: 0.0966, step time: 0.4869\n",
      "272/388, train_loss: 0.1327, step time: 0.4703\n",
      "273/388, train_loss: 0.1156, step time: 1.0313\n",
      "274/388, train_loss: 0.2086, step time: 0.5366\n",
      "275/388, train_loss: 0.1878, step time: 0.5065\n",
      "276/388, train_loss: 0.2396, step time: 0.4897\n",
      "277/388, train_loss: 0.0951, step time: 0.4963\n",
      "278/388, train_loss: 0.0948, step time: 0.4810\n",
      "279/388, train_loss: 0.1627, step time: 0.4960\n",
      "280/388, train_loss: 0.0589, step time: 0.5166\n",
      "281/388, train_loss: 0.1848, step time: 0.4951\n",
      "282/388, train_loss: 0.3418, step time: 0.4993\n",
      "283/388, train_loss: 0.2112, step time: 0.4807\n",
      "284/388, train_loss: 0.4927, step time: 0.5455\n",
      "285/388, train_loss: 0.1349, step time: 0.5122\n",
      "286/388, train_loss: 0.0838, step time: 0.5066\n",
      "287/388, train_loss: 0.2781, step time: 0.5365\n",
      "288/388, train_loss: 0.1848, step time: 0.5097\n",
      "289/388, train_loss: 0.1425, step time: 0.4996\n",
      "290/388, train_loss: 0.3489, step time: 1.0038\n",
      "291/388, train_loss: 0.2292, step time: 0.5372\n",
      "292/388, train_loss: 0.0792, step time: 0.5096\n",
      "293/388, train_loss: 0.1744, step time: 0.4950\n",
      "294/388, train_loss: 0.1949, step time: 0.4930\n",
      "295/388, train_loss: 0.2040, step time: 0.4822\n",
      "296/388, train_loss: 0.2557, step time: 0.4927\n",
      "297/388, train_loss: 0.1729, step time: 0.4767\n",
      "298/388, train_loss: 0.1581, step time: 0.4764\n",
      "299/388, train_loss: 0.1685, step time: 0.5017\n",
      "300/388, train_loss: 0.2570, step time: 0.4846\n",
      "301/388, train_loss: 0.1006, step time: 0.4896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/388, train_loss: 0.1782, step time: 1.1035\n",
      "303/388, train_loss: 0.1935, step time: 0.5284\n",
      "304/388, train_loss: 0.2311, step time: 0.5038\n",
      "305/388, train_loss: 0.0956, step time: 0.4871\n",
      "306/388, train_loss: 0.1307, step time: 0.4931\n",
      "307/388, train_loss: 0.1214, step time: 0.4787\n",
      "308/388, train_loss: 0.1438, step time: 0.4745\n",
      "309/388, train_loss: 0.0759, step time: 0.4781\n",
      "310/388, train_loss: 0.1727, step time: 0.4747\n",
      "311/388, train_loss: 0.2179, step time: 0.9161\n",
      "312/388, train_loss: 0.2316, step time: 0.5339\n",
      "313/388, train_loss: 0.5935, step time: 0.5082\n",
      "314/388, train_loss: 0.2304, step time: 0.4989\n",
      "315/388, train_loss: 0.2244, step time: 0.4991\n",
      "316/388, train_loss: 0.1922, step time: 0.4904\n",
      "317/388, train_loss: 0.0335, step time: 0.5361\n",
      "318/388, train_loss: 0.3524, step time: 0.5102\n",
      "319/388, train_loss: 0.1612, step time: 0.4962\n",
      "320/388, train_loss: 0.1034, step time: 0.4871\n",
      "321/388, train_loss: 0.4125, step time: 0.4884\n",
      "322/388, train_loss: 0.1700, step time: 0.4901\n",
      "323/388, train_loss: 0.0846, step time: 0.4971\n",
      "324/388, train_loss: 0.1545, step time: 0.4798\n",
      "325/388, train_loss: 0.2093, step time: 1.0299\n",
      "326/388, train_loss: 0.2004, step time: 0.5417\n",
      "327/388, train_loss: 0.3079, step time: 0.5207\n",
      "328/388, train_loss: 0.3887, step time: 0.4984\n",
      "329/388, train_loss: 0.1762, step time: 0.5063\n",
      "330/388, train_loss: 0.2530, step time: 0.4889\n",
      "331/388, train_loss: 0.1182, step time: 0.5183\n",
      "332/388, train_loss: 0.1416, step time: 0.5000\n",
      "333/388, train_loss: 0.1731, step time: 0.4959\n",
      "334/388, train_loss: 0.3277, step time: 0.4909\n",
      "335/388, train_loss: 0.2919, step time: 0.4823\n",
      "336/388, train_loss: 0.1942, step time: 0.4883\n",
      "337/388, train_loss: 0.1512, step time: 0.4852\n",
      "338/388, train_loss: 0.1310, step time: 0.8393\n",
      "339/388, train_loss: 0.1874, step time: 0.5440\n",
      "340/388, train_loss: 0.3277, step time: 0.5113\n",
      "341/388, train_loss: 0.0874, step time: 0.4916\n",
      "342/388, train_loss: 0.2631, step time: 0.5281\n",
      "343/388, train_loss: 0.2378, step time: 0.5201\n",
      "344/388, train_loss: 0.1889, step time: 0.5031\n",
      "345/388, train_loss: 0.1214, step time: 0.4886\n",
      "346/388, train_loss: 0.2768, step time: 0.5052\n",
      "347/388, train_loss: 0.2281, step time: 0.4806\n",
      "348/388, train_loss: 0.0784, step time: 0.4792\n",
      "349/388, train_loss: 0.2537, step time: 0.4758\n",
      "350/388, train_loss: 0.2298, step time: 0.4752\n",
      "351/388, train_loss: 0.3837, step time: 0.8200\n",
      "352/388, train_loss: 0.3238, step time: 0.5538\n",
      "353/388, train_loss: 0.1091, step time: 0.5186\n",
      "354/388, train_loss: 0.1422, step time: 0.5012\n",
      "355/388, train_loss: 0.1511, step time: 0.4941\n",
      "356/388, train_loss: 0.1352, step time: 0.4779\n",
      "357/388, train_loss: 0.1221, step time: 0.4786\n",
      "358/388, train_loss: 0.2333, step time: 0.4852\n",
      "359/388, train_loss: 0.1371, step time: 0.4852\n",
      "360/388, train_loss: 0.0939, step time: 0.4777\n",
      "361/388, train_loss: 0.2888, step time: 0.4996\n",
      "362/388, train_loss: 0.2959, step time: 0.5122\n",
      "363/388, train_loss: 0.1018, step time: 0.5200\n",
      "364/388, train_loss: 0.0982, step time: 0.4939\n",
      "365/388, train_loss: 0.0666, step time: 0.4984\n",
      "366/388, train_loss: 0.1318, step time: 0.4822\n",
      "367/388, train_loss: 0.0965, step time: 0.5024\n",
      "368/388, train_loss: 0.5580, step time: 0.4932\n",
      "369/388, train_loss: 0.1244, step time: 0.5009\n",
      "370/388, train_loss: 0.0800, step time: 0.5665\n",
      "371/388, train_loss: 0.2913, step time: 0.5399\n",
      "372/388, train_loss: 0.0697, step time: 0.5015\n",
      "373/388, train_loss: 0.2098, step time: 1.1525\n",
      "374/388, train_loss: 0.5299, step time: 0.5305\n",
      "375/388, train_loss: 0.1308, step time: 0.5153\n",
      "376/388, train_loss: 0.0912, step time: 0.4882\n",
      "377/388, train_loss: 0.0986, step time: 0.4952\n",
      "378/388, train_loss: 0.0481, step time: 0.4792\n",
      "379/388, train_loss: 0.1299, step time: 0.4914\n",
      "380/388, train_loss: 0.2460, step time: 0.4935\n",
      "381/388, train_loss: 0.1452, step time: 0.4815\n",
      "382/388, train_loss: 0.1755, step time: 0.4762\n",
      "383/388, train_loss: 0.0922, step time: 0.4785\n",
      "384/388, train_loss: 0.2851, step time: 0.4664\n",
      "385/388, train_loss: 0.2722, step time: 0.5435\n",
      "386/388, train_loss: 0.5167, step time: 0.7012\n",
      "387/388, train_loss: 0.0860, step time: 0.5857\n",
      "388/388, train_loss: 0.2093, step time: 0.5243\n",
      "epoch 54 average loss: 0.1887\n",
      "saved new best metric model\n",
      "current epoch: 54 current mean dice: 0.7673 tc: 0.8160 wt: 0.8995 et: 0.5864\n",
      "best mean dice: 0.7673 at epoch: 54\n",
      "time consuming of epoch 54 is: 299.4925\n",
      "----------\n",
      "epoch 55/300\n",
      "1/388, train_loss: 0.0676, step time: 0.4659\n",
      "2/388, train_loss: 0.1170, step time: 0.4883\n",
      "3/388, train_loss: 0.3488, step time: 0.6029\n",
      "4/388, train_loss: 0.1663, step time: 0.5451\n",
      "5/388, train_loss: 0.1444, step time: 0.4988\n",
      "6/388, train_loss: 0.1465, step time: 0.5497\n",
      "7/388, train_loss: 0.0837, step time: 0.6955\n",
      "8/388, train_loss: 0.0877, step time: 0.5911\n",
      "9/388, train_loss: 0.1180, step time: 0.5273\n",
      "10/388, train_loss: 0.2390, step time: 0.5498\n",
      "11/388, train_loss: 0.2225, step time: 0.5452\n",
      "12/388, train_loss: 0.1038, step time: 0.5273\n",
      "13/388, train_loss: 0.1384, step time: 0.5361\n",
      "14/388, train_loss: 0.2200, step time: 0.5312\n",
      "15/388, train_loss: 0.2508, step time: 0.4860\n",
      "16/388, train_loss: 0.3063, step time: 0.5144\n",
      "17/388, train_loss: 0.1341, step time: 0.5305\n",
      "18/388, train_loss: 0.2108, step time: 0.6483\n",
      "19/388, train_loss: 0.1229, step time: 0.5737\n",
      "20/388, train_loss: 0.0687, step time: 0.5312\n",
      "21/388, train_loss: 0.1631, step time: 0.5058\n",
      "22/388, train_loss: 0.1007, step time: 0.5300\n",
      "23/388, train_loss: 0.5006, step time: 0.7105\n",
      "24/388, train_loss: 0.0922, step time: 0.5751\n",
      "25/388, train_loss: 0.1581, step time: 0.5365\n",
      "26/388, train_loss: 0.1603, step time: 0.5025\n",
      "27/388, train_loss: 0.1326, step time: 0.9573\n",
      "28/388, train_loss: 0.0877, step time: 0.5463\n",
      "29/388, train_loss: 0.2286, step time: 0.5165\n",
      "30/388, train_loss: 0.1721, step time: 0.4905\n",
      "31/388, train_loss: 0.2298, step time: 0.5208\n",
      "32/388, train_loss: 0.2247, step time: 0.5082\n",
      "33/388, train_loss: 0.1405, step time: 0.5238\n",
      "34/388, train_loss: 0.2162, step time: 0.5114\n",
      "35/388, train_loss: 0.2040, step time: 0.5090\n",
      "36/388, train_loss: 0.1052, step time: 0.8884\n",
      "37/388, train_loss: 0.0587, step time: 0.5474\n",
      "38/388, train_loss: 0.1639, step time: 0.5213\n",
      "39/388, train_loss: 0.1055, step time: 0.5018\n",
      "40/388, train_loss: 0.1582, step time: 0.4843\n",
      "41/388, train_loss: 0.0341, step time: 0.5944\n",
      "42/388, train_loss: 0.0835, step time: 0.5471\n",
      "43/388, train_loss: 0.0640, step time: 0.5249\n",
      "44/388, train_loss: 0.2555, step time: 0.5101\n",
      "45/388, train_loss: 0.1208, step time: 0.4991\n",
      "46/388, train_loss: 0.1510, step time: 0.4999\n",
      "47/388, train_loss: 0.2323, step time: 1.1647\n",
      "48/388, train_loss: 0.1031, step time: 0.5608\n",
      "49/388, train_loss: 0.1844, step time: 0.5183\n",
      "50/388, train_loss: 0.1042, step time: 0.5164\n",
      "51/388, train_loss: 0.3060, step time: 0.5619\n",
      "52/388, train_loss: 0.1305, step time: 0.5387\n",
      "53/388, train_loss: 0.4734, step time: 0.5149\n",
      "54/388, train_loss: 0.1952, step time: 0.5353\n",
      "55/388, train_loss: 0.0670, step time: 0.5847\n",
      "56/388, train_loss: 0.2392, step time: 0.5781\n",
      "57/388, train_loss: 0.1715, step time: 0.7577\n",
      "58/388, train_loss: 0.1913, step time: 0.5802\n",
      "59/388, train_loss: 0.1887, step time: 0.5126\n",
      "60/388, train_loss: 0.1719, step time: 0.4818\n",
      "61/388, train_loss: 0.1220, step time: 0.4892\n",
      "62/388, train_loss: 0.1401, step time: 0.5143\n",
      "63/388, train_loss: 0.1902, step time: 0.4943\n",
      "64/388, train_loss: 0.4853, step time: 0.5026\n",
      "65/388, train_loss: 0.3097, step time: 0.8561\n",
      "66/388, train_loss: 0.1239, step time: 0.5670\n",
      "67/388, train_loss: 0.1482, step time: 0.5121\n",
      "68/388, train_loss: 0.5075, step time: 0.4893\n",
      "69/388, train_loss: 0.1134, step time: 0.5074\n",
      "70/388, train_loss: 0.0793, step time: 0.5535\n",
      "71/388, train_loss: 0.2577, step time: 0.5327\n",
      "72/388, train_loss: 0.1022, step time: 0.5109\n",
      "73/388, train_loss: 0.1727, step time: 0.5552\n",
      "74/388, train_loss: 0.2333, step time: 0.5368\n",
      "75/388, train_loss: 0.2510, step time: 0.5156\n",
      "76/388, train_loss: 0.5409, step time: 0.5877\n",
      "77/388, train_loss: 0.0533, step time: 0.5362\n",
      "78/388, train_loss: 0.2214, step time: 0.5190\n",
      "79/388, train_loss: 0.1417, step time: 0.5036\n",
      "80/388, train_loss: 0.1093, step time: 0.4970\n",
      "81/388, train_loss: 0.2917, step time: 0.4789\n",
      "82/388, train_loss: 0.0945, step time: 0.6037\n",
      "83/388, train_loss: 0.1189, step time: 0.5671\n",
      "84/388, train_loss: 0.2740, step time: 0.5212\n",
      "85/388, train_loss: 0.1110, step time: 0.4872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/388, train_loss: 0.2382, step time: 0.4989\n",
      "87/388, train_loss: 0.1320, step time: 0.4899\n",
      "88/388, train_loss: 0.1754, step time: 0.5141\n",
      "89/388, train_loss: 0.2689, step time: 0.6206\n",
      "90/388, train_loss: 0.0976, step time: 0.5520\n",
      "91/388, train_loss: 0.1239, step time: 0.4982\n",
      "92/388, train_loss: 0.2678, step time: 0.6855\n",
      "93/388, train_loss: 0.0569, step time: 0.5448\n",
      "94/388, train_loss: 0.3832, step time: 0.5186\n",
      "95/388, train_loss: 0.1719, step time: 0.5128\n",
      "96/388, train_loss: 0.4902, step time: 1.1407\n",
      "97/388, train_loss: 0.1802, step time: 0.5501\n",
      "98/388, train_loss: 0.5698, step time: 0.5173\n",
      "99/388, train_loss: 0.0601, step time: 0.5358\n",
      "100/388, train_loss: 0.1363, step time: 0.5139\n",
      "101/388, train_loss: 0.1459, step time: 0.4940\n",
      "102/388, train_loss: 0.6318, step time: 0.4894\n",
      "103/388, train_loss: 0.2465, step time: 1.1470\n",
      "104/388, train_loss: 0.0668, step time: 0.5338\n",
      "105/388, train_loss: 0.3232, step time: 0.5054\n",
      "106/388, train_loss: 0.1731, step time: 0.4814\n",
      "107/388, train_loss: 0.1116, step time: 0.4779\n",
      "108/388, train_loss: 0.3100, step time: 1.1870\n",
      "109/388, train_loss: 0.4347, step time: 0.5288\n",
      "110/388, train_loss: 0.2629, step time: 0.5054\n",
      "111/388, train_loss: 0.1302, step time: 0.4974\n",
      "112/388, train_loss: 0.0931, step time: 0.4945\n",
      "113/388, train_loss: 0.3223, step time: 0.4880\n",
      "114/388, train_loss: 0.3471, step time: 1.0591\n",
      "115/388, train_loss: 0.1977, step time: 0.5387\n",
      "116/388, train_loss: 0.1035, step time: 0.5064\n",
      "117/388, train_loss: 0.3734, step time: 0.4942\n",
      "118/388, train_loss: 0.0751, step time: 1.1367\n",
      "119/388, train_loss: 0.3782, step time: 0.5240\n",
      "120/388, train_loss: 0.2103, step time: 0.4991\n",
      "121/388, train_loss: 0.0982, step time: 0.4906\n",
      "122/388, train_loss: 0.1188, step time: 0.4937\n",
      "123/388, train_loss: 0.1293, step time: 0.4771\n",
      "124/388, train_loss: 0.1760, step time: 0.5046\n",
      "125/388, train_loss: 0.1444, step time: 0.4935\n",
      "126/388, train_loss: 0.1800, step time: 0.4939\n",
      "127/388, train_loss: 0.1697, step time: 0.4797\n",
      "128/388, train_loss: 0.1787, step time: 0.5465\n",
      "129/388, train_loss: 0.1079, step time: 0.5110\n",
      "130/388, train_loss: 0.2131, step time: 0.5029\n",
      "131/388, train_loss: 0.1428, step time: 0.4823\n",
      "132/388, train_loss: 0.0916, step time: 0.4806\n",
      "133/388, train_loss: 0.2161, step time: 1.1988\n",
      "134/388, train_loss: 0.0737, step time: 0.5377\n",
      "135/388, train_loss: 0.1303, step time: 0.5095\n",
      "136/388, train_loss: 0.1373, step time: 0.4954\n",
      "137/388, train_loss: 0.2792, step time: 0.4912\n",
      "138/388, train_loss: 0.2473, step time: 0.4940\n",
      "139/388, train_loss: 0.1159, step time: 0.4828\n",
      "140/388, train_loss: 0.1308, step time: 0.4988\n",
      "141/388, train_loss: 0.2283, step time: 0.4917\n",
      "142/388, train_loss: 0.2435, step time: 0.4980\n",
      "143/388, train_loss: 0.2820, step time: 0.6903\n",
      "144/388, train_loss: 0.1175, step time: 0.5557\n",
      "145/388, train_loss: 0.3437, step time: 0.5274\n",
      "146/388, train_loss: 0.3139, step time: 0.5114\n",
      "147/388, train_loss: 0.1249, step time: 0.5450\n",
      "148/388, train_loss: 0.3064, step time: 0.5280\n",
      "149/388, train_loss: 0.0505, step time: 0.5615\n",
      "150/388, train_loss: 0.2239, step time: 0.5214\n",
      "151/388, train_loss: 0.3729, step time: 0.6130\n",
      "152/388, train_loss: 0.3088, step time: 0.5595\n",
      "153/388, train_loss: 0.2730, step time: 0.5240\n",
      "154/388, train_loss: 0.0495, step time: 0.5070\n",
      "155/388, train_loss: 0.2457, step time: 0.4917\n",
      "156/388, train_loss: 0.6307, step time: 0.4936\n",
      "157/388, train_loss: 0.3823, step time: 0.4867\n",
      "158/388, train_loss: 0.2122, step time: 1.1572\n",
      "159/388, train_loss: 0.2841, step time: 0.5489\n",
      "160/388, train_loss: 0.2655, step time: 0.5219\n",
      "161/388, train_loss: 0.2778, step time: 0.4986\n",
      "162/388, train_loss: 0.2144, step time: 0.4929\n",
      "163/388, train_loss: 0.1148, step time: 0.4803\n",
      "164/388, train_loss: 0.1015, step time: 0.4784\n",
      "165/388, train_loss: 0.2377, step time: 0.4856\n",
      "166/388, train_loss: 0.0942, step time: 0.4863\n",
      "167/388, train_loss: 0.0812, step time: 0.4792\n",
      "168/388, train_loss: 0.1678, step time: 0.4818\n",
      "169/388, train_loss: 0.4043, step time: 1.1585\n",
      "170/388, train_loss: 0.3342, step time: 0.5448\n",
      "171/388, train_loss: 0.1918, step time: 0.5203\n",
      "172/388, train_loss: 0.1603, step time: 0.5071\n",
      "173/388, train_loss: 0.1180, step time: 0.5152\n",
      "174/388, train_loss: 0.2835, step time: 0.5029\n",
      "175/388, train_loss: 0.2406, step time: 0.7349\n",
      "176/388, train_loss: 0.2731, step time: 0.5280\n",
      "177/388, train_loss: 0.1077, step time: 0.5042\n",
      "178/388, train_loss: 0.0969, step time: 0.5166\n",
      "179/388, train_loss: 0.1135, step time: 0.5125\n",
      "180/388, train_loss: 0.0580, step time: 0.4958\n",
      "181/388, train_loss: 0.0771, step time: 0.4951\n",
      "182/388, train_loss: 0.0858, step time: 0.4790\n",
      "183/388, train_loss: 0.1100, step time: 0.4778\n",
      "184/388, train_loss: 0.3061, step time: 1.1668\n",
      "185/388, train_loss: 0.0867, step time: 0.5426\n",
      "186/388, train_loss: 0.3133, step time: 0.5156\n",
      "187/388, train_loss: 0.0978, step time: 0.4940\n",
      "188/388, train_loss: 0.0938, step time: 0.4979\n",
      "189/388, train_loss: 0.1392, step time: 0.4809\n",
      "190/388, train_loss: 0.1149, step time: 0.4934\n",
      "191/388, train_loss: 0.1010, step time: 0.4947\n",
      "192/388, train_loss: 0.0907, step time: 0.4945\n",
      "193/388, train_loss: 0.2498, step time: 0.5268\n",
      "194/388, train_loss: 0.0565, step time: 0.5274\n",
      "195/388, train_loss: 0.3388, step time: 0.5071\n",
      "196/388, train_loss: 0.2556, step time: 0.4983\n",
      "197/388, train_loss: 0.1453, step time: 0.4909\n",
      "198/388, train_loss: 0.0525, step time: 0.7308\n",
      "199/388, train_loss: 0.3274, step time: 0.5434\n",
      "200/388, train_loss: 0.1395, step time: 0.5234\n",
      "201/388, train_loss: 0.1890, step time: 0.5080\n",
      "202/388, train_loss: 0.1324, step time: 0.5019\n",
      "203/388, train_loss: 0.0891, step time: 0.4802\n",
      "204/388, train_loss: 0.1410, step time: 0.4947\n",
      "205/388, train_loss: 0.0986, step time: 0.4743\n",
      "206/388, train_loss: 0.0765, step time: 0.9705\n",
      "207/388, train_loss: 0.2302, step time: 0.5392\n",
      "208/388, train_loss: 0.1348, step time: 0.5177\n",
      "209/388, train_loss: 0.1211, step time: 0.5023\n",
      "210/388, train_loss: 0.4813, step time: 0.5161\n",
      "211/388, train_loss: 0.1096, step time: 0.4982\n",
      "212/388, train_loss: 0.1467, step time: 0.4893\n",
      "213/388, train_loss: 0.1662, step time: 0.5212\n",
      "214/388, train_loss: 0.0963, step time: 0.5301\n",
      "215/388, train_loss: 0.4164, step time: 0.5221\n",
      "216/388, train_loss: 0.1801, step time: 0.5092\n",
      "217/388, train_loss: 0.2031, step time: 0.5034\n",
      "218/388, train_loss: 0.0960, step time: 0.4933\n",
      "219/388, train_loss: 0.2108, step time: 0.5244\n",
      "220/388, train_loss: 0.1591, step time: 0.5040\n",
      "221/388, train_loss: 0.0889, step time: 0.4894\n",
      "222/388, train_loss: 0.1960, step time: 0.4810\n",
      "223/388, train_loss: 0.4790, step time: 0.5112\n",
      "224/388, train_loss: 0.3758, step time: 0.5427\n",
      "225/388, train_loss: 0.3186, step time: 0.5309\n",
      "226/388, train_loss: 0.2056, step time: 0.5472\n",
      "227/388, train_loss: 0.2387, step time: 0.5250\n",
      "228/388, train_loss: 0.2513, step time: 0.4991\n",
      "229/388, train_loss: 0.4418, step time: 0.5033\n",
      "230/388, train_loss: 0.2531, step time: 0.4858\n",
      "231/388, train_loss: 0.2139, step time: 0.4786\n",
      "232/388, train_loss: 0.2034, step time: 0.5077\n",
      "233/388, train_loss: 0.1901, step time: 0.4890\n",
      "234/388, train_loss: 0.4908, step time: 0.4897\n",
      "235/388, train_loss: 0.1872, step time: 0.7368\n",
      "236/388, train_loss: 0.1697, step time: 0.5510\n",
      "237/388, train_loss: 0.0700, step time: 0.5281\n",
      "238/388, train_loss: 0.3135, step time: 0.5005\n",
      "239/388, train_loss: 0.2043, step time: 0.5689\n",
      "240/388, train_loss: 0.1542, step time: 0.5471\n",
      "241/388, train_loss: 0.0891, step time: 0.5242\n",
      "242/388, train_loss: 0.1061, step time: 0.5189\n",
      "243/388, train_loss: 0.2073, step time: 0.4893\n",
      "244/388, train_loss: 0.3162, step time: 0.4800\n",
      "245/388, train_loss: 0.3718, step time: 1.2247\n",
      "246/388, train_loss: 0.2297, step time: 0.5348\n",
      "247/388, train_loss: 0.0628, step time: 0.4938\n",
      "248/388, train_loss: 0.1743, step time: 0.4926\n",
      "249/388, train_loss: 0.1909, step time: 0.4927\n",
      "250/388, train_loss: 0.2229, step time: 0.4845\n",
      "251/388, train_loss: 0.2353, step time: 0.4896\n",
      "252/388, train_loss: 0.2038, step time: 0.4786\n",
      "253/388, train_loss: 0.1496, step time: 0.4782\n",
      "254/388, train_loss: 0.1515, step time: 0.4924\n",
      "255/388, train_loss: 0.1795, step time: 0.4796\n",
      "256/388, train_loss: 0.0970, step time: 1.0068\n",
      "257/388, train_loss: 0.1861, step time: 0.5544\n",
      "258/388, train_loss: 0.0754, step time: 0.5292\n",
      "259/388, train_loss: 0.1408, step time: 0.5086\n",
      "260/388, train_loss: 0.1072, step time: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/388, train_loss: 0.1505, step time: 0.4908\n",
      "262/388, train_loss: 0.1599, step time: 0.4968\n",
      "263/388, train_loss: 0.2133, step time: 0.5104\n",
      "264/388, train_loss: 0.2745, step time: 0.5039\n",
      "265/388, train_loss: 0.1595, step time: 1.0139\n",
      "266/388, train_loss: 0.2488, step time: 0.5400\n",
      "267/388, train_loss: 0.1012, step time: 0.5114\n",
      "268/388, train_loss: 0.0367, step time: 0.4928\n",
      "269/388, train_loss: 0.2483, step time: 0.4981\n",
      "270/388, train_loss: 0.4669, step time: 0.4843\n",
      "271/388, train_loss: 0.0747, step time: 0.4936\n",
      "272/388, train_loss: 0.3386, step time: 0.5167\n",
      "273/388, train_loss: 0.1090, step time: 0.5025\n",
      "274/388, train_loss: 0.2495, step time: 0.4885\n",
      "275/388, train_loss: 0.2761, step time: 0.5049\n",
      "276/388, train_loss: 0.0519, step time: 0.5250\n",
      "277/388, train_loss: 0.1923, step time: 0.5201\n",
      "278/388, train_loss: 0.1569, step time: 0.4969\n",
      "279/388, train_loss: 0.0678, step time: 0.4936\n",
      "280/388, train_loss: 0.3738, step time: 1.1783\n",
      "281/388, train_loss: 0.2065, step time: 0.5266\n",
      "282/388, train_loss: 0.2245, step time: 0.4985\n",
      "283/388, train_loss: 0.1248, step time: 0.5021\n",
      "284/388, train_loss: 0.1253, step time: 0.4887\n",
      "285/388, train_loss: 0.0830, step time: 0.4944\n",
      "286/388, train_loss: 0.0928, step time: 0.4818\n",
      "287/388, train_loss: 0.3490, step time: 0.4794\n",
      "288/388, train_loss: 0.0884, step time: 0.4788\n",
      "289/388, train_loss: 0.1643, step time: 0.8363\n",
      "290/388, train_loss: 0.2923, step time: 0.5391\n",
      "291/388, train_loss: 0.1509, step time: 0.5086\n",
      "292/388, train_loss: 0.1850, step time: 0.4905\n",
      "293/388, train_loss: 0.1574, step time: 0.5063\n",
      "294/388, train_loss: 0.1205, step time: 0.4883\n",
      "295/388, train_loss: 0.2409, step time: 0.4909\n",
      "296/388, train_loss: 0.1791, step time: 0.4810\n",
      "297/388, train_loss: 0.2335, step time: 0.4888\n",
      "298/388, train_loss: 0.1219, step time: 0.4894\n",
      "299/388, train_loss: 0.2345, step time: 0.5167\n",
      "300/388, train_loss: 0.3816, step time: 0.5022\n",
      "301/388, train_loss: 0.3286, step time: 0.5169\n",
      "302/388, train_loss: 0.2491, step time: 0.5061\n",
      "303/388, train_loss: 0.2300, step time: 0.4841\n",
      "304/388, train_loss: 0.1837, step time: 0.4867\n",
      "305/388, train_loss: 0.1483, step time: 0.4763\n",
      "306/388, train_loss: 0.0819, step time: 0.4866\n",
      "307/388, train_loss: 0.2789, step time: 0.4924\n",
      "308/388, train_loss: 0.1567, step time: 0.4924\n",
      "309/388, train_loss: 0.3596, step time: 0.4976\n",
      "310/388, train_loss: 0.2129, step time: 0.4801\n",
      "311/388, train_loss: 0.4659, step time: 0.9955\n",
      "312/388, train_loss: 0.2085, step time: 0.5589\n",
      "313/388, train_loss: 0.1907, step time: 0.5094\n",
      "314/388, train_loss: 0.1578, step time: 0.4838\n",
      "315/388, train_loss: 0.4485, step time: 0.4885\n",
      "316/388, train_loss: 0.2635, step time: 0.5032\n",
      "317/388, train_loss: 0.2583, step time: 0.9313\n",
      "318/388, train_loss: 0.2146, step time: 0.5649\n",
      "319/388, train_loss: 0.2396, step time: 0.5206\n",
      "320/388, train_loss: 0.1864, step time: 0.5110\n",
      "321/388, train_loss: 0.1004, step time: 0.4929\n",
      "322/388, train_loss: 0.2940, step time: 0.4868\n",
      "323/388, train_loss: 0.3281, step time: 0.4912\n",
      "324/388, train_loss: 0.0698, step time: 0.4806\n",
      "325/388, train_loss: 0.1183, step time: 0.5061\n",
      "326/388, train_loss: 0.1060, step time: 0.4905\n",
      "327/388, train_loss: 0.1970, step time: 0.5188\n",
      "328/388, train_loss: 0.1608, step time: 0.5386\n",
      "329/388, train_loss: 0.1618, step time: 0.5184\n",
      "330/388, train_loss: 0.1232, step time: 0.5005\n",
      "331/388, train_loss: 0.2537, step time: 0.4990\n",
      "332/388, train_loss: 0.1429, step time: 0.4813\n",
      "333/388, train_loss: 0.2492, step time: 0.5097\n",
      "334/388, train_loss: 0.0746, step time: 0.4932\n",
      "335/388, train_loss: 0.1163, step time: 0.5303\n",
      "336/388, train_loss: 0.1412, step time: 0.5042\n",
      "337/388, train_loss: 0.2586, step time: 0.5021\n",
      "338/388, train_loss: 0.3131, step time: 0.4893\n",
      "339/388, train_loss: 0.0875, step time: 0.5149\n",
      "340/388, train_loss: 0.1162, step time: 0.5102\n",
      "341/388, train_loss: 0.1871, step time: 0.4969\n",
      "342/388, train_loss: 0.1564, step time: 0.4987\n",
      "343/388, train_loss: 0.3608, step time: 0.4812\n",
      "344/388, train_loss: 0.1528, step time: 1.1181\n",
      "345/388, train_loss: 0.4419, step time: 0.5336\n",
      "346/388, train_loss: 0.0397, step time: 0.5070\n",
      "347/388, train_loss: 0.2326, step time: 0.5002\n",
      "348/388, train_loss: 0.1441, step time: 0.4880\n",
      "349/388, train_loss: 0.0852, step time: 0.4839\n",
      "350/388, train_loss: 0.2139, step time: 0.4845\n",
      "351/388, train_loss: 0.1314, step time: 0.6278\n",
      "352/388, train_loss: 0.1015, step time: 0.5304\n",
      "353/388, train_loss: 0.3007, step time: 0.5027\n",
      "354/388, train_loss: 0.1639, step time: 0.4803\n",
      "355/388, train_loss: 0.0704, step time: 0.4831\n",
      "356/388, train_loss: 0.1650, step time: 0.4747\n",
      "357/388, train_loss: 0.0646, step time: 0.4892\n",
      "358/388, train_loss: 0.1201, step time: 0.4926\n",
      "359/388, train_loss: 0.0839, step time: 0.4832\n",
      "360/388, train_loss: 0.1437, step time: 0.4863\n",
      "361/388, train_loss: 0.2743, step time: 0.4940\n",
      "362/388, train_loss: 0.2063, step time: 0.5085\n",
      "363/388, train_loss: 0.1134, step time: 0.4983\n",
      "364/388, train_loss: 0.1412, step time: 0.5035\n",
      "365/388, train_loss: 0.0886, step time: 0.5016\n",
      "366/388, train_loss: 0.0926, step time: 0.5686\n",
      "367/388, train_loss: 0.2126, step time: 0.5320\n",
      "368/388, train_loss: 0.2747, step time: 0.4950\n",
      "369/388, train_loss: 0.0915, step time: 0.5003\n",
      "370/388, train_loss: 0.2014, step time: 0.5047\n",
      "371/388, train_loss: 0.1014, step time: 0.5008\n",
      "372/388, train_loss: 0.1669, step time: 0.4886\n",
      "373/388, train_loss: 0.2232, step time: 0.4976\n",
      "374/388, train_loss: 0.1795, step time: 0.4860\n",
      "375/388, train_loss: 0.1123, step time: 0.7198\n",
      "376/388, train_loss: 0.0687, step time: 0.5656\n",
      "377/388, train_loss: 0.1140, step time: 0.5278\n",
      "378/388, train_loss: 0.0668, step time: 0.5019\n",
      "379/388, train_loss: 0.1685, step time: 0.4980\n",
      "380/388, train_loss: 0.2582, step time: 0.4996\n",
      "381/388, train_loss: 0.3941, step time: 0.5064\n",
      "382/388, train_loss: 0.0711, step time: 0.4934\n",
      "383/388, train_loss: 0.0928, step time: 0.4764\n",
      "384/388, train_loss: 0.4016, step time: 0.5187\n",
      "385/388, train_loss: 0.0957, step time: 0.4959\n",
      "386/388, train_loss: 0.1117, step time: 0.5557\n",
      "387/388, train_loss: 0.1914, step time: 0.5185\n",
      "388/388, train_loss: 0.0979, step time: 0.5092\n",
      "epoch 55 average loss: 0.1928\n",
      "current epoch: 55 current mean dice: 0.7659 tc: 0.8117 wt: 0.8990 et: 0.5869\n",
      "best mean dice: 0.7673 at epoch: 54\n",
      "time consuming of epoch 55 is: 301.4426\n",
      "----------\n",
      "epoch 56/300\n",
      "1/388, train_loss: 0.0993, step time: 0.4813\n",
      "2/388, train_loss: 0.2561, step time: 0.4837\n",
      "3/388, train_loss: 0.1144, step time: 0.6589\n",
      "4/388, train_loss: 0.0963, step time: 0.5672\n",
      "5/388, train_loss: 0.1442, step time: 0.5302\n",
      "6/388, train_loss: 0.1808, step time: 0.5719\n",
      "7/388, train_loss: 0.1018, step time: 0.5395\n",
      "8/388, train_loss: 0.2084, step time: 0.4993\n",
      "9/388, train_loss: 0.1031, step time: 0.7753\n",
      "10/388, train_loss: 0.2324, step time: 0.5697\n",
      "11/388, train_loss: 0.2111, step time: 0.6181\n",
      "12/388, train_loss: 0.0373, step time: 0.5477\n",
      "13/388, train_loss: 0.1023, step time: 0.5148\n",
      "14/388, train_loss: 0.0927, step time: 0.4991\n",
      "15/388, train_loss: 0.1173, step time: 0.4947\n",
      "16/388, train_loss: 0.1932, step time: 0.7651\n",
      "17/388, train_loss: 0.2210, step time: 0.5567\n",
      "18/388, train_loss: 0.2662, step time: 0.5258\n",
      "19/388, train_loss: 0.0818, step time: 0.5035\n",
      "20/388, train_loss: 0.1666, step time: 0.5133\n",
      "21/388, train_loss: 0.1308, step time: 0.4989\n",
      "22/388, train_loss: 0.3189, step time: 0.5001\n",
      "23/388, train_loss: 0.0554, step time: 0.9607\n",
      "24/388, train_loss: 0.2094, step time: 0.5723\n",
      "25/388, train_loss: 0.1570, step time: 0.5376\n",
      "26/388, train_loss: 0.2101, step time: 0.5079\n",
      "27/388, train_loss: 0.1386, step time: 0.5088\n",
      "28/388, train_loss: 0.0427, step time: 0.4959\n",
      "29/388, train_loss: 0.2974, step time: 0.5231\n",
      "30/388, train_loss: 0.2668, step time: 0.4967\n",
      "31/388, train_loss: 0.1238, step time: 0.4952\n",
      "32/388, train_loss: 0.1265, step time: 0.4894\n",
      "33/388, train_loss: 0.1478, step time: 0.4887\n",
      "34/388, train_loss: 0.0930, step time: 0.8229\n",
      "35/388, train_loss: 0.1388, step time: 0.5489\n",
      "36/388, train_loss: 0.1428, step time: 0.5165\n",
      "37/388, train_loss: 0.1940, step time: 0.4883\n",
      "38/388, train_loss: 0.1845, step time: 0.4839\n",
      "39/388, train_loss: 0.0776, step time: 0.5141\n",
      "40/388, train_loss: 0.2654, step time: 0.4892\n",
      "41/388, train_loss: 0.3017, step time: 0.5028\n",
      "42/388, train_loss: 0.0808, step time: 0.4966\n",
      "43/388, train_loss: 0.3172, step time: 0.8223\n",
      "44/388, train_loss: 0.3461, step time: 0.5642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/388, train_loss: 0.2135, step time: 0.5175\n",
      "46/388, train_loss: 0.0870, step time: 0.5075\n",
      "47/388, train_loss: 0.1436, step time: 0.4974\n",
      "48/388, train_loss: 0.1082, step time: 0.5154\n",
      "49/388, train_loss: 0.1061, step time: 0.5180\n",
      "50/388, train_loss: 0.1140, step time: 0.5106\n",
      "51/388, train_loss: 0.0784, step time: 0.4894\n",
      "52/388, train_loss: 0.3011, step time: 0.5804\n",
      "53/388, train_loss: 0.2384, step time: 0.5809\n",
      "54/388, train_loss: 0.2324, step time: 0.5301\n",
      "55/388, train_loss: 0.1220, step time: 0.5019\n",
      "56/388, train_loss: 0.1074, step time: 0.5254\n",
      "57/388, train_loss: 0.2466, step time: 0.5166\n",
      "58/388, train_loss: 0.2552, step time: 0.5118\n",
      "59/388, train_loss: 0.1779, step time: 0.4978\n",
      "60/388, train_loss: 0.3089, step time: 0.4981\n",
      "61/388, train_loss: 0.2110, step time: 0.4768\n",
      "62/388, train_loss: 0.2084, step time: 1.1796\n",
      "63/388, train_loss: 0.2375, step time: 0.5452\n",
      "64/388, train_loss: 0.1101, step time: 0.5128\n",
      "65/388, train_loss: 0.3910, step time: 0.5000\n",
      "66/388, train_loss: 0.1887, step time: 0.5006\n",
      "67/388, train_loss: 0.2320, step time: 0.4867\n",
      "68/388, train_loss: 0.3549, step time: 0.4985\n",
      "69/388, train_loss: 0.4635, step time: 0.5119\n",
      "70/388, train_loss: 0.2535, step time: 0.4996\n",
      "71/388, train_loss: 0.0938, step time: 0.5078\n",
      "72/388, train_loss: 0.1135, step time: 0.5814\n",
      "73/388, train_loss: 0.1307, step time: 0.5497\n",
      "74/388, train_loss: 0.0925, step time: 0.5136\n",
      "75/388, train_loss: 0.0504, step time: 0.5064\n",
      "76/388, train_loss: 0.0976, step time: 0.5067\n",
      "77/388, train_loss: 0.0996, step time: 0.4948\n",
      "78/388, train_loss: 0.1122, step time: 0.4909\n",
      "79/388, train_loss: 0.2175, step time: 0.4861\n",
      "80/388, train_loss: 0.0976, step time: 1.0942\n",
      "81/388, train_loss: 0.1951, step time: 0.5219\n",
      "82/388, train_loss: 0.2750, step time: 0.4992\n",
      "83/388, train_loss: 0.1140, step time: 0.4815\n",
      "84/388, train_loss: 0.5551, step time: 0.4960\n",
      "85/388, train_loss: 0.1661, step time: 0.4956\n",
      "86/388, train_loss: 0.1166, step time: 0.5439\n",
      "87/388, train_loss: 0.3242, step time: 0.5246\n",
      "88/388, train_loss: 0.1130, step time: 0.5117\n",
      "89/388, train_loss: 0.2075, step time: 0.4900\n",
      "90/388, train_loss: 0.2671, step time: 0.5222\n",
      "91/388, train_loss: 0.2234, step time: 0.5849\n",
      "92/388, train_loss: 0.2724, step time: 0.5499\n",
      "93/388, train_loss: 0.3758, step time: 0.5231\n",
      "94/388, train_loss: 0.1421, step time: 0.5062\n",
      "95/388, train_loss: 0.1100, step time: 0.4962\n",
      "96/388, train_loss: 0.1047, step time: 0.4895\n",
      "97/388, train_loss: 0.4467, step time: 0.4923\n",
      "98/388, train_loss: 0.1547, step time: 0.4935\n",
      "99/388, train_loss: 0.1490, step time: 0.5058\n",
      "100/388, train_loss: 0.1312, step time: 0.5279\n",
      "101/388, train_loss: 0.1562, step time: 0.4897\n",
      "102/388, train_loss: 0.0999, step time: 0.6935\n",
      "103/388, train_loss: 0.1725, step time: 0.5792\n",
      "104/388, train_loss: 0.0686, step time: 0.5235\n",
      "105/388, train_loss: 0.2252, step time: 0.5060\n",
      "106/388, train_loss: 0.4051, step time: 0.5069\n",
      "107/388, train_loss: 0.0588, step time: 0.5630\n",
      "108/388, train_loss: 0.1936, step time: 0.5315\n",
      "109/388, train_loss: 0.2221, step time: 0.4914\n",
      "110/388, train_loss: 0.0871, step time: 0.5022\n",
      "111/388, train_loss: 0.0755, step time: 0.5130\n",
      "112/388, train_loss: 0.2867, step time: 0.5204\n",
      "113/388, train_loss: 0.2433, step time: 0.5108\n",
      "114/388, train_loss: 0.1956, step time: 0.4932\n",
      "115/388, train_loss: 0.2033, step time: 0.5044\n",
      "116/388, train_loss: 0.1244, step time: 0.5059\n",
      "117/388, train_loss: 0.1995, step time: 0.4939\n",
      "118/388, train_loss: 0.2502, step time: 0.5343\n",
      "119/388, train_loss: 0.1944, step time: 0.5124\n",
      "120/388, train_loss: 0.2055, step time: 0.4903\n",
      "121/388, train_loss: 0.3180, step time: 0.4922\n",
      "122/388, train_loss: 0.1247, step time: 0.7290\n",
      "123/388, train_loss: 0.1798, step time: 0.5540\n",
      "124/388, train_loss: 0.1938, step time: 0.5089\n",
      "125/388, train_loss: 0.1386, step time: 0.4823\n",
      "126/388, train_loss: 0.2404, step time: 1.1117\n",
      "127/388, train_loss: 0.2670, step time: 0.5462\n",
      "128/388, train_loss: 0.1002, step time: 0.5116\n",
      "129/388, train_loss: 0.1422, step time: 0.4996\n",
      "130/388, train_loss: 0.0876, step time: 0.4840\n",
      "131/388, train_loss: 0.2079, step time: 0.4977\n",
      "132/388, train_loss: 0.0737, step time: 0.5332\n",
      "133/388, train_loss: 0.1897, step time: 0.5421\n",
      "134/388, train_loss: 0.0514, step time: 0.5356\n",
      "135/388, train_loss: 0.3325, step time: 0.5116\n",
      "136/388, train_loss: 0.3169, step time: 0.5281\n",
      "137/388, train_loss: 0.2320, step time: 0.5703\n",
      "138/388, train_loss: 0.1897, step time: 0.5397\n",
      "139/388, train_loss: 0.0999, step time: 0.5188\n",
      "140/388, train_loss: 0.2620, step time: 0.5074\n",
      "141/388, train_loss: 0.1088, step time: 0.5373\n",
      "142/388, train_loss: 0.2008, step time: 0.5322\n",
      "143/388, train_loss: 0.0925, step time: 0.6207\n",
      "144/388, train_loss: 0.1529, step time: 0.5633\n",
      "145/388, train_loss: 0.5283, step time: 0.6535\n",
      "146/388, train_loss: 0.1578, step time: 0.5565\n",
      "147/388, train_loss: 0.0958, step time: 0.5148\n",
      "148/388, train_loss: 0.2474, step time: 0.5048\n",
      "149/388, train_loss: 0.0746, step time: 0.4886\n",
      "150/388, train_loss: 0.1378, step time: 0.4914\n",
      "151/388, train_loss: 0.2707, step time: 0.4777\n",
      "152/388, train_loss: 0.1889, step time: 0.4830\n",
      "153/388, train_loss: 0.1467, step time: 0.6786\n",
      "154/388, train_loss: 0.0828, step time: 0.5387\n",
      "155/388, train_loss: 0.1232, step time: 0.5122\n",
      "156/388, train_loss: 0.1538, step time: 0.5025\n",
      "157/388, train_loss: 0.2097, step time: 0.5003\n",
      "158/388, train_loss: 0.1186, step time: 0.4961\n",
      "159/388, train_loss: 0.3705, step time: 1.1302\n",
      "160/388, train_loss: 0.4292, step time: 0.5664\n",
      "161/388, train_loss: 0.2860, step time: 0.5231\n",
      "162/388, train_loss: 0.2238, step time: 0.5110\n",
      "163/388, train_loss: 0.0896, step time: 0.4936\n",
      "164/388, train_loss: 0.2186, step time: 0.4935\n",
      "165/388, train_loss: 0.0388, step time: 1.1237\n",
      "166/388, train_loss: 0.1484, step time: 0.5324\n",
      "167/388, train_loss: 0.2043, step time: 0.5168\n",
      "168/388, train_loss: 0.4822, step time: 0.4989\n",
      "169/388, train_loss: 0.0788, step time: 0.4832\n",
      "170/388, train_loss: 0.1025, step time: 0.4889\n",
      "171/388, train_loss: 0.1355, step time: 0.4964\n",
      "172/388, train_loss: 0.1523, step time: 0.4970\n",
      "173/388, train_loss: 0.1387, step time: 0.4917\n",
      "174/388, train_loss: 0.0721, step time: 0.4811\n",
      "175/388, train_loss: 0.3126, step time: 0.4866\n",
      "176/388, train_loss: 0.0868, step time: 0.5080\n",
      "177/388, train_loss: 0.1019, step time: 0.5280\n",
      "178/388, train_loss: 0.1917, step time: 0.5056\n",
      "179/388, train_loss: 0.1355, step time: 0.5084\n",
      "180/388, train_loss: 0.1316, step time: 0.5018\n",
      "181/388, train_loss: 0.2414, step time: 0.5236\n",
      "182/388, train_loss: 0.1804, step time: 0.4952\n",
      "183/388, train_loss: 0.4370, step time: 0.5477\n",
      "184/388, train_loss: 0.1809, step time: 0.6540\n",
      "185/388, train_loss: 0.1773, step time: 0.5586\n",
      "186/388, train_loss: 0.1735, step time: 0.5236\n",
      "187/388, train_loss: 0.1892, step time: 0.5077\n",
      "188/388, train_loss: 0.2647, step time: 0.5026\n",
      "189/388, train_loss: 0.1388, step time: 0.4899\n",
      "190/388, train_loss: 0.1854, step time: 1.0427\n",
      "191/388, train_loss: 0.1552, step time: 0.5473\n",
      "192/388, train_loss: 0.1102, step time: 0.5202\n",
      "193/388, train_loss: 0.1315, step time: 0.6047\n",
      "194/388, train_loss: 0.1844, step time: 0.5358\n",
      "195/388, train_loss: 0.2580, step time: 0.5180\n",
      "196/388, train_loss: 0.5553, step time: 0.4919\n",
      "197/388, train_loss: 0.2460, step time: 0.5211\n",
      "198/388, train_loss: 0.1265, step time: 0.6179\n",
      "199/388, train_loss: 0.0650, step time: 0.5462\n",
      "200/388, train_loss: 0.1513, step time: 0.5192\n",
      "201/388, train_loss: 0.2711, step time: 0.5106\n",
      "202/388, train_loss: 0.1633, step time: 0.4983\n",
      "203/388, train_loss: 0.0748, step time: 0.5157\n",
      "204/388, train_loss: 0.0958, step time: 0.5096\n",
      "205/388, train_loss: 0.1333, step time: 0.5973\n",
      "206/388, train_loss: 0.0691, step time: 0.5492\n",
      "207/388, train_loss: 0.0425, step time: 0.5273\n",
      "208/388, train_loss: 0.4634, step time: 0.5127\n",
      "209/388, train_loss: 0.5338, step time: 0.5698\n",
      "210/388, train_loss: 0.1486, step time: 0.6606\n",
      "211/388, train_loss: 0.2043, step time: 0.5516\n",
      "212/388, train_loss: 0.1526, step time: 0.5306\n",
      "213/388, train_loss: 0.0790, step time: 0.5068\n",
      "214/388, train_loss: 0.1330, step time: 0.4926\n",
      "215/388, train_loss: 0.2864, step time: 0.5458\n",
      "216/388, train_loss: 0.1372, step time: 0.5297\n",
      "217/388, train_loss: 0.2818, step time: 0.5369\n",
      "218/388, train_loss: 0.1337, step time: 0.5103\n",
      "219/388, train_loss: 0.1041, step time: 0.4957\n",
      "220/388, train_loss: 0.1278, step time: 0.4901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/388, train_loss: 0.0507, step time: 0.4964\n",
      "222/388, train_loss: 0.0940, step time: 0.5380\n",
      "223/388, train_loss: 0.2710, step time: 0.5224\n",
      "224/388, train_loss: 0.2248, step time: 0.4891\n",
      "225/388, train_loss: 0.1383, step time: 1.0101\n",
      "226/388, train_loss: 0.2010, step time: 0.5376\n",
      "227/388, train_loss: 0.1549, step time: 0.5161\n",
      "228/388, train_loss: 0.5013, step time: 0.4958\n",
      "229/388, train_loss: 0.1051, step time: 0.5000\n",
      "230/388, train_loss: 0.1824, step time: 0.5013\n",
      "231/388, train_loss: 0.2841, step time: 0.5016\n",
      "232/388, train_loss: 0.1847, step time: 0.9067\n",
      "233/388, train_loss: 0.0658, step time: 0.5306\n",
      "234/388, train_loss: 0.4575, step time: 0.5009\n",
      "235/388, train_loss: 0.3803, step time: 0.4940\n",
      "236/388, train_loss: 0.0833, step time: 0.5097\n",
      "237/388, train_loss: 0.1557, step time: 0.5851\n",
      "238/388, train_loss: 0.1556, step time: 0.5540\n",
      "239/388, train_loss: 0.1707, step time: 0.5147\n",
      "240/388, train_loss: 0.1486, step time: 0.4996\n",
      "241/388, train_loss: 0.1137, step time: 0.5537\n",
      "242/388, train_loss: 0.1409, step time: 0.5528\n",
      "243/388, train_loss: 0.3514, step time: 0.5229\n",
      "244/388, train_loss: 0.3450, step time: 0.5184\n",
      "245/388, train_loss: 0.2837, step time: 0.4963\n",
      "246/388, train_loss: 0.2962, step time: 0.4983\n",
      "247/388, train_loss: 0.3571, step time: 1.2170\n",
      "248/388, train_loss: 0.0482, step time: 0.5138\n",
      "249/388, train_loss: 0.3824, step time: 0.4941\n",
      "250/388, train_loss: 0.3308, step time: 0.4967\n",
      "251/388, train_loss: 0.1016, step time: 0.4835\n",
      "252/388, train_loss: 0.0811, step time: 0.4870\n",
      "253/388, train_loss: 0.2316, step time: 0.4944\n",
      "254/388, train_loss: 0.2883, step time: 0.4880\n",
      "255/388, train_loss: 0.0887, step time: 1.1701\n",
      "256/388, train_loss: 0.1944, step time: 0.5295\n",
      "257/388, train_loss: 0.2187, step time: 0.5033\n",
      "258/388, train_loss: 0.1390, step time: 0.4830\n",
      "259/388, train_loss: 0.1930, step time: 0.4802\n",
      "260/388, train_loss: 0.1070, step time: 0.4939\n",
      "261/388, train_loss: 0.1957, step time: 0.4797\n",
      "262/388, train_loss: 0.0890, step time: 0.4751\n",
      "263/388, train_loss: 0.1037, step time: 0.6202\n",
      "264/388, train_loss: 0.1075, step time: 0.5388\n",
      "265/388, train_loss: 0.1837, step time: 0.4994\n",
      "266/388, train_loss: 0.1030, step time: 0.4870\n",
      "267/388, train_loss: 0.2523, step time: 0.4860\n",
      "268/388, train_loss: 0.3823, step time: 0.4929\n",
      "269/388, train_loss: 0.0365, step time: 0.5985\n",
      "270/388, train_loss: 0.1946, step time: 0.5402\n",
      "271/388, train_loss: 0.1108, step time: 0.4995\n",
      "272/388, train_loss: 0.1344, step time: 0.4851\n",
      "273/388, train_loss: 0.1151, step time: 0.4961\n",
      "274/388, train_loss: 0.1296, step time: 0.4848\n",
      "275/388, train_loss: 0.2782, step time: 0.5312\n",
      "276/388, train_loss: 0.1076, step time: 0.5681\n",
      "277/388, train_loss: 0.1763, step time: 0.5389\n",
      "278/388, train_loss: 0.0811, step time: 0.5146\n",
      "279/388, train_loss: 0.0909, step time: 0.5044\n",
      "280/388, train_loss: 0.3933, step time: 0.5633\n",
      "281/388, train_loss: 0.0999, step time: 0.5331\n",
      "282/388, train_loss: 0.2267, step time: 0.5090\n",
      "283/388, train_loss: 0.0645, step time: 0.5087\n",
      "284/388, train_loss: 0.2908, step time: 0.5326\n",
      "285/388, train_loss: 0.0636, step time: 0.5410\n",
      "286/388, train_loss: 0.1809, step time: 0.5205\n",
      "287/388, train_loss: 0.2913, step time: 0.5175\n",
      "288/388, train_loss: 0.0825, step time: 0.5084\n",
      "289/388, train_loss: 0.0737, step time: 0.5114\n",
      "290/388, train_loss: 0.1969, step time: 0.5014\n",
      "291/388, train_loss: 0.1176, step time: 0.4857\n",
      "292/388, train_loss: 0.2164, step time: 1.0398\n",
      "293/388, train_loss: 0.0813, step time: 0.5366\n",
      "294/388, train_loss: 0.0459, step time: 0.5111\n",
      "295/388, train_loss: 0.1252, step time: 0.4898\n",
      "296/388, train_loss: 0.1841, step time: 0.4803\n",
      "297/388, train_loss: 0.2433, step time: 0.5159\n",
      "298/388, train_loss: 0.0864, step time: 0.5004\n",
      "299/388, train_loss: 0.1124, step time: 0.5011\n",
      "300/388, train_loss: 0.2098, step time: 0.4818\n",
      "301/388, train_loss: 0.1779, step time: 0.5092\n",
      "302/388, train_loss: 0.1368, step time: 0.5180\n",
      "303/388, train_loss: 0.5470, step time: 0.5047\n",
      "304/388, train_loss: 0.1659, step time: 0.5149\n",
      "305/388, train_loss: 0.0838, step time: 0.5024\n",
      "306/388, train_loss: 0.0424, step time: 0.4804\n",
      "307/388, train_loss: 0.2403, step time: 0.6912\n",
      "308/388, train_loss: 0.0806, step time: 0.5816\n",
      "309/388, train_loss: 0.1089, step time: 0.5280\n",
      "310/388, train_loss: 0.0451, step time: 0.5011\n",
      "311/388, train_loss: 0.1200, step time: 0.5046\n",
      "312/388, train_loss: 0.1494, step time: 0.5109\n",
      "313/388, train_loss: 0.1788, step time: 0.4910\n",
      "314/388, train_loss: 0.1270, step time: 0.4967\n",
      "315/388, train_loss: 0.3861, step time: 1.1511\n",
      "316/388, train_loss: 0.5238, step time: 0.5668\n",
      "317/388, train_loss: 0.0891, step time: 0.5223\n",
      "318/388, train_loss: 0.1211, step time: 0.5101\n",
      "319/388, train_loss: 0.0742, step time: 0.4946\n",
      "320/388, train_loss: 0.6027, step time: 0.4950\n",
      "321/388, train_loss: 0.2622, step time: 0.4971\n",
      "322/388, train_loss: 0.0794, step time: 1.1578\n",
      "323/388, train_loss: 0.4102, step time: 0.5460\n",
      "324/388, train_loss: 0.2267, step time: 0.5059\n",
      "325/388, train_loss: 0.0888, step time: 0.4975\n",
      "326/388, train_loss: 0.1918, step time: 0.4833\n",
      "327/388, train_loss: 0.1628, step time: 0.4792\n",
      "328/388, train_loss: 0.3594, step time: 0.4879\n",
      "329/388, train_loss: 0.1100, step time: 1.2482\n",
      "330/388, train_loss: 0.0833, step time: 0.5357\n",
      "331/388, train_loss: 0.3414, step time: 0.5088\n",
      "332/388, train_loss: 0.2314, step time: 0.4991\n",
      "333/388, train_loss: 0.0682, step time: 0.4886\n",
      "334/388, train_loss: 0.2103, step time: 0.4875\n",
      "335/388, train_loss: 0.2228, step time: 0.4899\n",
      "336/388, train_loss: 0.1832, step time: 1.1476\n",
      "337/388, train_loss: 0.1766, step time: 0.5383\n",
      "338/388, train_loss: 0.1352, step time: 0.5125\n",
      "339/388, train_loss: 0.0879, step time: 0.4983\n",
      "340/388, train_loss: 0.1783, step time: 0.4851\n",
      "341/388, train_loss: 0.1625, step time: 0.4945\n",
      "342/388, train_loss: 0.2713, step time: 1.1287\n",
      "343/388, train_loss: 0.0597, step time: 0.5251\n",
      "344/388, train_loss: 0.3478, step time: 0.5064\n",
      "345/388, train_loss: 0.0703, step time: 0.4817\n",
      "346/388, train_loss: 0.0960, step time: 0.4948\n",
      "347/388, train_loss: 0.2725, step time: 0.4821\n",
      "348/388, train_loss: 0.2729, step time: 1.1150\n",
      "349/388, train_loss: 0.1749, step time: 0.5302\n",
      "350/388, train_loss: 0.0840, step time: 0.4998\n",
      "351/388, train_loss: 0.2309, step time: 0.4891\n",
      "352/388, train_loss: 0.0957, step time: 0.4893\n",
      "353/388, train_loss: 0.1111, step time: 0.4732\n",
      "354/388, train_loss: 0.1765, step time: 0.4728\n",
      "355/388, train_loss: 0.1127, step time: 0.4747\n",
      "356/388, train_loss: 0.0866, step time: 0.4713\n",
      "357/388, train_loss: 0.1061, step time: 0.4778\n",
      "358/388, train_loss: 0.1394, step time: 0.4854\n",
      "359/388, train_loss: 0.2428, step time: 0.4921\n",
      "360/388, train_loss: 0.2046, step time: 0.5099\n",
      "361/388, train_loss: 0.1399, step time: 0.5847\n",
      "362/388, train_loss: 0.4195, step time: 0.5552\n",
      "363/388, train_loss: 0.2035, step time: 0.5169\n",
      "364/388, train_loss: 0.1830, step time: 0.4979\n",
      "365/388, train_loss: 0.2659, step time: 0.5236\n",
      "366/388, train_loss: 0.3030, step time: 0.5181\n",
      "367/388, train_loss: 0.1667, step time: 0.5594\n",
      "368/388, train_loss: 0.1578, step time: 0.5349\n",
      "369/388, train_loss: 0.7439, step time: 0.5024\n",
      "370/388, train_loss: 0.1906, step time: 0.4988\n",
      "371/388, train_loss: 0.0710, step time: 0.4836\n",
      "372/388, train_loss: 0.3156, step time: 0.4791\n",
      "373/388, train_loss: 0.2674, step time: 0.4957\n",
      "374/388, train_loss: 0.1747, step time: 0.4882\n",
      "375/388, train_loss: 0.3138, step time: 1.2205\n",
      "376/388, train_loss: 0.0848, step time: 0.5448\n",
      "377/388, train_loss: 0.3806, step time: 0.5053\n",
      "378/388, train_loss: 0.1207, step time: 0.5022\n",
      "379/388, train_loss: 0.3028, step time: 0.4897\n",
      "380/388, train_loss: 0.1707, step time: 0.4975\n",
      "381/388, train_loss: 0.1235, step time: 0.4860\n",
      "382/388, train_loss: 0.2130, step time: 0.4885\n",
      "383/388, train_loss: 0.1400, step time: 0.4784\n",
      "384/388, train_loss: 0.2918, step time: 0.9943\n",
      "385/388, train_loss: 0.1672, step time: 0.5412\n",
      "386/388, train_loss: 0.1373, step time: 0.4985\n",
      "387/388, train_loss: 0.2045, step time: 0.4968\n",
      "388/388, train_loss: 0.0530, step time: 0.4869\n",
      "epoch 56 average loss: 0.1878\n",
      "current epoch: 56 current mean dice: 0.7470 tc: 0.7954 wt: 0.8775 et: 0.5682\n",
      "best mean dice: 0.7673 at epoch: 54\n",
      "time consuming of epoch 56 is: 302.4454\n",
      "----------\n",
      "epoch 57/300\n",
      "1/388, train_loss: 0.1281, step time: 0.4746\n",
      "2/388, train_loss: 0.2273, step time: 0.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/388, train_loss: 0.2099, step time: 1.1713\n",
      "4/388, train_loss: 0.4691, step time: 0.5559\n",
      "5/388, train_loss: 0.1122, step time: 0.5327\n",
      "6/388, train_loss: 0.0882, step time: 0.5241\n",
      "7/388, train_loss: 0.1760, step time: 0.5109\n",
      "8/388, train_loss: 0.5591, step time: 0.5035\n",
      "9/388, train_loss: 0.2059, step time: 0.4940\n",
      "10/388, train_loss: 0.1806, step time: 1.0486\n",
      "11/388, train_loss: 0.1815, step time: 0.5521\n",
      "12/388, train_loss: 0.1570, step time: 0.5288\n",
      "13/388, train_loss: 0.1045, step time: 0.5044\n",
      "14/388, train_loss: 0.2410, step time: 0.4922\n",
      "15/388, train_loss: 0.1997, step time: 0.4878\n",
      "16/388, train_loss: 0.0697, step time: 0.7825\n",
      "17/388, train_loss: 0.1049, step time: 0.5513\n",
      "18/388, train_loss: 0.2040, step time: 0.5239\n",
      "19/388, train_loss: 0.1008, step time: 0.5063\n",
      "20/388, train_loss: 0.1535, step time: 0.4984\n",
      "21/388, train_loss: 0.2409, step time: 0.4986\n",
      "22/388, train_loss: 0.2896, step time: 0.5886\n",
      "23/388, train_loss: 0.0652, step time: 0.5302\n",
      "24/388, train_loss: 0.2894, step time: 0.5104\n",
      "25/388, train_loss: 0.0978, step time: 0.5239\n",
      "26/388, train_loss: 0.1141, step time: 0.5204\n",
      "27/388, train_loss: 0.1196, step time: 0.5623\n",
      "28/388, train_loss: 0.1093, step time: 0.5209\n",
      "29/388, train_loss: 0.0746, step time: 0.5128\n",
      "30/388, train_loss: 0.1618, step time: 0.5109\n",
      "31/388, train_loss: 0.2308, step time: 0.5406\n",
      "32/388, train_loss: 0.1575, step time: 0.5330\n",
      "33/388, train_loss: 0.1620, step time: 0.5569\n",
      "34/388, train_loss: 0.3777, step time: 0.5281\n",
      "35/388, train_loss: 0.2971, step time: 0.5110\n",
      "36/388, train_loss: 0.1523, step time: 0.5027\n",
      "37/388, train_loss: 0.0647, step time: 0.5074\n",
      "38/388, train_loss: 0.2992, step time: 0.4942\n",
      "39/388, train_loss: 0.3655, step time: 0.5276\n",
      "40/388, train_loss: 0.0616, step time: 0.5145\n",
      "41/388, train_loss: 0.1037, step time: 0.5026\n",
      "42/388, train_loss: 0.1509, step time: 0.4959\n",
      "43/388, train_loss: 0.1126, step time: 0.4883\n",
      "44/388, train_loss: 0.0304, step time: 0.4843\n",
      "45/388, train_loss: 0.3596, step time: 0.4978\n",
      "46/388, train_loss: 0.0487, step time: 0.5219\n",
      "47/388, train_loss: 0.6327, step time: 0.5121\n",
      "48/388, train_loss: 0.2310, step time: 0.5184\n",
      "49/388, train_loss: 0.1031, step time: 0.5231\n",
      "50/388, train_loss: 0.1329, step time: 0.5423\n",
      "51/388, train_loss: 0.2666, step time: 0.5256\n",
      "52/388, train_loss: 0.2994, step time: 0.5205\n",
      "53/388, train_loss: 0.2195, step time: 0.5169\n",
      "54/388, train_loss: 0.1194, step time: 0.5042\n",
      "55/388, train_loss: 0.3083, step time: 0.5017\n",
      "56/388, train_loss: 0.1817, step time: 0.5159\n",
      "57/388, train_loss: 0.0962, step time: 0.5077\n",
      "58/388, train_loss: 0.1358, step time: 0.5459\n",
      "59/388, train_loss: 0.1227, step time: 0.6202\n",
      "60/388, train_loss: 0.1547, step time: 0.5505\n",
      "61/388, train_loss: 0.0710, step time: 0.5214\n",
      "62/388, train_loss: 0.1630, step time: 0.5103\n",
      "63/388, train_loss: 0.1033, step time: 0.4938\n",
      "64/388, train_loss: 0.1899, step time: 0.5238\n",
      "65/388, train_loss: 0.1956, step time: 0.4951\n",
      "66/388, train_loss: 0.1559, step time: 1.0873\n",
      "67/388, train_loss: 0.0811, step time: 0.5451\n",
      "68/388, train_loss: 0.4505, step time: 0.5115\n",
      "69/388, train_loss: 0.2143, step time: 0.4956\n",
      "70/388, train_loss: 0.0999, step time: 0.5283\n",
      "71/388, train_loss: 0.3852, step time: 0.5108\n",
      "72/388, train_loss: 0.1684, step time: 0.5023\n",
      "73/388, train_loss: 0.0877, step time: 0.4902\n",
      "74/388, train_loss: 0.0337, step time: 0.5070\n",
      "75/388, train_loss: 0.0976, step time: 0.4956\n",
      "76/388, train_loss: 0.2951, step time: 0.4967\n",
      "77/388, train_loss: 0.2506, step time: 0.4839\n",
      "78/388, train_loss: 0.2109, step time: 0.7209\n",
      "79/388, train_loss: 0.1045, step time: 0.5556\n",
      "80/388, train_loss: 0.2032, step time: 0.5106\n",
      "81/388, train_loss: 0.1353, step time: 0.4992\n",
      "82/388, train_loss: 0.2966, step time: 0.5022\n",
      "83/388, train_loss: 0.2433, step time: 0.5420\n",
      "84/388, train_loss: 0.0715, step time: 0.5152\n",
      "85/388, train_loss: 0.1544, step time: 0.4990\n",
      "86/388, train_loss: 0.1835, step time: 0.4820\n",
      "87/388, train_loss: 0.1262, step time: 0.5038\n",
      "88/388, train_loss: 0.1523, step time: 0.4880\n",
      "89/388, train_loss: 0.1202, step time: 0.5056\n",
      "90/388, train_loss: 0.0913, step time: 0.5340\n",
      "91/388, train_loss: 0.0817, step time: 0.5394\n",
      "92/388, train_loss: 0.1795, step time: 0.5175\n",
      "93/388, train_loss: 0.1137, step time: 0.5079\n",
      "94/388, train_loss: 0.2883, step time: 0.9039\n",
      "95/388, train_loss: 0.1220, step time: 0.5531\n",
      "96/388, train_loss: 0.3140, step time: 0.5332\n",
      "97/388, train_loss: 0.3094, step time: 0.5041\n",
      "98/388, train_loss: 0.2526, step time: 0.4897\n",
      "99/388, train_loss: 0.0776, step time: 0.4795\n",
      "100/388, train_loss: 0.0775, step time: 0.4889\n",
      "101/388, train_loss: 0.1468, step time: 0.4820\n",
      "102/388, train_loss: 0.3279, step time: 1.0323\n",
      "103/388, train_loss: 0.3577, step time: 0.5357\n",
      "104/388, train_loss: 0.1085, step time: 0.5138\n",
      "105/388, train_loss: 0.2514, step time: 0.4852\n",
      "106/388, train_loss: 0.2709, step time: 0.4841\n",
      "107/388, train_loss: 0.3475, step time: 0.5082\n",
      "108/388, train_loss: 0.1259, step time: 0.5133\n",
      "109/388, train_loss: 0.1968, step time: 0.4942\n",
      "110/388, train_loss: 0.1400, step time: 0.4891\n",
      "111/388, train_loss: 0.1021, step time: 0.5220\n",
      "112/388, train_loss: 0.2537, step time: 0.5082\n",
      "113/388, train_loss: 0.2515, step time: 0.4997\n",
      "114/388, train_loss: 0.0839, step time: 0.5141\n",
      "115/388, train_loss: 0.1317, step time: 0.5955\n",
      "116/388, train_loss: 0.4379, step time: 0.5444\n",
      "117/388, train_loss: 0.3147, step time: 0.5178\n",
      "118/388, train_loss: 0.1656, step time: 0.4980\n",
      "119/388, train_loss: 0.3921, step time: 0.5435\n",
      "120/388, train_loss: 0.2356, step time: 0.5174\n",
      "121/388, train_loss: 0.0812, step time: 0.5004\n",
      "122/388, train_loss: 0.1840, step time: 0.5029\n",
      "123/388, train_loss: 0.1517, step time: 0.4871\n",
      "124/388, train_loss: 0.2551, step time: 0.5145\n",
      "125/388, train_loss: 0.2315, step time: 0.5068\n",
      "126/388, train_loss: 0.1102, step time: 0.5058\n",
      "127/388, train_loss: 0.0665, step time: 0.4937\n",
      "128/388, train_loss: 0.1849, step time: 0.5026\n",
      "129/388, train_loss: 0.3167, step time: 1.2020\n",
      "130/388, train_loss: 0.0845, step time: 0.5363\n",
      "131/388, train_loss: 0.1106, step time: 0.5149\n",
      "132/388, train_loss: 0.2684, step time: 0.5046\n",
      "133/388, train_loss: 0.1302, step time: 0.5168\n",
      "134/388, train_loss: 0.1723, step time: 0.4957\n",
      "135/388, train_loss: 0.1876, step time: 0.4984\n",
      "136/388, train_loss: 0.3083, step time: 0.4775\n",
      "137/388, train_loss: 0.0853, step time: 0.4906\n",
      "138/388, train_loss: 0.1393, step time: 0.5244\n",
      "139/388, train_loss: 0.2032, step time: 0.5268\n",
      "140/388, train_loss: 0.2437, step time: 0.5093\n",
      "141/388, train_loss: 0.2974, step time: 0.4934\n",
      "142/388, train_loss: 0.1639, step time: 1.0562\n",
      "143/388, train_loss: 0.0900, step time: 0.5304\n",
      "144/388, train_loss: 0.3464, step time: 0.5030\n",
      "145/388, train_loss: 0.3209, step time: 0.4871\n",
      "146/388, train_loss: 0.1430, step time: 0.4826\n",
      "147/388, train_loss: 0.1111, step time: 0.4766\n",
      "148/388, train_loss: 0.2485, step time: 0.4750\n",
      "149/388, train_loss: 0.0912, step time: 0.4780\n",
      "150/388, train_loss: 0.1827, step time: 0.5094\n",
      "151/388, train_loss: 0.0301, step time: 0.4847\n",
      "152/388, train_loss: 0.1472, step time: 0.5103\n",
      "153/388, train_loss: 0.1339, step time: 0.4983\n",
      "154/388, train_loss: 0.3184, step time: 0.5036\n",
      "155/388, train_loss: 0.0634, step time: 0.4960\n",
      "156/388, train_loss: 0.1910, step time: 0.5069\n",
      "157/388, train_loss: 0.0659, step time: 0.4980\n",
      "158/388, train_loss: 0.5794, step time: 0.4910\n",
      "159/388, train_loss: 0.1340, step time: 1.1138\n",
      "160/388, train_loss: 0.0965, step time: 0.5389\n",
      "161/388, train_loss: 0.1107, step time: 0.5108\n",
      "162/388, train_loss: 0.2310, step time: 0.4980\n",
      "163/388, train_loss: 0.3671, step time: 0.5024\n",
      "164/388, train_loss: 0.2526, step time: 0.5315\n",
      "165/388, train_loss: 0.2317, step time: 0.5225\n",
      "166/388, train_loss: 0.1560, step time: 0.5128\n",
      "167/388, train_loss: 0.1236, step time: 0.4965\n",
      "168/388, train_loss: 0.1358, step time: 0.4960\n",
      "169/388, train_loss: 0.1177, step time: 0.5528\n",
      "170/388, train_loss: 0.0454, step time: 0.5255\n",
      "171/388, train_loss: 0.2891, step time: 0.4946\n",
      "172/388, train_loss: 0.1346, step time: 0.4971\n",
      "173/388, train_loss: 0.0909, step time: 0.4954\n",
      "174/388, train_loss: 0.1451, step time: 0.4921\n",
      "175/388, train_loss: 0.0834, step time: 0.6959\n",
      "176/388, train_loss: 0.1331, step time: 0.5632\n",
      "177/388, train_loss: 0.0839, step time: 0.5278\n",
      "178/388, train_loss: 0.1853, step time: 0.5060\n",
      "179/388, train_loss: 0.1776, step time: 0.4915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/388, train_loss: 0.1032, step time: 0.4931\n",
      "181/388, train_loss: 0.2071, step time: 0.4827\n",
      "182/388, train_loss: 0.1312, step time: 0.4831\n",
      "183/388, train_loss: 0.3571, step time: 0.6857\n",
      "184/388, train_loss: 0.1132, step time: 0.5413\n",
      "185/388, train_loss: 0.1767, step time: 0.5367\n",
      "186/388, train_loss: 0.0933, step time: 0.5258\n",
      "187/388, train_loss: 0.2836, step time: 0.5429\n",
      "188/388, train_loss: 0.1066, step time: 0.5111\n",
      "189/388, train_loss: 0.2256, step time: 0.5085\n",
      "190/388, train_loss: 0.2592, step time: 0.4937\n",
      "191/388, train_loss: 0.1376, step time: 0.4881\n",
      "192/388, train_loss: 0.2014, step time: 0.5050\n",
      "193/388, train_loss: 0.2722, step time: 0.4889\n",
      "194/388, train_loss: 0.2547, step time: 0.9606\n",
      "195/388, train_loss: 0.1230, step time: 0.5447\n",
      "196/388, train_loss: 0.0528, step time: 0.5264\n",
      "197/388, train_loss: 0.3783, step time: 0.5123\n",
      "198/388, train_loss: 0.1657, step time: 0.4984\n",
      "199/388, train_loss: 0.2313, step time: 0.5088\n",
      "200/388, train_loss: 0.1017, step time: 0.4964\n",
      "201/388, train_loss: 0.5074, step time: 0.5023\n",
      "202/388, train_loss: 0.2252, step time: 0.4982\n",
      "203/388, train_loss: 0.1031, step time: 0.6286\n",
      "204/388, train_loss: 0.1916, step time: 0.5531\n",
      "205/388, train_loss: 0.3893, step time: 0.5140\n",
      "206/388, train_loss: 0.1862, step time: 0.5036\n",
      "207/388, train_loss: 0.1015, step time: 0.4912\n",
      "208/388, train_loss: 0.1892, step time: 0.4932\n",
      "209/388, train_loss: 0.1353, step time: 0.4769\n",
      "210/388, train_loss: 0.1050, step time: 0.4735\n",
      "211/388, train_loss: 0.0649, step time: 0.4905\n",
      "212/388, train_loss: 0.1810, step time: 0.4819\n",
      "213/388, train_loss: 0.1563, step time: 0.4924\n",
      "214/388, train_loss: 0.4196, step time: 0.8162\n",
      "215/388, train_loss: 0.2721, step time: 0.5591\n",
      "216/388, train_loss: 0.0589, step time: 0.5226\n",
      "217/388, train_loss: 0.0686, step time: 0.5093\n",
      "218/388, train_loss: 0.1156, step time: 0.5322\n",
      "219/388, train_loss: 0.1390, step time: 0.5082\n",
      "220/388, train_loss: 0.0692, step time: 0.4969\n",
      "221/388, train_loss: 0.1957, step time: 0.4949\n",
      "222/388, train_loss: 0.0769, step time: 0.4762\n",
      "223/388, train_loss: 0.1972, step time: 0.7228\n",
      "224/388, train_loss: 0.1972, step time: 0.6034\n",
      "225/388, train_loss: 0.2274, step time: 0.5373\n",
      "226/388, train_loss: 0.2544, step time: 0.5209\n",
      "227/388, train_loss: 0.1223, step time: 0.4911\n",
      "228/388, train_loss: 0.1760, step time: 0.4921\n",
      "229/388, train_loss: 0.2217, step time: 1.1096\n",
      "230/388, train_loss: 0.1447, step time: 0.5437\n",
      "231/388, train_loss: 0.2714, step time: 0.5179\n",
      "232/388, train_loss: 0.0856, step time: 0.4937\n",
      "233/388, train_loss: 0.1383, step time: 0.5005\n",
      "234/388, train_loss: 0.0879, step time: 0.4798\n",
      "235/388, train_loss: 0.2386, step time: 0.4763\n",
      "236/388, train_loss: 0.0754, step time: 0.4983\n",
      "237/388, train_loss: 0.0873, step time: 0.4963\n",
      "238/388, train_loss: 0.3464, step time: 0.4982\n",
      "239/388, train_loss: 0.3071, step time: 0.4938\n",
      "240/388, train_loss: 0.2912, step time: 1.0490\n",
      "241/388, train_loss: 0.1774, step time: 0.5405\n",
      "242/388, train_loss: 0.1320, step time: 0.5128\n",
      "243/388, train_loss: 0.1963, step time: 0.4972\n",
      "244/388, train_loss: 0.3141, step time: 0.4820\n",
      "245/388, train_loss: 0.2940, step time: 0.4753\n",
      "246/388, train_loss: 0.0889, step time: 0.5145\n",
      "247/388, train_loss: 0.1123, step time: 0.4955\n",
      "248/388, train_loss: 0.1511, step time: 0.5134\n",
      "249/388, train_loss: 0.0616, step time: 0.5490\n",
      "250/388, train_loss: 0.1683, step time: 0.5355\n",
      "251/388, train_loss: 0.1062, step time: 0.5513\n",
      "252/388, train_loss: 0.1132, step time: 0.5113\n",
      "253/388, train_loss: 0.1284, step time: 0.5237\n",
      "254/388, train_loss: 0.1968, step time: 0.4944\n",
      "255/388, train_loss: 0.1662, step time: 0.4866\n",
      "256/388, train_loss: 0.5252, step time: 0.5055\n",
      "257/388, train_loss: 0.2120, step time: 0.5021\n",
      "258/388, train_loss: 0.1202, step time: 0.5792\n",
      "259/388, train_loss: 0.4094, step time: 0.5479\n",
      "260/388, train_loss: 0.0739, step time: 0.5235\n",
      "261/388, train_loss: 0.1364, step time: 0.5111\n",
      "262/388, train_loss: 0.1045, step time: 0.5003\n",
      "263/388, train_loss: 0.1060, step time: 0.5548\n",
      "264/388, train_loss: 0.0822, step time: 0.5467\n",
      "265/388, train_loss: 0.1630, step time: 0.5156\n",
      "266/388, train_loss: 0.0835, step time: 0.4943\n",
      "267/388, train_loss: 0.0858, step time: 0.4911\n",
      "268/388, train_loss: 0.0967, step time: 0.4946\n",
      "269/388, train_loss: 0.0686, step time: 0.4827\n",
      "270/388, train_loss: 0.1898, step time: 1.1841\n",
      "271/388, train_loss: 0.4815, step time: 0.5432\n",
      "272/388, train_loss: 0.2231, step time: 0.5211\n",
      "273/388, train_loss: 0.1084, step time: 0.5062\n",
      "274/388, train_loss: 0.2190, step time: 0.5102\n",
      "275/388, train_loss: 0.0918, step time: 0.4995\n",
      "276/388, train_loss: 0.1584, step time: 0.4947\n",
      "277/388, train_loss: 0.1461, step time: 0.4815\n",
      "278/388, train_loss: 0.0896, step time: 0.4926\n",
      "279/388, train_loss: 0.0895, step time: 0.4844\n",
      "280/388, train_loss: 0.1954, step time: 0.5811\n",
      "281/388, train_loss: 0.1143, step time: 0.5615\n",
      "282/388, train_loss: 0.1896, step time: 0.5374\n",
      "283/388, train_loss: 0.1438, step time: 0.5087\n",
      "284/388, train_loss: 0.3254, step time: 0.5070\n",
      "285/388, train_loss: 0.2000, step time: 0.4869\n",
      "286/388, train_loss: 0.3396, step time: 0.5067\n",
      "287/388, train_loss: 0.4245, step time: 0.5254\n",
      "288/388, train_loss: 0.0843, step time: 0.4968\n",
      "289/388, train_loss: 0.1681, step time: 0.5024\n",
      "290/388, train_loss: 0.0802, step time: 0.4892\n",
      "291/388, train_loss: 0.3521, step time: 0.4855\n",
      "292/388, train_loss: 0.0734, step time: 1.0869\n",
      "293/388, train_loss: 0.2993, step time: 0.5367\n",
      "294/388, train_loss: 0.1564, step time: 0.5132\n",
      "295/388, train_loss: 0.1380, step time: 0.4853\n",
      "296/388, train_loss: 0.1004, step time: 1.1148\n",
      "297/388, train_loss: 0.0503, step time: 0.5384\n",
      "298/388, train_loss: 0.1252, step time: 0.4968\n",
      "299/388, train_loss: 0.1087, step time: 0.4965\n",
      "300/388, train_loss: 0.1185, step time: 0.4822\n",
      "301/388, train_loss: 0.1111, step time: 0.4860\n",
      "302/388, train_loss: 0.1645, step time: 0.4888\n",
      "303/388, train_loss: 0.1512, step time: 0.9942\n",
      "304/388, train_loss: 0.4562, step time: 0.5439\n",
      "305/388, train_loss: 0.2155, step time: 0.5157\n",
      "306/388, train_loss: 0.1012, step time: 0.4905\n",
      "307/388, train_loss: 0.0935, step time: 0.4968\n",
      "308/388, train_loss: 0.1673, step time: 0.4946\n",
      "309/388, train_loss: 0.2485, step time: 0.5325\n",
      "310/388, train_loss: 0.1566, step time: 0.5036\n",
      "311/388, train_loss: 0.9755, step time: 0.5009\n",
      "312/388, train_loss: 0.2495, step time: 0.4821\n",
      "313/388, train_loss: 0.0820, step time: 0.4809\n",
      "314/388, train_loss: 0.2376, step time: 0.7622\n",
      "315/388, train_loss: 0.2761, step time: 0.5555\n",
      "316/388, train_loss: 0.1583, step time: 0.5526\n",
      "317/388, train_loss: 0.0742, step time: 0.5256\n",
      "318/388, train_loss: 0.2802, step time: 0.5136\n",
      "319/388, train_loss: 0.3398, step time: 0.5497\n",
      "320/388, train_loss: 0.2170, step time: 0.5753\n",
      "321/388, train_loss: 0.3929, step time: 0.5390\n",
      "322/388, train_loss: 0.1293, step time: 0.4982\n",
      "323/388, train_loss: 0.1098, step time: 0.4865\n",
      "324/388, train_loss: 0.1172, step time: 0.5009\n",
      "325/388, train_loss: 0.1228, step time: 0.4874\n",
      "326/388, train_loss: 0.1478, step time: 0.5111\n",
      "327/388, train_loss: 0.2048, step time: 0.5329\n",
      "328/388, train_loss: 0.2075, step time: 0.5224\n",
      "329/388, train_loss: 0.2134, step time: 0.5047\n",
      "330/388, train_loss: 0.4626, step time: 0.9382\n",
      "331/388, train_loss: 0.0689, step time: 0.5345\n",
      "332/388, train_loss: 0.2454, step time: 0.5137\n",
      "333/388, train_loss: 0.0796, step time: 0.5004\n",
      "334/388, train_loss: 0.0892, step time: 0.5157\n",
      "335/388, train_loss: 0.2179, step time: 0.5660\n",
      "336/388, train_loss: 0.0971, step time: 0.5327\n",
      "337/388, train_loss: 0.2809, step time: 0.5197\n",
      "338/388, train_loss: 0.3384, step time: 0.4965\n",
      "339/388, train_loss: 0.3485, step time: 0.5242\n",
      "340/388, train_loss: 0.0804, step time: 0.5018\n",
      "341/388, train_loss: 0.1560, step time: 0.5062\n",
      "342/388, train_loss: 0.4070, step time: 0.4937\n",
      "343/388, train_loss: 0.1751, step time: 1.0990\n",
      "344/388, train_loss: 0.1436, step time: 0.5232\n",
      "345/388, train_loss: 0.1489, step time: 0.5055\n",
      "346/388, train_loss: 0.3837, step time: 0.4891\n",
      "347/388, train_loss: 0.1424, step time: 0.4921\n",
      "348/388, train_loss: 0.0415, step time: 0.4835\n",
      "349/388, train_loss: 0.0993, step time: 0.9377\n",
      "350/388, train_loss: 0.2579, step time: 0.5325\n",
      "351/388, train_loss: 0.0674, step time: 0.5059\n",
      "352/388, train_loss: 0.1588, step time: 0.5011\n",
      "353/388, train_loss: 0.2322, step time: 0.4866\n",
      "354/388, train_loss: 0.2166, step time: 0.5060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/388, train_loss: 0.1483, step time: 0.6048\n",
      "356/388, train_loss: 0.2340, step time: 0.5487\n",
      "357/388, train_loss: 0.0958, step time: 0.5305\n",
      "358/388, train_loss: 0.7240, step time: 0.5208\n",
      "359/388, train_loss: 0.2198, step time: 0.5553\n",
      "360/388, train_loss: 0.3772, step time: 0.5344\n",
      "361/388, train_loss: 0.1757, step time: 0.5124\n",
      "362/388, train_loss: 0.0632, step time: 0.4993\n",
      "363/388, train_loss: 0.2475, step time: 0.5047\n",
      "364/388, train_loss: 0.1475, step time: 0.4802\n",
      "365/388, train_loss: 0.2878, step time: 0.9914\n",
      "366/388, train_loss: 0.2598, step time: 0.5490\n",
      "367/388, train_loss: 0.1027, step time: 0.5122\n",
      "368/388, train_loss: 0.3758, step time: 0.4923\n",
      "369/388, train_loss: 0.1680, step time: 0.4941\n",
      "370/388, train_loss: 0.3158, step time: 0.4862\n",
      "371/388, train_loss: 0.1940, step time: 0.4929\n",
      "372/388, train_loss: 0.0734, step time: 0.5358\n",
      "373/388, train_loss: 0.0540, step time: 0.5209\n",
      "374/388, train_loss: 0.0938, step time: 0.5142\n",
      "375/388, train_loss: 0.1146, step time: 0.5079\n",
      "376/388, train_loss: 0.2507, step time: 0.4864\n",
      "377/388, train_loss: 0.5674, step time: 0.4788\n",
      "378/388, train_loss: 0.1885, step time: 0.5423\n",
      "379/388, train_loss: 0.1114, step time: 0.5211\n",
      "380/388, train_loss: 0.0932, step time: 0.5027\n",
      "381/388, train_loss: 0.1747, step time: 0.4994\n",
      "382/388, train_loss: 0.1867, step time: 0.5052\n",
      "383/388, train_loss: 0.1156, step time: 0.4947\n",
      "384/388, train_loss: 0.1486, step time: 0.4875\n",
      "385/388, train_loss: 0.1342, step time: 1.1969\n",
      "386/388, train_loss: 0.1552, step time: 0.5283\n",
      "387/388, train_loss: 0.1937, step time: 0.5102\n",
      "388/388, train_loss: 0.0430, step time: 0.5045\n",
      "epoch 57 average loss: 0.1887\n",
      "current epoch: 57 current mean dice: 0.7568 tc: 0.8125 wt: 0.8790 et: 0.5790\n",
      "best mean dice: 0.7673 at epoch: 54\n",
      "time consuming of epoch 57 is: 299.8916\n",
      "----------\n",
      "epoch 58/300\n",
      "1/388, train_loss: 0.1478, step time: 0.4820\n",
      "2/388, train_loss: 0.1501, step time: 0.4864\n",
      "3/388, train_loss: 0.0886, step time: 0.8758\n",
      "4/388, train_loss: 0.1960, step time: 0.5383\n",
      "5/388, train_loss: 0.1693, step time: 0.5133\n",
      "6/388, train_loss: 0.3753, step time: 0.5054\n",
      "7/388, train_loss: 0.1666, step time: 0.7789\n",
      "8/388, train_loss: 0.1654, step time: 0.5783\n",
      "9/388, train_loss: 0.2042, step time: 0.5371\n",
      "10/388, train_loss: 0.1129, step time: 0.5004\n",
      "11/388, train_loss: 0.3591, step time: 0.4929\n",
      "12/388, train_loss: 0.1564, step time: 0.9968\n",
      "13/388, train_loss: 0.1618, step time: 0.5348\n",
      "14/388, train_loss: 0.1161, step time: 0.5170\n",
      "15/388, train_loss: 0.1043, step time: 0.5019\n",
      "16/388, train_loss: 0.3778, step time: 0.5182\n",
      "17/388, train_loss: 0.1162, step time: 0.4985\n",
      "18/388, train_loss: 0.2331, step time: 0.5062\n",
      "19/388, train_loss: 0.1908, step time: 0.4886\n",
      "20/388, train_loss: 0.1052, step time: 0.9326\n",
      "21/388, train_loss: 0.3291, step time: 0.5733\n",
      "22/388, train_loss: 0.0687, step time: 0.5309\n",
      "23/388, train_loss: 0.1024, step time: 0.5059\n",
      "24/388, train_loss: 0.2651, step time: 0.4902\n",
      "25/388, train_loss: 0.1543, step time: 0.5001\n",
      "26/388, train_loss: 0.2245, step time: 0.5116\n",
      "27/388, train_loss: 0.1368, step time: 1.0478\n",
      "28/388, train_loss: 0.1854, step time: 0.5618\n",
      "29/388, train_loss: 0.0793, step time: 0.5014\n",
      "30/388, train_loss: 0.2201, step time: 0.5020\n",
      "31/388, train_loss: 0.2697, step time: 0.5105\n",
      "32/388, train_loss: 0.1012, step time: 0.4961\n",
      "33/388, train_loss: 0.0664, step time: 0.9375\n",
      "34/388, train_loss: 0.2857, step time: 0.5580\n",
      "35/388, train_loss: 0.1100, step time: 0.5193\n",
      "36/388, train_loss: 0.0848, step time: 0.4960\n",
      "37/388, train_loss: 0.2222, step time: 0.5149\n",
      "38/388, train_loss: 0.2167, step time: 0.4913\n",
      "39/388, train_loss: 0.2798, step time: 0.4818\n",
      "40/388, train_loss: 0.1058, step time: 0.4844\n",
      "41/388, train_loss: 0.1504, step time: 0.5849\n",
      "42/388, train_loss: 0.1496, step time: 0.5672\n",
      "43/388, train_loss: 0.2131, step time: 0.5427\n",
      "44/388, train_loss: 0.1192, step time: 0.5274\n",
      "45/388, train_loss: 0.2687, step time: 0.4930\n",
      "46/388, train_loss: 0.1084, step time: 0.5018\n",
      "47/388, train_loss: 0.2037, step time: 0.4971\n",
      "48/388, train_loss: 0.1846, step time: 0.4971\n",
      "49/388, train_loss: 0.2181, step time: 0.4859\n",
      "50/388, train_loss: 0.1642, step time: 0.4869\n",
      "51/388, train_loss: 0.0882, step time: 1.0082\n",
      "52/388, train_loss: 0.1890, step time: 0.5583\n",
      "53/388, train_loss: 0.1730, step time: 0.5195\n",
      "54/388, train_loss: 0.2963, step time: 0.5034\n",
      "55/388, train_loss: 0.2320, step time: 0.4976\n",
      "56/388, train_loss: 0.0641, step time: 0.4889\n",
      "57/388, train_loss: 0.0757, step time: 1.2042\n",
      "58/388, train_loss: 0.4765, step time: 0.5292\n",
      "59/388, train_loss: 0.1747, step time: 0.5049\n",
      "60/388, train_loss: 0.0925, step time: 0.4882\n",
      "61/388, train_loss: 0.4732, step time: 0.4955\n",
      "62/388, train_loss: 0.2550, step time: 0.4812\n",
      "63/388, train_loss: 0.2238, step time: 0.4887\n",
      "64/388, train_loss: 0.1104, step time: 0.5688\n",
      "65/388, train_loss: 0.1771, step time: 0.5232\n",
      "66/388, train_loss: 0.2565, step time: 0.5122\n",
      "67/388, train_loss: 0.1417, step time: 0.5437\n",
      "68/388, train_loss: 0.3345, step time: 0.5182\n",
      "69/388, train_loss: 0.1511, step time: 0.5195\n",
      "70/388, train_loss: 0.1173, step time: 0.4955\n",
      "71/388, train_loss: 0.1690, step time: 0.4924\n",
      "72/388, train_loss: 0.1183, step time: 0.4842\n",
      "73/388, train_loss: 0.3770, step time: 0.4779\n",
      "74/388, train_loss: 0.0898, step time: 1.0662\n",
      "75/388, train_loss: 0.1669, step time: 0.5388\n",
      "76/388, train_loss: 0.2091, step time: 0.5117\n",
      "77/388, train_loss: 0.6400, step time: 0.5162\n",
      "78/388, train_loss: 0.1057, step time: 0.5110\n",
      "79/388, train_loss: 0.1188, step time: 0.5054\n",
      "80/388, train_loss: 0.2168, step time: 0.4846\n",
      "81/388, train_loss: 0.2282, step time: 0.9088\n",
      "82/388, train_loss: 0.2470, step time: 0.5461\n",
      "83/388, train_loss: 0.1750, step time: 0.5026\n",
      "84/388, train_loss: 0.0938, step time: 0.4939\n",
      "85/388, train_loss: 0.2841, step time: 0.4952\n",
      "86/388, train_loss: 0.1313, step time: 0.4886\n",
      "87/388, train_loss: 0.2105, step time: 0.5000\n",
      "88/388, train_loss: 0.1851, step time: 0.4911\n",
      "89/388, train_loss: 0.0779, step time: 0.5550\n",
      "90/388, train_loss: 0.0685, step time: 0.5117\n",
      "91/388, train_loss: 0.2693, step time: 0.4892\n",
      "92/388, train_loss: 0.3406, step time: 1.0837\n",
      "93/388, train_loss: 0.1302, step time: 0.5444\n",
      "94/388, train_loss: 0.2407, step time: 0.5082\n",
      "95/388, train_loss: 0.0961, step time: 0.4938\n",
      "96/388, train_loss: 0.0757, step time: 0.4960\n",
      "97/388, train_loss: 0.3894, step time: 0.4844\n",
      "98/388, train_loss: 0.1243, step time: 0.4968\n",
      "99/388, train_loss: 0.0581, step time: 0.4883\n",
      "100/388, train_loss: 0.1020, step time: 0.4907\n",
      "101/388, train_loss: 0.3476, step time: 0.4939\n",
      "102/388, train_loss: 0.1464, step time: 1.1223\n",
      "103/388, train_loss: 0.1354, step time: 0.5284\n",
      "104/388, train_loss: 0.1258, step time: 0.5019\n",
      "105/388, train_loss: 0.1407, step time: 0.4931\n",
      "106/388, train_loss: 0.2426, step time: 0.4848\n",
      "107/388, train_loss: 0.0700, step time: 0.4794\n",
      "108/388, train_loss: 0.1171, step time: 0.4814\n",
      "109/388, train_loss: 0.1380, step time: 0.4758\n",
      "110/388, train_loss: 0.1764, step time: 0.5196\n",
      "111/388, train_loss: 0.1363, step time: 0.5165\n",
      "112/388, train_loss: 0.0980, step time: 0.5064\n",
      "113/388, train_loss: 0.1257, step time: 0.4981\n",
      "114/388, train_loss: 0.1628, step time: 0.4979\n",
      "115/388, train_loss: 0.1941, step time: 0.4952\n",
      "116/388, train_loss: 0.2788, step time: 0.4990\n",
      "117/388, train_loss: 0.2452, step time: 0.5056\n",
      "118/388, train_loss: 0.0929, step time: 0.4866\n",
      "119/388, train_loss: 0.4638, step time: 0.4987\n",
      "120/388, train_loss: 0.1889, step time: 0.4884\n",
      "121/388, train_loss: 0.2800, step time: 0.4784\n",
      "122/388, train_loss: 0.0929, step time: 0.4797\n",
      "123/388, train_loss: 0.4014, step time: 0.4802\n",
      "124/388, train_loss: 0.6488, step time: 1.1790\n",
      "125/388, train_loss: 0.2906, step time: 0.5409\n",
      "126/388, train_loss: 0.2137, step time: 0.5137\n",
      "127/388, train_loss: 0.1269, step time: 0.4980\n",
      "128/388, train_loss: 0.2494, step time: 0.4879\n",
      "129/388, train_loss: 0.2421, step time: 1.1637\n",
      "130/388, train_loss: 0.2973, step time: 0.5342\n",
      "131/388, train_loss: 0.1677, step time: 0.5084\n",
      "132/388, train_loss: 0.3419, step time: 0.4854\n",
      "133/388, train_loss: 0.1950, step time: 0.4967\n",
      "134/388, train_loss: 0.0856, step time: 0.4789\n",
      "135/388, train_loss: 0.0687, step time: 0.4764\n",
      "136/388, train_loss: 0.5232, step time: 0.4796\n",
      "137/388, train_loss: 0.1856, step time: 0.9953\n",
      "138/388, train_loss: 0.2814, step time: 0.5331\n",
      "139/388, train_loss: 0.2992, step time: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/388, train_loss: 0.0951, step time: 0.4968\n",
      "141/388, train_loss: 0.2478, step time: 0.4902\n",
      "142/388, train_loss: 0.0335, step time: 0.4901\n",
      "143/388, train_loss: 0.1089, step time: 0.4872\n",
      "144/388, train_loss: 0.1465, step time: 0.4816\n",
      "145/388, train_loss: 0.1887, step time: 0.4713\n",
      "146/388, train_loss: 0.4769, step time: 0.4758\n",
      "147/388, train_loss: 0.0973, step time: 0.5009\n",
      "148/388, train_loss: 0.0875, step time: 0.5077\n",
      "149/388, train_loss: 0.2773, step time: 0.5861\n",
      "150/388, train_loss: 0.1630, step time: 0.5457\n",
      "151/388, train_loss: 0.0601, step time: 0.5375\n",
      "152/388, train_loss: 0.1411, step time: 0.4976\n",
      "153/388, train_loss: 0.0887, step time: 0.5249\n",
      "154/388, train_loss: 0.1075, step time: 0.5128\n",
      "155/388, train_loss: 0.1745, step time: 0.4978\n",
      "156/388, train_loss: 0.1308, step time: 0.4873\n",
      "157/388, train_loss: 0.0834, step time: 0.4862\n",
      "158/388, train_loss: 0.2334, step time: 0.4785\n",
      "159/388, train_loss: 0.1552, step time: 0.4917\n",
      "160/388, train_loss: 0.1024, step time: 0.4958\n",
      "161/388, train_loss: 0.0853, step time: 0.4946\n",
      "162/388, train_loss: 0.3215, step time: 0.4820\n",
      "163/388, train_loss: 0.0828, step time: 0.4865\n",
      "164/388, train_loss: 0.0951, step time: 0.4793\n",
      "165/388, train_loss: 0.0926, step time: 0.4785\n",
      "166/388, train_loss: 0.0839, step time: 0.5343\n",
      "167/388, train_loss: 0.2118, step time: 0.5378\n",
      "168/388, train_loss: 0.2040, step time: 0.4954\n",
      "169/388, train_loss: 0.2428, step time: 0.4940\n",
      "170/388, train_loss: 0.3560, step time: 0.5249\n",
      "171/388, train_loss: 0.2178, step time: 0.5216\n",
      "172/388, train_loss: 0.1223, step time: 0.4980\n",
      "173/388, train_loss: 0.2101, step time: 0.5220\n",
      "174/388, train_loss: 0.2914, step time: 0.5142\n",
      "175/388, train_loss: 0.0620, step time: 0.5011\n",
      "176/388, train_loss: 0.0356, step time: 0.4783\n",
      "177/388, train_loss: 0.2585, step time: 0.4844\n",
      "178/388, train_loss: 0.1094, step time: 0.4770\n",
      "179/388, train_loss: 0.0761, step time: 1.0752\n",
      "180/388, train_loss: 0.0941, step time: 0.5325\n",
      "181/388, train_loss: 0.3896, step time: 0.5129\n",
      "182/388, train_loss: 0.3561, step time: 0.4863\n",
      "183/388, train_loss: 0.1355, step time: 0.4988\n",
      "184/388, train_loss: 0.6757, step time: 0.4838\n",
      "185/388, train_loss: 0.1941, step time: 1.1179\n",
      "186/388, train_loss: 0.1481, step time: 0.5076\n",
      "187/388, train_loss: 0.4123, step time: 0.4981\n",
      "188/388, train_loss: 0.1868, step time: 0.4785\n",
      "189/388, train_loss: 0.1449, step time: 0.4799\n",
      "190/388, train_loss: 0.2064, step time: 0.4866\n",
      "191/388, train_loss: 0.0978, step time: 0.4751\n",
      "192/388, train_loss: 0.1338, step time: 0.5151\n",
      "193/388, train_loss: 0.2992, step time: 0.5173\n",
      "194/388, train_loss: 0.1064, step time: 0.5024\n",
      "195/388, train_loss: 0.1103, step time: 0.4865\n",
      "196/388, train_loss: 0.1404, step time: 0.4811\n",
      "197/388, train_loss: 0.2839, step time: 1.0694\n",
      "198/388, train_loss: 0.2019, step time: 0.5500\n",
      "199/388, train_loss: 0.1161, step time: 0.5106\n",
      "200/388, train_loss: 0.0704, step time: 0.5025\n",
      "201/388, train_loss: 0.1278, step time: 0.5026\n",
      "202/388, train_loss: 0.0646, step time: 0.4837\n",
      "203/388, train_loss: 0.1598, step time: 0.4846\n",
      "204/388, train_loss: 0.2366, step time: 0.5416\n",
      "205/388, train_loss: 0.4312, step time: 0.5206\n",
      "206/388, train_loss: 0.1535, step time: 0.5055\n",
      "207/388, train_loss: 0.0851, step time: 0.4975\n",
      "208/388, train_loss: 0.0556, step time: 0.4831\n",
      "209/388, train_loss: 0.2716, step time: 0.4945\n",
      "210/388, train_loss: 0.0785, step time: 0.4968\n",
      "211/388, train_loss: 0.0954, step time: 0.4979\n",
      "212/388, train_loss: 0.1728, step time: 0.4888\n",
      "213/388, train_loss: 0.0469, step time: 0.4781\n",
      "214/388, train_loss: 0.0874, step time: 0.4831\n",
      "215/388, train_loss: 0.0996, step time: 1.0379\n",
      "216/388, train_loss: 0.3530, step time: 0.5522\n",
      "217/388, train_loss: 0.0911, step time: 0.5231\n",
      "218/388, train_loss: 0.1265, step time: 0.4993\n",
      "219/388, train_loss: 0.2903, step time: 0.4873\n",
      "220/388, train_loss: 0.1306, step time: 0.5389\n",
      "221/388, train_loss: 0.2511, step time: 0.5401\n",
      "222/388, train_loss: 0.0673, step time: 0.5074\n",
      "223/388, train_loss: 0.1176, step time: 0.5056\n",
      "224/388, train_loss: 0.1739, step time: 0.4882\n",
      "225/388, train_loss: 0.0896, step time: 0.5005\n",
      "226/388, train_loss: 0.3840, step time: 0.4894\n",
      "227/388, train_loss: 0.1837, step time: 1.0197\n",
      "228/388, train_loss: 0.1715, step time: 0.5224\n",
      "229/388, train_loss: 0.0529, step time: 0.5018\n",
      "230/388, train_loss: 0.0487, step time: 0.4848\n",
      "231/388, train_loss: 0.2001, step time: 0.4974\n",
      "232/388, train_loss: 0.0890, step time: 0.5441\n",
      "233/388, train_loss: 0.0781, step time: 0.5281\n",
      "234/388, train_loss: 0.4606, step time: 0.4975\n",
      "235/388, train_loss: 0.0910, step time: 0.4963\n",
      "236/388, train_loss: 0.2020, step time: 0.4826\n",
      "237/388, train_loss: 0.1621, step time: 0.4855\n",
      "238/388, train_loss: 0.2708, step time: 0.4885\n",
      "239/388, train_loss: 0.1111, step time: 0.4775\n",
      "240/388, train_loss: 0.1581, step time: 0.4742\n",
      "241/388, train_loss: 0.1963, step time: 0.4847\n",
      "242/388, train_loss: 0.0754, step time: 0.5553\n",
      "243/388, train_loss: 0.3744, step time: 0.5224\n",
      "244/388, train_loss: 0.1096, step time: 0.4984\n",
      "245/388, train_loss: 0.0495, step time: 0.4790\n",
      "246/388, train_loss: 0.1789, step time: 0.4824\n",
      "247/388, train_loss: 0.5166, step time: 1.0113\n",
      "248/388, train_loss: 0.1744, step time: 0.5418\n",
      "249/388, train_loss: 0.1739, step time: 0.5104\n",
      "250/388, train_loss: 0.3029, step time: 0.4931\n",
      "251/388, train_loss: 0.2540, step time: 0.4917\n",
      "252/388, train_loss: 0.0659, step time: 0.4963\n",
      "253/388, train_loss: 0.1527, step time: 0.4969\n",
      "254/388, train_loss: 0.2344, step time: 0.4808\n",
      "255/388, train_loss: 0.1915, step time: 0.4901\n",
      "256/388, train_loss: 0.2019, step time: 0.8456\n",
      "257/388, train_loss: 0.1586, step time: 0.5624\n",
      "258/388, train_loss: 0.1937, step time: 0.5243\n",
      "259/388, train_loss: 0.2319, step time: 0.5062\n",
      "260/388, train_loss: 0.1077, step time: 0.4897\n",
      "261/388, train_loss: 0.1705, step time: 0.4928\n",
      "262/388, train_loss: 0.1848, step time: 0.4757\n",
      "263/388, train_loss: 0.1783, step time: 0.4739\n",
      "264/388, train_loss: 0.4969, step time: 0.6002\n",
      "265/388, train_loss: 0.1621, step time: 0.5555\n",
      "266/388, train_loss: 0.1209, step time: 0.5278\n",
      "267/388, train_loss: 0.1957, step time: 0.5085\n",
      "268/388, train_loss: 0.2758, step time: 0.4996\n",
      "269/388, train_loss: 0.2988, step time: 0.5059\n",
      "270/388, train_loss: 0.2020, step time: 0.5017\n",
      "271/388, train_loss: 0.2479, step time: 0.4994\n",
      "272/388, train_loss: 0.2159, step time: 0.5025\n",
      "273/388, train_loss: 0.1097, step time: 0.4855\n",
      "274/388, train_loss: 0.1723, step time: 0.4940\n",
      "275/388, train_loss: 0.1659, step time: 1.0081\n",
      "276/388, train_loss: 0.1821, step time: 0.5413\n",
      "277/388, train_loss: 0.0850, step time: 0.5166\n",
      "278/388, train_loss: 0.0958, step time: 0.4970\n",
      "279/388, train_loss: 0.1225, step time: 0.5183\n",
      "280/388, train_loss: 0.2497, step time: 0.5017\n",
      "281/388, train_loss: 0.1648, step time: 0.4994\n",
      "282/388, train_loss: 0.2528, step time: 0.4847\n",
      "283/388, train_loss: 0.1479, step time: 0.5123\n",
      "284/388, train_loss: 0.2189, step time: 0.4907\n",
      "285/388, train_loss: 0.2147, step time: 0.4865\n",
      "286/388, train_loss: 0.3368, step time: 0.4797\n",
      "287/388, train_loss: 0.0676, step time: 0.4947\n",
      "288/388, train_loss: 0.1841, step time: 1.1602\n",
      "289/388, train_loss: 0.0441, step time: 0.5402\n",
      "290/388, train_loss: 0.1215, step time: 0.5086\n",
      "291/388, train_loss: 0.1457, step time: 0.5058\n",
      "292/388, train_loss: 0.2791, step time: 0.4847\n",
      "293/388, train_loss: 0.5181, step time: 0.4768\n",
      "294/388, train_loss: 0.1915, step time: 0.4984\n",
      "295/388, train_loss: 0.1661, step time: 0.4882\n",
      "296/388, train_loss: 0.2033, step time: 1.1984\n",
      "297/388, train_loss: 0.1845, step time: 0.5301\n",
      "298/388, train_loss: 0.0960, step time: 0.5077\n",
      "299/388, train_loss: 0.2059, step time: 0.4902\n",
      "300/388, train_loss: 0.2464, step time: 0.5018\n",
      "301/388, train_loss: 0.1613, step time: 0.4953\n",
      "302/388, train_loss: 0.1671, step time: 0.4850\n",
      "303/388, train_loss: 0.4141, step time: 1.1743\n",
      "304/388, train_loss: 0.2726, step time: 0.5222\n",
      "305/388, train_loss: 0.1510, step time: 0.5109\n",
      "306/388, train_loss: 0.1712, step time: 0.4856\n",
      "307/388, train_loss: 0.1153, step time: 0.4869\n",
      "308/388, train_loss: 0.1924, step time: 0.4898\n",
      "309/388, train_loss: 0.4224, step time: 0.4782\n",
      "310/388, train_loss: 0.3558, step time: 0.4851\n",
      "311/388, train_loss: 0.1574, step time: 0.4937\n",
      "312/388, train_loss: 0.0278, step time: 0.4817\n",
      "313/388, train_loss: 0.0660, step time: 0.5057\n",
      "314/388, train_loss: 0.1567, step time: 0.4830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/388, train_loss: 0.1353, step time: 0.4801\n",
      "316/388, train_loss: 0.1436, step time: 0.4824\n",
      "317/388, train_loss: 0.0892, step time: 0.4874\n",
      "318/388, train_loss: 0.3329, step time: 0.5001\n",
      "319/388, train_loss: 0.1637, step time: 0.6896\n",
      "320/388, train_loss: 0.3727, step time: 0.5410\n",
      "321/388, train_loss: 0.2856, step time: 0.5397\n",
      "322/388, train_loss: 0.1823, step time: 0.5291\n",
      "323/388, train_loss: 0.1641, step time: 0.5206\n",
      "324/388, train_loss: 0.2903, step time: 0.5170\n",
      "325/388, train_loss: 0.0840, step time: 0.5058\n",
      "326/388, train_loss: 0.5281, step time: 0.4956\n",
      "327/388, train_loss: 0.2974, step time: 0.4964\n",
      "328/388, train_loss: 0.3065, step time: 0.4834\n",
      "329/388, train_loss: 0.1380, step time: 0.4895\n",
      "330/388, train_loss: 0.1532, step time: 1.0028\n",
      "331/388, train_loss: 0.0698, step time: 0.5353\n",
      "332/388, train_loss: 0.2138, step time: 0.5135\n",
      "333/388, train_loss: 0.1049, step time: 0.4945\n",
      "334/388, train_loss: 0.0755, step time: 0.4981\n",
      "335/388, train_loss: 0.0531, step time: 0.4821\n",
      "336/388, train_loss: 0.1517, step time: 0.4860\n",
      "337/388, train_loss: 0.1361, step time: 0.9857\n",
      "338/388, train_loss: 0.3205, step time: 0.5329\n",
      "339/388, train_loss: 0.1027, step time: 0.5066\n",
      "340/388, train_loss: 0.3025, step time: 0.4971\n",
      "341/388, train_loss: 0.1957, step time: 1.0629\n",
      "342/388, train_loss: 0.0947, step time: 0.5265\n",
      "343/388, train_loss: 0.1695, step time: 0.5034\n",
      "344/388, train_loss: 0.0843, step time: 0.4993\n",
      "345/388, train_loss: 0.0480, step time: 0.4893\n",
      "346/388, train_loss: 0.1118, step time: 0.4904\n",
      "347/388, train_loss: 0.3819, step time: 0.4752\n",
      "348/388, train_loss: 0.1095, step time: 0.4749\n",
      "349/388, train_loss: 0.2254, step time: 0.4775\n",
      "350/388, train_loss: 0.1296, step time: 0.5594\n",
      "351/388, train_loss: 0.1206, step time: 0.5085\n",
      "352/388, train_loss: 0.0846, step time: 0.5003\n",
      "353/388, train_loss: 0.2641, step time: 0.5064\n",
      "354/388, train_loss: 0.1516, step time: 0.4962\n",
      "355/388, train_loss: 0.1681, step time: 0.4896\n",
      "356/388, train_loss: 0.0932, step time: 0.4752\n",
      "357/388, train_loss: 0.1543, step time: 0.4764\n",
      "358/388, train_loss: 0.0839, step time: 0.6123\n",
      "359/388, train_loss: 0.1268, step time: 0.5467\n",
      "360/388, train_loss: 0.1929, step time: 0.5127\n",
      "361/388, train_loss: 0.2561, step time: 0.5010\n",
      "362/388, train_loss: 0.1839, step time: 1.0984\n",
      "363/388, train_loss: 0.0824, step time: 0.5301\n",
      "364/388, train_loss: 0.2725, step time: 0.5025\n",
      "365/388, train_loss: 0.4980, step time: 0.4908\n",
      "366/388, train_loss: 0.0598, step time: 0.4949\n",
      "367/388, train_loss: 0.1756, step time: 0.4821\n",
      "368/388, train_loss: 0.2419, step time: 1.0087\n",
      "369/388, train_loss: 0.0936, step time: 0.5202\n",
      "370/388, train_loss: 0.1464, step time: 0.4964\n",
      "371/388, train_loss: 0.1382, step time: 0.4966\n",
      "372/388, train_loss: 0.1409, step time: 0.4828\n",
      "373/388, train_loss: 0.1179, step time: 0.4886\n",
      "374/388, train_loss: 0.0578, step time: 0.4814\n",
      "375/388, train_loss: 0.2347, step time: 0.4764\n",
      "376/388, train_loss: 0.3123, step time: 0.4750\n",
      "377/388, train_loss: 0.1589, step time: 0.9486\n",
      "378/388, train_loss: 0.0978, step time: 0.5259\n",
      "379/388, train_loss: 0.4575, step time: 0.5135\n",
      "380/388, train_loss: 0.1825, step time: 0.4942\n",
      "381/388, train_loss: 0.2568, step time: 0.4903\n",
      "382/388, train_loss: 0.2203, step time: 0.4749\n",
      "383/388, train_loss: 0.0391, step time: 0.6292\n",
      "384/388, train_loss: 0.2938, step time: 0.5404\n",
      "385/388, train_loss: 0.2233, step time: 0.5156\n",
      "386/388, train_loss: 0.1279, step time: 0.4926\n",
      "387/388, train_loss: 0.1991, step time: 0.4819\n",
      "388/388, train_loss: 0.2741, step time: 0.4809\n",
      "epoch 58 average loss: 0.1888\n",
      "saved new best metric model\n",
      "current epoch: 58 current mean dice: 0.7711 tc: 0.8188 wt: 0.9005 et: 0.5940\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 58 is: 301.8759\n",
      "----------\n",
      "epoch 59/300\n",
      "1/388, train_loss: 0.2030, step time: 0.4821\n",
      "2/388, train_loss: 0.1948, step time: 0.4869\n",
      "3/388, train_loss: 0.1559, step time: 1.2901\n",
      "4/388, train_loss: 0.1374, step time: 0.5548\n",
      "5/388, train_loss: 0.5780, step time: 0.5265\n",
      "6/388, train_loss: 0.1789, step time: 0.5001\n",
      "7/388, train_loss: 0.2517, step time: 0.5278\n",
      "8/388, train_loss: 0.0642, step time: 0.5148\n",
      "9/388, train_loss: 0.0965, step time: 0.5372\n",
      "10/388, train_loss: 0.2416, step time: 0.5203\n",
      "11/388, train_loss: 0.0986, step time: 0.6276\n",
      "12/388, train_loss: 0.0410, step time: 0.5748\n",
      "13/388, train_loss: 0.1795, step time: 0.5160\n",
      "14/388, train_loss: 0.1018, step time: 0.4972\n",
      "15/388, train_loss: 0.1262, step time: 0.4963\n",
      "16/388, train_loss: 0.1144, step time: 0.5826\n",
      "17/388, train_loss: 0.0716, step time: 0.5786\n",
      "18/388, train_loss: 0.2181, step time: 0.5358\n",
      "19/388, train_loss: 0.0818, step time: 0.5182\n",
      "20/388, train_loss: 0.2218, step time: 0.6683\n",
      "21/388, train_loss: 0.1436, step time: 0.5870\n",
      "22/388, train_loss: 0.1386, step time: 0.5668\n",
      "23/388, train_loss: 0.0492, step time: 0.5303\n",
      "24/388, train_loss: 0.2805, step time: 0.5057\n",
      "25/388, train_loss: 0.3263, step time: 0.4954\n",
      "26/388, train_loss: 0.2728, step time: 0.4926\n",
      "27/388, train_loss: 0.2840, step time: 0.9556\n",
      "28/388, train_loss: 0.1980, step time: 0.5520\n",
      "29/388, train_loss: 0.5044, step time: 0.5414\n",
      "30/388, train_loss: 0.2163, step time: 0.5156\n",
      "31/388, train_loss: 0.2784, step time: 0.4974\n",
      "32/388, train_loss: 0.2678, step time: 1.1811\n",
      "33/388, train_loss: 0.2157, step time: 0.5513\n",
      "34/388, train_loss: 0.3203, step time: 0.5288\n",
      "35/388, train_loss: 0.0836, step time: 0.5006\n",
      "36/388, train_loss: 0.1588, step time: 0.4961\n",
      "37/388, train_loss: 0.6910, step time: 0.4924\n",
      "38/388, train_loss: 0.1416, step time: 1.1515\n",
      "39/388, train_loss: 0.3901, step time: 0.5570\n",
      "40/388, train_loss: 0.0794, step time: 0.5244\n",
      "41/388, train_loss: 0.4258, step time: 0.5103\n",
      "42/388, train_loss: 0.2773, step time: 0.4908\n",
      "43/388, train_loss: 0.1595, step time: 0.4938\n",
      "44/388, train_loss: 0.2329, step time: 0.5376\n",
      "45/388, train_loss: 0.2793, step time: 0.5015\n",
      "46/388, train_loss: 0.1189, step time: 0.4888\n",
      "47/388, train_loss: 0.2393, step time: 1.0535\n",
      "48/388, train_loss: 0.1862, step time: 0.5565\n",
      "49/388, train_loss: 0.0827, step time: 0.5202\n",
      "50/388, train_loss: 0.2168, step time: 0.5079\n",
      "51/388, train_loss: 0.1871, step time: 0.4897\n",
      "52/388, train_loss: 0.2834, step time: 0.4832\n",
      "53/388, train_loss: 0.1258, step time: 1.0056\n",
      "54/388, train_loss: 0.2978, step time: 0.5524\n",
      "55/388, train_loss: 0.1147, step time: 0.5186\n",
      "56/388, train_loss: 0.0559, step time: 0.4955\n",
      "57/388, train_loss: 0.2536, step time: 0.4989\n",
      "58/388, train_loss: 0.0587, step time: 0.4880\n",
      "59/388, train_loss: 0.0739, step time: 0.5009\n",
      "60/388, train_loss: 0.4873, step time: 0.4804\n",
      "61/388, train_loss: 0.1255, step time: 1.0282\n",
      "62/388, train_loss: 0.1972, step time: 0.5464\n",
      "63/388, train_loss: 0.2245, step time: 0.5195\n",
      "64/388, train_loss: 0.2797, step time: 0.4930\n",
      "65/388, train_loss: 0.1086, step time: 0.4958\n",
      "66/388, train_loss: 0.1962, step time: 0.4843\n",
      "67/388, train_loss: 0.2976, step time: 0.4935\n",
      "68/388, train_loss: 0.0873, step time: 0.4890\n",
      "69/388, train_loss: 0.1121, step time: 0.5362\n",
      "70/388, train_loss: 0.1593, step time: 0.5312\n",
      "71/388, train_loss: 0.2067, step time: 0.5095\n",
      "72/388, train_loss: 0.2178, step time: 1.1327\n",
      "73/388, train_loss: 0.1779, step time: 0.5418\n",
      "74/388, train_loss: 0.0768, step time: 0.5072\n",
      "75/388, train_loss: 0.1350, step time: 0.5395\n",
      "76/388, train_loss: 0.0505, step time: 0.5238\n",
      "77/388, train_loss: 0.1947, step time: 0.4977\n",
      "78/388, train_loss: 0.1136, step time: 0.4809\n",
      "79/388, train_loss: 0.0526, step time: 1.1670\n",
      "80/388, train_loss: 0.2993, step time: 0.5365\n",
      "81/388, train_loss: 0.2566, step time: 0.5162\n",
      "82/388, train_loss: 0.1638, step time: 0.4965\n",
      "83/388, train_loss: 0.1904, step time: 0.4993\n",
      "84/388, train_loss: 0.0914, step time: 0.4835\n",
      "85/388, train_loss: 0.1247, step time: 0.5514\n",
      "86/388, train_loss: 0.1741, step time: 0.6040\n",
      "87/388, train_loss: 0.0353, step time: 0.5445\n",
      "88/388, train_loss: 0.0895, step time: 0.5151\n",
      "89/388, train_loss: 0.0591, step time: 0.5514\n",
      "90/388, train_loss: 0.0945, step time: 0.5305\n",
      "91/388, train_loss: 0.2507, step time: 0.4957\n",
      "92/388, train_loss: 0.3463, step time: 0.5065\n",
      "93/388, train_loss: 0.4921, step time: 0.4828\n",
      "94/388, train_loss: 0.1692, step time: 0.9546\n",
      "95/388, train_loss: 0.3088, step time: 0.5429\n",
      "96/388, train_loss: 0.0865, step time: 0.5101\n",
      "97/388, train_loss: 0.1763, step time: 0.5002\n",
      "98/388, train_loss: 0.1315, step time: 0.4859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/388, train_loss: 0.2614, step time: 0.4939\n",
      "100/388, train_loss: 0.0723, step time: 1.1673\n",
      "101/388, train_loss: 0.1444, step time: 0.5352\n",
      "102/388, train_loss: 0.1684, step time: 0.5163\n",
      "103/388, train_loss: 0.1609, step time: 0.5022\n",
      "104/388, train_loss: 0.1384, step time: 0.4905\n",
      "105/388, train_loss: 0.1090, step time: 0.4876\n",
      "106/388, train_loss: 0.1235, step time: 0.4909\n",
      "107/388, train_loss: 0.1832, step time: 0.4868\n",
      "108/388, train_loss: 0.3097, step time: 0.5121\n",
      "109/388, train_loss: 0.1442, step time: 0.5042\n",
      "110/388, train_loss: 0.2945, step time: 0.5026\n",
      "111/388, train_loss: 0.0772, step time: 0.5677\n",
      "112/388, train_loss: 0.1923, step time: 0.5247\n",
      "113/388, train_loss: 0.1301, step time: 0.4889\n",
      "114/388, train_loss: 0.2153, step time: 0.4848\n",
      "115/388, train_loss: 0.3351, step time: 0.5020\n",
      "116/388, train_loss: 0.1680, step time: 0.4918\n",
      "117/388, train_loss: 0.3604, step time: 1.1585\n",
      "118/388, train_loss: 0.1327, step time: 0.5281\n",
      "119/388, train_loss: 0.2664, step time: 0.5229\n",
      "120/388, train_loss: 0.1664, step time: 0.4965\n",
      "121/388, train_loss: 0.1915, step time: 0.5002\n",
      "122/388, train_loss: 0.0712, step time: 0.9519\n",
      "123/388, train_loss: 0.0809, step time: 0.5305\n",
      "124/388, train_loss: 0.1976, step time: 0.5141\n",
      "125/388, train_loss: 0.1084, step time: 0.4836\n",
      "126/388, train_loss: 0.1362, step time: 0.4790\n",
      "127/388, train_loss: 0.1395, step time: 0.4839\n",
      "128/388, train_loss: 0.0632, step time: 0.4734\n",
      "129/388, train_loss: 0.1506, step time: 0.4761\n",
      "130/388, train_loss: 0.3628, step time: 0.4841\n",
      "131/388, train_loss: 0.2264, step time: 0.4920\n",
      "132/388, train_loss: 0.1000, step time: 0.5522\n",
      "133/388, train_loss: 0.0914, step time: 0.5232\n",
      "134/388, train_loss: 0.1366, step time: 0.4929\n",
      "135/388, train_loss: 0.1665, step time: 0.4946\n",
      "136/388, train_loss: 0.1082, step time: 0.4986\n",
      "137/388, train_loss: 0.1114, step time: 0.5086\n",
      "138/388, train_loss: 0.1981, step time: 0.5475\n",
      "139/388, train_loss: 0.1980, step time: 0.5416\n",
      "140/388, train_loss: 0.0895, step time: 0.5211\n",
      "141/388, train_loss: 0.0943, step time: 0.4859\n",
      "142/388, train_loss: 0.2735, step time: 1.0687\n",
      "143/388, train_loss: 0.0991, step time: 0.5331\n",
      "144/388, train_loss: 0.0780, step time: 0.5163\n",
      "145/388, train_loss: 0.1348, step time: 0.5026\n",
      "146/388, train_loss: 0.2771, step time: 0.5011\n",
      "147/388, train_loss: 0.1029, step time: 0.5209\n",
      "148/388, train_loss: 0.1446, step time: 0.5093\n",
      "149/388, train_loss: 0.1277, step time: 0.4884\n",
      "150/388, train_loss: 0.2825, step time: 0.5009\n",
      "151/388, train_loss: 0.3320, step time: 0.4853\n",
      "152/388, train_loss: 0.0646, step time: 0.6148\n",
      "153/388, train_loss: 0.0891, step time: 0.5312\n",
      "154/388, train_loss: 0.0956, step time: 0.5133\n",
      "155/388, train_loss: 0.1400, step time: 0.4987\n",
      "156/388, train_loss: 0.2087, step time: 1.1291\n",
      "157/388, train_loss: 0.0613, step time: 0.5227\n",
      "158/388, train_loss: 0.3090, step time: 0.5061\n",
      "159/388, train_loss: 0.0875, step time: 0.4957\n",
      "160/388, train_loss: 0.4129, step time: 0.4831\n",
      "161/388, train_loss: 0.2031, step time: 0.4972\n",
      "162/388, train_loss: 0.1921, step time: 0.4992\n",
      "163/388, train_loss: 0.0960, step time: 0.4914\n",
      "164/388, train_loss: 0.0820, step time: 0.4898\n",
      "165/388, train_loss: 0.2095, step time: 0.4876\n",
      "166/388, train_loss: 0.2761, step time: 0.5296\n",
      "167/388, train_loss: 0.1538, step time: 0.5113\n",
      "168/388, train_loss: 0.0956, step time: 0.4996\n",
      "169/388, train_loss: 0.1987, step time: 0.4840\n",
      "170/388, train_loss: 0.2028, step time: 0.5137\n",
      "171/388, train_loss: 0.1657, step time: 0.4931\n",
      "172/388, train_loss: 0.1662, step time: 0.5859\n",
      "173/388, train_loss: 0.1750, step time: 0.5772\n",
      "174/388, train_loss: 0.1648, step time: 0.5330\n",
      "175/388, train_loss: 0.2121, step time: 0.5156\n",
      "176/388, train_loss: 0.2271, step time: 0.5365\n",
      "177/388, train_loss: 0.1580, step time: 0.5270\n",
      "178/388, train_loss: 0.1610, step time: 0.5227\n",
      "179/388, train_loss: 0.0781, step time: 0.6350\n",
      "180/388, train_loss: 0.2413, step time: 0.5509\n",
      "181/388, train_loss: 0.0420, step time: 0.5151\n",
      "182/388, train_loss: 0.1097, step time: 0.5024\n",
      "183/388, train_loss: 0.2413, step time: 0.5039\n",
      "184/388, train_loss: 0.0891, step time: 0.6651\n",
      "185/388, train_loss: 0.3343, step time: 0.6368\n",
      "186/388, train_loss: 0.2962, step time: 0.5532\n",
      "187/388, train_loss: 0.0731, step time: 0.5184\n",
      "188/388, train_loss: 0.1388, step time: 0.5174\n",
      "189/388, train_loss: 0.3820, step time: 0.5117\n",
      "190/388, train_loss: 0.1380, step time: 0.5060\n",
      "191/388, train_loss: 0.2644, step time: 0.4964\n",
      "192/388, train_loss: 0.0815, step time: 0.4850\n",
      "193/388, train_loss: 0.1593, step time: 0.4914\n",
      "194/388, train_loss: 0.1068, step time: 0.4734\n",
      "195/388, train_loss: 0.1810, step time: 0.4761\n",
      "196/388, train_loss: 0.2862, step time: 0.5310\n",
      "197/388, train_loss: 0.1293, step time: 0.5137\n",
      "198/388, train_loss: 0.5197, step time: 0.7411\n",
      "199/388, train_loss: 0.0892, step time: 0.6091\n",
      "200/388, train_loss: 0.2163, step time: 0.5346\n",
      "201/388, train_loss: 0.1369, step time: 0.5122\n",
      "202/388, train_loss: 0.1330, step time: 0.5047\n",
      "203/388, train_loss: 0.1524, step time: 0.4998\n",
      "204/388, train_loss: 0.1705, step time: 0.4858\n",
      "205/388, train_loss: 0.1858, step time: 0.5017\n",
      "206/388, train_loss: 0.3564, step time: 0.5045\n",
      "207/388, train_loss: 0.2117, step time: 0.5747\n",
      "208/388, train_loss: 0.1986, step time: 0.5020\n",
      "209/388, train_loss: 0.3348, step time: 0.4959\n",
      "210/388, train_loss: 0.0743, step time: 0.4994\n",
      "211/388, train_loss: 0.1613, step time: 0.4972\n",
      "212/388, train_loss: 0.0626, step time: 1.0331\n",
      "213/388, train_loss: 0.1483, step time: 0.5424\n",
      "214/388, train_loss: 0.0783, step time: 0.5150\n",
      "215/388, train_loss: 0.1621, step time: 0.4961\n",
      "216/388, train_loss: 0.0870, step time: 0.4967\n",
      "217/388, train_loss: 0.1356, step time: 0.4774\n",
      "218/388, train_loss: 0.1958, step time: 0.4886\n",
      "219/388, train_loss: 0.2311, step time: 0.5303\n",
      "220/388, train_loss: 0.1030, step time: 0.5133\n",
      "221/388, train_loss: 0.0891, step time: 0.5256\n",
      "222/388, train_loss: 0.0871, step time: 0.5406\n",
      "223/388, train_loss: 0.0725, step time: 0.5950\n",
      "224/388, train_loss: 0.2745, step time: 0.5630\n",
      "225/388, train_loss: 0.1280, step time: 0.5548\n",
      "226/388, train_loss: 0.1319, step time: 0.5421\n",
      "227/388, train_loss: 0.3066, step time: 0.5177\n",
      "228/388, train_loss: 0.0991, step time: 0.5053\n",
      "229/388, train_loss: 0.0928, step time: 0.5364\n",
      "230/388, train_loss: 0.2790, step time: 0.5381\n",
      "231/388, train_loss: 0.0857, step time: 0.6206\n",
      "232/388, train_loss: 0.1777, step time: 0.5604\n",
      "233/388, train_loss: 0.0655, step time: 0.5372\n",
      "234/388, train_loss: 0.0910, step time: 0.5146\n",
      "235/388, train_loss: 0.0442, step time: 0.5309\n",
      "236/388, train_loss: 0.4174, step time: 0.5341\n",
      "237/388, train_loss: 0.2559, step time: 0.5451\n",
      "238/388, train_loss: 0.2436, step time: 0.5147\n",
      "239/388, train_loss: 0.0961, step time: 0.4988\n",
      "240/388, train_loss: 0.2818, step time: 0.5089\n",
      "241/388, train_loss: 0.1527, step time: 0.5187\n",
      "242/388, train_loss: 0.1235, step time: 0.4933\n",
      "243/388, train_loss: 0.1778, step time: 0.4976\n",
      "244/388, train_loss: 0.2064, step time: 0.5699\n",
      "245/388, train_loss: 0.0912, step time: 0.5281\n",
      "246/388, train_loss: 0.3973, step time: 0.5093\n",
      "247/388, train_loss: 0.1698, step time: 0.4991\n",
      "248/388, train_loss: 0.2684, step time: 0.5454\n",
      "249/388, train_loss: 0.1267, step time: 0.5362\n",
      "250/388, train_loss: 0.1594, step time: 0.5124\n",
      "251/388, train_loss: 0.1108, step time: 0.4896\n",
      "252/388, train_loss: 0.1023, step time: 0.4930\n",
      "253/388, train_loss: 0.1625, step time: 0.4928\n",
      "254/388, train_loss: 0.2529, step time: 1.1437\n",
      "255/388, train_loss: 0.1798, step time: 0.5320\n",
      "256/388, train_loss: 0.1839, step time: 0.5069\n",
      "257/388, train_loss: 0.1365, step time: 0.4896\n",
      "258/388, train_loss: 0.0944, step time: 0.4969\n",
      "259/388, train_loss: 0.0676, step time: 0.4814\n",
      "260/388, train_loss: 0.5292, step time: 0.5010\n",
      "261/388, train_loss: 0.1106, step time: 0.4929\n",
      "262/388, train_loss: 0.2073, step time: 0.4775\n",
      "263/388, train_loss: 0.1205, step time: 0.6025\n",
      "264/388, train_loss: 0.3478, step time: 0.5547\n",
      "265/388, train_loss: 0.0821, step time: 0.5247\n",
      "266/388, train_loss: 0.0799, step time: 0.5084\n",
      "267/388, train_loss: 0.1303, step time: 0.5007\n",
      "268/388, train_loss: 0.1545, step time: 0.4900\n",
      "269/388, train_loss: 0.4954, step time: 0.5392\n",
      "270/388, train_loss: 0.2049, step time: 0.5055\n",
      "271/388, train_loss: 0.1155, step time: 0.4962\n",
      "272/388, train_loss: 0.1770, step time: 0.4788\n",
      "273/388, train_loss: 0.1057, step time: 0.5120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/388, train_loss: 0.1756, step time: 0.4872\n",
      "275/388, train_loss: 0.2886, step time: 1.1539\n",
      "276/388, train_loss: 0.1374, step time: 0.5349\n",
      "277/388, train_loss: 0.1910, step time: 0.5078\n",
      "278/388, train_loss: 0.1281, step time: 0.4917\n",
      "279/388, train_loss: 0.1041, step time: 0.5000\n",
      "280/388, train_loss: 0.1117, step time: 0.4827\n",
      "281/388, train_loss: 0.1388, step time: 0.5071\n",
      "282/388, train_loss: 0.2626, step time: 0.4977\n",
      "283/388, train_loss: 0.0886, step time: 0.4863\n",
      "284/388, train_loss: 0.1998, step time: 1.2338\n",
      "285/388, train_loss: 0.1566, step time: 0.5418\n",
      "286/388, train_loss: 0.1096, step time: 0.5209\n",
      "287/388, train_loss: 0.1657, step time: 0.5122\n",
      "288/388, train_loss: 0.2522, step time: 0.5048\n",
      "289/388, train_loss: 0.0386, step time: 0.4999\n",
      "290/388, train_loss: 0.2452, step time: 0.5003\n",
      "291/388, train_loss: 0.2544, step time: 0.5006\n",
      "292/388, train_loss: 0.2267, step time: 0.4912\n",
      "293/388, train_loss: 0.4119, step time: 0.4939\n",
      "294/388, train_loss: 0.2177, step time: 0.4774\n",
      "295/388, train_loss: 0.2424, step time: 0.7828\n",
      "296/388, train_loss: 0.1940, step time: 0.5527\n",
      "297/388, train_loss: 0.1215, step time: 0.5225\n",
      "298/388, train_loss: 0.1039, step time: 0.5124\n",
      "299/388, train_loss: 0.0942, step time: 0.4955\n",
      "300/388, train_loss: 0.1172, step time: 0.4967\n",
      "301/388, train_loss: 0.5048, step time: 0.4845\n",
      "302/388, train_loss: 0.2791, step time: 0.4814\n",
      "303/388, train_loss: 0.3640, step time: 0.4799\n",
      "304/388, train_loss: 0.1673, step time: 0.9787\n",
      "305/388, train_loss: 0.1249, step time: 0.5274\n",
      "306/388, train_loss: 0.2093, step time: 0.5002\n",
      "307/388, train_loss: 0.1891, step time: 0.4879\n",
      "308/388, train_loss: 0.3842, step time: 0.4829\n",
      "309/388, train_loss: 0.1695, step time: 0.4913\n",
      "310/388, train_loss: 0.2166, step time: 0.5040\n",
      "311/388, train_loss: 0.1513, step time: 0.4997\n",
      "312/388, train_loss: 0.1058, step time: 0.4904\n",
      "313/388, train_loss: 0.1076, step time: 0.5253\n",
      "314/388, train_loss: 0.2235, step time: 0.5033\n",
      "315/388, train_loss: 0.4469, step time: 0.5103\n",
      "316/388, train_loss: 0.1452, step time: 0.4886\n",
      "317/388, train_loss: 0.1416, step time: 0.4884\n",
      "318/388, train_loss: 0.3207, step time: 0.4963\n",
      "319/388, train_loss: 0.0839, step time: 0.4810\n",
      "320/388, train_loss: 0.0824, step time: 0.4856\n",
      "321/388, train_loss: 0.1110, step time: 0.5556\n",
      "322/388, train_loss: 0.1088, step time: 0.5237\n",
      "323/388, train_loss: 0.1003, step time: 0.5070\n",
      "324/388, train_loss: 0.2631, step time: 0.4860\n",
      "325/388, train_loss: 0.0502, step time: 0.4859\n",
      "326/388, train_loss: 0.2901, step time: 0.5050\n",
      "327/388, train_loss: 0.0729, step time: 0.9509\n",
      "328/388, train_loss: 0.2199, step time: 0.5364\n",
      "329/388, train_loss: 0.1247, step time: 0.5052\n",
      "330/388, train_loss: 0.2755, step time: 0.4935\n",
      "331/388, train_loss: 0.3147, step time: 0.5039\n",
      "332/388, train_loss: 0.0277, step time: 0.4813\n",
      "333/388, train_loss: 0.1323, step time: 0.5011\n",
      "334/388, train_loss: 0.1925, step time: 0.4894\n",
      "335/388, train_loss: 0.1121, step time: 0.4931\n",
      "336/388, train_loss: 0.1203, step time: 0.4729\n",
      "337/388, train_loss: 0.2022, step time: 0.4748\n",
      "338/388, train_loss: 0.0932, step time: 0.7943\n",
      "339/388, train_loss: 0.3741, step time: 0.5434\n",
      "340/388, train_loss: 0.1559, step time: 0.5251\n",
      "341/388, train_loss: 0.1143, step time: 0.4919\n",
      "342/388, train_loss: 0.3358, step time: 0.4920\n",
      "343/388, train_loss: 0.1394, step time: 1.1457\n",
      "344/388, train_loss: 0.1293, step time: 0.5358\n",
      "345/388, train_loss: 0.5114, step time: 0.5046\n",
      "346/388, train_loss: 0.1963, step time: 0.4973\n",
      "347/388, train_loss: 0.1760, step time: 0.4922\n",
      "348/388, train_loss: 0.0521, step time: 0.4851\n",
      "349/388, train_loss: 0.0908, step time: 0.4871\n",
      "350/388, train_loss: 0.0676, step time: 0.4810\n",
      "351/388, train_loss: 0.1421, step time: 0.4782\n",
      "352/388, train_loss: 0.2958, step time: 1.1515\n",
      "353/388, train_loss: 0.0642, step time: 0.5335\n",
      "354/388, train_loss: 0.3598, step time: 0.4936\n",
      "355/388, train_loss: 0.2315, step time: 0.4852\n",
      "356/388, train_loss: 0.2073, step time: 0.4927\n",
      "357/388, train_loss: 0.3594, step time: 0.4748\n",
      "358/388, train_loss: 0.1211, step time: 0.4806\n",
      "359/388, train_loss: 0.2196, step time: 0.4917\n",
      "360/388, train_loss: 0.3703, step time: 0.4793\n",
      "361/388, train_loss: 0.2365, step time: 1.1016\n",
      "362/388, train_loss: 0.1595, step time: 0.5415\n",
      "363/388, train_loss: 0.3286, step time: 0.5089\n",
      "364/388, train_loss: 0.2229, step time: 0.4988\n",
      "365/388, train_loss: 0.1533, step time: 0.4913\n",
      "366/388, train_loss: 0.0767, step time: 0.4836\n",
      "367/388, train_loss: 0.1047, step time: 0.4844\n",
      "368/388, train_loss: 0.1618, step time: 0.4748\n",
      "369/388, train_loss: 0.1659, step time: 0.4792\n",
      "370/388, train_loss: 0.2032, step time: 0.4710\n",
      "371/388, train_loss: 0.1197, step time: 0.9467\n",
      "372/388, train_loss: 0.2382, step time: 0.5415\n",
      "373/388, train_loss: 0.0860, step time: 0.5200\n",
      "374/388, train_loss: 0.0958, step time: 0.4985\n",
      "375/388, train_loss: 0.1168, step time: 0.4857\n",
      "376/388, train_loss: 0.2090, step time: 0.4846\n",
      "377/388, train_loss: 0.0725, step time: 0.4899\n",
      "378/388, train_loss: 0.1641, step time: 0.4744\n",
      "379/388, train_loss: 0.3429, step time: 0.4793\n",
      "380/388, train_loss: 0.0654, step time: 0.9449\n",
      "381/388, train_loss: 0.1101, step time: 0.5390\n",
      "382/388, train_loss: 0.0988, step time: 0.5033\n",
      "383/388, train_loss: 0.1130, step time: 0.4838\n",
      "384/388, train_loss: 0.2128, step time: 0.4790\n",
      "385/388, train_loss: 0.1309, step time: 0.4949\n",
      "386/388, train_loss: 0.1063, step time: 0.4759\n",
      "387/388, train_loss: 0.3148, step time: 0.4711\n",
      "388/388, train_loss: 0.2655, step time: 0.4667\n",
      "epoch 59 average loss: 0.1830\n",
      "current epoch: 59 current mean dice: 0.7559 tc: 0.8019 wt: 0.8957 et: 0.5700\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 59 is: 304.0680\n",
      "----------\n",
      "epoch 60/300\n",
      "1/388, train_loss: 0.1568, step time: 0.4776\n",
      "2/388, train_loss: 0.0345, step time: 1.0366\n",
      "3/388, train_loss: 0.1767, step time: 0.5667\n",
      "4/388, train_loss: 0.0808, step time: 0.5326\n",
      "5/388, train_loss: 0.0750, step time: 0.5041\n",
      "6/388, train_loss: 0.0967, step time: 0.4995\n",
      "7/388, train_loss: 0.1589, step time: 0.4836\n",
      "8/388, train_loss: 0.1526, step time: 0.9960\n",
      "9/388, train_loss: 0.2919, step time: 0.5321\n",
      "10/388, train_loss: 0.1852, step time: 0.5130\n",
      "11/388, train_loss: 0.1298, step time: 0.5143\n",
      "12/388, train_loss: 0.1309, step time: 0.5400\n",
      "13/388, train_loss: 0.1418, step time: 0.5238\n",
      "14/388, train_loss: 0.1211, step time: 0.5082\n",
      "15/388, train_loss: 0.1032, step time: 0.5371\n",
      "16/388, train_loss: 0.0757, step time: 0.6737\n",
      "17/388, train_loss: 0.2553, step time: 0.5354\n",
      "18/388, train_loss: 0.0686, step time: 0.5028\n",
      "19/388, train_loss: 0.2598, step time: 0.5029\n",
      "20/388, train_loss: 0.1268, step time: 0.5615\n",
      "21/388, train_loss: 0.0829, step time: 0.5109\n",
      "22/388, train_loss: 0.0946, step time: 0.5035\n",
      "23/388, train_loss: 0.1541, step time: 0.5111\n",
      "24/388, train_loss: 0.1163, step time: 0.5935\n",
      "25/388, train_loss: 0.2168, step time: 0.5416\n",
      "26/388, train_loss: 0.1173, step time: 0.5211\n",
      "27/388, train_loss: 0.1757, step time: 0.4958\n",
      "28/388, train_loss: 0.2706, step time: 0.5137\n",
      "29/388, train_loss: 0.4454, step time: 0.5665\n",
      "30/388, train_loss: 0.3948, step time: 0.5362\n",
      "31/388, train_loss: 0.0554, step time: 0.5120\n",
      "32/388, train_loss: 0.1306, step time: 0.4977\n",
      "33/388, train_loss: 0.1799, step time: 0.5002\n",
      "34/388, train_loss: 0.0941, step time: 0.5092\n",
      "35/388, train_loss: 0.1099, step time: 0.5019\n",
      "36/388, train_loss: 0.2302, step time: 0.4922\n",
      "37/388, train_loss: 0.2372, step time: 0.5427\n",
      "38/388, train_loss: 0.0816, step time: 0.5361\n",
      "39/388, train_loss: 0.1845, step time: 0.5155\n",
      "40/388, train_loss: 0.0442, step time: 0.4878\n",
      "41/388, train_loss: 0.2926, step time: 1.1596\n",
      "42/388, train_loss: 0.1753, step time: 0.5424\n",
      "43/388, train_loss: 0.2879, step time: 0.5216\n",
      "44/388, train_loss: 0.1315, step time: 0.5017\n",
      "45/388, train_loss: 0.1317, step time: 0.4982\n",
      "46/388, train_loss: 0.0879, step time: 0.4952\n",
      "47/388, train_loss: 0.1295, step time: 0.4938\n",
      "48/388, train_loss: 0.1141, step time: 0.4932\n",
      "49/388, train_loss: 0.1084, step time: 0.5226\n",
      "50/388, train_loss: 0.1041, step time: 0.4908\n",
      "51/388, train_loss: 0.3012, step time: 0.5012\n",
      "52/388, train_loss: 0.1013, step time: 0.5086\n",
      "53/388, train_loss: 0.1375, step time: 0.5208\n",
      "54/388, train_loss: 0.0867, step time: 0.5226\n",
      "55/388, train_loss: 0.0659, step time: 0.5014\n",
      "56/388, train_loss: 0.2706, step time: 0.4858\n",
      "57/388, train_loss: 0.0935, step time: 0.4903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/388, train_loss: 0.3549, step time: 0.5117\n",
      "59/388, train_loss: 0.1822, step time: 0.5333\n",
      "60/388, train_loss: 0.1076, step time: 0.5180\n",
      "61/388, train_loss: 0.1918, step time: 0.5059\n",
      "62/388, train_loss: 0.4972, step time: 0.4903\n",
      "63/388, train_loss: 0.1283, step time: 0.5009\n",
      "64/388, train_loss: 0.1961, step time: 0.4963\n",
      "65/388, train_loss: 0.0726, step time: 0.4856\n",
      "66/388, train_loss: 0.2072, step time: 0.5660\n",
      "67/388, train_loss: 0.1866, step time: 0.5649\n",
      "68/388, train_loss: 0.2767, step time: 0.5929\n",
      "69/388, train_loss: 0.1453, step time: 0.5532\n",
      "70/388, train_loss: 0.2510, step time: 0.5212\n",
      "71/388, train_loss: 0.1493, step time: 0.5337\n",
      "72/388, train_loss: 0.1513, step time: 0.6235\n",
      "73/388, train_loss: 0.3609, step time: 0.5622\n",
      "74/388, train_loss: 0.1253, step time: 0.5190\n",
      "75/388, train_loss: 0.1682, step time: 0.5036\n",
      "76/388, train_loss: 0.3825, step time: 0.5656\n",
      "77/388, train_loss: 0.2996, step time: 0.7038\n",
      "78/388, train_loss: 0.0925, step time: 0.5565\n",
      "79/388, train_loss: 0.1037, step time: 0.5236\n",
      "80/388, train_loss: 0.0784, step time: 0.4977\n",
      "81/388, train_loss: 0.1684, step time: 0.4988\n",
      "82/388, train_loss: 0.2370, step time: 0.5147\n",
      "83/388, train_loss: 0.2369, step time: 0.6059\n",
      "84/388, train_loss: 0.4267, step time: 0.5830\n",
      "85/388, train_loss: 0.2334, step time: 0.5499\n",
      "86/388, train_loss: 0.2137, step time: 0.5099\n",
      "87/388, train_loss: 0.1344, step time: 0.4921\n",
      "88/388, train_loss: 0.0827, step time: 0.4833\n",
      "89/388, train_loss: 0.1923, step time: 1.1975\n",
      "90/388, train_loss: 0.2481, step time: 0.5318\n",
      "91/388, train_loss: 0.3669, step time: 0.5127\n",
      "92/388, train_loss: 0.0717, step time: 0.4909\n",
      "93/388, train_loss: 0.0981, step time: 0.5007\n",
      "94/388, train_loss: 0.1115, step time: 0.4808\n",
      "95/388, train_loss: 0.1544, step time: 0.5058\n",
      "96/388, train_loss: 0.1948, step time: 0.5758\n",
      "97/388, train_loss: 0.2489, step time: 0.5292\n",
      "98/388, train_loss: 0.2016, step time: 0.5105\n",
      "99/388, train_loss: 0.1305, step time: 0.4951\n",
      "100/388, train_loss: 0.2234, step time: 0.4950\n",
      "101/388, train_loss: 0.1138, step time: 0.4795\n",
      "102/388, train_loss: 0.2715, step time: 0.5198\n",
      "103/388, train_loss: 0.1540, step time: 0.5151\n",
      "104/388, train_loss: 0.2780, step time: 0.5116\n",
      "105/388, train_loss: 0.0489, step time: 0.4996\n",
      "106/388, train_loss: 0.2268, step time: 0.5534\n",
      "107/388, train_loss: 0.0992, step time: 0.5385\n",
      "108/388, train_loss: 0.4685, step time: 0.5069\n",
      "109/388, train_loss: 0.1018, step time: 0.5134\n",
      "110/388, train_loss: 0.0464, step time: 0.5532\n",
      "111/388, train_loss: 0.2672, step time: 0.5313\n",
      "112/388, train_loss: 0.2714, step time: 0.5033\n",
      "113/388, train_loss: 0.0395, step time: 0.4916\n",
      "114/388, train_loss: 0.2899, step time: 1.1984\n",
      "115/388, train_loss: 0.0887, step time: 0.5402\n",
      "116/388, train_loss: 0.1869, step time: 0.5088\n",
      "117/388, train_loss: 0.1377, step time: 0.5040\n",
      "118/388, train_loss: 0.1871, step time: 0.4871\n",
      "119/388, train_loss: 0.1041, step time: 0.4858\n",
      "120/388, train_loss: 0.1735, step time: 1.1301\n",
      "121/388, train_loss: 0.1240, step time: 0.5176\n",
      "122/388, train_loss: 0.1285, step time: 0.4982\n",
      "123/388, train_loss: 0.2962, step time: 0.4937\n",
      "124/388, train_loss: 0.5451, step time: 0.4838\n",
      "125/388, train_loss: 0.0970, step time: 0.5076\n",
      "126/388, train_loss: 0.0648, step time: 0.5199\n",
      "127/388, train_loss: 0.1253, step time: 0.5010\n",
      "128/388, train_loss: 0.1876, step time: 0.5027\n",
      "129/388, train_loss: 0.0974, step time: 0.5671\n",
      "130/388, train_loss: 0.1273, step time: 0.5199\n",
      "131/388, train_loss: 0.1918, step time: 0.4952\n",
      "132/388, train_loss: 0.1007, step time: 0.5002\n",
      "133/388, train_loss: 0.0658, step time: 0.5018\n",
      "134/388, train_loss: 0.0890, step time: 0.5139\n",
      "135/388, train_loss: 0.4600, step time: 0.5053\n",
      "136/388, train_loss: 0.0955, step time: 0.4937\n",
      "137/388, train_loss: 0.0965, step time: 0.4875\n",
      "138/388, train_loss: 0.1699, step time: 0.6737\n",
      "139/388, train_loss: 0.2873, step time: 0.5896\n",
      "140/388, train_loss: 0.0843, step time: 0.5482\n",
      "141/388, train_loss: 0.2674, step time: 0.5200\n",
      "142/388, train_loss: 0.1198, step time: 0.5036\n",
      "143/388, train_loss: 0.3281, step time: 0.4993\n",
      "144/388, train_loss: 0.1499, step time: 0.4866\n",
      "145/388, train_loss: 0.3269, step time: 0.4991\n",
      "146/388, train_loss: 0.1188, step time: 0.5189\n",
      "147/388, train_loss: 0.1116, step time: 0.5281\n",
      "148/388, train_loss: 0.1728, step time: 0.5046\n",
      "149/388, train_loss: 0.1999, step time: 0.4998\n",
      "150/388, train_loss: 0.0832, step time: 0.4772\n",
      "151/388, train_loss: 0.2630, step time: 0.4918\n",
      "152/388, train_loss: 0.0986, step time: 1.1662\n",
      "153/388, train_loss: 0.1526, step time: 0.5377\n",
      "154/388, train_loss: 0.0986, step time: 0.5172\n",
      "155/388, train_loss: 0.1954, step time: 0.4973\n",
      "156/388, train_loss: 0.2556, step time: 0.4970\n",
      "157/388, train_loss: 0.1299, step time: 0.4803\n",
      "158/388, train_loss: 0.1441, step time: 0.4911\n",
      "159/388, train_loss: 0.1376, step time: 0.8123\n",
      "160/388, train_loss: 0.0574, step time: 0.5499\n",
      "161/388, train_loss: 0.0846, step time: 0.5203\n",
      "162/388, train_loss: 0.2996, step time: 0.4921\n",
      "163/388, train_loss: 0.1460, step time: 0.4927\n",
      "164/388, train_loss: 0.1978, step time: 0.4958\n",
      "165/388, train_loss: 0.2011, step time: 0.5042\n",
      "166/388, train_loss: 0.2801, step time: 0.4960\n",
      "167/388, train_loss: 0.4781, step time: 0.5705\n",
      "168/388, train_loss: 0.1986, step time: 0.5563\n",
      "169/388, train_loss: 0.0836, step time: 0.5427\n",
      "170/388, train_loss: 0.1288, step time: 0.5337\n",
      "171/388, train_loss: 0.1053, step time: 0.5214\n",
      "172/388, train_loss: 0.2249, step time: 0.5162\n",
      "173/388, train_loss: 0.0697, step time: 1.0701\n",
      "174/388, train_loss: 0.2742, step time: 0.5286\n",
      "175/388, train_loss: 0.0837, step time: 0.5012\n",
      "176/388, train_loss: 0.1762, step time: 0.4985\n",
      "177/388, train_loss: 0.0672, step time: 0.4959\n",
      "178/388, train_loss: 0.1674, step time: 0.4844\n",
      "179/388, train_loss: 0.1601, step time: 0.4947\n",
      "180/388, train_loss: 0.3737, step time: 0.4961\n",
      "181/388, train_loss: 0.1640, step time: 0.4993\n",
      "182/388, train_loss: 0.0881, step time: 0.4789\n",
      "183/388, train_loss: 0.0802, step time: 0.5307\n",
      "184/388, train_loss: 0.1993, step time: 0.5021\n",
      "185/388, train_loss: 0.1410, step time: 1.1214\n",
      "186/388, train_loss: 0.1647, step time: 0.5423\n",
      "187/388, train_loss: 0.1425, step time: 0.5207\n",
      "188/388, train_loss: 0.1325, step time: 0.4970\n",
      "189/388, train_loss: 0.0770, step time: 0.4936\n",
      "190/388, train_loss: 0.3152, step time: 0.4886\n",
      "191/388, train_loss: 0.0957, step time: 0.5025\n",
      "192/388, train_loss: 0.0999, step time: 1.0435\n",
      "193/388, train_loss: 0.1166, step time: 0.5527\n",
      "194/388, train_loss: 0.2258, step time: 0.5190\n",
      "195/388, train_loss: 0.0983, step time: 0.4966\n",
      "196/388, train_loss: 0.0888, step time: 0.4980\n",
      "197/388, train_loss: 0.0516, step time: 0.4869\n",
      "198/388, train_loss: 0.1338, step time: 0.5035\n",
      "199/388, train_loss: 0.2062, step time: 0.4929\n",
      "200/388, train_loss: 0.3488, step time: 0.5014\n",
      "201/388, train_loss: 0.1122, step time: 1.0978\n",
      "202/388, train_loss: 0.3649, step time: 0.5573\n",
      "203/388, train_loss: 0.2120, step time: 0.5152\n",
      "204/388, train_loss: 0.1394, step time: 0.4894\n",
      "205/388, train_loss: 0.1053, step time: 0.4830\n",
      "206/388, train_loss: 0.2288, step time: 0.4958\n",
      "207/388, train_loss: 0.1359, step time: 0.4819\n",
      "208/388, train_loss: 0.1334, step time: 0.7113\n",
      "209/388, train_loss: 0.2146, step time: 0.5577\n",
      "210/388, train_loss: 0.1109, step time: 0.5178\n",
      "211/388, train_loss: 0.1956, step time: 0.4992\n",
      "212/388, train_loss: 0.0836, step time: 0.4892\n",
      "213/388, train_loss: 0.2748, step time: 0.4977\n",
      "214/388, train_loss: 0.1062, step time: 0.5100\n",
      "215/388, train_loss: 0.1307, step time: 0.5026\n",
      "216/388, train_loss: 0.2950, step time: 0.4851\n",
      "217/388, train_loss: 0.1006, step time: 0.5150\n",
      "218/388, train_loss: 0.1060, step time: 0.4919\n",
      "219/388, train_loss: 0.0991, step time: 0.5192\n",
      "220/388, train_loss: 0.0972, step time: 0.5148\n",
      "221/388, train_loss: 0.1594, step time: 0.5646\n",
      "222/388, train_loss: 0.1396, step time: 0.5363\n",
      "223/388, train_loss: 0.2545, step time: 0.5062\n",
      "224/388, train_loss: 0.1872, step time: 0.5168\n",
      "225/388, train_loss: 0.2701, step time: 0.6546\n",
      "226/388, train_loss: 0.1857, step time: 0.5591\n",
      "227/388, train_loss: 0.1149, step time: 0.5114\n",
      "228/388, train_loss: 0.0608, step time: 0.4927\n",
      "229/388, train_loss: 0.1236, step time: 0.5153\n",
      "230/388, train_loss: 0.1664, step time: 0.4902\n",
      "231/388, train_loss: 0.2085, step time: 0.5886\n",
      "232/388, train_loss: 0.2694, step time: 0.5976\n",
      "233/388, train_loss: 0.2079, step time: 0.5356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/388, train_loss: 0.0711, step time: 0.4979\n",
      "235/388, train_loss: 0.2978, step time: 0.5269\n",
      "236/388, train_loss: 0.3074, step time: 0.5268\n",
      "237/388, train_loss: 0.0493, step time: 0.5789\n",
      "238/388, train_loss: 0.0952, step time: 0.5269\n",
      "239/388, train_loss: 0.1038, step time: 0.5038\n",
      "240/388, train_loss: 0.1248, step time: 0.4912\n",
      "241/388, train_loss: 0.2341, step time: 0.4995\n",
      "242/388, train_loss: 0.5512, step time: 0.5445\n",
      "243/388, train_loss: 0.1848, step time: 0.5359\n",
      "244/388, train_loss: 0.1804, step time: 0.5104\n",
      "245/388, train_loss: 0.3549, step time: 1.1488\n",
      "246/388, train_loss: 0.1460, step time: 0.5380\n",
      "247/388, train_loss: 0.0685, step time: 0.5018\n",
      "248/388, train_loss: 0.1002, step time: 0.4992\n",
      "249/388, train_loss: 0.2324, step time: 0.6744\n",
      "250/388, train_loss: 0.1319, step time: 0.5830\n",
      "251/388, train_loss: 0.1679, step time: 0.5359\n",
      "252/388, train_loss: 0.0783, step time: 0.5188\n",
      "253/388, train_loss: 0.2078, step time: 0.4965\n",
      "254/388, train_loss: 0.2143, step time: 0.5023\n",
      "255/388, train_loss: 0.4013, step time: 0.5429\n",
      "256/388, train_loss: 0.2969, step time: 0.5196\n",
      "257/388, train_loss: 0.0366, step time: 0.5023\n",
      "258/388, train_loss: 0.1662, step time: 0.4958\n",
      "259/388, train_loss: 0.1057, step time: 1.1668\n",
      "260/388, train_loss: 0.2130, step time: 0.5467\n",
      "261/388, train_loss: 0.2077, step time: 0.5194\n",
      "262/388, train_loss: 0.2702, step time: 0.5039\n",
      "263/388, train_loss: 0.0958, step time: 0.4918\n",
      "264/388, train_loss: 0.0876, step time: 0.4945\n",
      "265/388, train_loss: 0.4376, step time: 1.1194\n",
      "266/388, train_loss: 0.1513, step time: 0.5265\n",
      "267/388, train_loss: 0.1920, step time: 0.5084\n",
      "268/388, train_loss: 0.1454, step time: 0.4900\n",
      "269/388, train_loss: 0.5308, step time: 0.4946\n",
      "270/388, train_loss: 0.2559, step time: 0.4830\n",
      "271/388, train_loss: 0.1606, step time: 0.5369\n",
      "272/388, train_loss: 0.0841, step time: 0.5123\n",
      "273/388, train_loss: 0.0748, step time: 0.4931\n",
      "274/388, train_loss: 0.2249, step time: 0.4954\n",
      "275/388, train_loss: 0.1985, step time: 1.0766\n",
      "276/388, train_loss: 0.2104, step time: 0.5359\n",
      "277/388, train_loss: 0.2417, step time: 0.5053\n",
      "278/388, train_loss: 0.2360, step time: 0.4862\n",
      "279/388, train_loss: 0.1860, step time: 0.5389\n",
      "280/388, train_loss: 0.2024, step time: 0.5068\n",
      "281/388, train_loss: 0.6133, step time: 0.5018\n",
      "282/388, train_loss: 0.1395, step time: 0.4815\n",
      "283/388, train_loss: 0.0994, step time: 0.4825\n",
      "284/388, train_loss: 0.0323, step time: 0.8373\n",
      "285/388, train_loss: 0.0663, step time: 0.5462\n",
      "286/388, train_loss: 0.1479, step time: 0.5114\n",
      "287/388, train_loss: 0.3794, step time: 0.5057\n",
      "288/388, train_loss: 0.1213, step time: 0.4900\n",
      "289/388, train_loss: 0.2565, step time: 0.4865\n",
      "290/388, train_loss: 0.1991, step time: 0.4906\n",
      "291/388, train_loss: 0.1015, step time: 0.4747\n",
      "292/388, train_loss: 0.4592, step time: 0.5978\n",
      "293/388, train_loss: 0.2931, step time: 0.5531\n",
      "294/388, train_loss: 0.1741, step time: 0.5192\n",
      "295/388, train_loss: 0.1772, step time: 0.4970\n",
      "296/388, train_loss: 0.2191, step time: 0.4995\n",
      "297/388, train_loss: 0.1124, step time: 0.5346\n",
      "298/388, train_loss: 0.1290, step time: 0.5131\n",
      "299/388, train_loss: 0.1325, step time: 0.4982\n",
      "300/388, train_loss: 0.2002, step time: 0.4926\n",
      "301/388, train_loss: 0.1733, step time: 0.4850\n",
      "302/388, train_loss: 0.1390, step time: 0.5136\n",
      "303/388, train_loss: 0.1372, step time: 0.4903\n",
      "304/388, train_loss: 0.1692, step time: 0.9819\n",
      "305/388, train_loss: 0.1438, step time: 0.5361\n",
      "306/388, train_loss: 0.1778, step time: 0.5184\n",
      "307/388, train_loss: 0.1422, step time: 0.4954\n",
      "308/388, train_loss: 0.4482, step time: 0.4913\n",
      "309/388, train_loss: 0.2706, step time: 0.4999\n",
      "310/388, train_loss: 0.0997, step time: 0.5395\n",
      "311/388, train_loss: 0.1100, step time: 0.5248\n",
      "312/388, train_loss: 0.1050, step time: 0.5082\n",
      "313/388, train_loss: 0.5161, step time: 0.4988\n",
      "314/388, train_loss: 0.0614, step time: 0.5013\n",
      "315/388, train_loss: 0.2150, step time: 0.4987\n",
      "316/388, train_loss: 0.1392, step time: 0.4833\n",
      "317/388, train_loss: 0.1918, step time: 0.5033\n",
      "318/388, train_loss: 0.1645, step time: 0.5326\n",
      "319/388, train_loss: 0.3578, step time: 0.5322\n",
      "320/388, train_loss: 0.1427, step time: 0.5681\n",
      "321/388, train_loss: 0.3419, step time: 0.5312\n",
      "322/388, train_loss: 0.1522, step time: 0.4992\n",
      "323/388, train_loss: 0.1134, step time: 0.4998\n",
      "324/388, train_loss: 0.3068, step time: 0.4901\n",
      "325/388, train_loss: 0.3248, step time: 0.4868\n",
      "326/388, train_loss: 0.0806, step time: 0.4961\n",
      "327/388, train_loss: 0.1063, step time: 0.5141\n",
      "328/388, train_loss: 0.2429, step time: 0.5049\n",
      "329/388, train_loss: 0.0502, step time: 0.4835\n",
      "330/388, train_loss: 0.0725, step time: 0.5047\n",
      "331/388, train_loss: 0.2199, step time: 0.4966\n",
      "332/388, train_loss: 0.1087, step time: 0.4950\n",
      "333/388, train_loss: 0.1168, step time: 0.4835\n",
      "334/388, train_loss: 0.1718, step time: 0.5055\n",
      "335/388, train_loss: 0.0726, step time: 0.4924\n",
      "336/388, train_loss: 0.0947, step time: 0.5002\n",
      "337/388, train_loss: 0.0886, step time: 0.4836\n",
      "338/388, train_loss: 0.1116, step time: 0.5149\n",
      "339/388, train_loss: 0.0533, step time: 0.5517\n",
      "340/388, train_loss: 0.0751, step time: 0.5052\n",
      "341/388, train_loss: 0.3797, step time: 0.4987\n",
      "342/388, train_loss: 0.0820, step time: 0.4951\n",
      "343/388, train_loss: 0.1186, step time: 0.5672\n",
      "344/388, train_loss: 0.1212, step time: 0.5323\n",
      "345/388, train_loss: 0.1900, step time: 0.6277\n",
      "346/388, train_loss: 0.0726, step time: 0.5606\n",
      "347/388, train_loss: 0.1065, step time: 0.5187\n",
      "348/388, train_loss: 0.2015, step time: 0.5003\n",
      "349/388, train_loss: 0.2010, step time: 1.0288\n",
      "350/388, train_loss: 0.1960, step time: 0.5594\n",
      "351/388, train_loss: 0.2730, step time: 0.5172\n",
      "352/388, train_loss: 0.2343, step time: 0.5016\n",
      "353/388, train_loss: 0.1842, step time: 0.5219\n",
      "354/388, train_loss: 0.2592, step time: 0.5070\n",
      "355/388, train_loss: 0.1812, step time: 0.5040\n",
      "356/388, train_loss: 0.2374, step time: 0.4894\n",
      "357/388, train_loss: 0.3406, step time: 0.4880\n",
      "358/388, train_loss: 0.1512, step time: 1.1449\n",
      "359/388, train_loss: 0.1483, step time: 0.5368\n",
      "360/388, train_loss: 0.3919, step time: 0.5142\n",
      "361/388, train_loss: 0.0978, step time: 0.5016\n",
      "362/388, train_loss: 0.0912, step time: 0.4861\n",
      "363/388, train_loss: 0.1334, step time: 0.4833\n",
      "364/388, train_loss: 0.1436, step time: 1.1516\n",
      "365/388, train_loss: 0.2108, step time: 0.5320\n",
      "366/388, train_loss: 0.3647, step time: 0.5089\n",
      "367/388, train_loss: 0.3313, step time: 0.4865\n",
      "368/388, train_loss: 0.0681, step time: 0.4967\n",
      "369/388, train_loss: 0.2064, step time: 0.4921\n",
      "370/388, train_loss: 0.1584, step time: 0.4994\n",
      "371/388, train_loss: 0.0593, step time: 0.4822\n",
      "372/388, train_loss: 0.2406, step time: 0.4836\n",
      "373/388, train_loss: 0.1769, step time: 0.4835\n",
      "374/388, train_loss: 0.3355, step time: 0.9722\n",
      "375/388, train_loss: 0.4891, step time: 0.5445\n",
      "376/388, train_loss: 0.1652, step time: 0.4993\n",
      "377/388, train_loss: 0.3996, step time: 0.4983\n",
      "378/388, train_loss: 0.2189, step time: 0.4797\n",
      "379/388, train_loss: 0.0733, step time: 0.4796\n",
      "380/388, train_loss: 0.2896, step time: 1.0209\n",
      "381/388, train_loss: 0.1909, step time: 0.5229\n",
      "382/388, train_loss: 0.2473, step time: 0.5019\n",
      "383/388, train_loss: 0.1142, step time: 0.4806\n",
      "384/388, train_loss: 0.1062, step time: 0.4813\n",
      "385/388, train_loss: 0.0773, step time: 0.4817\n",
      "386/388, train_loss: 0.2224, step time: 0.4690\n",
      "387/388, train_loss: 0.2244, step time: 0.4865\n",
      "388/388, train_loss: 0.3304, step time: 0.4867\n",
      "epoch 60 average loss: 0.1812\n",
      "current epoch: 60 current mean dice: 0.7642 tc: 0.8141 wt: 0.9030 et: 0.5754\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 60 is: 302.8278\n",
      "----------\n",
      "epoch 61/300\n",
      "1/388, train_loss: 0.3426, step time: 0.4704\n",
      "2/388, train_loss: 0.0725, step time: 0.4760\n",
      "3/388, train_loss: 0.0899, step time: 0.4897\n",
      "4/388, train_loss: 0.2943, step time: 0.5648\n",
      "5/388, train_loss: 0.0951, step time: 0.5343\n",
      "6/388, train_loss: 0.3784, step time: 0.5635\n",
      "7/388, train_loss: 0.1127, step time: 0.5480\n",
      "8/388, train_loss: 0.2575, step time: 0.4952\n",
      "9/388, train_loss: 0.2347, step time: 0.5070\n",
      "10/388, train_loss: 0.1405, step time: 0.5230\n",
      "11/388, train_loss: 0.2836, step time: 0.5280\n",
      "12/388, train_loss: 0.1291, step time: 0.5018\n",
      "13/388, train_loss: 0.2085, step time: 0.5472\n",
      "14/388, train_loss: 0.1447, step time: 0.6944\n",
      "15/388, train_loss: 0.2346, step time: 0.5528\n",
      "16/388, train_loss: 0.0652, step time: 0.4997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/388, train_loss: 0.0959, step time: 0.4903\n",
      "18/388, train_loss: 0.0767, step time: 0.4807\n",
      "19/388, train_loss: 0.0916, step time: 1.0276\n",
      "20/388, train_loss: 0.2631, step time: 0.5432\n",
      "21/388, train_loss: 0.2156, step time: 0.4875\n",
      "22/388, train_loss: 0.1568, step time: 0.5186\n",
      "23/388, train_loss: 0.2703, step time: 0.5711\n",
      "24/388, train_loss: 0.1488, step time: 0.5342\n",
      "25/388, train_loss: 0.5020, step time: 0.5040\n",
      "26/388, train_loss: 0.0795, step time: 0.5110\n",
      "27/388, train_loss: 0.1394, step time: 0.5134\n",
      "28/388, train_loss: 0.1240, step time: 0.5608\n",
      "29/388, train_loss: 0.2257, step time: 0.7416\n",
      "30/388, train_loss: 0.2099, step time: 0.5657\n",
      "31/388, train_loss: 0.5277, step time: 0.5249\n",
      "32/388, train_loss: 0.1385, step time: 0.5109\n",
      "33/388, train_loss: 0.0849, step time: 0.5031\n",
      "34/388, train_loss: 0.1092, step time: 0.4950\n",
      "35/388, train_loss: 0.0910, step time: 0.5041\n",
      "36/388, train_loss: 0.1801, step time: 0.5174\n",
      "37/388, train_loss: 0.2800, step time: 0.4966\n",
      "38/388, train_loss: 0.1127, step time: 0.5015\n",
      "39/388, train_loss: 0.0946, step time: 0.4872\n",
      "40/388, train_loss: 0.1612, step time: 0.4871\n",
      "41/388, train_loss: 0.0695, step time: 0.8577\n",
      "42/388, train_loss: 0.1466, step time: 0.5301\n",
      "43/388, train_loss: 0.0825, step time: 0.5023\n",
      "44/388, train_loss: 0.2517, step time: 0.5112\n",
      "45/388, train_loss: 0.2042, step time: 0.6377\n",
      "46/388, train_loss: 0.3774, step time: 0.5721\n",
      "47/388, train_loss: 0.2305, step time: 0.5280\n",
      "48/388, train_loss: 0.1602, step time: 0.4959\n",
      "49/388, train_loss: 0.1377, step time: 0.5374\n",
      "50/388, train_loss: 0.4633, step time: 0.5844\n",
      "51/388, train_loss: 0.1080, step time: 0.5326\n",
      "52/388, train_loss: 0.2658, step time: 0.5182\n",
      "53/388, train_loss: 0.0687, step time: 0.4918\n",
      "54/388, train_loss: 0.2488, step time: 0.5010\n",
      "55/388, train_loss: 0.1109, step time: 0.5009\n",
      "56/388, train_loss: 0.1707, step time: 0.4825\n",
      "57/388, train_loss: 0.0441, step time: 1.1283\n",
      "58/388, train_loss: 0.1084, step time: 0.5459\n",
      "59/388, train_loss: 0.1628, step time: 0.5171\n",
      "60/388, train_loss: 0.4851, step time: 0.4948\n",
      "61/388, train_loss: 0.3166, step time: 0.5049\n",
      "62/388, train_loss: 0.1108, step time: 0.4841\n",
      "63/388, train_loss: 0.2064, step time: 1.1156\n",
      "64/388, train_loss: 0.3278, step time: 0.5278\n",
      "65/388, train_loss: 0.2282, step time: 0.5163\n",
      "66/388, train_loss: 0.1493, step time: 0.5033\n",
      "67/388, train_loss: 0.1017, step time: 0.4866\n",
      "68/388, train_loss: 0.1269, step time: 0.4950\n",
      "69/388, train_loss: 0.1921, step time: 0.4784\n",
      "70/388, train_loss: 0.0889, step time: 0.4769\n",
      "71/388, train_loss: 0.0982, step time: 0.4804\n",
      "72/388, train_loss: 0.1791, step time: 0.4726\n",
      "73/388, train_loss: 0.0935, step time: 0.4768\n",
      "74/388, train_loss: 0.1299, step time: 0.4958\n",
      "75/388, train_loss: 0.1255, step time: 0.4805\n",
      "76/388, train_loss: 0.1781, step time: 0.4890\n",
      "77/388, train_loss: 0.0967, step time: 0.5514\n",
      "78/388, train_loss: 0.1547, step time: 0.5292\n",
      "79/388, train_loss: 0.2646, step time: 0.5059\n",
      "80/388, train_loss: 0.0879, step time: 0.4977\n",
      "81/388, train_loss: 0.1727, step time: 1.1082\n",
      "82/388, train_loss: 0.2113, step time: 0.5220\n",
      "83/388, train_loss: 0.1073, step time: 0.5014\n",
      "84/388, train_loss: 0.1943, step time: 0.4836\n",
      "85/388, train_loss: 0.3457, step time: 0.5108\n",
      "86/388, train_loss: 0.0518, step time: 0.4888\n",
      "87/388, train_loss: 0.0926, step time: 0.4939\n",
      "88/388, train_loss: 0.0873, step time: 0.4751\n",
      "89/388, train_loss: 0.1592, step time: 0.5892\n",
      "90/388, train_loss: 0.1861, step time: 0.5321\n",
      "91/388, train_loss: 0.2051, step time: 0.5027\n",
      "92/388, train_loss: 0.1354, step time: 0.4963\n",
      "93/388, train_loss: 0.1151, step time: 0.4777\n",
      "94/388, train_loss: 0.2630, step time: 0.4748\n",
      "95/388, train_loss: 0.1081, step time: 0.4789\n",
      "96/388, train_loss: 0.2171, step time: 0.4929\n",
      "97/388, train_loss: 0.1861, step time: 0.8651\n",
      "98/388, train_loss: 0.1450, step time: 0.5544\n",
      "99/388, train_loss: 0.1772, step time: 0.5238\n",
      "100/388, train_loss: 0.0787, step time: 0.5164\n",
      "101/388, train_loss: 0.0601, step time: 0.5090\n",
      "102/388, train_loss: 0.2673, step time: 0.4974\n",
      "103/388, train_loss: 0.0466, step time: 0.4882\n",
      "104/388, train_loss: 0.0943, step time: 0.4904\n",
      "105/388, train_loss: 0.3368, step time: 1.0085\n",
      "106/388, train_loss: 0.3918, step time: 0.5363\n",
      "107/388, train_loss: 0.0349, step time: 0.4972\n",
      "108/388, train_loss: 0.1936, step time: 0.4903\n",
      "109/388, train_loss: 0.2515, step time: 0.5111\n",
      "110/388, train_loss: 0.1697, step time: 0.5076\n",
      "111/388, train_loss: 0.1459, step time: 0.5052\n",
      "112/388, train_loss: 0.2165, step time: 0.4980\n",
      "113/388, train_loss: 0.1083, step time: 0.4853\n",
      "114/388, train_loss: 0.3410, step time: 0.4802\n",
      "115/388, train_loss: 0.0918, step time: 0.6495\n",
      "116/388, train_loss: 0.3658, step time: 0.5558\n",
      "117/388, train_loss: 0.1060, step time: 0.5198\n",
      "118/388, train_loss: 0.3554, step time: 0.4914\n",
      "119/388, train_loss: 0.0482, step time: 0.4980\n",
      "120/388, train_loss: 0.3543, step time: 0.4819\n",
      "121/388, train_loss: 0.3621, step time: 0.4848\n",
      "122/388, train_loss: 0.1894, step time: 0.4861\n",
      "123/388, train_loss: 0.2536, step time: 0.9987\n",
      "124/388, train_loss: 0.1216, step time: 0.5532\n",
      "125/388, train_loss: 0.1118, step time: 0.5093\n",
      "126/388, train_loss: 0.0670, step time: 0.4846\n",
      "127/388, train_loss: 0.4295, step time: 0.4858\n",
      "128/388, train_loss: 0.0859, step time: 0.4982\n",
      "129/388, train_loss: 0.1363, step time: 0.4831\n",
      "130/388, train_loss: 0.4487, step time: 0.4916\n",
      "131/388, train_loss: 0.0770, step time: 0.4774\n",
      "132/388, train_loss: 0.1168, step time: 0.4756\n",
      "133/388, train_loss: 0.2245, step time: 0.5243\n",
      "134/388, train_loss: 0.3965, step time: 0.5679\n",
      "135/388, train_loss: 0.1044, step time: 0.5309\n",
      "136/388, train_loss: 0.1518, step time: 0.5049\n",
      "137/388, train_loss: 0.1157, step time: 0.5023\n",
      "138/388, train_loss: 0.1467, step time: 0.4846\n",
      "139/388, train_loss: 0.0676, step time: 0.4755\n",
      "140/388, train_loss: 0.1988, step time: 0.4778\n",
      "141/388, train_loss: 0.1610, step time: 1.0173\n",
      "142/388, train_loss: 0.1261, step time: 0.5408\n",
      "143/388, train_loss: 0.1090, step time: 0.5185\n",
      "144/388, train_loss: 0.1056, step time: 0.5071\n",
      "145/388, train_loss: 0.0905, step time: 0.4866\n",
      "146/388, train_loss: 0.4619, step time: 0.4912\n",
      "147/388, train_loss: 0.3437, step time: 0.4940\n",
      "148/388, train_loss: 0.1028, step time: 0.4797\n",
      "149/388, train_loss: 0.1716, step time: 0.5172\n",
      "150/388, train_loss: 0.1119, step time: 0.4915\n",
      "151/388, train_loss: 0.3069, step time: 0.4794\n",
      "152/388, train_loss: 0.2045, step time: 0.5068\n",
      "153/388, train_loss: 0.0626, step time: 0.4941\n",
      "154/388, train_loss: 0.3129, step time: 0.4990\n",
      "155/388, train_loss: 0.3502, step time: 0.4925\n",
      "156/388, train_loss: 0.1327, step time: 0.5484\n",
      "157/388, train_loss: 0.0791, step time: 0.5129\n",
      "158/388, train_loss: 0.1523, step time: 0.5031\n",
      "159/388, train_loss: 0.1541, step time: 0.4838\n",
      "160/388, train_loss: 0.2306, step time: 0.5033\n",
      "161/388, train_loss: 0.3184, step time: 0.5076\n",
      "162/388, train_loss: 0.1566, step time: 0.5113\n",
      "163/388, train_loss: 0.0662, step time: 0.4948\n",
      "164/388, train_loss: 0.1769, step time: 0.4965\n",
      "165/388, train_loss: 0.4122, step time: 0.5145\n",
      "166/388, train_loss: 0.1919, step time: 0.5056\n",
      "167/388, train_loss: 0.1730, step time: 0.4875\n",
      "168/388, train_loss: 0.0882, step time: 0.4904\n",
      "169/388, train_loss: 0.1799, step time: 0.4783\n",
      "170/388, train_loss: 0.0813, step time: 0.9294\n",
      "171/388, train_loss: 0.1145, step time: 0.5480\n",
      "172/388, train_loss: 0.0507, step time: 0.5182\n",
      "173/388, train_loss: 0.0906, step time: 0.5038\n",
      "174/388, train_loss: 0.3185, step time: 0.4944\n",
      "175/388, train_loss: 0.4099, step time: 0.4988\n",
      "176/388, train_loss: 0.1220, step time: 0.4946\n",
      "177/388, train_loss: 0.1354, step time: 0.4980\n",
      "178/388, train_loss: 0.1755, step time: 0.4934\n",
      "179/388, train_loss: 0.1059, step time: 0.5082\n",
      "180/388, train_loss: 0.1551, step time: 0.5122\n",
      "181/388, train_loss: 0.3114, step time: 0.5017\n",
      "182/388, train_loss: 0.1776, step time: 0.5637\n",
      "183/388, train_loss: 0.3992, step time: 0.5370\n",
      "184/388, train_loss: 0.0950, step time: 0.5137\n",
      "185/388, train_loss: 0.0871, step time: 0.4963\n",
      "186/388, train_loss: 0.0382, step time: 0.4961\n",
      "187/388, train_loss: 0.0740, step time: 0.4797\n",
      "188/388, train_loss: 0.1718, step time: 0.4797\n",
      "189/388, train_loss: 0.0991, step time: 0.9388\n",
      "190/388, train_loss: 0.1974, step time: 0.5365\n",
      "191/388, train_loss: 0.2692, step time: 0.5134\n",
      "192/388, train_loss: 0.1005, step time: 0.4947\n",
      "193/388, train_loss: 0.1521, step time: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/388, train_loss: 0.1444, step time: 0.5327\n",
      "195/388, train_loss: 0.1802, step time: 0.5057\n",
      "196/388, train_loss: 0.1194, step time: 0.4897\n",
      "197/388, train_loss: 0.1165, step time: 0.5078\n",
      "198/388, train_loss: 0.2699, step time: 0.5099\n",
      "199/388, train_loss: 0.0924, step time: 0.5031\n",
      "200/388, train_loss: 0.2146, step time: 0.4887\n",
      "201/388, train_loss: 0.0691, step time: 0.4986\n",
      "202/388, train_loss: 0.0754, step time: 0.4829\n",
      "203/388, train_loss: 0.1814, step time: 0.7538\n",
      "204/388, train_loss: 0.2900, step time: 0.5554\n",
      "205/388, train_loss: 0.1971, step time: 0.5333\n",
      "206/388, train_loss: 0.0827, step time: 0.4979\n",
      "207/388, train_loss: 0.1732, step time: 0.4983\n",
      "208/388, train_loss: 0.1253, step time: 0.4829\n",
      "209/388, train_loss: 0.2093, step time: 1.0478\n",
      "210/388, train_loss: 0.1180, step time: 0.5419\n",
      "211/388, train_loss: 0.1983, step time: 0.5282\n",
      "212/388, train_loss: 0.1268, step time: 0.4967\n",
      "213/388, train_loss: 0.2295, step time: 0.4933\n",
      "214/388, train_loss: 0.2122, step time: 0.4934\n",
      "215/388, train_loss: 0.2178, step time: 0.5014\n",
      "216/388, train_loss: 0.3247, step time: 0.4924\n",
      "217/388, train_loss: 0.1356, step time: 0.4800\n",
      "218/388, train_loss: 0.0995, step time: 1.1268\n",
      "219/388, train_loss: 0.1208, step time: 0.5285\n",
      "220/388, train_loss: 0.0782, step time: 0.4970\n",
      "221/388, train_loss: 0.2395, step time: 0.4922\n",
      "222/388, train_loss: 0.1159, step time: 0.4921\n",
      "223/388, train_loss: 0.1012, step time: 0.4791\n",
      "224/388, train_loss: 0.2201, step time: 0.4884\n",
      "225/388, train_loss: 0.1707, step time: 0.5009\n",
      "226/388, train_loss: 0.4089, step time: 0.5066\n",
      "227/388, train_loss: 0.1078, step time: 0.4996\n",
      "228/388, train_loss: 0.2374, step time: 0.4844\n",
      "229/388, train_loss: 0.2542, step time: 0.5020\n",
      "230/388, train_loss: 0.2744, step time: 0.5383\n",
      "231/388, train_loss: 0.1120, step time: 0.5310\n",
      "232/388, train_loss: 0.2021, step time: 0.5199\n",
      "233/388, train_loss: 0.2025, step time: 0.5075\n",
      "234/388, train_loss: 0.2503, step time: 0.5061\n",
      "235/388, train_loss: 0.0582, step time: 0.4818\n",
      "236/388, train_loss: 0.0550, step time: 0.4796\n",
      "237/388, train_loss: 0.2343, step time: 0.4870\n",
      "238/388, train_loss: 0.2248, step time: 1.0845\n",
      "239/388, train_loss: 0.1447, step time: 0.5356\n",
      "240/388, train_loss: 0.1204, step time: 0.5077\n",
      "241/388, train_loss: 0.4046, step time: 0.4901\n",
      "242/388, train_loss: 0.2895, step time: 0.5303\n",
      "243/388, train_loss: 0.5246, step time: 0.4994\n",
      "244/388, train_loss: 0.4256, step time: 0.4950\n",
      "245/388, train_loss: 0.2844, step time: 0.4900\n",
      "246/388, train_loss: 0.0919, step time: 1.1807\n",
      "247/388, train_loss: 0.0738, step time: 0.5262\n",
      "248/388, train_loss: 0.1854, step time: 0.5082\n",
      "249/388, train_loss: 0.1683, step time: 0.4937\n",
      "250/388, train_loss: 0.1270, step time: 0.4847\n",
      "251/388, train_loss: 0.1227, step time: 0.4920\n",
      "252/388, train_loss: 0.1602, step time: 0.4751\n",
      "253/388, train_loss: 0.2299, step time: 0.4766\n",
      "254/388, train_loss: 0.1233, step time: 0.4812\n",
      "255/388, train_loss: 0.3543, step time: 0.4715\n",
      "256/388, train_loss: 0.1361, step time: 0.7221\n",
      "257/388, train_loss: 0.1167, step time: 0.5410\n",
      "258/388, train_loss: 0.1554, step time: 0.5225\n",
      "259/388, train_loss: 0.1119, step time: 0.5022\n",
      "260/388, train_loss: 0.2122, step time: 0.5077\n",
      "261/388, train_loss: 0.2208, step time: 0.4927\n",
      "262/388, train_loss: 0.2444, step time: 0.4857\n",
      "263/388, train_loss: 0.2478, step time: 0.4899\n",
      "264/388, train_loss: 0.2591, step time: 1.0800\n",
      "265/388, train_loss: 0.2055, step time: 0.5356\n",
      "266/388, train_loss: 0.1804, step time: 0.5106\n",
      "267/388, train_loss: 0.0671, step time: 0.4944\n",
      "268/388, train_loss: 0.1848, step time: 0.4965\n",
      "269/388, train_loss: 0.1046, step time: 0.4806\n",
      "270/388, train_loss: 0.0457, step time: 0.4811\n",
      "271/388, train_loss: 0.2798, step time: 0.4773\n",
      "272/388, train_loss: 0.1292, step time: 0.4751\n",
      "273/388, train_loss: 0.2711, step time: 0.4923\n",
      "274/388, train_loss: 0.2926, step time: 0.4903\n",
      "275/388, train_loss: 0.2750, step time: 0.5048\n",
      "276/388, train_loss: 0.1499, step time: 0.7026\n",
      "277/388, train_loss: 0.1334, step time: 0.5596\n",
      "278/388, train_loss: 0.1849, step time: 0.5167\n",
      "279/388, train_loss: 0.3383, step time: 0.4915\n",
      "280/388, train_loss: 0.2621, step time: 0.5148\n",
      "281/388, train_loss: 0.1131, step time: 0.5080\n",
      "282/388, train_loss: 0.1261, step time: 0.4839\n",
      "283/388, train_loss: 0.0914, step time: 0.4812\n",
      "284/388, train_loss: 0.2764, step time: 0.4893\n",
      "285/388, train_loss: 0.0830, step time: 0.5038\n",
      "286/388, train_loss: 0.3767, step time: 0.5094\n",
      "287/388, train_loss: 0.1378, step time: 0.5266\n",
      "288/388, train_loss: 0.3195, step time: 0.5233\n",
      "289/388, train_loss: 0.1943, step time: 0.5917\n",
      "290/388, train_loss: 0.5833, step time: 0.5462\n",
      "291/388, train_loss: 0.0844, step time: 0.5132\n",
      "292/388, train_loss: 0.2441, step time: 0.4919\n",
      "293/388, train_loss: 0.0913, step time: 0.4948\n",
      "294/388, train_loss: 0.2646, step time: 0.4918\n",
      "295/388, train_loss: 0.1346, step time: 0.4970\n",
      "296/388, train_loss: 0.1012, step time: 0.4809\n",
      "297/388, train_loss: 0.1059, step time: 1.1218\n",
      "298/388, train_loss: 0.1060, step time: 0.5417\n",
      "299/388, train_loss: 0.0972, step time: 0.5180\n",
      "300/388, train_loss: 0.3119, step time: 0.5033\n",
      "301/388, train_loss: 0.3259, step time: 0.4973\n",
      "302/388, train_loss: 0.3640, step time: 0.4953\n",
      "303/388, train_loss: 0.2073, step time: 0.4914\n",
      "304/388, train_loss: 0.2657, step time: 0.4945\n",
      "305/388, train_loss: 0.1261, step time: 0.4786\n",
      "306/388, train_loss: 0.0453, step time: 0.5140\n",
      "307/388, train_loss: 0.1965, step time: 0.4878\n",
      "308/388, train_loss: 0.2047, step time: 0.4897\n",
      "309/388, train_loss: 0.1575, step time: 0.5084\n",
      "310/388, train_loss: 0.1724, step time: 0.4944\n",
      "311/388, train_loss: 0.1185, step time: 0.4942\n",
      "312/388, train_loss: 0.3791, step time: 0.5229\n",
      "313/388, train_loss: 0.2752, step time: 0.4971\n",
      "314/388, train_loss: 0.0701, step time: 0.4824\n",
      "315/388, train_loss: 0.3470, step time: 0.5033\n",
      "316/388, train_loss: 0.2017, step time: 0.5084\n",
      "317/388, train_loss: 0.1385, step time: 0.5179\n",
      "318/388, train_loss: 0.1299, step time: 0.4945\n",
      "319/388, train_loss: 0.2156, step time: 0.4893\n",
      "320/388, train_loss: 0.0833, step time: 0.5070\n",
      "321/388, train_loss: 0.2482, step time: 1.1760\n",
      "322/388, train_loss: 0.2298, step time: 0.5323\n",
      "323/388, train_loss: 0.1582, step time: 0.5129\n",
      "324/388, train_loss: 0.0596, step time: 0.5064\n",
      "325/388, train_loss: 0.3097, step time: 0.4943\n",
      "326/388, train_loss: 0.1785, step time: 0.4982\n",
      "327/388, train_loss: 0.2758, step time: 0.4951\n",
      "328/388, train_loss: 0.0604, step time: 0.5464\n",
      "329/388, train_loss: 0.2287, step time: 0.5320\n",
      "330/388, train_loss: 0.2755, step time: 0.5046\n",
      "331/388, train_loss: 0.0526, step time: 0.5002\n",
      "332/388, train_loss: 0.1345, step time: 0.4956\n",
      "333/388, train_loss: 0.1165, step time: 0.5070\n",
      "334/388, train_loss: 0.1646, step time: 0.4888\n",
      "335/388, train_loss: 0.1449, step time: 1.1152\n",
      "336/388, train_loss: 0.4542, step time: 0.5478\n",
      "337/388, train_loss: 0.1374, step time: 0.5223\n",
      "338/388, train_loss: 0.0669, step time: 0.5046\n",
      "339/388, train_loss: 0.2596, step time: 0.4985\n",
      "340/388, train_loss: 0.3719, step time: 0.4848\n",
      "341/388, train_loss: 0.3528, step time: 1.1253\n",
      "342/388, train_loss: 0.0976, step time: 0.5250\n",
      "343/388, train_loss: 0.0828, step time: 0.4951\n",
      "344/388, train_loss: 0.1771, step time: 0.4880\n",
      "345/388, train_loss: 0.0840, step time: 0.4924\n",
      "346/388, train_loss: 0.2855, step time: 0.4775\n",
      "347/388, train_loss: 0.1166, step time: 0.4848\n",
      "348/388, train_loss: 0.4808, step time: 0.5358\n",
      "349/388, train_loss: 0.0963, step time: 0.5122\n",
      "350/388, train_loss: 0.1862, step time: 0.5042\n",
      "351/388, train_loss: 0.0986, step time: 0.5073\n",
      "352/388, train_loss: 0.0936, step time: 0.5335\n",
      "353/388, train_loss: 0.0900, step time: 0.5209\n",
      "354/388, train_loss: 0.3032, step time: 0.5559\n",
      "355/388, train_loss: 0.2495, step time: 0.6360\n",
      "356/388, train_loss: 0.1022, step time: 0.5348\n",
      "357/388, train_loss: 0.0706, step time: 0.5145\n",
      "358/388, train_loss: 0.1301, step time: 0.5076\n",
      "359/388, train_loss: 0.1405, step time: 0.4969\n",
      "360/388, train_loss: 0.1724, step time: 0.5044\n",
      "361/388, train_loss: 0.1875, step time: 0.4825\n",
      "362/388, train_loss: 0.0893, step time: 1.0303\n",
      "363/388, train_loss: 0.1398, step time: 0.5416\n",
      "364/388, train_loss: 0.1803, step time: 0.5127\n",
      "365/388, train_loss: 0.0997, step time: 0.4957\n",
      "366/388, train_loss: 0.0883, step time: 0.4907\n",
      "367/388, train_loss: 0.2174, step time: 0.4966\n",
      "368/388, train_loss: 0.2008, step time: 0.4960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/388, train_loss: 0.1607, step time: 0.5983\n",
      "370/388, train_loss: 0.0967, step time: 0.5464\n",
      "371/388, train_loss: 0.1486, step time: 0.5274\n",
      "372/388, train_loss: 0.1373, step time: 0.5077\n",
      "373/388, train_loss: 0.2363, step time: 0.5065\n",
      "374/388, train_loss: 0.1887, step time: 0.4806\n",
      "375/388, train_loss: 0.2094, step time: 0.5337\n",
      "376/388, train_loss: 0.1436, step time: 0.5406\n",
      "377/388, train_loss: 0.1705, step time: 0.5084\n",
      "378/388, train_loss: 0.1196, step time: 0.4907\n",
      "379/388, train_loss: 0.4165, step time: 0.5217\n",
      "380/388, train_loss: 0.3182, step time: 0.5107\n",
      "381/388, train_loss: 0.1455, step time: 0.5111\n",
      "382/388, train_loss: 0.0318, step time: 0.4922\n",
      "383/388, train_loss: 0.0493, step time: 0.4964\n",
      "384/388, train_loss: 0.1686, step time: 0.4881\n",
      "385/388, train_loss: 0.4652, step time: 0.9364\n",
      "386/388, train_loss: 0.1467, step time: 0.5468\n",
      "387/388, train_loss: 0.2222, step time: 0.5233\n",
      "388/388, train_loss: 0.1945, step time: 0.4933\n",
      "epoch 61 average loss: 0.1850\n",
      "current epoch: 61 current mean dice: 0.7678 tc: 0.8156 wt: 0.9004 et: 0.5874\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 61 is: 295.6154\n",
      "----------\n",
      "epoch 62/300\n",
      "1/388, train_loss: 0.1117, step time: 0.4801\n",
      "2/388, train_loss: 0.3689, step time: 0.4901\n",
      "3/388, train_loss: 0.3056, step time: 0.8428\n",
      "4/388, train_loss: 0.0295, step time: 0.5322\n",
      "5/388, train_loss: 0.1392, step time: 0.5097\n",
      "6/388, train_loss: 0.0854, step time: 0.4982\n",
      "7/388, train_loss: 0.1940, step time: 1.0029\n",
      "8/388, train_loss: 0.1916, step time: 0.5909\n",
      "9/388, train_loss: 0.2911, step time: 0.5446\n",
      "10/388, train_loss: 0.1086, step time: 0.5016\n",
      "11/388, train_loss: 0.3916, step time: 0.7257\n",
      "12/388, train_loss: 0.0937, step time: 0.5933\n",
      "13/388, train_loss: 0.1034, step time: 0.5189\n",
      "14/388, train_loss: 0.3098, step time: 0.5268\n",
      "15/388, train_loss: 0.0961, step time: 0.5235\n",
      "16/388, train_loss: 0.1914, step time: 0.5170\n",
      "17/388, train_loss: 0.0623, step time: 0.5121\n",
      "18/388, train_loss: 0.1693, step time: 0.4944\n",
      "19/388, train_loss: 0.1718, step time: 0.5174\n",
      "20/388, train_loss: 0.0986, step time: 0.5364\n",
      "21/388, train_loss: 0.1037, step time: 0.6401\n",
      "22/388, train_loss: 0.3033, step time: 0.5602\n",
      "23/388, train_loss: 0.1751, step time: 0.5307\n",
      "24/388, train_loss: 0.1114, step time: 0.5252\n",
      "25/388, train_loss: 0.1155, step time: 0.5042\n",
      "26/388, train_loss: 0.2017, step time: 0.4934\n",
      "27/388, train_loss: 0.0482, step time: 1.0678\n",
      "28/388, train_loss: 0.1000, step time: 0.5414\n",
      "29/388, train_loss: 0.1158, step time: 0.5109\n",
      "30/388, train_loss: 0.2359, step time: 0.5112\n",
      "31/388, train_loss: 0.1853, step time: 0.5554\n",
      "32/388, train_loss: 0.2827, step time: 0.5172\n",
      "33/388, train_loss: 0.1181, step time: 0.4866\n",
      "34/388, train_loss: 0.0912, step time: 1.1412\n",
      "35/388, train_loss: 0.0945, step time: 0.5690\n",
      "36/388, train_loss: 0.1286, step time: 0.5347\n",
      "37/388, train_loss: 0.2153, step time: 0.5063\n",
      "38/388, train_loss: 0.2399, step time: 0.5113\n",
      "39/388, train_loss: 0.2488, step time: 0.5224\n",
      "40/388, train_loss: 0.1379, step time: 0.5064\n",
      "41/388, train_loss: 0.3004, step time: 0.4998\n",
      "42/388, train_loss: 0.2383, step time: 1.0163\n",
      "43/388, train_loss: 0.2185, step time: 0.5379\n",
      "44/388, train_loss: 0.0947, step time: 0.5021\n",
      "45/388, train_loss: 0.1681, step time: 0.4953\n",
      "46/388, train_loss: 0.1061, step time: 0.5122\n",
      "47/388, train_loss: 0.1435, step time: 0.5013\n",
      "48/388, train_loss: 0.4867, step time: 0.4872\n",
      "49/388, train_loss: 0.0924, step time: 0.4953\n",
      "50/388, train_loss: 0.1667, step time: 0.5091\n",
      "51/388, train_loss: 0.0723, step time: 0.5599\n",
      "52/388, train_loss: 0.1032, step time: 0.5349\n",
      "53/388, train_loss: 0.2792, step time: 0.5476\n",
      "54/388, train_loss: 0.1756, step time: 0.5224\n",
      "55/388, train_loss: 0.1886, step time: 0.5104\n",
      "56/388, train_loss: 0.1406, step time: 0.4930\n",
      "57/388, train_loss: 0.1395, step time: 0.4966\n",
      "58/388, train_loss: 0.1595, step time: 0.4747\n",
      "59/388, train_loss: 0.1515, step time: 0.6989\n",
      "60/388, train_loss: 0.1418, step time: 0.5539\n",
      "61/388, train_loss: 0.1490, step time: 0.5379\n",
      "62/388, train_loss: 0.1056, step time: 0.5142\n",
      "63/388, train_loss: 0.1067, step time: 0.4911\n",
      "64/388, train_loss: 0.1035, step time: 1.1449\n",
      "65/388, train_loss: 0.4307, step time: 0.5428\n",
      "66/388, train_loss: 0.1810, step time: 0.5189\n",
      "67/388, train_loss: 0.1029, step time: 0.5272\n",
      "68/388, train_loss: 0.1316, step time: 0.5215\n",
      "69/388, train_loss: 0.1754, step time: 0.5160\n",
      "70/388, train_loss: 0.2372, step time: 0.4920\n",
      "71/388, train_loss: 0.2389, step time: 0.5037\n",
      "72/388, train_loss: 0.1658, step time: 0.7022\n",
      "73/388, train_loss: 0.4002, step time: 0.5484\n",
      "74/388, train_loss: 0.0677, step time: 0.5112\n",
      "75/388, train_loss: 0.0777, step time: 0.5002\n",
      "76/388, train_loss: 0.1809, step time: 0.4946\n",
      "77/388, train_loss: 0.1158, step time: 0.4927\n",
      "78/388, train_loss: 0.3968, step time: 1.1515\n",
      "79/388, train_loss: 0.0796, step time: 0.5355\n",
      "80/388, train_loss: 0.3390, step time: 0.5191\n",
      "81/388, train_loss: 0.1400, step time: 0.5167\n",
      "82/388, train_loss: 0.1420, step time: 0.5128\n",
      "83/388, train_loss: 0.2698, step time: 0.5370\n",
      "84/388, train_loss: 0.2725, step time: 0.5765\n",
      "85/388, train_loss: 0.0814, step time: 0.6213\n",
      "86/388, train_loss: 0.1871, step time: 0.5400\n",
      "87/388, train_loss: 0.0748, step time: 0.5139\n",
      "88/388, train_loss: 0.1011, step time: 0.5043\n",
      "89/388, train_loss: 0.1974, step time: 0.5396\n",
      "90/388, train_loss: 0.0603, step time: 0.6395\n",
      "91/388, train_loss: 0.0945, step time: 0.5423\n",
      "92/388, train_loss: 0.1381, step time: 0.5105\n",
      "93/388, train_loss: 0.1096, step time: 0.5827\n",
      "94/388, train_loss: 0.1578, step time: 0.5568\n",
      "95/388, train_loss: 0.2019, step time: 0.5144\n",
      "96/388, train_loss: 0.0936, step time: 0.5016\n",
      "97/388, train_loss: 0.0796, step time: 0.4838\n",
      "98/388, train_loss: 0.4800, step time: 0.4765\n",
      "99/388, train_loss: 0.1773, step time: 0.4774\n",
      "100/388, train_loss: 0.2582, step time: 0.9450\n",
      "101/388, train_loss: 0.1342, step time: 0.5561\n",
      "102/388, train_loss: 0.2408, step time: 0.5220\n",
      "103/388, train_loss: 0.0713, step time: 0.4962\n",
      "104/388, train_loss: 0.1096, step time: 0.5014\n",
      "105/388, train_loss: 0.3625, step time: 0.4990\n",
      "106/388, train_loss: 0.3035, step time: 0.4849\n",
      "107/388, train_loss: 0.2413, step time: 0.4891\n",
      "108/388, train_loss: 0.1018, step time: 0.4916\n",
      "109/388, train_loss: 0.1945, step time: 1.1931\n",
      "110/388, train_loss: 0.0825, step time: 0.5406\n",
      "111/388, train_loss: 0.1205, step time: 0.5104\n",
      "112/388, train_loss: 0.1405, step time: 0.4955\n",
      "113/388, train_loss: 0.2413, step time: 0.4884\n",
      "114/388, train_loss: 0.0813, step time: 0.4918\n",
      "115/388, train_loss: 0.1296, step time: 1.1019\n",
      "116/388, train_loss: 0.0786, step time: 0.5347\n",
      "117/388, train_loss: 0.1950, step time: 0.5067\n",
      "118/388, train_loss: 0.1612, step time: 0.4934\n",
      "119/388, train_loss: 0.2305, step time: 0.6812\n",
      "120/388, train_loss: 0.2142, step time: 0.5689\n",
      "121/388, train_loss: 0.0756, step time: 0.5375\n",
      "122/388, train_loss: 0.2296, step time: 0.5101\n",
      "123/388, train_loss: 0.0513, step time: 0.5536\n",
      "124/388, train_loss: 0.2427, step time: 0.5364\n",
      "125/388, train_loss: 0.5157, step time: 0.5093\n",
      "126/388, train_loss: 0.0670, step time: 0.5651\n",
      "127/388, train_loss: 0.2191, step time: 0.5378\n",
      "128/388, train_loss: 0.0618, step time: 0.5023\n",
      "129/388, train_loss: 0.2559, step time: 0.4899\n",
      "130/388, train_loss: 0.1293, step time: 0.4872\n",
      "131/388, train_loss: 0.1718, step time: 0.4843\n",
      "132/388, train_loss: 0.1628, step time: 0.4989\n",
      "133/388, train_loss: 0.2363, step time: 0.4788\n",
      "134/388, train_loss: 0.0842, step time: 1.0032\n",
      "135/388, train_loss: 0.0988, step time: 0.5525\n",
      "136/388, train_loss: 0.1071, step time: 0.5137\n",
      "137/388, train_loss: 0.1223, step time: 0.4952\n",
      "138/388, train_loss: 0.1198, step time: 0.4998\n",
      "139/388, train_loss: 0.1559, step time: 0.4869\n",
      "140/388, train_loss: 0.1476, step time: 0.5124\n",
      "141/388, train_loss: 0.1554, step time: 0.4855\n",
      "142/388, train_loss: 0.1359, step time: 0.4909\n",
      "143/388, train_loss: 0.1320, step time: 0.5016\n",
      "144/388, train_loss: 0.1178, step time: 0.5293\n",
      "145/388, train_loss: 0.0936, step time: 0.5054\n",
      "146/388, train_loss: 0.2039, step time: 0.4942\n",
      "147/388, train_loss: 0.1224, step time: 0.4929\n",
      "148/388, train_loss: 0.0909, step time: 0.9294\n",
      "149/388, train_loss: 0.2790, step time: 0.5404\n",
      "150/388, train_loss: 0.1289, step time: 0.5193\n",
      "151/388, train_loss: 0.2653, step time: 0.4989\n",
      "152/388, train_loss: 0.4201, step time: 0.4923\n",
      "153/388, train_loss: 0.0526, step time: 0.5380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/388, train_loss: 0.0972, step time: 0.5120\n",
      "155/388, train_loss: 0.2981, step time: 0.5010\n",
      "156/388, train_loss: 0.1565, step time: 0.4882\n",
      "157/388, train_loss: 0.1299, step time: 0.5355\n",
      "158/388, train_loss: 0.0865, step time: 0.5195\n",
      "159/388, train_loss: 0.3076, step time: 0.5065\n",
      "160/388, train_loss: 0.2520, step time: 0.4847\n",
      "161/388, train_loss: 0.1505, step time: 0.5276\n",
      "162/388, train_loss: 0.2343, step time: 0.5018\n",
      "163/388, train_loss: 0.1134, step time: 0.4937\n",
      "164/388, train_loss: 0.2796, step time: 0.5449\n",
      "165/388, train_loss: 0.0866, step time: 0.5399\n",
      "166/388, train_loss: 0.2564, step time: 0.6312\n",
      "167/388, train_loss: 0.2706, step time: 0.5537\n",
      "168/388, train_loss: 0.1401, step time: 0.5332\n",
      "169/388, train_loss: 0.1148, step time: 0.5038\n",
      "170/388, train_loss: 0.2277, step time: 0.5176\n",
      "171/388, train_loss: 0.1419, step time: 0.5353\n",
      "172/388, train_loss: 0.1015, step time: 0.5315\n",
      "173/388, train_loss: 0.0934, step time: 0.5383\n",
      "174/388, train_loss: 0.1676, step time: 0.5332\n",
      "175/388, train_loss: 0.2191, step time: 0.5561\n",
      "176/388, train_loss: 0.0945, step time: 0.7125\n",
      "177/388, train_loss: 0.0647, step time: 0.5382\n",
      "178/388, train_loss: 0.0687, step time: 0.5069\n",
      "179/388, train_loss: 0.2079, step time: 0.5058\n",
      "180/388, train_loss: 0.1815, step time: 0.5042\n",
      "181/388, train_loss: 0.0702, step time: 0.4922\n",
      "182/388, train_loss: 0.0440, step time: 0.4905\n",
      "183/388, train_loss: 0.1946, step time: 0.5180\n",
      "184/388, train_loss: 0.1525, step time: 0.5035\n",
      "185/388, train_loss: 0.1253, step time: 1.1288\n",
      "186/388, train_loss: 0.1448, step time: 0.5281\n",
      "187/388, train_loss: 0.1613, step time: 0.4943\n",
      "188/388, train_loss: 0.0848, step time: 0.5186\n",
      "189/388, train_loss: 0.0817, step time: 0.5276\n",
      "190/388, train_loss: 0.1771, step time: 0.5130\n",
      "191/388, train_loss: 0.1215, step time: 0.5402\n",
      "192/388, train_loss: 0.2838, step time: 0.5277\n",
      "193/388, train_loss: 0.2767, step time: 0.5170\n",
      "194/388, train_loss: 0.2170, step time: 0.5939\n",
      "195/388, train_loss: 0.1812, step time: 0.5331\n",
      "196/388, train_loss: 0.1377, step time: 0.5045\n",
      "197/388, train_loss: 0.0964, step time: 0.4862\n",
      "198/388, train_loss: 0.1791, step time: 0.4921\n",
      "199/388, train_loss: 0.3323, step time: 0.4937\n",
      "200/388, train_loss: 0.0538, step time: 1.0621\n",
      "201/388, train_loss: 0.1249, step time: 0.5491\n",
      "202/388, train_loss: 0.1624, step time: 0.5216\n",
      "203/388, train_loss: 0.2013, step time: 0.4980\n",
      "204/388, train_loss: 0.3469, step time: 0.5054\n",
      "205/388, train_loss: 0.1895, step time: 0.5190\n",
      "206/388, train_loss: 0.3239, step time: 0.4909\n",
      "207/388, train_loss: 0.3536, step time: 0.5474\n",
      "208/388, train_loss: 0.3259, step time: 0.6413\n",
      "209/388, train_loss: 0.1436, step time: 0.5477\n",
      "210/388, train_loss: 0.1080, step time: 0.5122\n",
      "211/388, train_loss: 0.0330, step time: 0.5008\n",
      "212/388, train_loss: 0.3804, step time: 0.4820\n",
      "213/388, train_loss: 0.1471, step time: 0.4782\n",
      "214/388, train_loss: 0.0549, step time: 0.4944\n",
      "215/388, train_loss: 0.5295, step time: 0.5842\n",
      "216/388, train_loss: 0.1509, step time: 0.5355\n",
      "217/388, train_loss: 0.1279, step time: 0.5178\n",
      "218/388, train_loss: 0.4163, step time: 0.4999\n",
      "219/388, train_loss: 0.0845, step time: 0.5065\n",
      "220/388, train_loss: 0.3854, step time: 0.4953\n",
      "221/388, train_loss: 0.3732, step time: 0.5245\n",
      "222/388, train_loss: 0.1854, step time: 0.5392\n",
      "223/388, train_loss: 0.0757, step time: 0.5158\n",
      "224/388, train_loss: 0.5805, step time: 0.5083\n",
      "225/388, train_loss: 0.1655, step time: 0.5508\n",
      "226/388, train_loss: 0.0443, step time: 0.5341\n",
      "227/388, train_loss: 0.2604, step time: 0.5093\n",
      "228/388, train_loss: 0.2060, step time: 0.4838\n",
      "229/388, train_loss: 0.2465, step time: 0.4812\n",
      "230/388, train_loss: 0.0585, step time: 0.6039\n",
      "231/388, train_loss: 0.4017, step time: 0.5454\n",
      "232/388, train_loss: 0.4334, step time: 0.5243\n",
      "233/388, train_loss: 0.1058, step time: 0.5387\n",
      "234/388, train_loss: 0.1005, step time: 0.5668\n",
      "235/388, train_loss: 0.3650, step time: 0.5251\n",
      "236/388, train_loss: 0.1828, step time: 0.4893\n",
      "237/388, train_loss: 0.2486, step time: 0.4931\n",
      "238/388, train_loss: 0.3934, step time: 0.5080\n",
      "239/388, train_loss: 0.2584, step time: 0.5074\n",
      "240/388, train_loss: 0.1179, step time: 0.4971\n",
      "241/388, train_loss: 0.4085, step time: 0.5128\n",
      "242/388, train_loss: 0.3420, step time: 0.4861\n",
      "243/388, train_loss: 0.2509, step time: 0.4856\n",
      "244/388, train_loss: 0.1896, step time: 0.5243\n",
      "245/388, train_loss: 0.2439, step time: 0.5120\n",
      "246/388, train_loss: 0.3005, step time: 0.6662\n",
      "247/388, train_loss: 0.0866, step time: 0.5803\n",
      "248/388, train_loss: 0.0989, step time: 0.5373\n",
      "249/388, train_loss: 0.2399, step time: 0.5321\n",
      "250/388, train_loss: 0.0620, step time: 0.6111\n",
      "251/388, train_loss: 0.0955, step time: 0.5552\n",
      "252/388, train_loss: 0.0706, step time: 0.5254\n",
      "253/388, train_loss: 0.1760, step time: 0.5039\n",
      "254/388, train_loss: 0.1459, step time: 0.5020\n",
      "255/388, train_loss: 0.2043, step time: 0.4845\n",
      "256/388, train_loss: 0.2017, step time: 0.4862\n",
      "257/388, train_loss: 0.1659, step time: 0.5138\n",
      "258/388, train_loss: 0.0966, step time: 0.4940\n",
      "259/388, train_loss: 0.2173, step time: 0.4842\n",
      "260/388, train_loss: 0.1478, step time: 0.4877\n",
      "261/388, train_loss: 0.1942, step time: 0.5884\n",
      "262/388, train_loss: 0.1189, step time: 0.5767\n",
      "263/388, train_loss: 0.4744, step time: 0.5533\n",
      "264/388, train_loss: 0.1058, step time: 0.5166\n",
      "265/388, train_loss: 0.1314, step time: 0.4971\n",
      "266/388, train_loss: 0.2694, step time: 0.4876\n",
      "267/388, train_loss: 0.1024, step time: 0.4898\n",
      "268/388, train_loss: 0.1558, step time: 0.4758\n",
      "269/388, train_loss: 0.0837, step time: 0.5310\n",
      "270/388, train_loss: 0.1907, step time: 0.6763\n",
      "271/388, train_loss: 0.0904, step time: 0.5607\n",
      "272/388, train_loss: 0.1007, step time: 0.5169\n",
      "273/388, train_loss: 0.1595, step time: 0.4874\n",
      "274/388, train_loss: 0.1139, step time: 0.5200\n",
      "275/388, train_loss: 0.0620, step time: 0.5653\n",
      "276/388, train_loss: 0.0618, step time: 0.5403\n",
      "277/388, train_loss: 0.1065, step time: 0.5214\n",
      "278/388, train_loss: 0.1062, step time: 0.5269\n",
      "279/388, train_loss: 0.1334, step time: 0.5273\n",
      "280/388, train_loss: 0.1557, step time: 0.5342\n",
      "281/388, train_loss: 0.1905, step time: 0.5185\n",
      "282/388, train_loss: 0.1384, step time: 0.5161\n",
      "283/388, train_loss: 0.6487, step time: 0.4981\n",
      "284/388, train_loss: 0.2691, step time: 0.5110\n",
      "285/388, train_loss: 0.1108, step time: 0.4977\n",
      "286/388, train_loss: 0.2478, step time: 1.1241\n",
      "287/388, train_loss: 0.1488, step time: 0.5424\n",
      "288/388, train_loss: 0.0901, step time: 0.5089\n",
      "289/388, train_loss: 0.0346, step time: 0.5004\n",
      "290/388, train_loss: 0.2625, step time: 0.4996\n",
      "291/388, train_loss: 0.1177, step time: 0.4823\n",
      "292/388, train_loss: 0.1028, step time: 0.4885\n",
      "293/388, train_loss: 0.0598, step time: 0.4931\n",
      "294/388, train_loss: 0.3357, step time: 1.1376\n",
      "295/388, train_loss: 0.3396, step time: 0.5351\n",
      "296/388, train_loss: 0.2863, step time: 0.5070\n",
      "297/388, train_loss: 0.2362, step time: 0.4883\n",
      "298/388, train_loss: 0.4897, step time: 0.4907\n",
      "299/388, train_loss: 0.1432, step time: 0.5011\n",
      "300/388, train_loss: 0.1118, step time: 0.4965\n",
      "301/388, train_loss: 0.3261, step time: 0.5097\n",
      "302/388, train_loss: 0.1026, step time: 0.4946\n",
      "303/388, train_loss: 0.1716, step time: 0.4981\n",
      "304/388, train_loss: 0.3059, step time: 0.4800\n",
      "305/388, train_loss: 0.1055, step time: 0.5057\n",
      "306/388, train_loss: 0.1993, step time: 0.5901\n",
      "307/388, train_loss: 0.2246, step time: 0.5912\n",
      "308/388, train_loss: 0.1822, step time: 0.5340\n",
      "309/388, train_loss: 0.0924, step time: 0.5136\n",
      "310/388, train_loss: 0.0698, step time: 0.4969\n",
      "311/388, train_loss: 0.2107, step time: 0.4891\n",
      "312/388, train_loss: 0.0941, step time: 0.5534\n",
      "313/388, train_loss: 0.2194, step time: 0.5386\n",
      "314/388, train_loss: 0.0875, step time: 0.5086\n",
      "315/388, train_loss: 0.2776, step time: 0.5027\n",
      "316/388, train_loss: 0.2337, step time: 0.4959\n",
      "317/388, train_loss: 0.0912, step time: 0.4925\n",
      "318/388, train_loss: 0.1836, step time: 0.5079\n",
      "319/388, train_loss: 0.0652, step time: 0.4990\n",
      "320/388, train_loss: 0.1706, step time: 0.5469\n",
      "321/388, train_loss: 0.1167, step time: 0.5370\n",
      "322/388, train_loss: 0.2695, step time: 0.5169\n",
      "323/388, train_loss: 0.2669, step time: 0.5711\n",
      "324/388, train_loss: 0.1824, step time: 0.5327\n",
      "325/388, train_loss: 0.1860, step time: 0.5051\n",
      "326/388, train_loss: 0.1132, step time: 0.4981\n",
      "327/388, train_loss: 0.0674, step time: 0.5181\n",
      "328/388, train_loss: 0.0876, step time: 0.4968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/388, train_loss: 0.0583, step time: 0.4982\n",
      "330/388, train_loss: 0.2722, step time: 0.4985\n",
      "331/388, train_loss: 0.2647, step time: 0.5020\n",
      "332/388, train_loss: 0.1820, step time: 0.5825\n",
      "333/388, train_loss: 0.1107, step time: 0.5271\n",
      "334/388, train_loss: 0.0818, step time: 0.4913\n",
      "335/388, train_loss: 0.3419, step time: 0.5585\n",
      "336/388, train_loss: 0.1920, step time: 0.5410\n",
      "337/388, train_loss: 0.2943, step time: 0.5079\n",
      "338/388, train_loss: 0.1812, step time: 0.5118\n",
      "339/388, train_loss: 0.2486, step time: 0.4943\n",
      "340/388, train_loss: 0.1438, step time: 0.4904\n",
      "341/388, train_loss: 0.1896, step time: 1.1904\n",
      "342/388, train_loss: 0.2091, step time: 0.5360\n",
      "343/388, train_loss: 0.1724, step time: 0.4991\n",
      "344/388, train_loss: 0.1946, step time: 0.4959\n",
      "345/388, train_loss: 0.2021, step time: 0.4965\n",
      "346/388, train_loss: 0.1203, step time: 0.4839\n",
      "347/388, train_loss: 0.1594, step time: 0.4950\n",
      "348/388, train_loss: 0.1894, step time: 0.4761\n",
      "349/388, train_loss: 0.1275, step time: 0.4831\n",
      "350/388, train_loss: 0.1901, step time: 0.4908\n",
      "351/388, train_loss: 0.0667, step time: 1.1265\n",
      "352/388, train_loss: 0.2826, step time: 0.5339\n",
      "353/388, train_loss: 0.2444, step time: 0.5195\n",
      "354/388, train_loss: 0.0600, step time: 0.4848\n",
      "355/388, train_loss: 0.0752, step time: 0.5048\n",
      "356/388, train_loss: 0.3723, step time: 0.4858\n",
      "357/388, train_loss: 0.1094, step time: 0.4883\n",
      "358/388, train_loss: 0.1393, step time: 0.5176\n",
      "359/388, train_loss: 0.4078, step time: 0.5049\n",
      "360/388, train_loss: 0.2191, step time: 0.4889\n",
      "361/388, train_loss: 0.1141, step time: 0.4980\n",
      "362/388, train_loss: 0.1268, step time: 0.4793\n",
      "363/388, train_loss: 0.0627, step time: 0.4799\n",
      "364/388, train_loss: 0.2750, step time: 0.4964\n",
      "365/388, train_loss: 0.2025, step time: 0.4879\n",
      "366/388, train_loss: 0.1327, step time: 0.4975\n",
      "367/388, train_loss: 0.2924, step time: 0.4903\n",
      "368/388, train_loss: 0.2414, step time: 0.4812\n",
      "369/388, train_loss: 0.1410, step time: 0.4935\n",
      "370/388, train_loss: 0.2443, step time: 0.4885\n",
      "371/388, train_loss: 0.2022, step time: 1.2447\n",
      "372/388, train_loss: 0.1915, step time: 0.5303\n",
      "373/388, train_loss: 0.1070, step time: 0.5061\n",
      "374/388, train_loss: 0.1722, step time: 0.4934\n",
      "375/388, train_loss: 0.0910, step time: 0.4926\n",
      "376/388, train_loss: 0.1520, step time: 0.4842\n",
      "377/388, train_loss: 0.1347, step time: 0.4928\n",
      "378/388, train_loss: 0.2899, step time: 0.5201\n",
      "379/388, train_loss: 0.1990, step time: 0.5749\n",
      "380/388, train_loss: 0.3050, step time: 0.5407\n",
      "381/388, train_loss: 0.1410, step time: 0.5059\n",
      "382/388, train_loss: 0.2062, step time: 0.4873\n",
      "383/388, train_loss: 0.1691, step time: 0.4851\n",
      "384/388, train_loss: 0.1479, step time: 0.4792\n",
      "385/388, train_loss: 0.2393, step time: 0.4725\n",
      "386/388, train_loss: 0.1359, step time: 0.8443\n",
      "387/388, train_loss: 0.0436, step time: 0.5353\n",
      "388/388, train_loss: 0.1782, step time: 0.5112\n",
      "epoch 62 average loss: 0.1808\n",
      "current epoch: 62 current mean dice: 0.7656 tc: 0.8196 wt: 0.8989 et: 0.5783\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 62 is: 300.4090\n",
      "----------\n",
      "epoch 63/300\n",
      "1/388, train_loss: 0.0581, step time: 0.4709\n",
      "2/388, train_loss: 0.1031, step time: 0.4766\n",
      "3/388, train_loss: 0.1012, step time: 0.4738\n",
      "4/388, train_loss: 0.1007, step time: 1.0490\n",
      "5/388, train_loss: 0.0853, step time: 0.5363\n",
      "6/388, train_loss: 0.0811, step time: 0.5267\n",
      "7/388, train_loss: 0.1572, step time: 0.5011\n",
      "8/388, train_loss: 0.1157, step time: 0.4830\n",
      "9/388, train_loss: 0.1261, step time: 0.5131\n",
      "10/388, train_loss: 0.1130, step time: 0.5074\n",
      "11/388, train_loss: 0.0911, step time: 0.5201\n",
      "12/388, train_loss: 0.5319, step time: 0.5243\n",
      "13/388, train_loss: 0.1290, step time: 0.5048\n",
      "14/388, train_loss: 0.2913, step time: 0.5004\n",
      "15/388, train_loss: 0.2892, step time: 0.6798\n",
      "16/388, train_loss: 0.0856, step time: 0.5492\n",
      "17/388, train_loss: 0.1542, step time: 0.5196\n",
      "18/388, train_loss: 0.1354, step time: 0.4971\n",
      "19/388, train_loss: 0.1407, step time: 0.4918\n",
      "20/388, train_loss: 0.1242, step time: 0.4828\n",
      "21/388, train_loss: 0.1911, step time: 1.0948\n",
      "22/388, train_loss: 0.1585, step time: 0.5545\n",
      "23/388, train_loss: 0.1000, step time: 0.5296\n",
      "24/388, train_loss: 0.0735, step time: 0.5038\n",
      "25/388, train_loss: 0.0520, step time: 0.5204\n",
      "26/388, train_loss: 0.4570, step time: 0.5253\n",
      "27/388, train_loss: 0.0998, step time: 0.5183\n",
      "28/388, train_loss: 0.0928, step time: 0.5031\n",
      "29/388, train_loss: 0.2443, step time: 0.4967\n",
      "30/388, train_loss: 0.0538, step time: 0.5357\n",
      "31/388, train_loss: 0.1978, step time: 0.5165\n",
      "32/388, train_loss: 0.1262, step time: 0.4985\n",
      "33/388, train_loss: 0.2109, step time: 0.4909\n",
      "34/388, train_loss: 0.2557, step time: 0.4958\n",
      "35/388, train_loss: 0.1754, step time: 0.4876\n",
      "36/388, train_loss: 0.1009, step time: 0.7090\n",
      "37/388, train_loss: 0.2744, step time: 0.5556\n",
      "38/388, train_loss: 0.0350, step time: 0.4970\n",
      "39/388, train_loss: 0.2357, step time: 0.4957\n",
      "40/388, train_loss: 0.3262, step time: 0.4816\n",
      "41/388, train_loss: 0.0456, step time: 0.5038\n",
      "42/388, train_loss: 0.1048, step time: 0.4972\n",
      "43/388, train_loss: 0.1148, step time: 0.5019\n",
      "44/388, train_loss: 0.5917, step time: 0.6954\n",
      "45/388, train_loss: 0.2366, step time: 0.5549\n",
      "46/388, train_loss: 0.1801, step time: 0.5250\n",
      "47/388, train_loss: 0.1087, step time: 0.5068\n",
      "48/388, train_loss: 0.0969, step time: 0.5299\n",
      "49/388, train_loss: 0.2675, step time: 0.5227\n",
      "50/388, train_loss: 0.1139, step time: 0.5027\n",
      "51/388, train_loss: 0.2309, step time: 0.4969\n",
      "52/388, train_loss: 0.1311, step time: 0.5180\n",
      "53/388, train_loss: 0.2010, step time: 0.5266\n",
      "54/388, train_loss: 0.1467, step time: 0.5342\n",
      "55/388, train_loss: 0.1701, step time: 0.5742\n",
      "56/388, train_loss: 0.3880, step time: 0.5441\n",
      "57/388, train_loss: 0.2021, step time: 0.5197\n",
      "58/388, train_loss: 0.0388, step time: 0.5205\n",
      "59/388, train_loss: 0.1527, step time: 0.5012\n",
      "60/388, train_loss: 0.0639, step time: 0.4909\n",
      "61/388, train_loss: 0.0897, step time: 0.4930\n",
      "62/388, train_loss: 0.1211, step time: 0.4872\n",
      "63/388, train_loss: 0.1585, step time: 0.4945\n",
      "64/388, train_loss: 0.1294, step time: 0.8343\n",
      "65/388, train_loss: 0.2348, step time: 0.5533\n",
      "66/388, train_loss: 0.2501, step time: 0.5092\n",
      "67/388, train_loss: 0.0828, step time: 0.5036\n",
      "68/388, train_loss: 0.2293, step time: 0.4805\n",
      "69/388, train_loss: 0.1017, step time: 0.4810\n",
      "70/388, train_loss: 0.2363, step time: 0.4813\n",
      "71/388, train_loss: 0.0619, step time: 0.6098\n",
      "72/388, train_loss: 0.1678, step time: 0.5651\n",
      "73/388, train_loss: 0.2643, step time: 0.5276\n",
      "74/388, train_loss: 0.2048, step time: 0.5008\n",
      "75/388, train_loss: 0.1691, step time: 0.4805\n",
      "76/388, train_loss: 0.2355, step time: 0.5285\n",
      "77/388, train_loss: 0.1168, step time: 0.5020\n",
      "78/388, train_loss: 0.1677, step time: 0.5002\n",
      "79/388, train_loss: 0.4071, step time: 0.4865\n",
      "80/388, train_loss: 0.2342, step time: 0.5023\n",
      "81/388, train_loss: 0.1018, step time: 0.5112\n",
      "82/388, train_loss: 0.1074, step time: 0.5015\n",
      "83/388, train_loss: 0.2071, step time: 0.5006\n",
      "84/388, train_loss: 0.2061, step time: 0.4913\n",
      "85/388, train_loss: 0.0631, step time: 1.1678\n",
      "86/388, train_loss: 0.1738, step time: 0.5371\n",
      "87/388, train_loss: 0.1167, step time: 0.5180\n",
      "88/388, train_loss: 0.0945, step time: 0.5026\n",
      "89/388, train_loss: 0.0954, step time: 0.5016\n",
      "90/388, train_loss: 0.2655, step time: 0.4859\n",
      "91/388, train_loss: 0.1071, step time: 1.0902\n",
      "92/388, train_loss: 0.1074, step time: 0.5235\n",
      "93/388, train_loss: 0.2004, step time: 0.5100\n",
      "94/388, train_loss: 0.0624, step time: 0.4895\n",
      "95/388, train_loss: 0.1826, step time: 0.4922\n",
      "96/388, train_loss: 0.1314, step time: 0.4773\n",
      "97/388, train_loss: 0.0969, step time: 0.5149\n",
      "98/388, train_loss: 0.1012, step time: 0.4897\n",
      "99/388, train_loss: 0.1865, step time: 0.4899\n",
      "100/388, train_loss: 0.0806, step time: 0.4868\n",
      "101/388, train_loss: 0.1052, step time: 0.9661\n",
      "102/388, train_loss: 0.0813, step time: 0.5419\n",
      "103/388, train_loss: 0.2248, step time: 0.5160\n",
      "104/388, train_loss: 0.2737, step time: 0.5055\n",
      "105/388, train_loss: 0.2868, step time: 0.4941\n",
      "106/388, train_loss: 0.0965, step time: 0.5162\n",
      "107/388, train_loss: 0.1594, step time: 0.4971\n",
      "108/388, train_loss: 0.1991, step time: 0.4910\n",
      "109/388, train_loss: 0.0964, step time: 0.4810\n",
      "110/388, train_loss: 0.3160, step time: 0.7555\n",
      "111/388, train_loss: 0.0705, step time: 0.5295\n",
      "112/388, train_loss: 0.2815, step time: 0.5002\n",
      "113/388, train_loss: 0.1308, step time: 0.4975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/388, train_loss: 0.3383, step time: 0.4971\n",
      "115/388, train_loss: 0.0976, step time: 0.5335\n",
      "116/388, train_loss: 0.3762, step time: 0.5228\n",
      "117/388, train_loss: 0.2181, step time: 0.4967\n",
      "118/388, train_loss: 0.1487, step time: 0.5055\n",
      "119/388, train_loss: 0.1212, step time: 0.4971\n",
      "120/388, train_loss: 0.2709, step time: 0.4960\n",
      "121/388, train_loss: 0.1969, step time: 1.1614\n",
      "122/388, train_loss: 0.1370, step time: 0.5320\n",
      "123/388, train_loss: 0.2180, step time: 0.5060\n",
      "124/388, train_loss: 0.3090, step time: 0.4965\n",
      "125/388, train_loss: 0.1745, step time: 0.4822\n",
      "126/388, train_loss: 0.3742, step time: 0.4817\n",
      "127/388, train_loss: 0.0879, step time: 0.4918\n",
      "128/388, train_loss: 0.0834, step time: 0.4852\n",
      "129/388, train_loss: 0.3636, step time: 0.4914\n",
      "130/388, train_loss: 0.1690, step time: 0.4918\n",
      "131/388, train_loss: 0.1067, step time: 0.4925\n",
      "132/388, train_loss: 0.1591, step time: 1.2275\n",
      "133/388, train_loss: 0.2373, step time: 0.5371\n",
      "134/388, train_loss: 0.1040, step time: 0.5035\n",
      "135/388, train_loss: 0.2134, step time: 0.5129\n",
      "136/388, train_loss: 0.0482, step time: 0.5131\n",
      "137/388, train_loss: 0.1453, step time: 0.5012\n",
      "138/388, train_loss: 0.2344, step time: 0.4946\n",
      "139/388, train_loss: 0.1710, step time: 0.4846\n",
      "140/388, train_loss: 0.0868, step time: 0.5181\n",
      "141/388, train_loss: 0.0386, step time: 0.4936\n",
      "142/388, train_loss: 0.1422, step time: 0.4877\n",
      "143/388, train_loss: 0.0483, step time: 0.5085\n",
      "144/388, train_loss: 0.1940, step time: 0.4951\n",
      "145/388, train_loss: 0.1500, step time: 0.5004\n",
      "146/388, train_loss: 0.2854, step time: 0.4847\n",
      "147/388, train_loss: 0.4967, step time: 0.9113\n",
      "148/388, train_loss: 0.1302, step time: 0.5619\n",
      "149/388, train_loss: 0.1994, step time: 0.5211\n",
      "150/388, train_loss: 0.0760, step time: 0.4940\n",
      "151/388, train_loss: 0.3432, step time: 0.4972\n",
      "152/388, train_loss: 0.2928, step time: 0.4810\n",
      "153/388, train_loss: 0.2383, step time: 0.4811\n",
      "154/388, train_loss: 0.1117, step time: 0.5347\n",
      "155/388, train_loss: 0.2191, step time: 0.5034\n",
      "156/388, train_loss: 0.0936, step time: 0.4856\n",
      "157/388, train_loss: 0.1838, step time: 0.4823\n",
      "158/388, train_loss: 0.2194, step time: 0.8649\n",
      "159/388, train_loss: 0.1134, step time: 0.5681\n",
      "160/388, train_loss: 0.0590, step time: 0.5293\n",
      "161/388, train_loss: 0.0609, step time: 0.5016\n",
      "162/388, train_loss: 0.2554, step time: 0.4987\n",
      "163/388, train_loss: 0.1161, step time: 0.4812\n",
      "164/388, train_loss: 0.0695, step time: 0.4867\n",
      "165/388, train_loss: 0.4323, step time: 0.5127\n",
      "166/388, train_loss: 0.1458, step time: 0.7860\n",
      "167/388, train_loss: 0.1660, step time: 0.5598\n",
      "168/388, train_loss: 0.0986, step time: 0.5269\n",
      "169/388, train_loss: 0.1269, step time: 0.5077\n",
      "170/388, train_loss: 0.4978, step time: 0.4916\n",
      "171/388, train_loss: 0.4154, step time: 0.4981\n",
      "172/388, train_loss: 0.0448, step time: 0.4794\n",
      "173/388, train_loss: 0.1438, step time: 1.1263\n",
      "174/388, train_loss: 0.2073, step time: 0.5367\n",
      "175/388, train_loss: 0.2720, step time: 0.5008\n",
      "176/388, train_loss: 0.2743, step time: 0.5000\n",
      "177/388, train_loss: 0.2368, step time: 0.4815\n",
      "178/388, train_loss: 0.1707, step time: 0.4818\n",
      "179/388, train_loss: 0.0595, step time: 0.4899\n",
      "180/388, train_loss: 0.3891, step time: 0.4937\n",
      "181/388, train_loss: 0.0906, step time: 0.4985\n",
      "182/388, train_loss: 0.1312, step time: 0.4797\n",
      "183/388, train_loss: 0.2394, step time: 0.4827\n",
      "184/388, train_loss: 0.0845, step time: 1.1030\n",
      "185/388, train_loss: 0.1481, step time: 0.5303\n",
      "186/388, train_loss: 0.2859, step time: 0.5163\n",
      "187/388, train_loss: 0.0595, step time: 0.4950\n",
      "188/388, train_loss: 0.0638, step time: 0.4961\n",
      "189/388, train_loss: 0.3124, step time: 0.4892\n",
      "190/388, train_loss: 0.1783, step time: 0.4940\n",
      "191/388, train_loss: 0.3733, step time: 0.5779\n",
      "192/388, train_loss: 0.3125, step time: 0.5779\n",
      "193/388, train_loss: 0.1323, step time: 0.5418\n",
      "194/388, train_loss: 0.2004, step time: 0.5060\n",
      "195/388, train_loss: 0.1229, step time: 0.5009\n",
      "196/388, train_loss: 0.1565, step time: 1.1648\n",
      "197/388, train_loss: 0.1837, step time: 0.5408\n",
      "198/388, train_loss: 0.2287, step time: 0.5127\n",
      "199/388, train_loss: 0.0961, step time: 0.4989\n",
      "200/388, train_loss: 0.2098, step time: 0.4890\n",
      "201/388, train_loss: 0.0634, step time: 0.4872\n",
      "202/388, train_loss: 0.1610, step time: 0.5042\n",
      "203/388, train_loss: 0.0949, step time: 0.5357\n",
      "204/388, train_loss: 0.1152, step time: 0.5152\n",
      "205/388, train_loss: 0.2836, step time: 0.5031\n",
      "206/388, train_loss: 0.2426, step time: 0.4845\n",
      "207/388, train_loss: 0.1625, step time: 0.4963\n",
      "208/388, train_loss: 0.1042, step time: 0.4881\n",
      "209/388, train_loss: 0.2186, step time: 0.4879\n",
      "210/388, train_loss: 0.1155, step time: 1.1739\n",
      "211/388, train_loss: 0.1083, step time: 0.5319\n",
      "212/388, train_loss: 0.2953, step time: 0.5097\n",
      "213/388, train_loss: 0.0382, step time: 0.4870\n",
      "214/388, train_loss: 0.0752, step time: 0.4947\n",
      "215/388, train_loss: 0.0998, step time: 0.4835\n",
      "216/388, train_loss: 0.2190, step time: 0.4810\n",
      "217/388, train_loss: 0.1478, step time: 0.5211\n",
      "218/388, train_loss: 0.1848, step time: 0.5171\n",
      "219/388, train_loss: 0.1690, step time: 0.5172\n",
      "220/388, train_loss: 0.0879, step time: 0.5191\n",
      "221/388, train_loss: 0.1220, step time: 0.5533\n",
      "222/388, train_loss: 0.2931, step time: 0.5309\n",
      "223/388, train_loss: 0.0875, step time: 0.5106\n",
      "224/388, train_loss: 0.2023, step time: 0.4967\n",
      "225/388, train_loss: 0.3543, step time: 0.4950\n",
      "226/388, train_loss: 0.3560, step time: 0.4801\n",
      "227/388, train_loss: 0.1258, step time: 1.0937\n",
      "228/388, train_loss: 0.3110, step time: 0.5391\n",
      "229/388, train_loss: 0.2705, step time: 0.5145\n",
      "230/388, train_loss: 0.2504, step time: 0.4885\n",
      "231/388, train_loss: 0.1198, step time: 0.4915\n",
      "232/388, train_loss: 0.1674, step time: 0.4945\n",
      "233/388, train_loss: 0.3250, step time: 0.5464\n",
      "234/388, train_loss: 0.3095, step time: 0.5195\n",
      "235/388, train_loss: 0.1630, step time: 0.5049\n",
      "236/388, train_loss: 0.2629, step time: 0.4888\n",
      "237/388, train_loss: 0.1204, step time: 0.5280\n",
      "238/388, train_loss: 0.2918, step time: 0.4930\n",
      "239/388, train_loss: 0.0845, step time: 0.4962\n",
      "240/388, train_loss: 0.0760, step time: 0.4806\n",
      "241/388, train_loss: 0.1426, step time: 0.5186\n",
      "242/388, train_loss: 0.5663, step time: 0.4976\n",
      "243/388, train_loss: 0.0842, step time: 1.0409\n",
      "244/388, train_loss: 0.2751, step time: 0.5409\n",
      "245/388, train_loss: 0.1105, step time: 0.5108\n",
      "246/388, train_loss: 0.1736, step time: 0.4917\n",
      "247/388, train_loss: 0.1227, step time: 0.4791\n",
      "248/388, train_loss: 0.1415, step time: 0.4808\n",
      "249/388, train_loss: 0.1328, step time: 0.5100\n",
      "250/388, train_loss: 0.4137, step time: 0.5071\n",
      "251/388, train_loss: 0.1579, step time: 0.5295\n",
      "252/388, train_loss: 0.1720, step time: 0.5471\n",
      "253/388, train_loss: 0.1973, step time: 0.5935\n",
      "254/388, train_loss: 0.0956, step time: 0.5333\n",
      "255/388, train_loss: 0.0913, step time: 0.5122\n",
      "256/388, train_loss: 0.1293, step time: 0.4997\n",
      "257/388, train_loss: 0.5014, step time: 0.5116\n",
      "258/388, train_loss: 0.1551, step time: 0.5778\n",
      "259/388, train_loss: 0.3357, step time: 0.5368\n",
      "260/388, train_loss: 0.1073, step time: 0.5030\n",
      "261/388, train_loss: 0.1248, step time: 0.5055\n",
      "262/388, train_loss: 0.1074, step time: 0.5435\n",
      "263/388, train_loss: 0.1085, step time: 0.5290\n",
      "264/388, train_loss: 0.1160, step time: 0.5149\n",
      "265/388, train_loss: 0.3410, step time: 0.5227\n",
      "266/388, train_loss: 0.1456, step time: 0.5109\n",
      "267/388, train_loss: 0.1893, step time: 0.5018\n",
      "268/388, train_loss: 0.2667, step time: 0.4875\n",
      "269/388, train_loss: 0.1326, step time: 0.4977\n",
      "270/388, train_loss: 0.4437, step time: 0.4841\n",
      "271/388, train_loss: 0.1943, step time: 1.0807\n",
      "272/388, train_loss: 0.0783, step time: 0.5616\n",
      "273/388, train_loss: 0.0608, step time: 0.5190\n",
      "274/388, train_loss: 0.1052, step time: 0.4993\n",
      "275/388, train_loss: 0.1837, step time: 0.5033\n",
      "276/388, train_loss: 0.2012, step time: 0.5159\n",
      "277/388, train_loss: 0.2820, step time: 0.5783\n",
      "278/388, train_loss: 0.1310, step time: 0.5468\n",
      "279/388, train_loss: 0.3460, step time: 0.5147\n",
      "280/388, train_loss: 0.0853, step time: 0.5043\n",
      "281/388, train_loss: 0.2632, step time: 0.5165\n",
      "282/388, train_loss: 0.2779, step time: 0.5150\n",
      "283/388, train_loss: 0.3859, step time: 0.5089\n",
      "284/388, train_loss: 0.2578, step time: 0.4860\n",
      "285/388, train_loss: 0.2000, step time: 1.0085\n",
      "286/388, train_loss: 0.1334, step time: 0.5506\n",
      "287/388, train_loss: 0.0806, step time: 0.5319\n",
      "288/388, train_loss: 0.2788, step time: 0.5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/388, train_loss: 0.2520, step time: 0.5934\n",
      "290/388, train_loss: 0.0561, step time: 0.5412\n",
      "291/388, train_loss: 0.1218, step time: 0.5260\n",
      "292/388, train_loss: 0.2453, step time: 0.4996\n",
      "293/388, train_loss: 0.1219, step time: 0.4944\n",
      "294/388, train_loss: 0.1505, step time: 0.4781\n",
      "295/388, train_loss: 0.1680, step time: 0.5114\n",
      "296/388, train_loss: 0.1322, step time: 0.5382\n",
      "297/388, train_loss: 0.1200, step time: 0.5235\n",
      "298/388, train_loss: 0.1681, step time: 0.5141\n",
      "299/388, train_loss: 0.0778, step time: 0.4914\n",
      "300/388, train_loss: 0.1095, step time: 0.4877\n",
      "301/388, train_loss: 0.1246, step time: 0.6901\n",
      "302/388, train_loss: 0.2787, step time: 0.5647\n",
      "303/388, train_loss: 0.2365, step time: 0.5495\n",
      "304/388, train_loss: 0.1727, step time: 0.5165\n",
      "305/388, train_loss: 0.0792, step time: 0.5040\n",
      "306/388, train_loss: 0.0414, step time: 0.5389\n",
      "307/388, train_loss: 0.0869, step time: 0.5477\n",
      "308/388, train_loss: 0.1804, step time: 0.5314\n",
      "309/388, train_loss: 0.2057, step time: 0.4981\n",
      "310/388, train_loss: 0.1566, step time: 0.4928\n",
      "311/388, train_loss: 0.1267, step time: 0.4927\n",
      "312/388, train_loss: 0.2176, step time: 1.0978\n",
      "313/388, train_loss: 0.2375, step time: 0.5360\n",
      "314/388, train_loss: 0.0936, step time: 0.5104\n",
      "315/388, train_loss: 0.0899, step time: 0.4974\n",
      "316/388, train_loss: 0.0897, step time: 0.5546\n",
      "317/388, train_loss: 0.1522, step time: 0.5295\n",
      "318/388, train_loss: 0.1857, step time: 0.5002\n",
      "319/388, train_loss: 0.1551, step time: 0.5083\n",
      "320/388, train_loss: 0.2652, step time: 0.4879\n",
      "321/388, train_loss: 0.2779, step time: 0.4995\n",
      "322/388, train_loss: 0.1674, step time: 0.4958\n",
      "323/388, train_loss: 0.0881, step time: 0.4895\n",
      "324/388, train_loss: 0.3353, step time: 1.1210\n",
      "325/388, train_loss: 0.1498, step time: 0.5540\n",
      "326/388, train_loss: 0.0856, step time: 0.5238\n",
      "327/388, train_loss: 0.6290, step time: 0.5014\n",
      "328/388, train_loss: 0.0825, step time: 0.4989\n",
      "329/388, train_loss: 0.2800, step time: 0.9813\n",
      "330/388, train_loss: 0.1788, step time: 0.5394\n",
      "331/388, train_loss: 0.4443, step time: 0.5077\n",
      "332/388, train_loss: 0.1085, step time: 0.4897\n",
      "333/388, train_loss: 0.2584, step time: 0.5002\n",
      "334/388, train_loss: 0.1732, step time: 0.4975\n",
      "335/388, train_loss: 0.0905, step time: 0.4829\n",
      "336/388, train_loss: 0.1655, step time: 0.4928\n",
      "337/388, train_loss: 0.1015, step time: 0.4981\n",
      "338/388, train_loss: 0.0652, step time: 0.4925\n",
      "339/388, train_loss: 0.2569, step time: 1.0569\n",
      "340/388, train_loss: 0.2474, step time: 0.5519\n",
      "341/388, train_loss: 0.2560, step time: 0.5218\n",
      "342/388, train_loss: 0.2890, step time: 0.4943\n",
      "343/388, train_loss: 0.1832, step time: 0.5095\n",
      "344/388, train_loss: 0.1312, step time: 0.5041\n",
      "345/388, train_loss: 0.2277, step time: 0.5027\n",
      "346/388, train_loss: 0.1318, step time: 0.4855\n",
      "347/388, train_loss: 0.1493, step time: 1.1132\n",
      "348/388, train_loss: 0.1427, step time: 0.5273\n",
      "349/388, train_loss: 0.1730, step time: 0.4993\n",
      "350/388, train_loss: 0.1409, step time: 0.4915\n",
      "351/388, train_loss: 0.0646, step time: 0.4958\n",
      "352/388, train_loss: 0.1720, step time: 0.5054\n",
      "353/388, train_loss: 0.1959, step time: 0.5032\n",
      "354/388, train_loss: 0.1856, step time: 0.5452\n",
      "355/388, train_loss: 0.2018, step time: 0.5257\n",
      "356/388, train_loss: 0.1018, step time: 0.5086\n",
      "357/388, train_loss: 0.1150, step time: 0.5283\n",
      "358/388, train_loss: 0.0680, step time: 0.6088\n",
      "359/388, train_loss: 0.1927, step time: 0.5378\n",
      "360/388, train_loss: 0.2182, step time: 0.5092\n",
      "361/388, train_loss: 0.1439, step time: 0.4863\n",
      "362/388, train_loss: 0.2803, step time: 0.4907\n",
      "363/388, train_loss: 0.0871, step time: 1.2023\n",
      "364/388, train_loss: 0.2046, step time: 0.5408\n",
      "365/388, train_loss: 0.2547, step time: 0.5041\n",
      "366/388, train_loss: 0.1463, step time: 0.4844\n",
      "367/388, train_loss: 0.4858, step time: 0.4788\n",
      "368/388, train_loss: 0.4483, step time: 0.4924\n",
      "369/388, train_loss: 0.1247, step time: 0.4798\n",
      "370/388, train_loss: 0.2093, step time: 1.0841\n",
      "371/388, train_loss: 0.0661, step time: 0.5248\n",
      "372/388, train_loss: 0.2427, step time: 0.5113\n",
      "373/388, train_loss: 0.2406, step time: 0.4878\n",
      "374/388, train_loss: 0.0898, step time: 0.5139\n",
      "375/388, train_loss: 0.0831, step time: 0.4956\n",
      "376/388, train_loss: 0.5004, step time: 0.4908\n",
      "377/388, train_loss: 0.1616, step time: 0.4957\n",
      "378/388, train_loss: 0.1230, step time: 0.5096\n",
      "379/388, train_loss: 0.4106, step time: 0.5030\n",
      "380/388, train_loss: 0.0872, step time: 0.4892\n",
      "381/388, train_loss: 0.1701, step time: 1.1502\n",
      "382/388, train_loss: 0.2372, step time: 0.5408\n",
      "383/388, train_loss: 0.1390, step time: 0.5002\n",
      "384/388, train_loss: 0.2640, step time: 0.4900\n",
      "385/388, train_loss: 0.1180, step time: 0.5147\n",
      "386/388, train_loss: 0.1047, step time: 0.5023\n",
      "387/388, train_loss: 0.1426, step time: 0.4912\n",
      "388/388, train_loss: 0.3313, step time: 0.4693\n",
      "epoch 63 average loss: 0.1819\n",
      "current epoch: 63 current mean dice: 0.7687 tc: 0.8146 wt: 0.8972 et: 0.5944\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 63 is: 302.5110\n",
      "----------\n",
      "epoch 64/300\n",
      "1/388, train_loss: 0.3841, step time: 0.4815\n",
      "2/388, train_loss: 0.1193, step time: 0.4825\n",
      "3/388, train_loss: 0.3254, step time: 1.0699\n",
      "4/388, train_loss: 0.2049, step time: 0.5753\n",
      "5/388, train_loss: 0.1217, step time: 0.5299\n",
      "6/388, train_loss: 0.1298, step time: 0.5046\n",
      "7/388, train_loss: 0.0975, step time: 0.5459\n",
      "8/388, train_loss: 0.2543, step time: 0.5577\n",
      "9/388, train_loss: 0.4042, step time: 0.6767\n",
      "10/388, train_loss: 0.0989, step time: 0.5558\n",
      "11/388, train_loss: 0.4131, step time: 0.5359\n",
      "12/388, train_loss: 0.0467, step time: 0.5051\n",
      "13/388, train_loss: 0.0836, step time: 0.5076\n",
      "14/388, train_loss: 0.0904, step time: 0.4911\n",
      "15/388, train_loss: 0.2551, step time: 0.4975\n",
      "16/388, train_loss: 0.3397, step time: 0.4901\n",
      "17/388, train_loss: 0.4202, step time: 0.4988\n",
      "18/388, train_loss: 0.0660, step time: 0.5015\n",
      "19/388, train_loss: 0.2348, step time: 0.4980\n",
      "20/388, train_loss: 0.2045, step time: 1.0742\n",
      "21/388, train_loss: 0.0964, step time: 0.5301\n",
      "22/388, train_loss: 0.1740, step time: 0.5147\n",
      "23/388, train_loss: 0.3916, step time: 0.4943\n",
      "24/388, train_loss: 0.0831, step time: 0.5008\n",
      "25/388, train_loss: 0.1727, step time: 0.4815\n",
      "26/388, train_loss: 0.2633, step time: 0.4834\n",
      "27/388, train_loss: 0.2199, step time: 0.7548\n",
      "28/388, train_loss: 0.1410, step time: 0.6347\n",
      "29/388, train_loss: 0.2763, step time: 0.5326\n",
      "30/388, train_loss: 0.2667, step time: 0.5024\n",
      "31/388, train_loss: 0.2400, step time: 0.4979\n",
      "32/388, train_loss: 0.0754, step time: 0.4809\n",
      "33/388, train_loss: 0.0794, step time: 0.5496\n",
      "34/388, train_loss: 0.1188, step time: 0.5150\n",
      "35/388, train_loss: 0.1148, step time: 0.4915\n",
      "36/388, train_loss: 0.1730, step time: 0.5179\n",
      "37/388, train_loss: 0.2657, step time: 0.5018\n",
      "38/388, train_loss: 0.0495, step time: 0.4983\n",
      "39/388, train_loss: 0.2032, step time: 0.5325\n",
      "40/388, train_loss: 0.1307, step time: 0.5345\n",
      "41/388, train_loss: 0.3363, step time: 0.5364\n",
      "42/388, train_loss: 0.0722, step time: 0.5267\n",
      "43/388, train_loss: 0.1399, step time: 0.4934\n",
      "44/388, train_loss: 0.0940, step time: 0.4952\n",
      "45/388, train_loss: 0.2714, step time: 0.4836\n",
      "46/388, train_loss: 0.0500, step time: 0.5311\n",
      "47/388, train_loss: 0.1612, step time: 0.5135\n",
      "48/388, train_loss: 0.3368, step time: 0.5012\n",
      "49/388, train_loss: 0.3731, step time: 1.1543\n",
      "50/388, train_loss: 0.1753, step time: 0.5397\n",
      "51/388, train_loss: 0.0979, step time: 0.5176\n",
      "52/388, train_loss: 0.5160, step time: 0.5207\n",
      "53/388, train_loss: 0.0769, step time: 0.5039\n",
      "54/388, train_loss: 0.0551, step time: 0.4992\n",
      "55/388, train_loss: 0.1593, step time: 0.4876\n",
      "56/388, train_loss: 0.0638, step time: 0.4915\n",
      "57/388, train_loss: 0.1698, step time: 0.5215\n",
      "58/388, train_loss: 0.1281, step time: 0.6022\n",
      "59/388, train_loss: 0.3605, step time: 0.5553\n",
      "60/388, train_loss: 0.4446, step time: 0.5322\n",
      "61/388, train_loss: 0.4367, step time: 0.5167\n",
      "62/388, train_loss: 0.3354, step time: 0.5090\n",
      "63/388, train_loss: 0.1322, step time: 0.4972\n",
      "64/388, train_loss: 0.1100, step time: 0.5046\n",
      "65/388, train_loss: 0.2070, step time: 0.6038\n",
      "66/388, train_loss: 0.4937, step time: 0.5403\n",
      "67/388, train_loss: 0.0918, step time: 0.5108\n",
      "68/388, train_loss: 0.1953, step time: 0.4822\n",
      "69/388, train_loss: 0.1113, step time: 0.4905\n",
      "70/388, train_loss: 0.2163, step time: 0.4953\n",
      "71/388, train_loss: 0.0973, step time: 0.5036\n",
      "72/388, train_loss: 0.2734, step time: 0.5706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/388, train_loss: 0.0804, step time: 0.6033\n",
      "74/388, train_loss: 0.2744, step time: 0.5439\n",
      "75/388, train_loss: 0.1164, step time: 0.5133\n",
      "76/388, train_loss: 0.1053, step time: 0.4940\n",
      "77/388, train_loss: 0.1994, step time: 0.4840\n",
      "78/388, train_loss: 0.1258, step time: 1.0421\n",
      "79/388, train_loss: 0.0988, step time: 0.5532\n",
      "80/388, train_loss: 0.1025, step time: 0.5182\n",
      "81/388, train_loss: 0.1759, step time: 0.5084\n",
      "82/388, train_loss: 0.0713, step time: 0.4924\n",
      "83/388, train_loss: 0.0791, step time: 0.4886\n",
      "84/388, train_loss: 0.1576, step time: 0.4927\n",
      "85/388, train_loss: 0.0724, step time: 0.4815\n",
      "86/388, train_loss: 0.2955, step time: 0.5269\n",
      "87/388, train_loss: 0.1060, step time: 0.5221\n",
      "88/388, train_loss: 0.2083, step time: 0.5227\n",
      "89/388, train_loss: 0.1821, step time: 0.5767\n",
      "90/388, train_loss: 0.2863, step time: 0.5315\n",
      "91/388, train_loss: 0.0907, step time: 0.5124\n",
      "92/388, train_loss: 0.2744, step time: 0.5070\n",
      "93/388, train_loss: 0.2655, step time: 0.4960\n",
      "94/388, train_loss: 0.3643, step time: 0.4937\n",
      "95/388, train_loss: 0.1435, step time: 0.7989\n",
      "96/388, train_loss: 0.1669, step time: 0.5415\n",
      "97/388, train_loss: 0.1729, step time: 0.5205\n",
      "98/388, train_loss: 0.0759, step time: 0.4950\n",
      "99/388, train_loss: 0.1134, step time: 0.5074\n",
      "100/388, train_loss: 0.0934, step time: 0.4850\n",
      "101/388, train_loss: 0.1858, step time: 0.4960\n",
      "102/388, train_loss: 0.1844, step time: 0.5416\n",
      "103/388, train_loss: 0.2183, step time: 0.5204\n",
      "104/388, train_loss: 0.0572, step time: 0.4981\n",
      "105/388, train_loss: 0.1517, step time: 0.4876\n",
      "106/388, train_loss: 0.1407, step time: 0.4765\n",
      "107/388, train_loss: 0.1484, step time: 0.9671\n",
      "108/388, train_loss: 0.1604, step time: 0.5569\n",
      "109/388, train_loss: 0.3214, step time: 0.5276\n",
      "110/388, train_loss: 0.1021, step time: 0.5028\n",
      "111/388, train_loss: 0.0588, step time: 0.4879\n",
      "112/388, train_loss: 0.1278, step time: 0.4828\n",
      "113/388, train_loss: 0.4489, step time: 0.4931\n",
      "114/388, train_loss: 0.1477, step time: 1.1917\n",
      "115/388, train_loss: 0.4239, step time: 0.5435\n",
      "116/388, train_loss: 0.0829, step time: 0.5031\n",
      "117/388, train_loss: 0.1497, step time: 0.4964\n",
      "118/388, train_loss: 0.2748, step time: 0.4816\n",
      "119/388, train_loss: 0.0991, step time: 0.4815\n",
      "120/388, train_loss: 0.0942, step time: 0.4830\n",
      "121/388, train_loss: 0.2846, step time: 0.4754\n",
      "122/388, train_loss: 0.1039, step time: 0.4822\n",
      "123/388, train_loss: 0.1620, step time: 0.5044\n",
      "124/388, train_loss: 0.1497, step time: 0.5053\n",
      "125/388, train_loss: 0.1511, step time: 0.4917\n",
      "126/388, train_loss: 0.1209, step time: 0.4772\n",
      "127/388, train_loss: 0.0783, step time: 0.4860\n",
      "128/388, train_loss: 0.2870, step time: 0.5142\n",
      "129/388, train_loss: 0.3350, step time: 0.5072\n",
      "130/388, train_loss: 0.1963, step time: 0.4949\n",
      "131/388, train_loss: 0.1260, step time: 1.0883\n",
      "132/388, train_loss: 0.5335, step time: 0.5417\n",
      "133/388, train_loss: 0.2997, step time: 0.5061\n",
      "134/388, train_loss: 0.1604, step time: 0.4928\n",
      "135/388, train_loss: 0.1146, step time: 0.5028\n",
      "136/388, train_loss: 0.1527, step time: 0.4973\n",
      "137/388, train_loss: 0.2453, step time: 0.5031\n",
      "138/388, train_loss: 0.1133, step time: 0.4930\n",
      "139/388, train_loss: 0.0711, step time: 0.5023\n",
      "140/388, train_loss: 0.1337, step time: 0.5231\n",
      "141/388, train_loss: 0.1794, step time: 0.4974\n",
      "142/388, train_loss: 0.6037, step time: 0.4906\n",
      "143/388, train_loss: 0.3913, step time: 0.4947\n",
      "144/388, train_loss: 0.1027, step time: 0.4774\n",
      "145/388, train_loss: 0.1964, step time: 0.4816\n",
      "146/388, train_loss: 0.1555, step time: 0.7974\n",
      "147/388, train_loss: 0.2806, step time: 0.5495\n",
      "148/388, train_loss: 0.1972, step time: 0.5177\n",
      "149/388, train_loss: 0.1867, step time: 0.4985\n",
      "150/388, train_loss: 0.1632, step time: 0.4957\n",
      "151/388, train_loss: 0.1727, step time: 0.4802\n",
      "152/388, train_loss: 0.1217, step time: 0.5357\n",
      "153/388, train_loss: 0.1307, step time: 0.5054\n",
      "154/388, train_loss: 0.2922, step time: 0.4966\n",
      "155/388, train_loss: 0.1232, step time: 0.4854\n",
      "156/388, train_loss: 0.0594, step time: 0.4895\n",
      "157/388, train_loss: 0.3017, step time: 1.0269\n",
      "158/388, train_loss: 0.1376, step time: 0.5326\n",
      "159/388, train_loss: 0.1128, step time: 0.5012\n",
      "160/388, train_loss: 0.3077, step time: 0.5012\n",
      "161/388, train_loss: 0.1829, step time: 0.4946\n",
      "162/388, train_loss: 0.2789, step time: 0.4772\n",
      "163/388, train_loss: 0.3063, step time: 1.1231\n",
      "164/388, train_loss: 0.3034, step time: 0.5372\n",
      "165/388, train_loss: 0.1126, step time: 0.5043\n",
      "166/388, train_loss: 0.2125, step time: 0.4857\n",
      "167/388, train_loss: 0.1737, step time: 0.4895\n",
      "168/388, train_loss: 0.2817, step time: 0.4918\n",
      "169/388, train_loss: 0.0343, step time: 0.9926\n",
      "170/388, train_loss: 0.2169, step time: 0.5401\n",
      "171/388, train_loss: 0.2281, step time: 0.4959\n",
      "172/388, train_loss: 0.0708, step time: 0.4835\n",
      "173/388, train_loss: 0.2590, step time: 0.4905\n",
      "174/388, train_loss: 0.1032, step time: 0.4758\n",
      "175/388, train_loss: 0.0824, step time: 0.4754\n",
      "176/388, train_loss: 0.1150, step time: 0.4744\n",
      "177/388, train_loss: 0.1086, step time: 0.4813\n",
      "178/388, train_loss: 0.1686, step time: 0.4905\n",
      "179/388, train_loss: 0.1973, step time: 0.4933\n",
      "180/388, train_loss: 0.0969, step time: 0.4977\n",
      "181/388, train_loss: 0.0994, step time: 0.4869\n",
      "182/388, train_loss: 0.1110, step time: 0.5000\n",
      "183/388, train_loss: 0.2791, step time: 0.4863\n",
      "184/388, train_loss: 0.1487, step time: 0.4983\n",
      "185/388, train_loss: 0.1566, step time: 0.5021\n",
      "186/388, train_loss: 0.0787, step time: 0.4870\n",
      "187/388, train_loss: 0.1887, step time: 0.5075\n",
      "188/388, train_loss: 0.1005, step time: 0.4872\n",
      "189/388, train_loss: 0.2323, step time: 0.4905\n",
      "190/388, train_loss: 0.2351, step time: 0.6778\n",
      "191/388, train_loss: 0.1354, step time: 0.5414\n",
      "192/388, train_loss: 0.2074, step time: 0.5113\n",
      "193/388, train_loss: 0.2120, step time: 0.5152\n",
      "194/388, train_loss: 0.2562, step time: 0.6031\n",
      "195/388, train_loss: 0.2929, step time: 0.5241\n",
      "196/388, train_loss: 0.1211, step time: 0.5270\n",
      "197/388, train_loss: 0.2056, step time: 0.5058\n",
      "198/388, train_loss: 0.2006, step time: 0.4958\n",
      "199/388, train_loss: 0.1209, step time: 0.4800\n",
      "200/388, train_loss: 0.1117, step time: 0.5071\n",
      "201/388, train_loss: 0.2001, step time: 0.4877\n",
      "202/388, train_loss: 0.0843, step time: 0.4915\n",
      "203/388, train_loss: 0.0829, step time: 0.4774\n",
      "204/388, train_loss: 0.1864, step time: 0.5036\n",
      "205/388, train_loss: 0.2019, step time: 0.5341\n",
      "206/388, train_loss: 0.3212, step time: 0.5035\n",
      "207/388, train_loss: 0.0781, step time: 0.4969\n",
      "208/388, train_loss: 0.1327, step time: 0.5014\n",
      "209/388, train_loss: 0.1440, step time: 0.4955\n",
      "210/388, train_loss: 0.1670, step time: 0.5320\n",
      "211/388, train_loss: 0.0690, step time: 0.5099\n",
      "212/388, train_loss: 0.2320, step time: 0.4942\n",
      "213/388, train_loss: 0.1364, step time: 0.4901\n",
      "214/388, train_loss: 0.1073, step time: 0.5213\n",
      "215/388, train_loss: 0.1930, step time: 0.5040\n",
      "216/388, train_loss: 0.1223, step time: 0.5018\n",
      "217/388, train_loss: 0.0945, step time: 0.5032\n",
      "218/388, train_loss: 0.2809, step time: 0.5085\n",
      "219/388, train_loss: 0.3262, step time: 0.4991\n",
      "220/388, train_loss: 0.1096, step time: 0.4960\n",
      "221/388, train_loss: 0.1411, step time: 1.1612\n",
      "222/388, train_loss: 0.1267, step time: 0.5568\n",
      "223/388, train_loss: 0.0821, step time: 0.5260\n",
      "224/388, train_loss: 0.2680, step time: 0.5027\n",
      "225/388, train_loss: 0.4305, step time: 0.4837\n",
      "226/388, train_loss: 0.0702, step time: 0.5212\n",
      "227/388, train_loss: 0.0862, step time: 0.5042\n",
      "228/388, train_loss: 0.1074, step time: 0.4929\n",
      "229/388, train_loss: 0.0637, step time: 0.4822\n",
      "230/388, train_loss: 0.0550, step time: 0.4799\n",
      "231/388, train_loss: 0.0883, step time: 0.4800\n",
      "232/388, train_loss: 0.3839, step time: 0.9135\n",
      "233/388, train_loss: 0.1831, step time: 0.5497\n",
      "234/388, train_loss: 0.2932, step time: 0.5080\n",
      "235/388, train_loss: 0.1212, step time: 0.4991\n",
      "236/388, train_loss: 0.0481, step time: 0.4823\n",
      "237/388, train_loss: 0.0859, step time: 0.4822\n",
      "238/388, train_loss: 0.1131, step time: 0.4849\n",
      "239/388, train_loss: 0.1571, step time: 0.9593\n",
      "240/388, train_loss: 0.1303, step time: 0.5277\n",
      "241/388, train_loss: 0.0908, step time: 0.4985\n",
      "242/388, train_loss: 0.1308, step time: 0.4930\n",
      "243/388, train_loss: 0.1653, step time: 0.4921\n",
      "244/388, train_loss: 0.2177, step time: 0.4837\n",
      "245/388, train_loss: 0.2658, step time: 0.5018\n",
      "246/388, train_loss: 0.2741, step time: 0.5246\n",
      "247/388, train_loss: 0.1472, step time: 0.5101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/388, train_loss: 0.1773, step time: 0.4950\n",
      "249/388, train_loss: 0.6065, step time: 0.4963\n",
      "250/388, train_loss: 0.1835, step time: 0.5236\n",
      "251/388, train_loss: 0.1893, step time: 0.5102\n",
      "252/388, train_loss: 0.2475, step time: 0.5092\n",
      "253/388, train_loss: 0.2162, step time: 0.5739\n",
      "254/388, train_loss: 0.2404, step time: 0.5354\n",
      "255/388, train_loss: 0.1770, step time: 0.5065\n",
      "256/388, train_loss: 0.1370, step time: 0.5014\n",
      "257/388, train_loss: 0.0647, step time: 0.4879\n",
      "258/388, train_loss: 0.4517, step time: 0.4912\n",
      "259/388, train_loss: 0.2391, step time: 1.1308\n",
      "260/388, train_loss: 0.2292, step time: 0.5396\n",
      "261/388, train_loss: 0.0857, step time: 0.5182\n",
      "262/388, train_loss: 0.2402, step time: 0.4965\n",
      "263/388, train_loss: 0.1208, step time: 0.4922\n",
      "264/388, train_loss: 0.0592, step time: 0.4910\n",
      "265/388, train_loss: 0.1650, step time: 0.4959\n",
      "266/388, train_loss: 0.1290, step time: 0.9021\n",
      "267/388, train_loss: 0.0962, step time: 0.5363\n",
      "268/388, train_loss: 0.0386, step time: 0.5061\n",
      "269/388, train_loss: 0.1359, step time: 0.4872\n",
      "270/388, train_loss: 0.1228, step time: 0.4878\n",
      "271/388, train_loss: 0.1541, step time: 0.4878\n",
      "272/388, train_loss: 0.1907, step time: 0.7523\n",
      "273/388, train_loss: 0.0728, step time: 0.5371\n",
      "274/388, train_loss: 0.2449, step time: 0.5073\n",
      "275/388, train_loss: 0.2025, step time: 0.4934\n",
      "276/388, train_loss: 0.3837, step time: 0.4922\n",
      "277/388, train_loss: 0.2791, step time: 1.0960\n",
      "278/388, train_loss: 0.2110, step time: 0.5237\n",
      "279/388, train_loss: 0.1318, step time: 0.5012\n",
      "280/388, train_loss: 0.2355, step time: 0.4866\n",
      "281/388, train_loss: 0.0624, step time: 0.5304\n",
      "282/388, train_loss: 0.0559, step time: 0.5056\n",
      "283/388, train_loss: 0.1886, step time: 0.4999\n",
      "284/388, train_loss: 0.2134, step time: 0.4821\n",
      "285/388, train_loss: 0.2308, step time: 0.5216\n",
      "286/388, train_loss: 0.2424, step time: 0.4989\n",
      "287/388, train_loss: 0.0894, step time: 0.4993\n",
      "288/388, train_loss: 0.3095, step time: 0.4824\n",
      "289/388, train_loss: 0.2039, step time: 0.7445\n",
      "290/388, train_loss: 0.3117, step time: 0.5276\n",
      "291/388, train_loss: 0.0807, step time: 0.5009\n",
      "292/388, train_loss: 0.1844, step time: 0.4877\n",
      "293/388, train_loss: 0.2000, step time: 0.5159\n",
      "294/388, train_loss: 0.0889, step time: 0.4895\n",
      "295/388, train_loss: 0.1426, step time: 0.4854\n",
      "296/388, train_loss: 0.1271, step time: 0.4879\n",
      "297/388, train_loss: 0.3206, step time: 0.4765\n",
      "298/388, train_loss: 0.0767, step time: 0.7114\n",
      "299/388, train_loss: 0.1101, step time: 0.5520\n",
      "300/388, train_loss: 0.2115, step time: 0.5226\n",
      "301/388, train_loss: 0.2896, step time: 0.5031\n",
      "302/388, train_loss: 0.2139, step time: 0.4932\n",
      "303/388, train_loss: 0.0615, step time: 0.4953\n",
      "304/388, train_loss: 0.1843, step time: 0.4837\n",
      "305/388, train_loss: 0.0496, step time: 1.1616\n",
      "306/388, train_loss: 0.2151, step time: 0.5405\n",
      "307/388, train_loss: 0.2442, step time: 0.5171\n",
      "308/388, train_loss: 0.0942, step time: 0.4992\n",
      "309/388, train_loss: 0.2670, step time: 0.4973\n",
      "310/388, train_loss: 0.0562, step time: 0.4832\n",
      "311/388, train_loss: 0.0921, step time: 0.4814\n",
      "312/388, train_loss: 0.0548, step time: 1.0288\n",
      "313/388, train_loss: 0.2092, step time: 0.5294\n",
      "314/388, train_loss: 0.1029, step time: 0.5155\n",
      "315/388, train_loss: 0.2127, step time: 0.4985\n",
      "316/388, train_loss: 0.0942, step time: 0.4953\n",
      "317/388, train_loss: 0.1826, step time: 0.4901\n",
      "318/388, train_loss: 0.1385, step time: 0.5428\n",
      "319/388, train_loss: 0.2229, step time: 0.5182\n",
      "320/388, train_loss: 0.1430, step time: 0.5041\n",
      "321/388, train_loss: 0.1126, step time: 0.4848\n",
      "322/388, train_loss: 0.1136, step time: 0.4818\n",
      "323/388, train_loss: 0.1909, step time: 0.4865\n",
      "324/388, train_loss: 0.2794, step time: 0.4834\n",
      "325/388, train_loss: 0.1878, step time: 0.7672\n",
      "326/388, train_loss: 0.1845, step time: 0.5597\n",
      "327/388, train_loss: 0.1896, step time: 0.5323\n",
      "328/388, train_loss: 0.1573, step time: 0.5108\n",
      "329/388, train_loss: 0.0797, step time: 0.4881\n",
      "330/388, train_loss: 0.1353, step time: 0.5487\n",
      "331/388, train_loss: 0.0839, step time: 0.5477\n",
      "332/388, train_loss: 0.1591, step time: 0.5230\n",
      "333/388, train_loss: 0.0974, step time: 0.5100\n",
      "334/388, train_loss: 0.0328, step time: 0.4949\n",
      "335/388, train_loss: 0.0726, step time: 0.4825\n",
      "336/388, train_loss: 0.0929, step time: 0.4833\n",
      "337/388, train_loss: 0.1475, step time: 1.0088\n",
      "338/388, train_loss: 0.1381, step time: 0.5496\n",
      "339/388, train_loss: 0.0849, step time: 0.5080\n",
      "340/388, train_loss: 0.1173, step time: 0.4936\n",
      "341/388, train_loss: 0.2023, step time: 0.4845\n",
      "342/388, train_loss: 0.0441, step time: 1.0249\n",
      "343/388, train_loss: 0.3870, step time: 0.5399\n",
      "344/388, train_loss: 0.0937, step time: 0.5143\n",
      "345/388, train_loss: 0.3399, step time: 0.5051\n",
      "346/388, train_loss: 0.2277, step time: 0.5011\n",
      "347/388, train_loss: 0.2122, step time: 0.4894\n",
      "348/388, train_loss: 0.2113, step time: 0.4927\n",
      "349/388, train_loss: 0.2528, step time: 0.4814\n",
      "350/388, train_loss: 0.1735, step time: 0.4760\n",
      "351/388, train_loss: 0.1638, step time: 0.4768\n",
      "352/388, train_loss: 0.0753, step time: 0.7017\n",
      "353/388, train_loss: 0.0489, step time: 0.5444\n",
      "354/388, train_loss: 0.3264, step time: 0.5157\n",
      "355/388, train_loss: 0.2073, step time: 0.5088\n",
      "356/388, train_loss: 0.1379, step time: 0.5077\n",
      "357/388, train_loss: 0.3294, step time: 0.4948\n",
      "358/388, train_loss: 0.1284, step time: 0.4903\n",
      "359/388, train_loss: 0.1211, step time: 0.4869\n",
      "360/388, train_loss: 0.0973, step time: 0.4985\n",
      "361/388, train_loss: 0.1911, step time: 0.4894\n",
      "362/388, train_loss: 0.1095, step time: 0.4870\n",
      "363/388, train_loss: 0.1938, step time: 0.4881\n",
      "364/388, train_loss: 0.2171, step time: 0.4934\n",
      "365/388, train_loss: 0.1474, step time: 0.4756\n",
      "366/388, train_loss: 0.0841, step time: 0.4941\n",
      "367/388, train_loss: 0.1113, step time: 0.4936\n",
      "368/388, train_loss: 0.1155, step time: 0.4862\n",
      "369/388, train_loss: 0.2864, step time: 0.8865\n",
      "370/388, train_loss: 0.3270, step time: 0.5409\n",
      "371/388, train_loss: 0.4496, step time: 0.5061\n",
      "372/388, train_loss: 0.0906, step time: 0.4923\n",
      "373/388, train_loss: 0.2666, step time: 0.5005\n",
      "374/388, train_loss: 0.2162, step time: 0.6454\n",
      "375/388, train_loss: 0.1301, step time: 0.5605\n",
      "376/388, train_loss: 0.1223, step time: 0.5215\n",
      "377/388, train_loss: 0.2298, step time: 0.4973\n",
      "378/388, train_loss: 0.1283, step time: 0.4887\n",
      "379/388, train_loss: 0.2468, step time: 0.5067\n",
      "380/388, train_loss: 0.2803, step time: 0.5041\n",
      "381/388, train_loss: 0.1324, step time: 0.4956\n",
      "382/388, train_loss: 0.0963, step time: 1.1125\n",
      "383/388, train_loss: 0.1647, step time: 0.5167\n",
      "384/388, train_loss: 0.2075, step time: 0.5012\n",
      "385/388, train_loss: 0.3703, step time: 0.4832\n",
      "386/388, train_loss: 0.1120, step time: 0.5051\n",
      "387/388, train_loss: 0.0984, step time: 0.5040\n",
      "388/388, train_loss: 0.1129, step time: 0.4896\n",
      "epoch 64 average loss: 0.1806\n",
      "current epoch: 64 current mean dice: 0.7603 tc: 0.8106 wt: 0.8916 et: 0.5787\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 64 is: 301.5855\n",
      "----------\n",
      "epoch 65/300\n",
      "1/388, train_loss: 0.1758, step time: 0.4716\n",
      "2/388, train_loss: 0.1343, step time: 0.4918\n",
      "3/388, train_loss: 0.1470, step time: 0.8434\n",
      "4/388, train_loss: 0.1649, step time: 0.6008\n",
      "5/388, train_loss: 0.2798, step time: 0.5435\n",
      "6/388, train_loss: 0.5181, step time: 0.5383\n",
      "7/388, train_loss: 0.0727, step time: 0.5781\n",
      "8/388, train_loss: 0.1811, step time: 0.5865\n",
      "9/388, train_loss: 0.1569, step time: 0.5561\n",
      "10/388, train_loss: 0.2588, step time: 0.5963\n",
      "11/388, train_loss: 0.3351, step time: 0.5284\n",
      "12/388, train_loss: 0.1687, step time: 0.5227\n",
      "13/388, train_loss: 0.1974, step time: 0.5423\n",
      "14/388, train_loss: 0.1599, step time: 0.6077\n",
      "15/388, train_loss: 0.2853, step time: 0.5660\n",
      "16/388, train_loss: 0.1974, step time: 0.5227\n",
      "17/388, train_loss: 0.1617, step time: 0.5029\n",
      "18/388, train_loss: 0.1277, step time: 0.5352\n",
      "19/388, train_loss: 0.3313, step time: 0.5188\n",
      "20/388, train_loss: 0.4075, step time: 0.6379\n",
      "21/388, train_loss: 0.2693, step time: 0.5812\n",
      "22/388, train_loss: 0.1089, step time: 0.5590\n",
      "23/388, train_loss: 0.1230, step time: 0.6482\n",
      "24/388, train_loss: 0.3426, step time: 0.5577\n",
      "25/388, train_loss: 0.1387, step time: 0.5220\n",
      "26/388, train_loss: 0.1496, step time: 0.5017\n",
      "27/388, train_loss: 0.3327, step time: 0.5821\n",
      "28/388, train_loss: 0.1757, step time: 0.6469\n",
      "29/388, train_loss: 0.1253, step time: 0.5609\n",
      "30/388, train_loss: 0.1811, step time: 0.5333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/388, train_loss: 0.0476, step time: 0.5231\n",
      "32/388, train_loss: 0.2091, step time: 0.5741\n",
      "33/388, train_loss: 0.2467, step time: 0.6724\n",
      "34/388, train_loss: 0.0865, step time: 0.5607\n",
      "35/388, train_loss: 0.0903, step time: 0.5114\n",
      "36/388, train_loss: 0.1422, step time: 0.5435\n",
      "37/388, train_loss: 0.1955, step time: 0.5357\n",
      "38/388, train_loss: 0.2992, step time: 0.5018\n",
      "39/388, train_loss: 0.1105, step time: 0.4971\n",
      "40/388, train_loss: 0.0675, step time: 0.4821\n",
      "41/388, train_loss: 0.2196, step time: 1.2010\n",
      "42/388, train_loss: 0.1863, step time: 0.5438\n",
      "43/388, train_loss: 0.2664, step time: 0.5112\n",
      "44/388, train_loss: 0.0581, step time: 0.4938\n",
      "45/388, train_loss: 0.1488, step time: 0.5020\n",
      "46/388, train_loss: 0.1020, step time: 0.5041\n",
      "47/388, train_loss: 0.0955, step time: 0.5885\n",
      "48/388, train_loss: 0.2922, step time: 0.6292\n",
      "49/388, train_loss: 0.3707, step time: 0.5225\n",
      "50/388, train_loss: 0.2015, step time: 1.2234\n",
      "51/388, train_loss: 0.0977, step time: 0.5546\n",
      "52/388, train_loss: 0.0744, step time: 0.5184\n",
      "53/388, train_loss: 0.1088, step time: 0.5017\n",
      "54/388, train_loss: 0.2730, step time: 0.4913\n",
      "55/388, train_loss: 0.0998, step time: 0.4957\n",
      "56/388, train_loss: 0.2109, step time: 0.4979\n",
      "57/388, train_loss: 0.3362, step time: 0.5040\n",
      "58/388, train_loss: 0.5283, step time: 0.5718\n",
      "59/388, train_loss: 0.0289, step time: 0.6837\n",
      "60/388, train_loss: 0.1330, step time: 0.5487\n",
      "61/388, train_loss: 0.0859, step time: 0.5092\n",
      "62/388, train_loss: 0.1036, step time: 0.5042\n",
      "63/388, train_loss: 0.0687, step time: 0.4868\n",
      "64/388, train_loss: 0.1376, step time: 0.4854\n",
      "65/388, train_loss: 0.1421, step time: 0.6986\n",
      "66/388, train_loss: 0.1042, step time: 0.5590\n",
      "67/388, train_loss: 0.0776, step time: 0.5158\n",
      "68/388, train_loss: 0.2790, step time: 0.4868\n",
      "69/388, train_loss: 0.1427, step time: 0.4840\n",
      "70/388, train_loss: 0.0401, step time: 0.4942\n",
      "71/388, train_loss: 0.1555, step time: 0.4901\n",
      "72/388, train_loss: 0.4086, step time: 0.5550\n",
      "73/388, train_loss: 0.1651, step time: 0.7275\n",
      "74/388, train_loss: 0.0890, step time: 0.5505\n",
      "75/388, train_loss: 0.1160, step time: 0.5223\n",
      "76/388, train_loss: 0.3911, step time: 0.5858\n",
      "77/388, train_loss: 0.2357, step time: 0.5469\n",
      "78/388, train_loss: 0.1000, step time: 0.5138\n",
      "79/388, train_loss: 0.1198, step time: 0.4958\n",
      "80/388, train_loss: 0.1852, step time: 0.4848\n",
      "81/388, train_loss: 0.1612, step time: 0.4985\n",
      "82/388, train_loss: 0.1011, step time: 0.4955\n",
      "83/388, train_loss: 0.1052, step time: 0.4883\n",
      "84/388, train_loss: 0.2516, step time: 0.4934\n",
      "85/388, train_loss: 0.0498, step time: 0.5420\n",
      "86/388, train_loss: 0.0466, step time: 0.5024\n",
      "87/388, train_loss: 0.1639, step time: 0.4961\n",
      "88/388, train_loss: 0.3975, step time: 1.1326\n",
      "89/388, train_loss: 0.1388, step time: 0.5343\n",
      "90/388, train_loss: 0.1873, step time: 0.5179\n",
      "91/388, train_loss: 0.0917, step time: 0.4959\n",
      "92/388, train_loss: 0.3875, step time: 0.4961\n",
      "93/388, train_loss: 0.0866, step time: 0.4886\n",
      "94/388, train_loss: 0.2061, step time: 0.4945\n",
      "95/388, train_loss: 0.3193, step time: 0.4922\n",
      "96/388, train_loss: 0.1136, step time: 0.4971\n",
      "97/388, train_loss: 0.1798, step time: 0.5210\n",
      "98/388, train_loss: 0.0965, step time: 0.5519\n",
      "99/388, train_loss: 0.0957, step time: 0.7292\n",
      "100/388, train_loss: 0.2132, step time: 0.5534\n",
      "101/388, train_loss: 0.2404, step time: 0.5254\n",
      "102/388, train_loss: 0.3202, step time: 0.5029\n",
      "103/388, train_loss: 0.3130, step time: 0.5386\n",
      "104/388, train_loss: 0.1172, step time: 0.5259\n",
      "105/388, train_loss: 0.1720, step time: 0.5094\n",
      "106/388, train_loss: 0.2432, step time: 0.4877\n",
      "107/388, train_loss: 0.1309, step time: 0.4932\n",
      "108/388, train_loss: 0.5331, step time: 1.1737\n",
      "109/388, train_loss: 0.1606, step time: 0.5672\n",
      "110/388, train_loss: 0.2290, step time: 0.5206\n",
      "111/388, train_loss: 0.0988, step time: 0.5083\n",
      "112/388, train_loss: 0.1403, step time: 0.4860\n",
      "113/388, train_loss: 0.0656, step time: 0.4879\n",
      "114/388, train_loss: 0.2659, step time: 0.4904\n",
      "115/388, train_loss: 0.2825, step time: 1.0510\n",
      "116/388, train_loss: 0.2474, step time: 0.5365\n",
      "117/388, train_loss: 0.0648, step time: 0.5048\n",
      "118/388, train_loss: 0.1361, step time: 0.4840\n",
      "119/388, train_loss: 0.1992, step time: 1.0519\n",
      "120/388, train_loss: 0.2228, step time: 0.5561\n",
      "121/388, train_loss: 0.1510, step time: 0.5243\n",
      "122/388, train_loss: 0.0911, step time: 0.4995\n",
      "123/388, train_loss: 0.0445, step time: 0.5196\n",
      "124/388, train_loss: 0.3492, step time: 0.5031\n",
      "125/388, train_loss: 0.0410, step time: 0.4978\n",
      "126/388, train_loss: 0.2555, step time: 0.4866\n",
      "127/388, train_loss: 0.1571, step time: 1.1797\n",
      "128/388, train_loss: 0.0827, step time: 0.5584\n",
      "129/388, train_loss: 0.1552, step time: 0.5261\n",
      "130/388, train_loss: 0.0275, step time: 0.5014\n",
      "131/388, train_loss: 0.2642, step time: 0.5047\n",
      "132/388, train_loss: 0.4703, step time: 0.4873\n",
      "133/388, train_loss: 0.1654, step time: 0.7993\n",
      "134/388, train_loss: 0.1182, step time: 0.5387\n",
      "135/388, train_loss: 0.4629, step time: 0.5058\n",
      "136/388, train_loss: 0.1529, step time: 0.4912\n",
      "137/388, train_loss: 0.1319, step time: 0.4869\n",
      "138/388, train_loss: 0.6264, step time: 0.5005\n",
      "139/388, train_loss: 0.2211, step time: 0.4860\n",
      "140/388, train_loss: 0.1054, step time: 0.4929\n",
      "141/388, train_loss: 0.1050, step time: 0.5066\n",
      "142/388, train_loss: 0.3384, step time: 0.5069\n",
      "143/388, train_loss: 0.5320, step time: 0.4970\n",
      "144/388, train_loss: 0.3377, step time: 0.9399\n",
      "145/388, train_loss: 0.5832, step time: 0.5648\n",
      "146/388, train_loss: 0.1404, step time: 0.5273\n",
      "147/388, train_loss: 0.2536, step time: 0.5035\n",
      "148/388, train_loss: 0.1921, step time: 0.5063\n",
      "149/388, train_loss: 0.1202, step time: 0.4900\n",
      "150/388, train_loss: 0.1999, step time: 0.4845\n",
      "151/388, train_loss: 0.1056, step time: 1.1202\n",
      "152/388, train_loss: 0.0807, step time: 0.5566\n",
      "153/388, train_loss: 0.1953, step time: 0.5171\n",
      "154/388, train_loss: 0.1223, step time: 0.4942\n",
      "155/388, train_loss: 0.1684, step time: 0.4918\n",
      "156/388, train_loss: 0.2019, step time: 0.4901\n",
      "157/388, train_loss: 0.1270, step time: 0.5159\n",
      "158/388, train_loss: 0.6003, step time: 0.6564\n",
      "159/388, train_loss: 0.2460, step time: 0.5963\n",
      "160/388, train_loss: 0.4612, step time: 0.5224\n",
      "161/388, train_loss: 0.2374, step time: 0.4970\n",
      "162/388, train_loss: 0.2241, step time: 0.4837\n",
      "163/388, train_loss: 0.1205, step time: 0.5216\n",
      "164/388, train_loss: 0.2346, step time: 0.5042\n",
      "165/388, train_loss: 0.1014, step time: 0.5056\n",
      "166/388, train_loss: 0.1049, step time: 0.8660\n",
      "167/388, train_loss: 0.1963, step time: 0.5516\n",
      "168/388, train_loss: 0.1359, step time: 0.5198\n",
      "169/388, train_loss: 0.2558, step time: 0.5045\n",
      "170/388, train_loss: 0.1585, step time: 0.5266\n",
      "171/388, train_loss: 0.2147, step time: 0.5001\n",
      "172/388, train_loss: 0.0681, step time: 0.4838\n",
      "173/388, train_loss: 0.0967, step time: 0.4933\n",
      "174/388, train_loss: 0.1468, step time: 0.4801\n",
      "175/388, train_loss: 0.0468, step time: 0.9694\n",
      "176/388, train_loss: 0.1661, step time: 0.5515\n",
      "177/388, train_loss: 0.1114, step time: 0.5273\n",
      "178/388, train_loss: 0.5994, step time: 0.4967\n",
      "179/388, train_loss: 0.1882, step time: 0.5045\n",
      "180/388, train_loss: 0.1196, step time: 0.5005\n",
      "181/388, train_loss: 0.0725, step time: 1.0424\n",
      "182/388, train_loss: 0.1177, step time: 0.5342\n",
      "183/388, train_loss: 0.3588, step time: 0.5094\n",
      "184/388, train_loss: 0.0774, step time: 0.6221\n",
      "185/388, train_loss: 0.1970, step time: 0.5397\n",
      "186/388, train_loss: 0.3129, step time: 0.5117\n",
      "187/388, train_loss: 0.3411, step time: 0.5032\n",
      "188/388, train_loss: 0.1226, step time: 0.5385\n",
      "189/388, train_loss: 0.3424, step time: 0.5226\n",
      "190/388, train_loss: 0.0830, step time: 0.4960\n",
      "191/388, train_loss: 0.1703, step time: 0.4963\n",
      "192/388, train_loss: 0.3167, step time: 0.5260\n",
      "193/388, train_loss: 0.1204, step time: 0.5012\n",
      "194/388, train_loss: 0.1095, step time: 0.4904\n",
      "195/388, train_loss: 0.0799, step time: 0.4912\n",
      "196/388, train_loss: 0.0850, step time: 1.0577\n",
      "197/388, train_loss: 0.0949, step time: 0.5358\n",
      "198/388, train_loss: 0.1723, step time: 0.5107\n",
      "199/388, train_loss: 0.3979, step time: 0.5139\n",
      "200/388, train_loss: 0.1555, step time: 0.5671\n",
      "201/388, train_loss: 0.1950, step time: 0.5356\n",
      "202/388, train_loss: 0.2120, step time: 0.5130\n",
      "203/388, train_loss: 0.1239, step time: 0.4976\n",
      "204/388, train_loss: 0.1465, step time: 0.4857\n",
      "205/388, train_loss: 0.1274, step time: 0.4935\n",
      "206/388, train_loss: 0.1239, step time: 0.6384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/388, train_loss: 0.0773, step time: 0.5413\n",
      "208/388, train_loss: 0.3489, step time: 0.5212\n",
      "209/388, train_loss: 0.2686, step time: 0.5011\n",
      "210/388, train_loss: 0.1563, step time: 0.4996\n",
      "211/388, train_loss: 0.1075, step time: 1.2237\n",
      "212/388, train_loss: 0.2106, step time: 0.5403\n",
      "213/388, train_loss: 0.0597, step time: 0.5087\n",
      "214/388, train_loss: 0.2341, step time: 0.4869\n",
      "215/388, train_loss: 0.1804, step time: 0.4950\n",
      "216/388, train_loss: 0.2405, step time: 0.4805\n",
      "217/388, train_loss: 0.1248, step time: 0.5472\n",
      "218/388, train_loss: 0.1200, step time: 0.5547\n",
      "219/388, train_loss: 0.1801, step time: 0.5410\n",
      "220/388, train_loss: 0.0697, step time: 0.5793\n",
      "221/388, train_loss: 0.2697, step time: 0.5461\n",
      "222/388, train_loss: 0.4550, step time: 0.5260\n",
      "223/388, train_loss: 0.4089, step time: 0.5027\n",
      "224/388, train_loss: 0.0914, step time: 1.1617\n",
      "225/388, train_loss: 0.1086, step time: 0.5387\n",
      "226/388, train_loss: 0.1000, step time: 0.5151\n",
      "227/388, train_loss: 0.1022, step time: 0.4966\n",
      "228/388, train_loss: 0.2631, step time: 0.4976\n",
      "229/388, train_loss: 0.0914, step time: 0.5111\n",
      "230/388, train_loss: 0.3148, step time: 0.4881\n",
      "231/388, train_loss: 0.2948, step time: 0.9970\n",
      "232/388, train_loss: 0.1578, step time: 0.5236\n",
      "233/388, train_loss: 0.0799, step time: 0.4948\n",
      "234/388, train_loss: 0.1658, step time: 0.4849\n",
      "235/388, train_loss: 0.1245, step time: 0.4838\n",
      "236/388, train_loss: 0.1566, step time: 0.4899\n",
      "237/388, train_loss: 0.1292, step time: 0.4841\n",
      "238/388, train_loss: 0.0884, step time: 0.4915\n",
      "239/388, train_loss: 0.2824, step time: 0.5611\n",
      "240/388, train_loss: 0.1354, step time: 0.5435\n",
      "241/388, train_loss: 0.1025, step time: 0.5072\n",
      "242/388, train_loss: 0.3212, step time: 0.5123\n",
      "243/388, train_loss: 0.2113, step time: 0.5015\n",
      "244/388, train_loss: 0.2487, step time: 0.5310\n",
      "245/388, train_loss: 0.0802, step time: 0.4984\n",
      "246/388, train_loss: 0.2792, step time: 0.4962\n",
      "247/388, train_loss: 0.0831, step time: 0.5137\n",
      "248/388, train_loss: 0.1258, step time: 0.4944\n",
      "249/388, train_loss: 0.1741, step time: 0.4939\n",
      "250/388, train_loss: 0.2122, step time: 0.4828\n",
      "251/388, train_loss: 0.0838, step time: 0.7165\n",
      "252/388, train_loss: 0.1692, step time: 0.5837\n",
      "253/388, train_loss: 0.2984, step time: 0.5552\n",
      "254/388, train_loss: 0.1346, step time: 0.5196\n",
      "255/388, train_loss: 0.0811, step time: 0.5656\n",
      "256/388, train_loss: 0.0729, step time: 0.5303\n",
      "257/388, train_loss: 0.1530, step time: 0.5220\n",
      "258/388, train_loss: 0.2190, step time: 0.6035\n",
      "259/388, train_loss: 0.1363, step time: 0.5450\n",
      "260/388, train_loss: 0.1932, step time: 0.5155\n",
      "261/388, train_loss: 0.0829, step time: 0.4941\n",
      "262/388, train_loss: 0.3638, step time: 0.4921\n",
      "263/388, train_loss: 0.0658, step time: 0.5140\n",
      "264/388, train_loss: 0.2498, step time: 0.5092\n",
      "265/388, train_loss: 0.1573, step time: 0.5334\n",
      "266/388, train_loss: 0.0998, step time: 0.5179\n",
      "267/388, train_loss: 0.2025, step time: 0.4993\n",
      "268/388, train_loss: 0.2194, step time: 0.5375\n",
      "269/388, train_loss: 0.1452, step time: 0.6634\n",
      "270/388, train_loss: 0.1714, step time: 0.5519\n",
      "271/388, train_loss: 0.0892, step time: 0.5200\n",
      "272/388, train_loss: 0.5115, step time: 0.5019\n",
      "273/388, train_loss: 0.3869, step time: 0.9646\n",
      "274/388, train_loss: 0.2304, step time: 0.5397\n",
      "275/388, train_loss: 0.0906, step time: 0.5144\n",
      "276/388, train_loss: 0.2436, step time: 0.4990\n",
      "277/388, train_loss: 0.0921, step time: 0.4994\n",
      "278/388, train_loss: 0.0881, step time: 0.4851\n",
      "279/388, train_loss: 0.0565, step time: 0.7688\n",
      "280/388, train_loss: 0.1021, step time: 0.5607\n",
      "281/388, train_loss: 0.1978, step time: 0.5299\n",
      "282/388, train_loss: 0.2245, step time: 0.4985\n",
      "283/388, train_loss: 0.1428, step time: 0.4897\n",
      "284/388, train_loss: 0.2159, step time: 0.4831\n",
      "285/388, train_loss: 0.1795, step time: 0.5076\n",
      "286/388, train_loss: 0.0604, step time: 0.4862\n",
      "287/388, train_loss: 0.2624, step time: 0.9861\n",
      "288/388, train_loss: 0.1155, step time: 0.5369\n",
      "289/388, train_loss: 0.2730, step time: 0.5180\n",
      "290/388, train_loss: 0.1206, step time: 0.5021\n",
      "291/388, train_loss: 0.0664, step time: 0.4881\n",
      "292/388, train_loss: 0.1414, step time: 0.4948\n",
      "293/388, train_loss: 0.1206, step time: 0.4846\n",
      "294/388, train_loss: 0.2752, step time: 0.5031\n",
      "295/388, train_loss: 0.2938, step time: 0.4838\n",
      "296/388, train_loss: 0.3363, step time: 0.4965\n",
      "297/388, train_loss: 0.2376, step time: 0.5002\n",
      "298/388, train_loss: 0.0592, step time: 0.5156\n",
      "299/388, train_loss: 0.1288, step time: 0.5814\n",
      "300/388, train_loss: 0.1035, step time: 0.5491\n",
      "301/388, train_loss: 0.1806, step time: 0.5360\n",
      "302/388, train_loss: 0.2787, step time: 0.5914\n",
      "303/388, train_loss: 0.1966, step time: 0.5392\n",
      "304/388, train_loss: 0.1581, step time: 0.5199\n",
      "305/388, train_loss: 0.3715, step time: 0.5011\n",
      "306/388, train_loss: 0.1589, step time: 0.4998\n",
      "307/388, train_loss: 0.1268, step time: 0.4855\n",
      "308/388, train_loss: 0.1408, step time: 1.1512\n",
      "309/388, train_loss: 0.1270, step time: 0.5486\n",
      "310/388, train_loss: 0.1031, step time: 0.5221\n",
      "311/388, train_loss: 0.1943, step time: 0.4998\n",
      "312/388, train_loss: 0.1762, step time: 0.4976\n",
      "313/388, train_loss: 0.2569, step time: 0.4833\n",
      "314/388, train_loss: 0.1575, step time: 1.1273\n",
      "315/388, train_loss: 0.2038, step time: 0.5340\n",
      "316/388, train_loss: 0.3222, step time: 0.4949\n",
      "317/388, train_loss: 0.0727, step time: 0.4877\n",
      "318/388, train_loss: 0.3425, step time: 0.4982\n",
      "319/388, train_loss: 0.2116, step time: 0.4941\n",
      "320/388, train_loss: 0.2066, step time: 0.4938\n",
      "321/388, train_loss: 0.1086, step time: 0.4850\n",
      "322/388, train_loss: 0.2745, step time: 0.6432\n",
      "323/388, train_loss: 0.0848, step time: 0.5555\n",
      "324/388, train_loss: 0.1082, step time: 0.5206\n",
      "325/388, train_loss: 0.0932, step time: 0.5050\n",
      "326/388, train_loss: 0.0537, step time: 0.4960\n",
      "327/388, train_loss: 0.5346, step time: 0.4854\n",
      "328/388, train_loss: 0.1340, step time: 0.4931\n",
      "329/388, train_loss: 0.2515, step time: 0.4850\n",
      "330/388, train_loss: 0.2412, step time: 1.0043\n",
      "331/388, train_loss: 0.2389, step time: 0.5495\n",
      "332/388, train_loss: 0.1442, step time: 0.5218\n",
      "333/388, train_loss: 0.2159, step time: 0.5134\n",
      "334/388, train_loss: 0.1782, step time: 0.5770\n",
      "335/388, train_loss: 0.3265, step time: 0.5374\n",
      "336/388, train_loss: 0.0692, step time: 0.5093\n",
      "337/388, train_loss: 0.1708, step time: 0.4858\n",
      "338/388, train_loss: 0.2568, step time: 0.4926\n",
      "339/388, train_loss: 0.0941, step time: 0.4918\n",
      "340/388, train_loss: 0.1139, step time: 0.9133\n",
      "341/388, train_loss: 0.2351, step time: 0.5292\n",
      "342/388, train_loss: 0.1856, step time: 0.5107\n",
      "343/388, train_loss: 0.6153, step time: 0.4867\n",
      "344/388, train_loss: 0.1767, step time: 0.4938\n",
      "345/388, train_loss: 0.1737, step time: 1.1275\n",
      "346/388, train_loss: 0.3876, step time: 0.5191\n",
      "347/388, train_loss: 0.2281, step time: 0.5054\n",
      "348/388, train_loss: 0.1133, step time: 0.4897\n",
      "349/388, train_loss: 0.1051, step time: 0.5218\n",
      "350/388, train_loss: 0.0357, step time: 0.5066\n",
      "351/388, train_loss: 0.1742, step time: 0.5265\n",
      "352/388, train_loss: 0.0594, step time: 0.5048\n",
      "353/388, train_loss: 0.2617, step time: 0.5040\n",
      "354/388, train_loss: 0.4336, step time: 0.4860\n",
      "355/388, train_loss: 0.1006, step time: 0.4890\n",
      "356/388, train_loss: 0.0944, step time: 0.4831\n",
      "357/388, train_loss: 0.1139, step time: 0.4807\n",
      "358/388, train_loss: 0.1053, step time: 0.5013\n",
      "359/388, train_loss: 0.1963, step time: 0.5170\n",
      "360/388, train_loss: 0.2264, step time: 0.5287\n",
      "361/388, train_loss: 0.1311, step time: 0.5156\n",
      "362/388, train_loss: 0.0771, step time: 0.5128\n",
      "363/388, train_loss: 0.2122, step time: 0.4910\n",
      "364/388, train_loss: 0.1804, step time: 0.4923\n",
      "365/388, train_loss: 0.0649, step time: 1.1797\n",
      "366/388, train_loss: 0.0952, step time: 0.5311\n",
      "367/388, train_loss: 0.2256, step time: 0.5037\n",
      "368/388, train_loss: 0.2083, step time: 0.4943\n",
      "369/388, train_loss: 0.3755, step time: 0.4917\n",
      "370/388, train_loss: 0.1086, step time: 0.4774\n",
      "371/388, train_loss: 0.1031, step time: 0.4772\n",
      "372/388, train_loss: 0.1607, step time: 0.5261\n",
      "373/388, train_loss: 0.5056, step time: 0.5111\n",
      "374/388, train_loss: 0.1392, step time: 0.5069\n",
      "375/388, train_loss: 0.1357, step time: 0.5676\n",
      "376/388, train_loss: 0.0856, step time: 0.5350\n",
      "377/388, train_loss: 0.1225, step time: 0.5076\n",
      "378/388, train_loss: 0.2592, step time: 0.4956\n",
      "379/388, train_loss: 0.1302, step time: 0.4972\n",
      "380/388, train_loss: 0.0831, step time: 0.4881\n",
      "381/388, train_loss: 0.1589, step time: 0.4924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/388, train_loss: 0.3990, step time: 1.1827\n",
      "383/388, train_loss: 0.2940, step time: 0.5387\n",
      "384/388, train_loss: 0.0912, step time: 0.5029\n",
      "385/388, train_loss: 0.1619, step time: 0.4949\n",
      "386/388, train_loss: 0.2547, step time: 0.4782\n",
      "387/388, train_loss: 0.3344, step time: 0.4757\n",
      "388/388, train_loss: 0.2598, step time: 1.0186\n",
      "epoch 65 average loss: 0.1899\n",
      "current epoch: 65 current mean dice: 0.7682 tc: 0.8111 wt: 0.9031 et: 0.5904\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 65 is: 306.2707\n",
      "----------\n",
      "epoch 66/300\n",
      "1/388, train_loss: 0.1895, step time: 0.4754\n",
      "2/388, train_loss: 0.0977, step time: 0.4793\n",
      "3/388, train_loss: 0.1745, step time: 1.1485\n",
      "4/388, train_loss: 0.2406, step time: 0.5339\n",
      "5/388, train_loss: 0.0953, step time: 0.5169\n",
      "6/388, train_loss: 0.1597, step time: 0.5076\n",
      "7/388, train_loss: 0.2474, step time: 0.5521\n",
      "8/388, train_loss: 0.0652, step time: 0.7927\n",
      "9/388, train_loss: 0.1621, step time: 0.5752\n",
      "10/388, train_loss: 0.1684, step time: 0.5292\n",
      "11/388, train_loss: 0.1906, step time: 0.5175\n",
      "12/388, train_loss: 0.0907, step time: 0.5053\n",
      "13/388, train_loss: 0.1345, step time: 0.4950\n",
      "14/388, train_loss: 0.2658, step time: 0.4975\n",
      "15/388, train_loss: 0.2255, step time: 0.4801\n",
      "16/388, train_loss: 0.3979, step time: 0.4800\n",
      "17/388, train_loss: 0.1704, step time: 1.1732\n",
      "18/388, train_loss: 0.4215, step time: 0.5639\n",
      "19/388, train_loss: 0.0937, step time: 0.5309\n",
      "20/388, train_loss: 0.2959, step time: 0.5077\n",
      "21/388, train_loss: 0.3872, step time: 0.4917\n",
      "22/388, train_loss: 0.1910, step time: 0.4808\n",
      "23/388, train_loss: 0.2065, step time: 1.0367\n",
      "24/388, train_loss: 0.1381, step time: 0.5363\n",
      "25/388, train_loss: 0.2577, step time: 0.5189\n",
      "26/388, train_loss: 0.2467, step time: 0.4979\n",
      "27/388, train_loss: 0.3368, step time: 0.4913\n",
      "28/388, train_loss: 0.2412, step time: 0.4799\n",
      "29/388, train_loss: 0.3853, step time: 0.4864\n",
      "30/388, train_loss: 0.0759, step time: 1.0147\n",
      "31/388, train_loss: 0.3411, step time: 0.5366\n",
      "32/388, train_loss: 0.1552, step time: 0.5081\n",
      "33/388, train_loss: 0.0842, step time: 0.4980\n",
      "34/388, train_loss: 0.0569, step time: 0.4808\n",
      "35/388, train_loss: 0.1610, step time: 0.4806\n",
      "36/388, train_loss: 0.1324, step time: 0.4806\n",
      "37/388, train_loss: 0.2471, step time: 0.4967\n",
      "38/388, train_loss: 0.1384, step time: 0.4906\n",
      "39/388, train_loss: 0.1636, step time: 0.4974\n",
      "40/388, train_loss: 0.1035, step time: 0.4781\n",
      "41/388, train_loss: 0.0729, step time: 0.4970\n",
      "42/388, train_loss: 0.1602, step time: 0.4967\n",
      "43/388, train_loss: 0.1598, step time: 0.4965\n",
      "44/388, train_loss: 0.0524, step time: 0.4963\n",
      "45/388, train_loss: 0.1814, step time: 0.5113\n",
      "46/388, train_loss: 0.5859, step time: 0.4923\n",
      "47/388, train_loss: 0.0722, step time: 0.4944\n",
      "48/388, train_loss: 0.2103, step time: 0.4791\n",
      "49/388, train_loss: 0.2661, step time: 0.9253\n",
      "50/388, train_loss: 0.3376, step time: 0.5470\n",
      "51/388, train_loss: 0.1101, step time: 0.5140\n",
      "52/388, train_loss: 0.2424, step time: 0.5100\n",
      "53/388, train_loss: 0.2144, step time: 0.4918\n",
      "54/388, train_loss: 0.2800, step time: 0.4847\n",
      "55/388, train_loss: 0.0931, step time: 0.4931\n",
      "56/388, train_loss: 0.2904, step time: 0.4830\n",
      "57/388, train_loss: 0.0589, step time: 0.4888\n",
      "58/388, train_loss: 0.1471, step time: 0.4932\n",
      "59/388, train_loss: 0.0294, step time: 0.4972\n",
      "60/388, train_loss: 0.3121, step time: 0.4977\n",
      "61/388, train_loss: 0.1311, step time: 0.5011\n",
      "62/388, train_loss: 0.0977, step time: 0.4927\n",
      "63/388, train_loss: 0.0859, step time: 0.4874\n",
      "64/388, train_loss: 0.1982, step time: 0.4963\n",
      "65/388, train_loss: 0.2545, step time: 0.5450\n",
      "66/388, train_loss: 0.1731, step time: 0.5112\n",
      "67/388, train_loss: 0.1721, step time: 0.4979\n",
      "68/388, train_loss: 0.2692, step time: 0.5179\n",
      "69/388, train_loss: 0.0847, step time: 0.5185\n",
      "70/388, train_loss: 0.1346, step time: 0.5009\n",
      "71/388, train_loss: 0.1363, step time: 0.5076\n",
      "72/388, train_loss: 0.0582, step time: 0.9723\n",
      "73/388, train_loss: 0.0379, step time: 0.5384\n",
      "74/388, train_loss: 0.0830, step time: 0.5095\n",
      "75/388, train_loss: 0.1049, step time: 0.4900\n",
      "76/388, train_loss: 0.0835, step time: 0.4954\n",
      "77/388, train_loss: 0.0907, step time: 0.4778\n",
      "78/388, train_loss: 0.1549, step time: 0.4774\n",
      "79/388, train_loss: 0.2010, step time: 0.4725\n",
      "80/388, train_loss: 0.0853, step time: 0.4990\n",
      "81/388, train_loss: 0.2029, step time: 0.7021\n",
      "82/388, train_loss: 0.1469, step time: 0.6028\n",
      "83/388, train_loss: 0.0517, step time: 0.5349\n",
      "84/388, train_loss: 0.0954, step time: 0.4990\n",
      "85/388, train_loss: 0.1858, step time: 0.4982\n",
      "86/388, train_loss: 0.1387, step time: 0.4899\n",
      "87/388, train_loss: 0.0954, step time: 0.4920\n",
      "88/388, train_loss: 0.1190, step time: 0.4744\n",
      "89/388, train_loss: 0.3580, step time: 1.0282\n",
      "90/388, train_loss: 0.2736, step time: 0.5514\n",
      "91/388, train_loss: 0.1539, step time: 0.5225\n",
      "92/388, train_loss: 0.0854, step time: 0.5096\n",
      "93/388, train_loss: 0.1027, step time: 0.4954\n",
      "94/388, train_loss: 0.2545, step time: 0.4845\n",
      "95/388, train_loss: 0.1131, step time: 0.4948\n",
      "96/388, train_loss: 0.0444, step time: 0.5074\n",
      "97/388, train_loss: 0.1993, step time: 0.4992\n",
      "98/388, train_loss: 0.1893, step time: 0.4859\n",
      "99/388, train_loss: 0.0476, step time: 0.4904\n",
      "100/388, train_loss: 0.1752, step time: 0.4879\n",
      "101/388, train_loss: 0.3556, step time: 1.1173\n",
      "102/388, train_loss: 0.3893, step time: 0.5595\n",
      "103/388, train_loss: 0.2268, step time: 0.5078\n",
      "104/388, train_loss: 0.1389, step time: 0.5023\n",
      "105/388, train_loss: 0.1295, step time: 0.4891\n",
      "106/388, train_loss: 0.1084, step time: 0.4851\n",
      "107/388, train_loss: 0.1124, step time: 0.5080\n",
      "108/388, train_loss: 0.1584, step time: 0.4877\n",
      "109/388, train_loss: 0.1725, step time: 0.5141\n",
      "110/388, train_loss: 0.4009, step time: 0.4932\n",
      "111/388, train_loss: 0.0926, step time: 0.5194\n",
      "112/388, train_loss: 0.0792, step time: 0.5214\n",
      "113/388, train_loss: 0.4448, step time: 0.5068\n",
      "114/388, train_loss: 0.1304, step time: 0.5003\n",
      "115/388, train_loss: 0.0787, step time: 0.4990\n",
      "116/388, train_loss: 0.4248, step time: 0.4900\n",
      "117/388, train_loss: 0.1325, step time: 0.4730\n",
      "118/388, train_loss: 0.0330, step time: 0.4716\n",
      "119/388, train_loss: 0.2534, step time: 0.4831\n",
      "120/388, train_loss: 0.1579, step time: 0.4897\n",
      "121/388, train_loss: 0.1031, step time: 0.4898\n",
      "122/388, train_loss: 0.2988, step time: 0.4906\n",
      "123/388, train_loss: 0.1147, step time: 0.4804\n",
      "124/388, train_loss: 0.1422, step time: 1.0419\n",
      "125/388, train_loss: 0.2379, step time: 0.5484\n",
      "126/388, train_loss: 0.1937, step time: 0.5059\n",
      "127/388, train_loss: 0.2268, step time: 0.4905\n",
      "128/388, train_loss: 0.1565, step time: 0.4973\n",
      "129/388, train_loss: 0.1368, step time: 0.4867\n",
      "130/388, train_loss: 0.1969, step time: 1.1795\n",
      "131/388, train_loss: 0.1926, step time: 0.5245\n",
      "132/388, train_loss: 0.0993, step time: 0.5004\n",
      "133/388, train_loss: 0.0640, step time: 0.4919\n",
      "134/388, train_loss: 0.1082, step time: 0.4890\n",
      "135/388, train_loss: 0.1925, step time: 0.4950\n",
      "136/388, train_loss: 0.2716, step time: 0.5045\n",
      "137/388, train_loss: 0.1001, step time: 0.4920\n",
      "138/388, train_loss: 0.0832, step time: 0.4745\n",
      "139/388, train_loss: 0.1838, step time: 1.0245\n",
      "140/388, train_loss: 0.1339, step time: 0.5231\n",
      "141/388, train_loss: 0.2092, step time: 0.5092\n",
      "142/388, train_loss: 0.2216, step time: 0.4933\n",
      "143/388, train_loss: 0.4557, step time: 0.5182\n",
      "144/388, train_loss: 0.1835, step time: 0.4962\n",
      "145/388, train_loss: 0.5437, step time: 0.4951\n",
      "146/388, train_loss: 0.1246, step time: 0.4803\n",
      "147/388, train_loss: 0.2015, step time: 0.5042\n",
      "148/388, train_loss: 0.1153, step time: 0.4918\n",
      "149/388, train_loss: 0.1414, step time: 0.4825\n",
      "150/388, train_loss: 0.2317, step time: 0.4856\n",
      "151/388, train_loss: 0.3984, step time: 0.4718\n",
      "152/388, train_loss: 0.0979, step time: 0.6430\n",
      "153/388, train_loss: 0.1733, step time: 0.5553\n",
      "154/388, train_loss: 0.1397, step time: 0.5232\n",
      "155/388, train_loss: 0.0602, step time: 0.4977\n",
      "156/388, train_loss: 0.2344, step time: 0.4976\n",
      "157/388, train_loss: 0.1798, step time: 0.4850\n",
      "158/388, train_loss: 0.4059, step time: 0.4819\n",
      "159/388, train_loss: 0.1922, step time: 0.8987\n",
      "160/388, train_loss: 0.1129, step time: 0.5326\n",
      "161/388, train_loss: 0.1123, step time: 0.5080\n",
      "162/388, train_loss: 0.0566, step time: 0.4911\n",
      "163/388, train_loss: 0.1826, step time: 0.4945\n",
      "164/388, train_loss: 0.1002, step time: 0.4910\n",
      "165/388, train_loss: 0.2101, step time: 0.4858\n",
      "166/388, train_loss: 0.0828, step time: 0.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/388, train_loss: 0.1190, step time: 0.5220\n",
      "168/388, train_loss: 0.1172, step time: 0.4911\n",
      "169/388, train_loss: 0.1233, step time: 0.4825\n",
      "170/388, train_loss: 0.2234, step time: 0.4985\n",
      "171/388, train_loss: 0.0601, step time: 0.4797\n",
      "172/388, train_loss: 0.2038, step time: 0.4845\n",
      "173/388, train_loss: 0.2744, step time: 0.4859\n",
      "174/388, train_loss: 0.2090, step time: 0.4720\n",
      "175/388, train_loss: 0.1856, step time: 0.4911\n",
      "176/388, train_loss: 0.1177, step time: 0.5186\n",
      "177/388, train_loss: 0.3451, step time: 0.5101\n",
      "178/388, train_loss: 0.0856, step time: 1.1417\n",
      "179/388, train_loss: 0.3107, step time: 0.5414\n",
      "180/388, train_loss: 0.0442, step time: 0.5060\n",
      "181/388, train_loss: 0.1479, step time: 0.5015\n",
      "182/388, train_loss: 0.1631, step time: 0.5474\n",
      "183/388, train_loss: 0.1535, step time: 0.5130\n",
      "184/388, train_loss: 0.3129, step time: 0.4994\n",
      "185/388, train_loss: 0.1634, step time: 0.4957\n",
      "186/388, train_loss: 0.0864, step time: 0.4787\n",
      "187/388, train_loss: 0.0630, step time: 0.4856\n",
      "188/388, train_loss: 0.1264, step time: 0.4755\n",
      "189/388, train_loss: 0.1839, step time: 0.8867\n",
      "190/388, train_loss: 0.2362, step time: 0.5338\n",
      "191/388, train_loss: 0.1022, step time: 0.5186\n",
      "192/388, train_loss: 0.0981, step time: 0.4866\n",
      "193/388, train_loss: 0.2953, step time: 0.4903\n",
      "194/388, train_loss: 0.0953, step time: 0.4905\n",
      "195/388, train_loss: 0.1689, step time: 0.4878\n",
      "196/388, train_loss: 0.1995, step time: 0.5314\n",
      "197/388, train_loss: 0.2383, step time: 0.5034\n",
      "198/388, train_loss: 0.1931, step time: 0.5101\n",
      "199/388, train_loss: 0.1075, step time: 0.5011\n",
      "200/388, train_loss: 0.0794, step time: 0.4876\n",
      "201/388, train_loss: 0.2000, step time: 0.4763\n",
      "202/388, train_loss: 0.0684, step time: 0.5219\n",
      "203/388, train_loss: 0.3529, step time: 0.5235\n",
      "204/388, train_loss: 0.1941, step time: 0.5109\n",
      "205/388, train_loss: 0.3062, step time: 0.4968\n",
      "206/388, train_loss: 0.0832, step time: 0.5161\n",
      "207/388, train_loss: 0.2112, step time: 0.4877\n",
      "208/388, train_loss: 0.1465, step time: 0.5307\n",
      "209/388, train_loss: 0.0795, step time: 0.6016\n",
      "210/388, train_loss: 0.1149, step time: 0.5614\n",
      "211/388, train_loss: 0.2920, step time: 0.5398\n",
      "212/388, train_loss: 0.0725, step time: 0.5125\n",
      "213/388, train_loss: 0.0878, step time: 0.5056\n",
      "214/388, train_loss: 0.1355, step time: 0.4844\n",
      "215/388, train_loss: 0.2805, step time: 0.4929\n",
      "216/388, train_loss: 0.0553, step time: 0.4939\n",
      "217/388, train_loss: 0.1449, step time: 0.5553\n",
      "218/388, train_loss: 0.3866, step time: 0.5885\n",
      "219/388, train_loss: 0.0671, step time: 0.5471\n",
      "220/388, train_loss: 0.1284, step time: 0.5174\n",
      "221/388, train_loss: 0.2024, step time: 0.5029\n",
      "222/388, train_loss: 0.2560, step time: 0.5026\n",
      "223/388, train_loss: 0.0976, step time: 0.9791\n",
      "224/388, train_loss: 0.1090, step time: 0.5491\n",
      "225/388, train_loss: 0.2523, step time: 0.5213\n",
      "226/388, train_loss: 0.1289, step time: 0.4948\n",
      "227/388, train_loss: 0.1645, step time: 0.4964\n",
      "228/388, train_loss: 0.0559, step time: 0.4929\n",
      "229/388, train_loss: 0.5983, step time: 0.4943\n",
      "230/388, train_loss: 0.4454, step time: 0.5246\n",
      "231/388, train_loss: 0.1132, step time: 0.5050\n",
      "232/388, train_loss: 0.1232, step time: 0.4962\n",
      "233/388, train_loss: 0.1844, step time: 0.4869\n",
      "234/388, train_loss: 0.1142, step time: 0.5006\n",
      "235/388, train_loss: 0.0993, step time: 0.5330\n",
      "236/388, train_loss: 0.1460, step time: 0.5173\n",
      "237/388, train_loss: 0.1603, step time: 0.5025\n",
      "238/388, train_loss: 0.1956, step time: 0.4879\n",
      "239/388, train_loss: 0.0888, step time: 0.7313\n",
      "240/388, train_loss: 0.2278, step time: 0.5705\n",
      "241/388, train_loss: 0.1935, step time: 0.5173\n",
      "242/388, train_loss: 0.0751, step time: 0.4998\n",
      "243/388, train_loss: 0.0779, step time: 0.4845\n",
      "244/388, train_loss: 0.0638, step time: 0.4838\n",
      "245/388, train_loss: 0.0861, step time: 0.4823\n",
      "246/388, train_loss: 0.0994, step time: 0.5087\n",
      "247/388, train_loss: 0.0692, step time: 0.5101\n",
      "248/388, train_loss: 0.0591, step time: 0.4846\n",
      "249/388, train_loss: 0.1175, step time: 0.4784\n",
      "250/388, train_loss: 0.1352, step time: 0.5009\n",
      "251/388, train_loss: 0.1668, step time: 0.5105\n",
      "252/388, train_loss: 0.1360, step time: 0.5214\n",
      "253/388, train_loss: 0.0679, step time: 0.4979\n",
      "254/388, train_loss: 0.2568, step time: 0.4914\n",
      "255/388, train_loss: 0.4568, step time: 1.0663\n",
      "256/388, train_loss: 0.2203, step time: 0.5482\n",
      "257/388, train_loss: 0.3930, step time: 0.5226\n",
      "258/388, train_loss: 0.0936, step time: 0.4917\n",
      "259/388, train_loss: 0.3943, step time: 0.4914\n",
      "260/388, train_loss: 0.1947, step time: 0.4929\n",
      "261/388, train_loss: 0.0687, step time: 0.4780\n",
      "262/388, train_loss: 0.4012, step time: 0.6343\n",
      "263/388, train_loss: 0.2817, step time: 0.5816\n",
      "264/388, train_loss: 0.2310, step time: 0.5385\n",
      "265/388, train_loss: 0.1245, step time: 0.5071\n",
      "266/388, train_loss: 0.1883, step time: 0.4993\n",
      "267/388, train_loss: 0.1436, step time: 0.4840\n",
      "268/388, train_loss: 0.1527, step time: 0.4847\n",
      "269/388, train_loss: 0.0954, step time: 0.4722\n",
      "270/388, train_loss: 0.1629, step time: 0.4701\n",
      "271/388, train_loss: 0.1010, step time: 0.4710\n",
      "272/388, train_loss: 0.2777, step time: 0.8946\n",
      "273/388, train_loss: 0.3737, step time: 0.5462\n",
      "274/388, train_loss: 0.1654, step time: 0.5142\n",
      "275/388, train_loss: 0.2589, step time: 0.4969\n",
      "276/388, train_loss: 0.2588, step time: 0.4995\n",
      "277/388, train_loss: 0.0865, step time: 0.4806\n",
      "278/388, train_loss: 0.1197, step time: 0.5019\n",
      "279/388, train_loss: 0.0665, step time: 0.4859\n",
      "280/388, train_loss: 0.5005, step time: 0.4862\n",
      "281/388, train_loss: 0.1568, step time: 0.4867\n",
      "282/388, train_loss: 0.1133, step time: 0.5162\n",
      "283/388, train_loss: 0.1317, step time: 0.5031\n",
      "284/388, train_loss: 0.2972, step time: 1.0486\n",
      "285/388, train_loss: 0.4846, step time: 0.5334\n",
      "286/388, train_loss: 0.1358, step time: 0.5169\n",
      "287/388, train_loss: 0.0732, step time: 0.5008\n",
      "288/388, train_loss: 0.0725, step time: 0.5006\n",
      "289/388, train_loss: 0.0875, step time: 0.4836\n",
      "290/388, train_loss: 0.1080, step time: 0.4815\n",
      "291/388, train_loss: 0.2095, step time: 0.9085\n",
      "292/388, train_loss: 0.1001, step time: 0.5255\n",
      "293/388, train_loss: 0.2542, step time: 0.5091\n",
      "294/388, train_loss: 0.1121, step time: 0.4947\n",
      "295/388, train_loss: 0.2547, step time: 0.4863\n",
      "296/388, train_loss: 0.2348, step time: 0.9800\n",
      "297/388, train_loss: 0.1251, step time: 0.5332\n",
      "298/388, train_loss: 0.1533, step time: 0.5134\n",
      "299/388, train_loss: 0.0976, step time: 0.4891\n",
      "300/388, train_loss: 0.1386, step time: 0.4917\n",
      "301/388, train_loss: 0.2884, step time: 0.4787\n",
      "302/388, train_loss: 0.3850, step time: 0.4771\n",
      "303/388, train_loss: 0.0751, step time: 0.4947\n",
      "304/388, train_loss: 0.2132, step time: 0.5030\n",
      "305/388, train_loss: 0.1744, step time: 0.4998\n",
      "306/388, train_loss: 0.2270, step time: 0.5007\n",
      "307/388, train_loss: 0.1202, step time: 0.4855\n",
      "308/388, train_loss: 0.1557, step time: 0.4920\n",
      "309/388, train_loss: 0.2406, step time: 0.4920\n",
      "310/388, train_loss: 0.0970, step time: 1.2716\n",
      "311/388, train_loss: 0.0841, step time: 0.5210\n",
      "312/388, train_loss: 0.0707, step time: 0.5003\n",
      "313/388, train_loss: 0.1081, step time: 0.4836\n",
      "314/388, train_loss: 0.3278, step time: 0.4918\n",
      "315/388, train_loss: 0.1532, step time: 0.4972\n",
      "316/388, train_loss: 0.2065, step time: 0.4868\n",
      "317/388, train_loss: 0.1865, step time: 1.2432\n",
      "318/388, train_loss: 0.3323, step time: 0.5370\n",
      "319/388, train_loss: 0.1773, step time: 0.4993\n",
      "320/388, train_loss: 0.2461, step time: 0.4937\n",
      "321/388, train_loss: 0.1292, step time: 0.4958\n",
      "322/388, train_loss: 0.1792, step time: 0.4851\n",
      "323/388, train_loss: 0.2837, step time: 0.5023\n",
      "324/388, train_loss: 0.1520, step time: 0.4991\n",
      "325/388, train_loss: 0.0930, step time: 0.4784\n",
      "326/388, train_loss: 0.4607, step time: 0.4874\n",
      "327/388, train_loss: 0.2036, step time: 1.1887\n",
      "328/388, train_loss: 0.2025, step time: 0.5336\n",
      "329/388, train_loss: 0.2156, step time: 0.5109\n",
      "330/388, train_loss: 0.3429, step time: 0.4917\n",
      "331/388, train_loss: 0.0979, step time: 0.4958\n",
      "332/388, train_loss: 0.3977, step time: 0.4792\n",
      "333/388, train_loss: 0.1967, step time: 0.4960\n",
      "334/388, train_loss: 0.2352, step time: 0.4898\n",
      "335/388, train_loss: 0.1343, step time: 0.4902\n",
      "336/388, train_loss: 0.2465, step time: 0.4757\n",
      "337/388, train_loss: 0.2089, step time: 0.5113\n",
      "338/388, train_loss: 0.3180, step time: 0.5289\n",
      "339/388, train_loss: 0.3587, step time: 0.5431\n",
      "340/388, train_loss: 0.1460, step time: 0.5201\n",
      "341/388, train_loss: 0.2083, step time: 0.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/388, train_loss: 0.2254, step time: 0.4975\n",
      "343/388, train_loss: 0.4327, step time: 0.4955\n",
      "344/388, train_loss: 0.2003, step time: 1.1169\n",
      "345/388, train_loss: 0.1010, step time: 0.5446\n",
      "346/388, train_loss: 0.2678, step time: 0.4978\n",
      "347/388, train_loss: 0.0960, step time: 0.4887\n",
      "348/388, train_loss: 0.1142, step time: 0.4917\n",
      "349/388, train_loss: 0.1128, step time: 0.4806\n",
      "350/388, train_loss: 0.1256, step time: 0.4891\n",
      "351/388, train_loss: 0.3049, step time: 0.5243\n",
      "352/388, train_loss: 0.0634, step time: 0.5061\n",
      "353/388, train_loss: 0.1477, step time: 0.4895\n",
      "354/388, train_loss: 0.0662, step time: 0.4933\n",
      "355/388, train_loss: 0.5222, step time: 1.1541\n",
      "356/388, train_loss: 0.2214, step time: 0.5312\n",
      "357/388, train_loss: 0.1452, step time: 0.5062\n",
      "358/388, train_loss: 0.0890, step time: 0.4912\n",
      "359/388, train_loss: 0.2076, step time: 0.4952\n",
      "360/388, train_loss: 0.0410, step time: 0.4811\n",
      "361/388, train_loss: 0.1732, step time: 0.4888\n",
      "362/388, train_loss: 0.1922, step time: 0.4829\n",
      "363/388, train_loss: 0.1508, step time: 0.5422\n",
      "364/388, train_loss: 0.0347, step time: 0.5191\n",
      "365/388, train_loss: 0.1658, step time: 0.5132\n",
      "366/388, train_loss: 0.2658, step time: 0.4888\n",
      "367/388, train_loss: 0.1419, step time: 1.2102\n",
      "368/388, train_loss: 0.1155, step time: 0.5173\n",
      "369/388, train_loss: 0.2534, step time: 0.5007\n",
      "370/388, train_loss: 0.2059, step time: 0.4828\n",
      "371/388, train_loss: 0.1915, step time: 0.4851\n",
      "372/388, train_loss: 0.1369, step time: 1.0126\n",
      "373/388, train_loss: 0.1794, step time: 0.5380\n",
      "374/388, train_loss: 0.1364, step time: 0.5134\n",
      "375/388, train_loss: 0.2323, step time: 0.4916\n",
      "376/388, train_loss: 0.2875, step time: 0.4984\n",
      "377/388, train_loss: 0.2400, step time: 0.4783\n",
      "378/388, train_loss: 0.5091, step time: 0.4804\n",
      "379/388, train_loss: 0.1540, step time: 0.4836\n",
      "380/388, train_loss: 0.2029, step time: 0.4729\n",
      "381/388, train_loss: 0.3194, step time: 0.4999\n",
      "382/388, train_loss: 0.0643, step time: 0.4923\n",
      "383/388, train_loss: 0.1227, step time: 0.5421\n",
      "384/388, train_loss: 0.0984, step time: 0.5264\n",
      "385/388, train_loss: 0.1062, step time: 0.5243\n",
      "386/388, train_loss: 0.2108, step time: 0.5016\n",
      "387/388, train_loss: 0.0689, step time: 0.4828\n",
      "388/388, train_loss: 0.1179, step time: 0.4910\n",
      "epoch 66 average loss: 0.1822\n",
      "current epoch: 66 current mean dice: 0.7684 tc: 0.8268 wt: 0.8973 et: 0.5810\n",
      "best mean dice: 0.7711 at epoch: 58\n",
      "time consuming of epoch 66 is: 300.7863\n",
      "----------\n",
      "epoch 67/300\n",
      "1/388, train_loss: 0.1434, step time: 0.4751\n",
      "2/388, train_loss: 0.1168, step time: 0.4809\n",
      "3/388, train_loss: 0.0869, step time: 0.8552\n",
      "4/388, train_loss: 0.6055, step time: 0.5710\n",
      "5/388, train_loss: 0.1137, step time: 0.5577\n",
      "6/388, train_loss: 0.3061, step time: 0.5305\n",
      "7/388, train_loss: 0.0719, step time: 0.5806\n",
      "8/388, train_loss: 0.2329, step time: 0.5720\n",
      "9/388, train_loss: 0.1482, step time: 0.5254\n",
      "10/388, train_loss: 0.1333, step time: 0.5152\n",
      "11/388, train_loss: 0.1264, step time: 0.9832\n",
      "12/388, train_loss: 0.0845, step time: 0.5673\n",
      "13/388, train_loss: 0.0713, step time: 0.5190\n",
      "14/388, train_loss: 0.2670, step time: 0.4968\n",
      "15/388, train_loss: 0.1104, step time: 0.4919\n",
      "16/388, train_loss: 0.1583, step time: 0.7248\n",
      "17/388, train_loss: 0.2151, step time: 0.5384\n",
      "18/388, train_loss: 0.1171, step time: 0.5453\n",
      "19/388, train_loss: 0.4479, step time: 0.5068\n",
      "20/388, train_loss: 0.2339, step time: 0.7050\n",
      "21/388, train_loss: 0.0470, step time: 0.6049\n",
      "22/388, train_loss: 0.1301, step time: 0.5455\n",
      "23/388, train_loss: 0.1922, step time: 0.5203\n",
      "24/388, train_loss: 0.1082, step time: 0.5056\n",
      "25/388, train_loss: 0.5524, step time: 0.5013\n",
      "26/388, train_loss: 0.1424, step time: 0.5118\n",
      "27/388, train_loss: 0.1006, step time: 0.6041\n",
      "28/388, train_loss: 0.1207, step time: 0.5692\n",
      "29/388, train_loss: 0.2537, step time: 0.5339\n",
      "30/388, train_loss: 0.1668, step time: 0.5066\n",
      "31/388, train_loss: 0.1562, step time: 0.5233\n",
      "32/388, train_loss: 0.2685, step time: 0.6509\n",
      "33/388, train_loss: 0.1305, step time: 0.5619\n",
      "34/388, train_loss: 0.0540, step time: 0.5200\n",
      "35/388, train_loss: 0.1064, step time: 0.4939\n",
      "36/388, train_loss: 0.1423, step time: 1.1253\n",
      "37/388, train_loss: 0.1989, step time: 0.5356\n",
      "38/388, train_loss: 0.1462, step time: 0.5046\n",
      "39/388, train_loss: 0.1658, step time: 0.4917\n",
      "40/388, train_loss: 0.1904, step time: 0.4866\n",
      "41/388, train_loss: 0.3449, step time: 1.2076\n",
      "42/388, train_loss: 0.1777, step time: 0.5296\n",
      "43/388, train_loss: 0.4449, step time: 0.5079\n",
      "44/388, train_loss: 0.1708, step time: 0.5011\n",
      "45/388, train_loss: 0.1290, step time: 0.5107\n",
      "46/388, train_loss: 0.4674, step time: 0.4941\n",
      "47/388, train_loss: 0.2443, step time: 0.4946\n",
      "48/388, train_loss: 0.1433, step time: 0.5368\n",
      "49/388, train_loss: 0.1465, step time: 0.5324\n",
      "50/388, train_loss: 0.5721, step time: 0.5152\n",
      "51/388, train_loss: 0.2372, step time: 0.4935\n",
      "52/388, train_loss: 0.1015, step time: 0.4958\n",
      "53/388, train_loss: 0.1249, step time: 0.5220\n",
      "54/388, train_loss: 0.4540, step time: 0.5057\n",
      "55/388, train_loss: 0.1780, step time: 0.4869\n",
      "56/388, train_loss: 0.0574, step time: 1.0474\n",
      "57/388, train_loss: 0.0626, step time: 0.5458\n",
      "58/388, train_loss: 0.1967, step time: 0.5151\n",
      "59/388, train_loss: 0.1935, step time: 0.4992\n",
      "60/388, train_loss: 0.1417, step time: 0.4984\n",
      "61/388, train_loss: 0.0968, step time: 0.4966\n",
      "62/388, train_loss: 0.1128, step time: 0.8325\n",
      "63/388, train_loss: 0.1944, step time: 0.5648\n",
      "64/388, train_loss: 0.1071, step time: 0.5249\n",
      "65/388, train_loss: 0.0927, step time: 0.5044\n",
      "66/388, train_loss: 0.0658, step time: 0.4899\n",
      "67/388, train_loss: 0.1035, step time: 0.5066\n",
      "68/388, train_loss: 0.2227, step time: 0.4875\n",
      "69/388, train_loss: 0.3113, step time: 1.1528\n",
      "70/388, train_loss: 0.3051, step time: 0.5369\n",
      "71/388, train_loss: 0.0778, step time: 0.5166\n",
      "72/388, train_loss: 0.2346, step time: 0.4967\n",
      "73/388, train_loss: 0.0935, step time: 0.4980\n",
      "74/388, train_loss: 0.1249, step time: 0.4938\n",
      "75/388, train_loss: 0.1309, step time: 0.4814\n",
      "76/388, train_loss: 0.3969, step time: 0.5253\n",
      "77/388, train_loss: 0.0915, step time: 0.4991\n",
      "78/388, train_loss: 0.1979, step time: 0.4917\n",
      "79/388, train_loss: 0.2093, step time: 0.4995\n",
      "80/388, train_loss: 0.0937, step time: 0.4941\n",
      "81/388, train_loss: 0.2430, step time: 0.5258\n",
      "82/388, train_loss: 0.2347, step time: 0.5074\n",
      "83/388, train_loss: 0.1676, step time: 0.5118\n",
      "84/388, train_loss: 0.1960, step time: 0.5690\n",
      "85/388, train_loss: 0.1240, step time: 0.5356\n",
      "86/388, train_loss: 0.1907, step time: 0.5064\n",
      "87/388, train_loss: 0.1848, step time: 0.5123\n",
      "88/388, train_loss: 0.0960, step time: 0.5742\n",
      "89/388, train_loss: 0.0631, step time: 0.5415\n",
      "90/388, train_loss: 0.0937, step time: 0.5207\n",
      "91/388, train_loss: 0.1404, step time: 0.4971\n",
      "92/388, train_loss: 0.2110, step time: 0.4891\n",
      "93/388, train_loss: 0.1050, step time: 0.5136\n",
      "94/388, train_loss: 0.0980, step time: 1.1957\n",
      "95/388, train_loss: 0.2259, step time: 0.5405\n",
      "96/388, train_loss: 0.2027, step time: 0.5072\n",
      "97/388, train_loss: 0.1875, step time: 0.4935\n",
      "98/388, train_loss: 0.1635, step time: 0.5304\n",
      "99/388, train_loss: 0.0925, step time: 0.5178\n",
      "100/388, train_loss: 0.3921, step time: 0.5405\n",
      "101/388, train_loss: 0.1728, step time: 0.5240\n",
      "102/388, train_loss: 0.1133, step time: 0.5019\n",
      "103/388, train_loss: 0.3348, step time: 0.5121\n",
      "104/388, train_loss: 0.1882, step time: 0.4986\n",
      "105/388, train_loss: 0.1090, step time: 0.5151\n",
      "106/388, train_loss: 0.1639, step time: 0.5010\n",
      "107/388, train_loss: 0.1454, step time: 0.5065\n",
      "108/388, train_loss: 0.1056, step time: 0.4981\n",
      "109/388, train_loss: 0.2487, step time: 0.4939\n",
      "110/388, train_loss: 0.2177, step time: 0.5995\n",
      "111/388, train_loss: 0.0616, step time: 0.5584\n",
      "112/388, train_loss: 0.2529, step time: 0.5307\n",
      "113/388, train_loss: 0.2253, step time: 0.4924\n",
      "114/388, train_loss: 0.0980, step time: 1.0725\n",
      "115/388, train_loss: 0.2047, step time: 0.5402\n",
      "116/388, train_loss: 0.1090, step time: 0.5102\n",
      "117/388, train_loss: 0.2826, step time: 0.4900\n",
      "118/388, train_loss: 0.1984, step time: 0.4994\n",
      "119/388, train_loss: 0.1247, step time: 0.4873\n",
      "120/388, train_loss: 0.2742, step time: 0.5080\n",
      "121/388, train_loss: 0.2988, step time: 0.4891\n",
      "122/388, train_loss: 0.0463, step time: 0.4966\n",
      "123/388, train_loss: 0.1607, step time: 0.5026\n",
      "124/388, train_loss: 0.1866, step time: 0.4828\n",
      "125/388, train_loss: 0.1017, step time: 0.5071\n",
      "126/388, train_loss: 0.0498, step time: 0.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/388, train_loss: 0.0792, step time: 0.4943\n",
      "128/388, train_loss: 0.1421, step time: 1.1443\n",
      "129/388, train_loss: 0.1154, step time: 0.5396\n",
      "130/388, train_loss: 0.1030, step time: 0.5068\n",
      "131/388, train_loss: 0.1684, step time: 0.4923\n",
      "132/388, train_loss: 0.2345, step time: 0.4943\n",
      "133/388, train_loss: 0.1300, step time: 0.4882\n",
      "134/388, train_loss: 0.2800, step time: 0.5804\n",
      "135/388, train_loss: 0.1685, step time: 0.5517\n",
      "136/388, train_loss: 0.1903, step time: 0.5178\n",
      "137/388, train_loss: 0.1433, step time: 0.4989\n",
      "138/388, train_loss: 0.0732, step time: 0.5006\n",
      "139/388, train_loss: 0.0968, step time: 0.4938\n",
      "140/388, train_loss: 0.3757, step time: 0.5017\n",
      "141/388, train_loss: 0.0911, step time: 0.5289\n",
      "142/388, train_loss: 0.4597, step time: 0.5331\n",
      "143/388, train_loss: 0.1317, step time: 0.5160\n",
      "144/388, train_loss: 0.2604, step time: 0.4979\n",
      "145/388, train_loss: 0.4395, step time: 0.4831\n",
      "146/388, train_loss: 0.0737, step time: 1.0582\n",
      "147/388, train_loss: 0.1071, step time: 0.5586\n",
      "148/388, train_loss: 0.0693, step time: 0.5223\n",
      "149/388, train_loss: 0.2288, step time: 0.5033\n",
      "150/388, train_loss: 0.0798, step time: 0.5362\n",
      "151/388, train_loss: 0.2012, step time: 0.5197\n",
      "152/388, train_loss: 0.1957, step time: 0.5003\n",
      "153/388, train_loss: 0.0703, step time: 0.4959\n",
      "154/388, train_loss: 0.3419, step time: 0.4790\n",
      "155/388, train_loss: 0.2678, step time: 0.4769\n",
      "156/388, train_loss: 0.2565, step time: 1.0054\n",
      "157/388, train_loss: 0.0779, step time: 0.5490\n",
      "158/388, train_loss: 0.1008, step time: 0.5252\n",
      "159/388, train_loss: 0.2852, step time: 0.5038\n",
      "160/388, train_loss: 0.1244, step time: 0.4960\n",
      "161/388, train_loss: 0.1105, step time: 0.4857\n",
      "162/388, train_loss: 0.2306, step time: 0.4854\n",
      "163/388, train_loss: 0.2347, step time: 0.5010\n",
      "164/388, train_loss: 0.1537, step time: 0.5233\n",
      "165/388, train_loss: 0.2356, step time: 0.4993\n",
      "166/388, train_loss: 0.0449, step time: 0.5009\n",
      "167/388, train_loss: 0.2630, step time: 0.4875\n",
      "168/388, train_loss: 0.1002, step time: 0.4847\n",
      "169/388, train_loss: 0.1175, step time: 0.4888\n",
      "170/388, train_loss: 0.1973, step time: 0.5067\n",
      "171/388, train_loss: 0.1716, step time: 0.5005\n",
      "172/388, train_loss: 0.2565, step time: 0.5621\n",
      "173/388, train_loss: 0.0883, step time: 0.5515\n",
      "174/388, train_loss: 0.2106, step time: 0.5168\n",
      "175/388, train_loss: 0.2326, step time: 0.5061\n",
      "176/388, train_loss: 0.1169, step time: 0.4961\n",
      "177/388, train_loss: 0.2151, step time: 1.0945\n",
      "178/388, train_loss: 0.1901, step time: 0.5415\n",
      "179/388, train_loss: 0.1843, step time: 0.5130\n",
      "180/388, train_loss: 0.1686, step time: 0.4911\n",
      "181/388, train_loss: 0.4372, step time: 0.5031\n",
      "182/388, train_loss: 0.1465, step time: 0.4944\n",
      "183/388, train_loss: 0.1893, step time: 0.4898\n",
      "184/388, train_loss: 0.3118, step time: 0.4959\n",
      "185/388, train_loss: 0.1538, step time: 0.5082\n",
      "186/388, train_loss: 0.2350, step time: 0.5059\n",
      "187/388, train_loss: 0.1186, step time: 0.5232\n",
      "188/388, train_loss: 0.1491, step time: 0.5149\n",
      "189/388, train_loss: 0.1105, step time: 0.5001\n",
      "190/388, train_loss: 0.1211, step time: 0.4919\n",
      "191/388, train_loss: 0.0332, step time: 0.4801\n",
      "192/388, train_loss: 0.0769, step time: 1.1816\n",
      "193/388, train_loss: 0.0828, step time: 0.5325\n",
      "194/388, train_loss: 0.1000, step time: 0.5105\n",
      "195/388, train_loss: 0.2908, step time: 0.4991\n",
      "196/388, train_loss: 0.3514, step time: 0.4970\n",
      "197/388, train_loss: 0.2678, step time: 0.4898\n",
      "198/388, train_loss: 0.2684, step time: 0.5296\n",
      "199/388, train_loss: 0.0551, step time: 0.6395\n",
      "200/388, train_loss: 0.3280, step time: 0.5338\n",
      "201/388, train_loss: 0.1536, step time: 0.5096\n",
      "202/388, train_loss: 0.0881, step time: 0.4910\n",
      "203/388, train_loss: 0.2823, step time: 0.4922\n",
      "204/388, train_loss: 0.2448, step time: 0.4921\n",
      "205/388, train_loss: 0.0371, step time: 0.4786\n",
      "206/388, train_loss: 0.0988, step time: 0.4713\n",
      "207/388, train_loss: 0.1601, step time: 0.5091\n",
      "208/388, train_loss: 0.1216, step time: 0.5025\n",
      "209/388, train_loss: 0.0382, step time: 0.4956\n",
      "210/388, train_loss: 0.0540, step time: 0.4791\n",
      "211/388, train_loss: 0.0836, step time: 1.0229\n",
      "212/388, train_loss: 0.1646, step time: 0.5469\n",
      "213/388, train_loss: 0.1799, step time: 0.5093\n",
      "214/388, train_loss: 0.1520, step time: 0.4910\n",
      "215/388, train_loss: 0.1529, step time: 0.4912\n",
      "216/388, train_loss: 0.1132, step time: 0.4798\n",
      "217/388, train_loss: 0.1220, step time: 0.4931\n",
      "218/388, train_loss: 0.1126, step time: 0.4757\n",
      "219/388, train_loss: 0.0640, step time: 0.7449\n",
      "220/388, train_loss: 0.0868, step time: 0.5609\n",
      "221/388, train_loss: 0.1626, step time: 0.5256\n",
      "222/388, train_loss: 0.1210, step time: 0.4982\n",
      "223/388, train_loss: 0.2121, step time: 0.4941\n",
      "224/388, train_loss: 0.0290, step time: 0.4848\n",
      "225/388, train_loss: 0.2145, step time: 0.4841\n",
      "226/388, train_loss: 0.1848, step time: 0.5652\n",
      "227/388, train_loss: 0.1412, step time: 0.5233\n",
      "228/388, train_loss: 0.3262, step time: 0.5080\n",
      "229/388, train_loss: 0.1692, step time: 0.5368\n",
      "230/388, train_loss: 0.2705, step time: 0.5068\n",
      "231/388, train_loss: 0.0708, step time: 0.5085\n",
      "232/388, train_loss: 0.2850, step time: 0.4875\n",
      "233/388, train_loss: 0.2654, step time: 0.9972\n",
      "234/388, train_loss: 0.3010, step time: 0.5362\n",
      "235/388, train_loss: 0.0973, step time: 0.5136\n",
      "236/388, train_loss: 0.0772, step time: 0.4950\n",
      "237/388, train_loss: 0.2338, step time: 0.4853\n",
      "238/388, train_loss: 0.3543, step time: 0.4932\n",
      "239/388, train_loss: 0.1301, step time: 0.4963\n",
      "240/388, train_loss: 0.3592, step time: 0.5044\n",
      "241/388, train_loss: 0.1154, step time: 0.5095\n",
      "242/388, train_loss: 0.2435, step time: 0.5033\n",
      "243/388, train_loss: 0.0457, step time: 0.4863\n",
      "244/388, train_loss: 0.1074, step time: 0.4849\n",
      "245/388, train_loss: 0.0868, step time: 0.4764\n",
      "246/388, train_loss: 0.0979, step time: 0.9319\n",
      "247/388, train_loss: 0.2018, step time: 0.5490\n",
      "248/388, train_loss: 0.2600, step time: 0.5198\n",
      "249/388, train_loss: 0.1939, step time: 0.5035\n",
      "250/388, train_loss: 0.1468, step time: 0.4881\n",
      "251/388, train_loss: 0.2465, step time: 0.4951\n",
      "252/388, train_loss: 0.0904, step time: 0.4827\n",
      "253/388, train_loss: 0.0797, step time: 0.4775\n",
      "254/388, train_loss: 0.1060, step time: 0.5055\n",
      "255/388, train_loss: 0.1511, step time: 1.1971\n",
      "256/388, train_loss: 0.1159, step time: 0.5246\n",
      "257/388, train_loss: 0.1168, step time: 0.5007\n",
      "258/388, train_loss: 0.1382, step time: 0.4859\n",
      "259/388, train_loss: 0.0457, step time: 0.4828\n",
      "260/388, train_loss: 0.0771, step time: 0.4913\n",
      "261/388, train_loss: 0.1467, step time: 0.4789\n",
      "262/388, train_loss: 0.1898, step time: 0.4758\n",
      "263/388, train_loss: 0.1928, step time: 1.2292\n",
      "264/388, train_loss: 0.1297, step time: 0.5148\n",
      "265/388, train_loss: 0.0976, step time: 0.4917\n",
      "266/388, train_loss: 0.1140, step time: 0.4839\n",
      "267/388, train_loss: 0.0918, step time: 0.4914\n",
      "268/388, train_loss: 0.1437, step time: 0.4751\n",
      "269/388, train_loss: 0.2254, step time: 0.4788\n",
      "270/388, train_loss: 0.2122, step time: 0.8547\n",
      "271/388, train_loss: 0.2191, step time: 0.5446\n",
      "272/388, train_loss: 0.1027, step time: 0.5061\n",
      "273/388, train_loss: 0.3376, step time: 0.4913\n",
      "274/388, train_loss: 0.3387, step time: 0.4937\n",
      "275/388, train_loss: 0.0836, step time: 0.4894\n",
      "276/388, train_loss: 0.1811, step time: 0.4763\n",
      "277/388, train_loss: 0.1202, step time: 0.6083\n",
      "278/388, train_loss: 0.4130, step time: 0.5593\n",
      "279/388, train_loss: 0.1637, step time: 0.5146\n",
      "280/388, train_loss: 0.4141, step time: 0.4957\n",
      "281/388, train_loss: 0.1180, step time: 0.7182\n",
      "282/388, train_loss: 0.0950, step time: 0.5351\n",
      "283/388, train_loss: 0.2275, step time: 0.5071\n",
      "284/388, train_loss: 0.1248, step time: 0.4949\n",
      "285/388, train_loss: 0.0798, step time: 0.4825\n",
      "286/388, train_loss: 0.1453, step time: 0.4837\n",
      "287/388, train_loss: 0.1324, step time: 0.4752\n",
      "288/388, train_loss: 0.1147, step time: 0.5070\n",
      "289/388, train_loss: 0.1130, step time: 0.4867\n",
      "290/388, train_loss: 0.2225, step time: 0.4985\n",
      "291/388, train_loss: 0.1728, step time: 1.1008\n",
      "292/388, train_loss: 0.1578, step time: 0.5365\n",
      "293/388, train_loss: 0.1102, step time: 0.5119\n",
      "294/388, train_loss: 0.1270, step time: 0.4907\n",
      "295/388, train_loss: 0.2750, step time: 0.5012\n",
      "296/388, train_loss: 0.0912, step time: 0.4853\n",
      "297/388, train_loss: 0.0953, step time: 0.4889\n",
      "298/388, train_loss: 0.1354, step time: 0.4764\n",
      "299/388, train_loss: 0.1046, step time: 0.9012\n",
      "300/388, train_loss: 0.1968, step time: 0.5349\n",
      "301/388, train_loss: 0.3592, step time: 0.5065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/388, train_loss: 0.0724, step time: 0.4895\n",
      "303/388, train_loss: 0.2782, step time: 0.4882\n",
      "304/388, train_loss: 0.3112, step time: 0.5078\n",
      "305/388, train_loss: 0.4349, step time: 0.4953\n",
      "306/388, train_loss: 0.1389, step time: 0.4932\n",
      "307/388, train_loss: 0.2526, step time: 0.4875\n",
      "308/388, train_loss: 0.2847, step time: 0.5848\n",
      "309/388, train_loss: 0.1599, step time: 0.5300\n",
      "310/388, train_loss: 0.2861, step time: 0.5002\n",
      "311/388, train_loss: 0.2053, step time: 0.5020\n",
      "312/388, train_loss: 0.1155, step time: 0.4843\n",
      "313/388, train_loss: 0.3251, step time: 0.4857\n",
      "314/388, train_loss: 0.2029, step time: 0.4861\n",
      "315/388, train_loss: 0.0896, step time: 0.4760\n",
      "316/388, train_loss: 0.0927, step time: 0.7189\n",
      "317/388, train_loss: 0.1712, step time: 0.5464\n",
      "318/388, train_loss: 0.1979, step time: 0.5148\n",
      "319/388, train_loss: 0.2284, step time: 0.5040\n",
      "320/388, train_loss: 0.0506, step time: 0.5442\n",
      "321/388, train_loss: 0.1905, step time: 0.5236\n",
      "322/388, train_loss: 0.0806, step time: 0.5057\n",
      "323/388, train_loss: 0.2316, step time: 0.5036\n",
      "324/388, train_loss: 0.1528, step time: 0.4908\n",
      "325/388, train_loss: 0.0799, step time: 0.4945\n",
      "326/388, train_loss: 0.4692, step time: 0.4835\n",
      "327/388, train_loss: 0.2519, step time: 0.6209\n",
      "328/388, train_loss: 0.0672, step time: 0.5447\n",
      "329/388, train_loss: 0.1620, step time: 0.5167\n",
      "330/388, train_loss: 0.1132, step time: 0.4910\n",
      "331/388, train_loss: 0.3389, step time: 0.4876\n",
      "332/388, train_loss: 0.1355, step time: 0.4908\n",
      "333/388, train_loss: 0.1752, step time: 0.4971\n",
      "334/388, train_loss: 0.1105, step time: 0.4996\n",
      "335/388, train_loss: 0.2654, step time: 0.4816\n",
      "336/388, train_loss: 0.2213, step time: 0.4840\n",
      "337/388, train_loss: 0.0889, step time: 0.4824\n",
      "338/388, train_loss: 0.1400, step time: 1.0757\n",
      "339/388, train_loss: 0.3047, step time: 0.5393\n",
      "340/388, train_loss: 0.0684, step time: 0.5085\n",
      "341/388, train_loss: 0.1810, step time: 0.4915\n",
      "342/388, train_loss: 0.0591, step time: 0.5005\n",
      "343/388, train_loss: 0.0675, step time: 0.4791\n",
      "344/388, train_loss: 0.2905, step time: 0.4903\n",
      "345/388, train_loss: 0.1756, step time: 0.4876\n",
      "346/388, train_loss: 0.1022, step time: 1.1485\n",
      "347/388, train_loss: 0.1900, step time: 0.5359\n",
      "348/388, train_loss: 0.0760, step time: 0.4983\n",
      "349/388, train_loss: 0.2820, step time: 0.5010\n",
      "350/388, train_loss: 0.1078, step time: 0.4890\n",
      "351/388, train_loss: 0.1586, step time: 0.5462\n",
      "352/388, train_loss: 0.1386, step time: 0.5212\n",
      "353/388, train_loss: 0.1905, step time: 0.5106\n",
      "354/388, train_loss: 0.0804, step time: 0.5140\n",
      "355/388, train_loss: 0.2321, step time: 0.5096\n",
      "356/388, train_loss: 0.3706, step time: 0.4978\n",
      "357/388, train_loss: 0.1176, step time: 0.4819\n",
      "358/388, train_loss: 0.5216, step time: 0.4797\n",
      "359/388, train_loss: 0.0701, step time: 0.5322\n",
      "360/388, train_loss: 0.2122, step time: 0.5232\n",
      "361/388, train_loss: 0.0769, step time: 0.5546\n",
      "362/388, train_loss: 0.0968, step time: 0.5156\n",
      "363/388, train_loss: 0.4242, step time: 0.5099\n",
      "364/388, train_loss: 0.1068, step time: 0.4884\n",
      "365/388, train_loss: 0.0801, step time: 0.4983\n",
      "366/388, train_loss: 0.2782, step time: 0.4864\n",
      "367/388, train_loss: 0.2679, step time: 1.2244\n",
      "368/388, train_loss: 0.2595, step time: 0.5440\n",
      "369/388, train_loss: 0.1700, step time: 0.5196\n",
      "370/388, train_loss: 0.2034, step time: 0.4950\n",
      "371/388, train_loss: 0.1530, step time: 0.4990\n",
      "372/388, train_loss: 0.1498, step time: 0.4813\n",
      "373/388, train_loss: 0.1091, step time: 0.4782\n",
      "374/388, train_loss: 0.2783, step time: 0.5076\n",
      "375/388, train_loss: 0.4931, step time: 0.5636\n",
      "376/388, train_loss: 0.1413, step time: 0.5451\n",
      "377/388, train_loss: 0.3376, step time: 0.5155\n",
      "378/388, train_loss: 0.2056, step time: 0.5039\n",
      "379/388, train_loss: 0.3120, step time: 0.4850\n",
      "380/388, train_loss: 0.2006, step time: 0.5009\n",
      "381/388, train_loss: 0.4831, step time: 1.1749\n",
      "382/388, train_loss: 0.1709, step time: 0.5391\n",
      "383/388, train_loss: 0.0664, step time: 0.5174\n",
      "384/388, train_loss: 0.0535, step time: 0.4945\n",
      "385/388, train_loss: 0.1744, step time: 0.4862\n",
      "386/388, train_loss: 0.3617, step time: 0.4757\n",
      "387/388, train_loss: 0.1556, step time: 0.4707\n",
      "388/388, train_loss: 0.1553, step time: 0.4948\n",
      "epoch 67 average loss: 0.1800\n",
      "saved new best metric model\n",
      "current epoch: 67 current mean dice: 0.7717 tc: 0.8204 wt: 0.9022 et: 0.5926\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 67 is: 304.2556\n",
      "----------\n",
      "epoch 68/300\n",
      "1/388, train_loss: 0.1686, step time: 0.4819\n",
      "2/388, train_loss: 0.1062, step time: 0.4828\n",
      "3/388, train_loss: 0.2321, step time: 0.4822\n",
      "4/388, train_loss: 0.1473, step time: 0.5857\n",
      "5/388, train_loss: 0.3438, step time: 0.6060\n",
      "6/388, train_loss: 0.1366, step time: 0.5746\n",
      "7/388, train_loss: 0.0792, step time: 0.5994\n",
      "8/388, train_loss: 0.2346, step time: 0.6815\n",
      "9/388, train_loss: 0.1862, step time: 0.5765\n",
      "10/388, train_loss: 0.1623, step time: 0.5807\n",
      "11/388, train_loss: 0.1517, step time: 0.5361\n",
      "12/388, train_loss: 0.0702, step time: 1.2009\n",
      "13/388, train_loss: 0.1932, step time: 0.5401\n",
      "14/388, train_loss: 0.1968, step time: 0.5277\n",
      "15/388, train_loss: 0.5997, step time: 0.5258\n",
      "16/388, train_loss: 0.1830, step time: 0.5980\n",
      "17/388, train_loss: 0.2249, step time: 0.5572\n",
      "18/388, train_loss: 0.2599, step time: 0.5299\n",
      "19/388, train_loss: 0.2420, step time: 0.5017\n",
      "20/388, train_loss: 0.0883, step time: 1.1173\n",
      "21/388, train_loss: 0.1766, step time: 0.5395\n",
      "22/388, train_loss: 0.0866, step time: 0.5001\n",
      "23/388, train_loss: 0.1210, step time: 0.5741\n",
      "24/388, train_loss: 0.2351, step time: 0.5514\n",
      "25/388, train_loss: 0.2120, step time: 0.5105\n",
      "26/388, train_loss: 0.1140, step time: 1.0531\n",
      "27/388, train_loss: 0.1023, step time: 0.5469\n",
      "28/388, train_loss: 0.0645, step time: 0.5234\n",
      "29/388, train_loss: 0.2508, step time: 0.5112\n",
      "30/388, train_loss: 0.0382, step time: 0.4956\n",
      "31/388, train_loss: 0.0738, step time: 0.4911\n",
      "32/388, train_loss: 0.1235, step time: 0.4910\n",
      "33/388, train_loss: 0.3110, step time: 1.1063\n",
      "34/388, train_loss: 0.2045, step time: 0.5271\n",
      "35/388, train_loss: 0.1180, step time: 0.5116\n",
      "36/388, train_loss: 0.0850, step time: 0.5282\n",
      "37/388, train_loss: 0.2981, step time: 0.5049\n",
      "38/388, train_loss: 0.3977, step time: 0.4944\n",
      "39/388, train_loss: 0.4255, step time: 0.4934\n",
      "40/388, train_loss: 0.0915, step time: 0.5066\n",
      "41/388, train_loss: 0.1377, step time: 0.4968\n",
      "42/388, train_loss: 0.3151, step time: 0.4970\n",
      "43/388, train_loss: 0.3120, step time: 1.1730\n",
      "44/388, train_loss: 0.3639, step time: 0.5304\n",
      "45/388, train_loss: 0.1465, step time: 0.5124\n",
      "46/388, train_loss: 0.0707, step time: 0.4989\n",
      "47/388, train_loss: 0.0955, step time: 0.4980\n",
      "48/388, train_loss: 0.5029, step time: 0.7701\n",
      "49/388, train_loss: 0.2628, step time: 0.5597\n",
      "50/388, train_loss: 0.1630, step time: 0.5452\n",
      "51/388, train_loss: 0.3718, step time: 0.5314\n",
      "52/388, train_loss: 0.0533, step time: 0.5600\n",
      "53/388, train_loss: 0.2588, step time: 0.5358\n",
      "54/388, train_loss: 0.1957, step time: 0.5212\n",
      "55/388, train_loss: 0.1533, step time: 0.5044\n",
      "56/388, train_loss: 0.2381, step time: 0.5420\n",
      "57/388, train_loss: 0.0856, step time: 0.5324\n",
      "58/388, train_loss: 0.0796, step time: 0.5155\n",
      "59/388, train_loss: 0.0900, step time: 0.5369\n",
      "60/388, train_loss: 0.1739, step time: 0.5530\n",
      "61/388, train_loss: 0.4631, step time: 0.5371\n",
      "62/388, train_loss: 0.1690, step time: 0.5319\n",
      "63/388, train_loss: 0.2943, step time: 0.5229\n",
      "64/388, train_loss: 0.2737, step time: 0.5007\n",
      "65/388, train_loss: 0.1742, step time: 0.5062\n",
      "66/388, train_loss: 0.0808, step time: 0.5869\n",
      "67/388, train_loss: 0.1859, step time: 0.5523\n",
      "68/388, train_loss: 0.1335, step time: 0.5304\n",
      "69/388, train_loss: 0.1391, step time: 0.5088\n",
      "70/388, train_loss: 0.2235, step time: 0.5469\n",
      "71/388, train_loss: 0.1024, step time: 0.5185\n",
      "72/388, train_loss: 0.1159, step time: 0.5349\n",
      "73/388, train_loss: 0.0310, step time: 0.5421\n",
      "74/388, train_loss: 0.0803, step time: 0.5430\n",
      "75/388, train_loss: 0.1060, step time: 0.6777\n",
      "76/388, train_loss: 0.1132, step time: 0.5418\n",
      "77/388, train_loss: 0.1311, step time: 0.5094\n",
      "78/388, train_loss: 0.1210, step time: 0.4838\n",
      "79/388, train_loss: 0.2172, step time: 0.4941\n",
      "80/388, train_loss: 0.5019, step time: 0.4904\n",
      "81/388, train_loss: 0.2514, step time: 0.5077\n",
      "82/388, train_loss: 0.2654, step time: 0.4979\n",
      "83/388, train_loss: 0.2072, step time: 0.5494\n",
      "84/388, train_loss: 0.4482, step time: 0.5220\n",
      "85/388, train_loss: 0.1267, step time: 0.5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/388, train_loss: 0.3083, step time: 0.5115\n",
      "87/388, train_loss: 0.1101, step time: 0.5057\n",
      "88/388, train_loss: 0.1531, step time: 0.4855\n",
      "89/388, train_loss: 0.0589, step time: 1.1463\n",
      "90/388, train_loss: 0.1635, step time: 0.5520\n",
      "91/388, train_loss: 0.1392, step time: 0.5217\n",
      "92/388, train_loss: 0.2580, step time: 0.5414\n",
      "93/388, train_loss: 0.0985, step time: 0.5310\n",
      "94/388, train_loss: 0.1716, step time: 0.5149\n",
      "95/388, train_loss: 0.1123, step time: 0.4850\n",
      "96/388, train_loss: 0.1249, step time: 0.5219\n",
      "97/388, train_loss: 0.1300, step time: 0.4999\n",
      "98/388, train_loss: 0.2126, step time: 0.5019\n",
      "99/388, train_loss: 0.1661, step time: 0.5006\n",
      "100/388, train_loss: 0.0748, step time: 1.2610\n",
      "101/388, train_loss: 0.0944, step time: 0.5327\n",
      "102/388, train_loss: 0.1429, step time: 0.5105\n",
      "103/388, train_loss: 0.0810, step time: 0.5086\n",
      "104/388, train_loss: 0.1405, step time: 0.4879\n",
      "105/388, train_loss: 0.2499, step time: 0.4964\n",
      "106/388, train_loss: 0.3625, step time: 1.1295\n",
      "107/388, train_loss: 0.2336, step time: 0.5443\n",
      "108/388, train_loss: 0.0477, step time: 0.5061\n",
      "109/388, train_loss: 0.2295, step time: 0.4991\n",
      "110/388, train_loss: 0.0862, step time: 0.4813\n",
      "111/388, train_loss: 0.1768, step time: 0.4942\n",
      "112/388, train_loss: 0.2564, step time: 0.4845\n",
      "113/388, train_loss: 0.1244, step time: 0.5096\n",
      "114/388, train_loss: 0.2028, step time: 0.4993\n",
      "115/388, train_loss: 0.2406, step time: 0.4950\n",
      "116/388, train_loss: 0.1516, step time: 1.1194\n",
      "117/388, train_loss: 0.1940, step time: 0.5250\n",
      "118/388, train_loss: 0.2290, step time: 0.5051\n",
      "119/388, train_loss: 0.0934, step time: 0.4926\n",
      "120/388, train_loss: 0.2465, step time: 0.4985\n",
      "121/388, train_loss: 0.1899, step time: 0.4855\n",
      "122/388, train_loss: 0.2304, step time: 0.4979\n",
      "123/388, train_loss: 0.1698, step time: 0.4955\n",
      "124/388, train_loss: 0.0408, step time: 0.4875\n",
      "125/388, train_loss: 0.1825, step time: 0.5020\n",
      "126/388, train_loss: 0.0816, step time: 0.4844\n",
      "127/388, train_loss: 0.1912, step time: 1.1149\n",
      "128/388, train_loss: 0.2053, step time: 0.5396\n",
      "129/388, train_loss: 0.3174, step time: 0.4983\n",
      "130/388, train_loss: 0.0482, step time: 0.4873\n",
      "131/388, train_loss: 0.0704, step time: 0.4794\n",
      "132/388, train_loss: 0.2101, step time: 0.6223\n",
      "133/388, train_loss: 0.2844, step time: 0.5403\n",
      "134/388, train_loss: 0.1844, step time: 0.5061\n",
      "135/388, train_loss: 0.1231, step time: 0.4834\n",
      "136/388, train_loss: 0.1158, step time: 0.4847\n",
      "137/388, train_loss: 0.1548, step time: 0.5212\n",
      "138/388, train_loss: 0.1147, step time: 0.5056\n",
      "139/388, train_loss: 0.4034, step time: 0.4946\n",
      "140/388, train_loss: 0.0684, step time: 0.4815\n",
      "141/388, train_loss: 0.2435, step time: 0.5121\n",
      "142/388, train_loss: 0.1549, step time: 0.5025\n",
      "143/388, train_loss: 0.1580, step time: 0.4936\n",
      "144/388, train_loss: 0.2146, step time: 0.5267\n",
      "145/388, train_loss: 0.3837, step time: 0.4979\n",
      "146/388, train_loss: 0.1168, step time: 0.4818\n",
      "147/388, train_loss: 0.1897, step time: 1.1474\n",
      "148/388, train_loss: 0.1202, step time: 0.5506\n",
      "149/388, train_loss: 0.1614, step time: 0.5339\n",
      "150/388, train_loss: 0.0890, step time: 0.5100\n",
      "151/388, train_loss: 0.0797, step time: 0.4888\n",
      "152/388, train_loss: 0.2254, step time: 0.4873\n",
      "153/388, train_loss: 0.1069, step time: 0.4860\n",
      "154/388, train_loss: 0.1870, step time: 0.4890\n",
      "155/388, train_loss: 0.1203, step time: 0.5070\n",
      "156/388, train_loss: 0.1048, step time: 0.4994\n",
      "157/388, train_loss: 0.1856, step time: 0.4901\n",
      "158/388, train_loss: 0.0742, step time: 1.0407\n",
      "159/388, train_loss: 0.2030, step time: 0.5400\n",
      "160/388, train_loss: 0.1179, step time: 0.5108\n",
      "161/388, train_loss: 0.1529, step time: 0.4959\n",
      "162/388, train_loss: 0.0865, step time: 0.4991\n",
      "163/388, train_loss: 0.1501, step time: 0.4937\n",
      "164/388, train_loss: 0.1024, step time: 0.4996\n",
      "165/388, train_loss: 0.0955, step time: 0.4939\n",
      "166/388, train_loss: 0.1089, step time: 0.4886\n",
      "167/388, train_loss: 0.0376, step time: 1.1539\n",
      "168/388, train_loss: 0.0917, step time: 0.5293\n",
      "169/388, train_loss: 0.0974, step time: 0.5017\n",
      "170/388, train_loss: 0.4806, step time: 0.4816\n",
      "171/388, train_loss: 0.0860, step time: 0.4797\n",
      "172/388, train_loss: 0.1947, step time: 0.4853\n",
      "173/388, train_loss: 0.0804, step time: 0.4741\n",
      "174/388, train_loss: 0.1125, step time: 0.4756\n",
      "175/388, train_loss: 0.1697, step time: 0.5037\n",
      "176/388, train_loss: 0.3176, step time: 0.5211\n",
      "177/388, train_loss: 0.2223, step time: 0.5010\n",
      "178/388, train_loss: 0.0936, step time: 0.4925\n",
      "179/388, train_loss: 0.1503, step time: 0.4928\n",
      "180/388, train_loss: 0.2573, step time: 0.4929\n",
      "181/388, train_loss: 0.1537, step time: 0.9829\n",
      "182/388, train_loss: 0.1074, step time: 0.5291\n",
      "183/388, train_loss: 0.4268, step time: 0.5019\n",
      "184/388, train_loss: 0.2946, step time: 0.4985\n",
      "185/388, train_loss: 0.2103, step time: 0.4871\n",
      "186/388, train_loss: 0.2297, step time: 1.0993\n",
      "187/388, train_loss: 0.2117, step time: 0.5535\n",
      "188/388, train_loss: 0.3256, step time: 0.5194\n",
      "189/388, train_loss: 0.3221, step time: 0.5135\n",
      "190/388, train_loss: 0.1196, step time: 0.5009\n",
      "191/388, train_loss: 0.1569, step time: 0.4968\n",
      "192/388, train_loss: 0.3387, step time: 0.4899\n",
      "193/388, train_loss: 0.1484, step time: 0.4955\n",
      "194/388, train_loss: 0.2285, step time: 0.5426\n",
      "195/388, train_loss: 0.4082, step time: 0.5181\n",
      "196/388, train_loss: 0.1187, step time: 0.5001\n",
      "197/388, train_loss: 0.0945, step time: 0.4930\n",
      "198/388, train_loss: 0.1972, step time: 0.5023\n",
      "199/388, train_loss: 0.1030, step time: 0.4829\n",
      "200/388, train_loss: 0.1282, step time: 0.4800\n",
      "201/388, train_loss: 0.1159, step time: 1.0746\n",
      "202/388, train_loss: 0.1586, step time: 0.5469\n",
      "203/388, train_loss: 0.1284, step time: 0.5181\n",
      "204/388, train_loss: 0.1944, step time: 0.4866\n",
      "205/388, train_loss: 0.2801, step time: 0.4913\n",
      "206/388, train_loss: 0.0932, step time: 0.4910\n",
      "207/388, train_loss: 0.2474, step time: 0.4774\n",
      "208/388, train_loss: 0.0966, step time: 0.5349\n",
      "209/388, train_loss: 0.2011, step time: 0.5085\n",
      "210/388, train_loss: 0.0606, step time: 0.5103\n",
      "211/388, train_loss: 0.0715, step time: 0.4910\n",
      "212/388, train_loss: 0.0721, step time: 0.5090\n",
      "213/388, train_loss: 0.1439, step time: 0.4921\n",
      "214/388, train_loss: 0.2370, step time: 0.4970\n",
      "215/388, train_loss: 0.2053, step time: 0.5072\n",
      "216/388, train_loss: 0.0756, step time: 0.4955\n",
      "217/388, train_loss: 0.2108, step time: 0.4961\n",
      "218/388, train_loss: 0.0898, step time: 1.1424\n",
      "219/388, train_loss: 0.0516, step time: 0.5272\n",
      "220/388, train_loss: 0.1404, step time: 0.5055\n",
      "221/388, train_loss: 0.0994, step time: 0.4824\n",
      "222/388, train_loss: 0.0692, step time: 0.4791\n",
      "223/388, train_loss: 0.0486, step time: 0.4805\n",
      "224/388, train_loss: 0.3835, step time: 0.9800\n",
      "225/388, train_loss: 0.2810, step time: 0.5452\n",
      "226/388, train_loss: 0.0669, step time: 0.5015\n",
      "227/388, train_loss: 0.1872, step time: 0.4938\n",
      "228/388, train_loss: 0.0969, step time: 0.4792\n",
      "229/388, train_loss: 0.1485, step time: 0.4806\n",
      "230/388, train_loss: 0.3201, step time: 0.4864\n",
      "231/388, train_loss: 0.2280, step time: 0.4944\n",
      "232/388, train_loss: 0.1197, step time: 0.4842\n",
      "233/388, train_loss: 0.0593, step time: 0.4805\n",
      "234/388, train_loss: 0.1547, step time: 0.5469\n",
      "235/388, train_loss: 0.0779, step time: 0.5207\n",
      "236/388, train_loss: 0.2059, step time: 0.4969\n",
      "237/388, train_loss: 0.1959, step time: 0.4952\n",
      "238/388, train_loss: 0.1971, step time: 0.4941\n",
      "239/388, train_loss: 0.0870, step time: 0.5022\n",
      "240/388, train_loss: 0.2835, step time: 0.4865\n",
      "241/388, train_loss: 0.1642, step time: 0.9405\n",
      "242/388, train_loss: 0.0984, step time: 0.5474\n",
      "243/388, train_loss: 0.0929, step time: 0.5187\n",
      "244/388, train_loss: 0.0732, step time: 0.4954\n",
      "245/388, train_loss: 0.1865, step time: 0.4918\n",
      "246/388, train_loss: 0.1982, step time: 0.4947\n",
      "247/388, train_loss: 0.3753, step time: 0.4841\n",
      "248/388, train_loss: 0.1300, step time: 0.4896\n",
      "249/388, train_loss: 0.1962, step time: 0.4786\n",
      "250/388, train_loss: 0.2268, step time: 0.4764\n",
      "251/388, train_loss: 0.1305, step time: 0.6427\n",
      "252/388, train_loss: 0.2481, step time: 0.5250\n",
      "253/388, train_loss: 0.2724, step time: 0.4961\n",
      "254/388, train_loss: 0.3963, step time: 0.4860\n",
      "255/388, train_loss: 0.1162, step time: 1.1765\n",
      "256/388, train_loss: 0.1626, step time: 0.5311\n",
      "257/388, train_loss: 0.2426, step time: 0.5069\n",
      "258/388, train_loss: 0.0924, step time: 0.4874\n",
      "259/388, train_loss: 0.2374, step time: 0.4983\n",
      "260/388, train_loss: 0.1254, step time: 0.4841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/388, train_loss: 0.0853, step time: 0.4949\n",
      "262/388, train_loss: 0.2418, step time: 0.4765\n",
      "263/388, train_loss: 0.0966, step time: 0.9687\n",
      "264/388, train_loss: 0.2210, step time: 0.5259\n",
      "265/388, train_loss: 0.1583, step time: 0.5044\n",
      "266/388, train_loss: 0.1751, step time: 0.4920\n",
      "267/388, train_loss: 0.0847, step time: 0.4982\n",
      "268/388, train_loss: 0.3451, step time: 0.4864\n",
      "269/388, train_loss: 0.2842, step time: 0.4861\n",
      "270/388, train_loss: 0.2143, step time: 0.4857\n",
      "271/388, train_loss: 0.1735, step time: 0.4941\n",
      "272/388, train_loss: 0.1569, step time: 0.4784\n",
      "273/388, train_loss: 0.1437, step time: 0.4873\n",
      "274/388, train_loss: 0.1259, step time: 0.4801\n",
      "275/388, train_loss: 0.2725, step time: 0.4960\n",
      "276/388, train_loss: 0.2059, step time: 0.4929\n",
      "277/388, train_loss: 0.2777, step time: 0.4894\n",
      "278/388, train_loss: 0.3139, step time: 0.4977\n",
      "279/388, train_loss: 0.3583, step time: 0.8380\n",
      "280/388, train_loss: 0.1113, step time: 0.5374\n",
      "281/388, train_loss: 0.0282, step time: 0.5043\n",
      "282/388, train_loss: 0.0892, step time: 0.4956\n",
      "283/388, train_loss: 0.5039, step time: 0.4945\n",
      "284/388, train_loss: 0.1772, step time: 0.4829\n",
      "285/388, train_loss: 0.0576, step time: 0.4857\n",
      "286/388, train_loss: 0.1599, step time: 0.4782\n",
      "287/388, train_loss: 0.1267, step time: 0.4772\n",
      "288/388, train_loss: 0.3720, step time: 0.4835\n",
      "289/388, train_loss: 0.1876, step time: 0.5044\n",
      "290/388, train_loss: 0.2440, step time: 0.4896\n",
      "291/388, train_loss: 0.0948, step time: 0.4903\n",
      "292/388, train_loss: 0.2312, step time: 0.4777\n",
      "293/388, train_loss: 0.0644, step time: 0.6805\n",
      "294/388, train_loss: 0.1345, step time: 0.5553\n",
      "295/388, train_loss: 0.2327, step time: 0.5263\n",
      "296/388, train_loss: 0.1869, step time: 0.5168\n",
      "297/388, train_loss: 0.2085, step time: 0.5497\n",
      "298/388, train_loss: 0.3150, step time: 0.5296\n",
      "299/388, train_loss: 0.1556, step time: 0.5144\n",
      "300/388, train_loss: 0.2150, step time: 0.5117\n",
      "301/388, train_loss: 0.0498, step time: 0.5123\n",
      "302/388, train_loss: 0.0794, step time: 0.4927\n",
      "303/388, train_loss: 0.3124, step time: 0.4857\n",
      "304/388, train_loss: 0.0680, step time: 1.1788\n",
      "305/388, train_loss: 0.2000, step time: 0.5206\n",
      "306/388, train_loss: 0.1698, step time: 0.4995\n",
      "307/388, train_loss: 0.1090, step time: 0.4904\n",
      "308/388, train_loss: 0.1112, step time: 0.4813\n",
      "309/388, train_loss: 0.3964, step time: 0.4892\n",
      "310/388, train_loss: 0.2854, step time: 0.4754\n",
      "311/388, train_loss: 0.2030, step time: 0.4851\n",
      "312/388, train_loss: 0.1058, step time: 0.8163\n",
      "313/388, train_loss: 0.4979, step time: 0.5453\n",
      "314/388, train_loss: 0.1308, step time: 0.5059\n",
      "315/388, train_loss: 0.2828, step time: 0.4854\n",
      "316/388, train_loss: 0.2403, step time: 0.4815\n",
      "317/388, train_loss: 0.0970, step time: 0.4906\n",
      "318/388, train_loss: 0.2086, step time: 0.4834\n",
      "319/388, train_loss: 0.1167, step time: 0.4927\n",
      "320/388, train_loss: 0.0838, step time: 0.5328\n",
      "321/388, train_loss: 0.0681, step time: 0.5299\n",
      "322/388, train_loss: 0.0523, step time: 0.4989\n",
      "323/388, train_loss: 0.1075, step time: 0.4957\n",
      "324/388, train_loss: 0.1276, step time: 0.4832\n",
      "325/388, train_loss: 0.0866, step time: 0.4852\n",
      "326/388, train_loss: 0.1205, step time: 0.4810\n",
      "327/388, train_loss: 0.2263, step time: 0.4875\n",
      "328/388, train_loss: 0.0892, step time: 0.8804\n",
      "329/388, train_loss: 0.3941, step time: 0.5540\n",
      "330/388, train_loss: 0.1939, step time: 0.5053\n",
      "331/388, train_loss: 0.1030, step time: 0.4951\n",
      "332/388, train_loss: 0.0717, step time: 0.4978\n",
      "333/388, train_loss: 0.1878, step time: 0.4833\n",
      "334/388, train_loss: 0.3004, step time: 0.4875\n",
      "335/388, train_loss: 0.3227, step time: 0.4766\n",
      "336/388, train_loss: 0.1372, step time: 1.0222\n",
      "337/388, train_loss: 0.1687, step time: 0.5396\n",
      "338/388, train_loss: 0.0909, step time: 0.5201\n",
      "339/388, train_loss: 0.1254, step time: 0.5182\n",
      "340/388, train_loss: 0.1647, step time: 0.4966\n",
      "341/388, train_loss: 0.1022, step time: 0.4938\n",
      "342/388, train_loss: 0.2897, step time: 0.4798\n",
      "343/388, train_loss: 0.3894, step time: 0.4802\n",
      "344/388, train_loss: 0.2767, step time: 0.5960\n",
      "345/388, train_loss: 0.1288, step time: 0.5351\n",
      "346/388, train_loss: 0.1014, step time: 0.5294\n",
      "347/388, train_loss: 0.1631, step time: 0.5145\n",
      "348/388, train_loss: 0.3716, step time: 0.4941\n",
      "349/388, train_loss: 0.1224, step time: 0.5145\n",
      "350/388, train_loss: 0.0837, step time: 0.5054\n",
      "351/388, train_loss: 0.1766, step time: 0.5052\n",
      "352/388, train_loss: 0.1022, step time: 0.4850\n",
      "353/388, train_loss: 0.0640, step time: 0.4860\n",
      "354/388, train_loss: 0.1031, step time: 0.4908\n",
      "355/388, train_loss: 0.1690, step time: 1.1015\n",
      "356/388, train_loss: 0.0792, step time: 0.5324\n",
      "357/388, train_loss: 0.1697, step time: 0.5008\n",
      "358/388, train_loss: 0.3302, step time: 0.4890\n",
      "359/388, train_loss: 0.0821, step time: 0.4936\n",
      "360/388, train_loss: 0.1038, step time: 0.4790\n",
      "361/388, train_loss: 0.3125, step time: 0.4875\n",
      "362/388, train_loss: 0.0571, step time: 0.5068\n",
      "363/388, train_loss: 0.1493, step time: 0.4930\n",
      "364/388, train_loss: 0.1400, step time: 0.4943\n",
      "365/388, train_loss: 0.5171, step time: 0.4798\n",
      "366/388, train_loss: 0.0916, step time: 0.4787\n",
      "367/388, train_loss: 0.1259, step time: 0.7347\n",
      "368/388, train_loss: 0.2573, step time: 0.5562\n",
      "369/388, train_loss: 0.2265, step time: 0.5262\n",
      "370/388, train_loss: 0.0653, step time: 0.5004\n",
      "371/388, train_loss: 0.1088, step time: 0.5057\n",
      "372/388, train_loss: 0.1094, step time: 0.4903\n",
      "373/388, train_loss: 0.0451, step time: 0.5111\n",
      "374/388, train_loss: 0.2606, step time: 0.4947\n",
      "375/388, train_loss: 0.1361, step time: 0.4918\n",
      "376/388, train_loss: 0.2737, step time: 0.5173\n",
      "377/388, train_loss: 0.1540, step time: 0.5208\n",
      "378/388, train_loss: 0.1307, step time: 0.4922\n",
      "379/388, train_loss: 0.1475, step time: 0.4895\n",
      "380/388, train_loss: 0.2485, step time: 0.4893\n",
      "381/388, train_loss: 0.1267, step time: 0.4771\n",
      "382/388, train_loss: 0.4670, step time: 0.4830\n",
      "383/388, train_loss: 0.1294, step time: 0.5014\n",
      "384/388, train_loss: 0.2865, step time: 0.4936\n",
      "385/388, train_loss: 0.1617, step time: 0.5200\n",
      "386/388, train_loss: 0.0988, step time: 0.4956\n",
      "387/388, train_loss: 0.0851, step time: 0.5029\n",
      "388/388, train_loss: 0.3107, step time: 0.5050\n",
      "epoch 68 average loss: 0.1798\n",
      "current epoch: 68 current mean dice: 0.7696 tc: 0.8184 wt: 0.9013 et: 0.5891\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 68 is: 303.0368\n",
      "----------\n",
      "epoch 69/300\n",
      "1/388, train_loss: 0.2411, step time: 0.4736\n",
      "2/388, train_loss: 0.1631, step time: 0.5324\n",
      "3/388, train_loss: 0.0852, step time: 0.5532\n",
      "4/388, train_loss: 0.2565, step time: 0.6569\n",
      "5/388, train_loss: 0.1215, step time: 0.5809\n",
      "6/388, train_loss: 0.1331, step time: 0.5510\n",
      "7/388, train_loss: 0.1236, step time: 0.5233\n",
      "8/388, train_loss: 0.1543, step time: 0.8123\n",
      "9/388, train_loss: 0.1984, step time: 0.6545\n",
      "10/388, train_loss: 0.2446, step time: 0.5458\n",
      "11/388, train_loss: 0.1591, step time: 0.5035\n",
      "12/388, train_loss: 0.1790, step time: 0.6195\n",
      "13/388, train_loss: 0.0699, step time: 0.6146\n",
      "14/388, train_loss: 0.1603, step time: 0.5534\n",
      "15/388, train_loss: 0.2230, step time: 0.5289\n",
      "16/388, train_loss: 0.1448, step time: 0.5109\n",
      "17/388, train_loss: 0.1084, step time: 0.5419\n",
      "18/388, train_loss: 0.0646, step time: 0.7558\n",
      "19/388, train_loss: 0.1150, step time: 0.5382\n",
      "20/388, train_loss: 0.0901, step time: 0.4957\n",
      "21/388, train_loss: 0.1718, step time: 0.5229\n",
      "22/388, train_loss: 0.1028, step time: 0.7012\n",
      "23/388, train_loss: 0.1102, step time: 0.5828\n",
      "24/388, train_loss: 0.1756, step time: 0.5480\n",
      "25/388, train_loss: 0.0961, step time: 0.6155\n",
      "26/388, train_loss: 0.0569, step time: 0.6045\n",
      "27/388, train_loss: 0.1269, step time: 0.5199\n",
      "28/388, train_loss: 0.0722, step time: 0.4963\n",
      "29/388, train_loss: 0.1062, step time: 0.5840\n",
      "30/388, train_loss: 0.0819, step time: 0.5638\n",
      "31/388, train_loss: 0.2288, step time: 0.5505\n",
      "32/388, train_loss: 0.0872, step time: 0.5120\n",
      "33/388, train_loss: 0.0286, step time: 0.5470\n",
      "34/388, train_loss: 0.1460, step time: 0.5241\n",
      "35/388, train_loss: 0.1196, step time: 0.4966\n",
      "36/388, train_loss: 0.1231, step time: 0.4975\n",
      "37/388, train_loss: 0.1147, step time: 0.5266\n",
      "38/388, train_loss: 0.1888, step time: 0.5083\n",
      "39/388, train_loss: 0.1101, step time: 1.1029\n",
      "40/388, train_loss: 0.1704, step time: 0.5511\n",
      "41/388, train_loss: 0.1496, step time: 0.5349\n",
      "42/388, train_loss: 0.1080, step time: 0.5576\n",
      "43/388, train_loss: 0.0754, step time: 0.5137\n",
      "44/388, train_loss: 0.2073, step time: 0.5344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/388, train_loss: 0.2872, step time: 0.6432\n",
      "46/388, train_loss: 0.1562, step time: 0.5619\n",
      "47/388, train_loss: 0.0871, step time: 0.5234\n",
      "48/388, train_loss: 0.3396, step time: 0.5034\n",
      "49/388, train_loss: 0.0864, step time: 0.5093\n",
      "50/388, train_loss: 0.3385, step time: 0.5003\n",
      "51/388, train_loss: 0.2819, step time: 0.6045\n",
      "52/388, train_loss: 0.0457, step time: 0.5699\n",
      "53/388, train_loss: 0.1577, step time: 0.5316\n",
      "54/388, train_loss: 0.2447, step time: 0.5064\n",
      "55/388, train_loss: 0.2556, step time: 0.5210\n",
      "56/388, train_loss: 0.1121, step time: 0.5707\n",
      "57/388, train_loss: 0.2602, step time: 0.5380\n",
      "58/388, train_loss: 0.1203, step time: 0.5141\n",
      "59/388, train_loss: 0.4297, step time: 0.4925\n",
      "60/388, train_loss: 0.0958, step time: 0.5180\n",
      "61/388, train_loss: 0.1194, step time: 0.4988\n",
      "62/388, train_loss: 0.2372, step time: 0.7155\n",
      "63/388, train_loss: 0.1828, step time: 0.5657\n",
      "64/388, train_loss: 0.1464, step time: 0.5376\n",
      "65/388, train_loss: 0.1191, step time: 0.5720\n",
      "66/388, train_loss: 0.1115, step time: 0.5440\n",
      "67/388, train_loss: 0.1276, step time: 0.5240\n",
      "68/388, train_loss: 0.3852, step time: 0.5097\n",
      "69/388, train_loss: 0.1782, step time: 0.4930\n",
      "70/388, train_loss: 0.1919, step time: 0.7420\n",
      "71/388, train_loss: 0.1211, step time: 0.5580\n",
      "72/388, train_loss: 0.1996, step time: 0.5155\n",
      "73/388, train_loss: 0.2343, step time: 0.5017\n",
      "74/388, train_loss: 0.0999, step time: 0.4828\n",
      "75/388, train_loss: 0.1075, step time: 0.5153\n",
      "76/388, train_loss: 0.1074, step time: 0.5306\n",
      "77/388, train_loss: 0.3088, step time: 0.5140\n",
      "78/388, train_loss: 0.1552, step time: 0.5198\n",
      "79/388, train_loss: 0.1297, step time: 0.6215\n",
      "80/388, train_loss: 0.1257, step time: 0.5632\n",
      "81/388, train_loss: 0.1496, step time: 0.5379\n",
      "82/388, train_loss: 0.1449, step time: 0.5103\n",
      "83/388, train_loss: 0.1094, step time: 0.5159\n",
      "84/388, train_loss: 0.1269, step time: 0.5162\n",
      "85/388, train_loss: 0.0653, step time: 0.4876\n",
      "86/388, train_loss: 0.1578, step time: 0.8201\n",
      "87/388, train_loss: 0.1994, step time: 0.5729\n",
      "88/388, train_loss: 0.1254, step time: 0.5417\n",
      "89/388, train_loss: 0.2357, step time: 0.6024\n",
      "90/388, train_loss: 0.1933, step time: 0.5680\n",
      "91/388, train_loss: 0.1379, step time: 0.5247\n",
      "92/388, train_loss: 0.1061, step time: 0.5133\n",
      "93/388, train_loss: 0.1750, step time: 0.5180\n",
      "94/388, train_loss: 0.2689, step time: 0.5135\n",
      "95/388, train_loss: 0.1268, step time: 0.5144\n",
      "96/388, train_loss: 0.3078, step time: 0.4881\n",
      "97/388, train_loss: 0.2404, step time: 1.1359\n",
      "98/388, train_loss: 0.1231, step time: 0.5458\n",
      "99/388, train_loss: 0.3477, step time: 0.5240\n",
      "100/388, train_loss: 0.2010, step time: 0.6097\n",
      "101/388, train_loss: 0.0682, step time: 0.5333\n",
      "102/388, train_loss: 0.1588, step time: 0.5181\n",
      "103/388, train_loss: 0.1903, step time: 0.4977\n",
      "104/388, train_loss: 0.0889, step time: 0.4960\n",
      "105/388, train_loss: 0.1802, step time: 0.5171\n",
      "106/388, train_loss: 0.1050, step time: 0.5009\n",
      "107/388, train_loss: 0.1564, step time: 0.4894\n",
      "108/388, train_loss: 0.1210, step time: 0.4943\n",
      "109/388, train_loss: 0.1791, step time: 0.4823\n",
      "110/388, train_loss: 0.0638, step time: 0.5033\n",
      "111/388, train_loss: 0.1066, step time: 0.4966\n",
      "112/388, train_loss: 0.3192, step time: 0.8541\n",
      "113/388, train_loss: 0.3970, step time: 0.5764\n",
      "114/388, train_loss: 0.1945, step time: 0.5417\n",
      "115/388, train_loss: 0.2402, step time: 0.5900\n",
      "116/388, train_loss: 0.1503, step time: 0.5460\n",
      "117/388, train_loss: 0.1272, step time: 0.5087\n",
      "118/388, train_loss: 0.0861, step time: 0.4834\n",
      "119/388, train_loss: 0.0755, step time: 0.4897\n",
      "120/388, train_loss: 0.2833, step time: 1.1726\n",
      "121/388, train_loss: 0.1689, step time: 0.5394\n",
      "122/388, train_loss: 0.1810, step time: 0.5122\n",
      "123/388, train_loss: 0.1444, step time: 0.5080\n",
      "124/388, train_loss: 0.1800, step time: 0.4915\n",
      "125/388, train_loss: 0.0981, step time: 0.5827\n",
      "126/388, train_loss: 0.1958, step time: 0.5385\n",
      "127/388, train_loss: 0.1439, step time: 0.5031\n",
      "128/388, train_loss: 0.2321, step time: 0.5019\n",
      "129/388, train_loss: 0.2004, step time: 0.4899\n",
      "130/388, train_loss: 0.1344, step time: 1.1136\n",
      "131/388, train_loss: 0.1318, step time: 0.5447\n",
      "132/388, train_loss: 0.4228, step time: 0.5180\n",
      "133/388, train_loss: 0.0660, step time: 0.5007\n",
      "134/388, train_loss: 0.1026, step time: 0.5359\n",
      "135/388, train_loss: 0.1134, step time: 0.5036\n",
      "136/388, train_loss: 0.1993, step time: 0.5009\n",
      "137/388, train_loss: 0.0883, step time: 0.4822\n",
      "138/388, train_loss: 0.1596, step time: 0.5391\n",
      "139/388, train_loss: 0.1297, step time: 0.5347\n",
      "140/388, train_loss: 0.0943, step time: 0.5094\n",
      "141/388, train_loss: 0.1785, step time: 0.5137\n",
      "142/388, train_loss: 0.0815, step time: 0.5908\n",
      "143/388, train_loss: 0.1354, step time: 0.5257\n",
      "144/388, train_loss: 0.2176, step time: 0.5005\n",
      "145/388, train_loss: 0.1094, step time: 0.4923\n",
      "146/388, train_loss: 0.0936, step time: 1.0276\n",
      "147/388, train_loss: 0.0761, step time: 0.5391\n",
      "148/388, train_loss: 0.1123, step time: 0.5053\n",
      "149/388, train_loss: 0.2321, step time: 0.4933\n",
      "150/388, train_loss: 0.3496, step time: 0.5118\n",
      "151/388, train_loss: 0.0368, step time: 0.5011\n",
      "152/388, train_loss: 0.1514, step time: 0.4982\n",
      "153/388, train_loss: 0.1404, step time: 0.4875\n",
      "154/388, train_loss: 0.3736, step time: 0.5198\n",
      "155/388, train_loss: 0.1529, step time: 0.5202\n",
      "156/388, train_loss: 0.1854, step time: 0.5829\n",
      "157/388, train_loss: 0.2505, step time: 0.5296\n",
      "158/388, train_loss: 0.1821, step time: 0.5019\n",
      "159/388, train_loss: 0.1315, step time: 1.1392\n",
      "160/388, train_loss: 0.3275, step time: 0.5355\n",
      "161/388, train_loss: 0.6192, step time: 0.5177\n",
      "162/388, train_loss: 0.1058, step time: 0.5808\n",
      "163/388, train_loss: 0.5337, step time: 0.5357\n",
      "164/388, train_loss: 0.2603, step time: 0.5064\n",
      "165/388, train_loss: 0.2073, step time: 0.4920\n",
      "166/388, train_loss: 0.0707, step time: 0.4936\n",
      "167/388, train_loss: 0.1664, step time: 0.7036\n",
      "168/388, train_loss: 0.0978, step time: 0.5455\n",
      "169/388, train_loss: 0.2214, step time: 0.5206\n",
      "170/388, train_loss: 0.1496, step time: 0.6053\n",
      "171/388, train_loss: 0.0731, step time: 0.5458\n",
      "172/388, train_loss: 0.1056, step time: 0.5322\n",
      "173/388, train_loss: 0.4311, step time: 0.5065\n",
      "174/388, train_loss: 0.1678, step time: 0.4952\n",
      "175/388, train_loss: 0.2115, step time: 0.4916\n",
      "176/388, train_loss: 0.1047, step time: 1.1347\n",
      "177/388, train_loss: 0.2775, step time: 0.5388\n",
      "178/388, train_loss: 0.1672, step time: 0.5051\n",
      "179/388, train_loss: 0.2658, step time: 0.4969\n",
      "180/388, train_loss: 0.2220, step time: 0.4967\n",
      "181/388, train_loss: 0.1800, step time: 0.4849\n",
      "182/388, train_loss: 0.0767, step time: 0.4901\n",
      "183/388, train_loss: 0.1291, step time: 0.9933\n",
      "184/388, train_loss: 0.2554, step time: 0.5483\n",
      "185/388, train_loss: 0.2470, step time: 0.5109\n",
      "186/388, train_loss: 0.1054, step time: 0.5061\n",
      "187/388, train_loss: 0.2175, step time: 0.4895\n",
      "188/388, train_loss: 0.1306, step time: 1.1447\n",
      "189/388, train_loss: 0.0586, step time: 0.5417\n",
      "190/388, train_loss: 0.1335, step time: 0.5190\n",
      "191/388, train_loss: 0.1163, step time: 0.4917\n",
      "192/388, train_loss: 0.0965, step time: 0.4869\n",
      "193/388, train_loss: 0.0334, step time: 0.4954\n",
      "194/388, train_loss: 0.2594, step time: 0.4783\n",
      "195/388, train_loss: 0.2665, step time: 0.4828\n",
      "196/388, train_loss: 0.3372, step time: 0.4924\n",
      "197/388, train_loss: 0.1022, step time: 1.0783\n",
      "198/388, train_loss: 0.2299, step time: 0.5331\n",
      "199/388, train_loss: 0.4058, step time: 0.5079\n",
      "200/388, train_loss: 0.3018, step time: 0.5012\n",
      "201/388, train_loss: 0.1547, step time: 0.5039\n",
      "202/388, train_loss: 0.1015, step time: 0.4878\n",
      "203/388, train_loss: 0.2189, step time: 0.5107\n",
      "204/388, train_loss: 0.0586, step time: 0.5149\n",
      "205/388, train_loss: 0.1916, step time: 0.4864\n",
      "206/388, train_loss: 0.1816, step time: 0.4853\n",
      "207/388, train_loss: 0.0803, step time: 0.4928\n",
      "208/388, train_loss: 0.2538, step time: 0.4763\n",
      "209/388, train_loss: 0.0834, step time: 1.0820\n",
      "210/388, train_loss: 0.0547, step time: 0.5260\n",
      "211/388, train_loss: 0.5259, step time: 0.5062\n",
      "212/388, train_loss: 0.0755, step time: 0.4942\n",
      "213/388, train_loss: 0.1065, step time: 0.4872\n",
      "214/388, train_loss: 0.2196, step time: 0.4807\n",
      "215/388, train_loss: 0.2148, step time: 1.0887\n",
      "216/388, train_loss: 0.1080, step time: 0.5335\n",
      "217/388, train_loss: 0.1970, step time: 0.5002\n",
      "218/388, train_loss: 0.0546, step time: 0.4922\n",
      "219/388, train_loss: 0.1590, step time: 0.4917\n",
      "220/388, train_loss: 0.1576, step time: 0.4807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/388, train_loss: 0.1904, step time: 0.4781\n",
      "222/388, train_loss: 0.0599, step time: 0.4728\n",
      "223/388, train_loss: 0.1417, step time: 0.4807\n",
      "224/388, train_loss: 0.0367, step time: 0.4907\n",
      "225/388, train_loss: 0.2570, step time: 0.4876\n",
      "226/388, train_loss: 0.0817, step time: 0.4937\n",
      "227/388, train_loss: 0.2022, step time: 0.5041\n",
      "228/388, train_loss: 0.2292, step time: 0.4906\n",
      "229/388, train_loss: 0.1690, step time: 0.5147\n",
      "230/388, train_loss: 0.3070, step time: 0.5031\n",
      "231/388, train_loss: 0.0898, step time: 0.5251\n",
      "232/388, train_loss: 0.1297, step time: 0.5311\n",
      "233/388, train_loss: 0.1292, step time: 0.5108\n",
      "234/388, train_loss: 0.4105, step time: 0.5175\n",
      "235/388, train_loss: 0.0807, step time: 0.5795\n",
      "236/388, train_loss: 0.0927, step time: 0.5350\n",
      "237/388, train_loss: 0.2657, step time: 0.5173\n",
      "238/388, train_loss: 0.1442, step time: 0.5033\n",
      "239/388, train_loss: 0.1453, step time: 0.5065\n",
      "240/388, train_loss: 0.1648, step time: 0.4842\n",
      "241/388, train_loss: 0.0925, step time: 0.4893\n",
      "242/388, train_loss: 0.1956, step time: 0.4876\n",
      "243/388, train_loss: 0.3262, step time: 1.1180\n",
      "244/388, train_loss: 0.0554, step time: 0.5412\n",
      "245/388, train_loss: 0.1544, step time: 0.5082\n",
      "246/388, train_loss: 0.3509, step time: 0.4954\n",
      "247/388, train_loss: 0.1118, step time: 0.4947\n",
      "248/388, train_loss: 0.1334, step time: 0.4850\n",
      "249/388, train_loss: 0.2869, step time: 0.5069\n",
      "250/388, train_loss: 0.1803, step time: 0.9757\n",
      "251/388, train_loss: 0.2008, step time: 0.5461\n",
      "252/388, train_loss: 0.1977, step time: 0.5073\n",
      "253/388, train_loss: 0.1541, step time: 0.5005\n",
      "254/388, train_loss: 0.1745, step time: 0.4859\n",
      "255/388, train_loss: 0.1356, step time: 0.4902\n",
      "256/388, train_loss: 0.2021, step time: 0.4887\n",
      "257/388, train_loss: 0.2084, step time: 0.4776\n",
      "258/388, train_loss: 0.0676, step time: 0.5693\n",
      "259/388, train_loss: 0.1678, step time: 0.5558\n",
      "260/388, train_loss: 0.1061, step time: 0.5235\n",
      "261/388, train_loss: 0.0855, step time: 0.5097\n",
      "262/388, train_loss: 0.0824, step time: 0.5020\n",
      "263/388, train_loss: 0.1376, step time: 0.4970\n",
      "264/388, train_loss: 0.2670, step time: 0.4800\n",
      "265/388, train_loss: 0.3430, step time: 0.4919\n",
      "266/388, train_loss: 0.0832, step time: 0.4907\n",
      "267/388, train_loss: 0.1633, step time: 0.4891\n",
      "268/388, train_loss: 0.1555, step time: 0.4933\n",
      "269/388, train_loss: 0.1511, step time: 0.5052\n",
      "270/388, train_loss: 0.3058, step time: 0.5025\n",
      "271/388, train_loss: 0.0822, step time: 0.4842\n",
      "272/388, train_loss: 0.2382, step time: 0.4845\n",
      "273/388, train_loss: 0.5532, step time: 0.5525\n",
      "274/388, train_loss: 0.2228, step time: 0.5279\n",
      "275/388, train_loss: 0.1447, step time: 0.4873\n",
      "276/388, train_loss: 0.1146, step time: 1.0661\n",
      "277/388, train_loss: 0.0732, step time: 0.5279\n",
      "278/388, train_loss: 0.2277, step time: 0.5007\n",
      "279/388, train_loss: 0.1534, step time: 0.4907\n",
      "280/388, train_loss: 0.0893, step time: 0.4780\n",
      "281/388, train_loss: 0.1089, step time: 0.9699\n",
      "282/388, train_loss: 0.2887, step time: 0.5352\n",
      "283/388, train_loss: 0.0692, step time: 0.5104\n",
      "284/388, train_loss: 0.1002, step time: 0.5001\n",
      "285/388, train_loss: 0.2001, step time: 0.4898\n",
      "286/388, train_loss: 0.2666, step time: 0.4877\n",
      "287/388, train_loss: 0.1171, step time: 0.4799\n",
      "288/388, train_loss: 0.1698, step time: 0.4827\n",
      "289/388, train_loss: 0.1327, step time: 1.1629\n",
      "290/388, train_loss: 0.0535, step time: 0.5234\n",
      "291/388, train_loss: 0.2477, step time: 0.4948\n",
      "292/388, train_loss: 0.3030, step time: 0.4904\n",
      "293/388, train_loss: 0.4125, step time: 0.4920\n",
      "294/388, train_loss: 0.2101, step time: 0.4787\n",
      "295/388, train_loss: 0.1054, step time: 0.4800\n",
      "296/388, train_loss: 0.1906, step time: 0.5025\n",
      "297/388, train_loss: 0.0720, step time: 0.4894\n",
      "298/388, train_loss: 0.1367, step time: 0.5075\n",
      "299/388, train_loss: 0.2916, step time: 0.4881\n",
      "300/388, train_loss: 0.1953, step time: 0.5411\n",
      "301/388, train_loss: 0.1084, step time: 0.5164\n",
      "302/388, train_loss: 0.0477, step time: 0.5004\n",
      "303/388, train_loss: 0.2963, step time: 0.4916\n",
      "304/388, train_loss: 0.0887, step time: 0.4939\n",
      "305/388, train_loss: 0.3199, step time: 0.5516\n",
      "306/388, train_loss: 0.1553, step time: 0.5191\n",
      "307/388, train_loss: 0.1406, step time: 0.4900\n",
      "308/388, train_loss: 0.0645, step time: 0.4952\n",
      "309/388, train_loss: 0.2397, step time: 0.4796\n",
      "310/388, train_loss: 0.2086, step time: 0.4998\n",
      "311/388, train_loss: 0.3770, step time: 0.5003\n",
      "312/388, train_loss: 0.2692, step time: 0.5169\n",
      "313/388, train_loss: 0.1137, step time: 0.5069\n",
      "314/388, train_loss: 0.2404, step time: 0.5600\n",
      "315/388, train_loss: 0.3497, step time: 0.5690\n",
      "316/388, train_loss: 0.0819, step time: 0.5254\n",
      "317/388, train_loss: 0.0639, step time: 0.5093\n",
      "318/388, train_loss: 0.3670, step time: 0.4925\n",
      "319/388, train_loss: 0.2181, step time: 0.4955\n",
      "320/388, train_loss: 0.0946, step time: 0.4838\n",
      "321/388, train_loss: 0.0898, step time: 0.4989\n",
      "322/388, train_loss: 0.1477, step time: 0.5530\n",
      "323/388, train_loss: 0.0680, step time: 0.6441\n",
      "324/388, train_loss: 0.1035, step time: 0.5400\n",
      "325/388, train_loss: 0.3026, step time: 0.5214\n",
      "326/388, train_loss: 0.2040, step time: 0.4971\n",
      "327/388, train_loss: 0.0786, step time: 0.5010\n",
      "328/388, train_loss: 0.0575, step time: 0.4881\n",
      "329/388, train_loss: 0.3719, step time: 0.4798\n",
      "330/388, train_loss: 0.2964, step time: 0.5071\n",
      "331/388, train_loss: 0.1923, step time: 0.4955\n",
      "332/388, train_loss: 0.2262, step time: 0.5207\n",
      "333/388, train_loss: 0.4086, step time: 0.5047\n",
      "334/388, train_loss: 0.0671, step time: 0.5167\n",
      "335/388, train_loss: 0.0828, step time: 0.4993\n",
      "336/388, train_loss: 0.1122, step time: 0.4920\n",
      "337/388, train_loss: 0.2658, step time: 1.0870\n",
      "338/388, train_loss: 0.1439, step time: 0.5560\n",
      "339/388, train_loss: 0.1848, step time: 0.5200\n",
      "340/388, train_loss: 0.2634, step time: 0.4968\n",
      "341/388, train_loss: 0.0985, step time: 0.5002\n",
      "342/388, train_loss: 0.0942, step time: 0.4829\n",
      "343/388, train_loss: 0.0986, step time: 0.4846\n",
      "344/388, train_loss: 0.0888, step time: 1.1306\n",
      "345/388, train_loss: 0.2247, step time: 0.5344\n",
      "346/388, train_loss: 0.0859, step time: 0.5047\n",
      "347/388, train_loss: 0.1780, step time: 0.4873\n",
      "348/388, train_loss: 0.2286, step time: 0.4955\n",
      "349/388, train_loss: 0.1047, step time: 0.4854\n",
      "350/388, train_loss: 0.1461, step time: 0.4765\n",
      "351/388, train_loss: 0.2696, step time: 0.4809\n",
      "352/388, train_loss: 0.4165, step time: 0.6142\n",
      "353/388, train_loss: 0.0957, step time: 0.5414\n",
      "354/388, train_loss: 0.1830, step time: 0.5112\n",
      "355/388, train_loss: 0.2285, step time: 0.5168\n",
      "356/388, train_loss: 0.4638, step time: 0.5137\n",
      "357/388, train_loss: 0.0622, step time: 0.4972\n",
      "358/388, train_loss: 0.4833, step time: 0.4908\n",
      "359/388, train_loss: 0.1582, step time: 0.4834\n",
      "360/388, train_loss: 0.2017, step time: 0.4911\n",
      "361/388, train_loss: 0.1912, step time: 0.4947\n",
      "362/388, train_loss: 0.0684, step time: 0.5480\n",
      "363/388, train_loss: 0.1106, step time: 0.5264\n",
      "364/388, train_loss: 0.1739, step time: 0.4989\n",
      "365/388, train_loss: 0.1729, step time: 0.5001\n",
      "366/388, train_loss: 0.2749, step time: 0.5199\n",
      "367/388, train_loss: 0.1156, step time: 0.5047\n",
      "368/388, train_loss: 0.2791, step time: 0.4832\n",
      "369/388, train_loss: 0.1049, step time: 0.9995\n",
      "370/388, train_loss: 0.1050, step time: 0.5279\n",
      "371/388, train_loss: 0.2791, step time: 0.5124\n",
      "372/388, train_loss: 0.2862, step time: 0.4869\n",
      "373/388, train_loss: 0.1661, step time: 0.5274\n",
      "374/388, train_loss: 0.1653, step time: 0.5123\n",
      "375/388, train_loss: 0.1103, step time: 0.4997\n",
      "376/388, train_loss: 0.4593, step time: 0.4897\n",
      "377/388, train_loss: 0.1486, step time: 0.4904\n",
      "378/388, train_loss: 0.1097, step time: 0.4851\n",
      "379/388, train_loss: 0.1323, step time: 0.4951\n",
      "380/388, train_loss: 0.2052, step time: 0.4871\n",
      "381/388, train_loss: 0.1126, step time: 0.4792\n",
      "382/388, train_loss: 0.1239, step time: 0.4927\n",
      "383/388, train_loss: 0.1722, step time: 0.5196\n",
      "384/388, train_loss: 0.2577, step time: 0.5114\n",
      "385/388, train_loss: 0.1616, step time: 0.4869\n",
      "386/388, train_loss: 0.4723, step time: 0.5088\n",
      "387/388, train_loss: 0.0463, step time: 0.4939\n",
      "388/388, train_loss: 0.2032, step time: 0.4776\n",
      "epoch 69 average loss: 0.1762\n",
      "current epoch: 69 current mean dice: 0.7626 tc: 0.8110 wt: 0.8988 et: 0.5780\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 69 is: 301.2760\n",
      "----------\n",
      "epoch 70/300\n",
      "1/388, train_loss: 0.1669, step time: 0.4772\n",
      "2/388, train_loss: 0.0977, step time: 0.4826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/388, train_loss: 0.0948, step time: 1.0351\n",
      "4/388, train_loss: 0.1461, step time: 0.5502\n",
      "5/388, train_loss: 0.2006, step time: 0.5018\n",
      "6/388, train_loss: 0.1493, step time: 0.5082\n",
      "7/388, train_loss: 0.1374, step time: 1.1558\n",
      "8/388, train_loss: 0.1452, step time: 0.5696\n",
      "9/388, train_loss: 0.1074, step time: 0.5144\n",
      "10/388, train_loss: 0.1048, step time: 0.4980\n",
      "11/388, train_loss: 0.2610, step time: 0.5281\n",
      "12/388, train_loss: 0.4233, step time: 0.5062\n",
      "13/388, train_loss: 0.1171, step time: 0.4909\n",
      "14/388, train_loss: 0.2214, step time: 0.9461\n",
      "15/388, train_loss: 0.0776, step time: 0.5728\n",
      "16/388, train_loss: 0.1889, step time: 0.5389\n",
      "17/388, train_loss: 0.1471, step time: 0.5077\n",
      "18/388, train_loss: 0.1021, step time: 0.4985\n",
      "19/388, train_loss: 0.2085, step time: 0.4997\n",
      "20/388, train_loss: 0.1094, step time: 0.5713\n",
      "21/388, train_loss: 0.0721, step time: 0.5043\n",
      "22/388, train_loss: 0.2933, step time: 0.5778\n",
      "23/388, train_loss: 0.2277, step time: 0.5925\n",
      "24/388, train_loss: 0.1763, step time: 0.5332\n",
      "25/388, train_loss: 0.1006, step time: 0.5019\n",
      "26/388, train_loss: 0.0805, step time: 1.0220\n",
      "27/388, train_loss: 0.1205, step time: 0.5400\n",
      "28/388, train_loss: 0.2528, step time: 0.5094\n",
      "29/388, train_loss: 0.1871, step time: 0.5052\n",
      "30/388, train_loss: 0.2525, step time: 0.5190\n",
      "31/388, train_loss: 0.3683, step time: 0.4919\n",
      "32/388, train_loss: 0.0943, step time: 0.5822\n",
      "33/388, train_loss: 0.0968, step time: 0.5658\n",
      "34/388, train_loss: 0.2087, step time: 0.5280\n",
      "35/388, train_loss: 0.2277, step time: 0.5042\n",
      "36/388, train_loss: 0.0863, step time: 0.4985\n",
      "37/388, train_loss: 0.2178, step time: 0.4828\n",
      "38/388, train_loss: 0.2231, step time: 0.8701\n",
      "39/388, train_loss: 0.2870, step time: 0.5678\n",
      "40/388, train_loss: 0.1688, step time: 0.5310\n",
      "41/388, train_loss: 0.1496, step time: 0.5138\n",
      "42/388, train_loss: 0.2720, step time: 0.5034\n",
      "43/388, train_loss: 0.0866, step time: 0.4958\n",
      "44/388, train_loss: 0.3234, step time: 0.4977\n",
      "45/388, train_loss: 0.1199, step time: 1.2065\n",
      "46/388, train_loss: 0.2195, step time: 0.5393\n",
      "47/388, train_loss: 0.3130, step time: 0.5020\n",
      "48/388, train_loss: 0.1478, step time: 0.4905\n",
      "49/388, train_loss: 0.0975, step time: 0.4960\n",
      "50/388, train_loss: 0.2866, step time: 0.4841\n",
      "51/388, train_loss: 0.2683, step time: 0.4902\n",
      "52/388, train_loss: 0.1753, step time: 0.4760\n",
      "53/388, train_loss: 0.2524, step time: 1.0373\n",
      "54/388, train_loss: 0.1255, step time: 0.5400\n",
      "55/388, train_loss: 0.1463, step time: 0.5112\n",
      "56/388, train_loss: 0.2388, step time: 0.4947\n",
      "57/388, train_loss: 0.0702, step time: 0.4952\n",
      "58/388, train_loss: 0.2555, step time: 0.4990\n",
      "59/388, train_loss: 0.1884, step time: 0.5683\n",
      "60/388, train_loss: 0.1499, step time: 0.5401\n",
      "61/388, train_loss: 0.1162, step time: 0.5297\n",
      "62/388, train_loss: 0.2726, step time: 0.5170\n",
      "63/388, train_loss: 0.1869, step time: 0.5059\n",
      "64/388, train_loss: 0.2819, step time: 0.4842\n",
      "65/388, train_loss: 0.1185, step time: 1.0575\n",
      "66/388, train_loss: 0.1649, step time: 0.5411\n",
      "67/388, train_loss: 0.2789, step time: 0.5220\n",
      "68/388, train_loss: 0.2827, step time: 0.4982\n",
      "69/388, train_loss: 0.2633, step time: 0.4949\n",
      "70/388, train_loss: 0.1526, step time: 0.4804\n",
      "71/388, train_loss: 0.1400, step time: 1.1310\n",
      "72/388, train_loss: 0.2506, step time: 0.5239\n",
      "73/388, train_loss: 0.1110, step time: 0.5012\n",
      "74/388, train_loss: 0.1114, step time: 0.4932\n",
      "75/388, train_loss: 0.1262, step time: 0.4844\n",
      "76/388, train_loss: 0.4097, step time: 0.4921\n",
      "77/388, train_loss: 0.1085, step time: 0.9161\n",
      "78/388, train_loss: 0.0906, step time: 0.5400\n",
      "79/388, train_loss: 0.0727, step time: 0.5081\n",
      "80/388, train_loss: 0.2033, step time: 0.4910\n",
      "81/388, train_loss: 0.1777, step time: 0.4959\n",
      "82/388, train_loss: 0.1130, step time: 0.5396\n",
      "83/388, train_loss: 0.0577, step time: 0.5119\n",
      "84/388, train_loss: 0.1731, step time: 0.4978\n",
      "85/388, train_loss: 0.0752, step time: 0.4925\n",
      "86/388, train_loss: 0.1176, step time: 0.4814\n",
      "87/388, train_loss: 0.1077, step time: 0.4770\n",
      "88/388, train_loss: 0.3031, step time: 0.4760\n",
      "89/388, train_loss: 0.1206, step time: 1.0114\n",
      "90/388, train_loss: 0.0766, step time: 0.5350\n",
      "91/388, train_loss: 0.2798, step time: 0.5144\n",
      "92/388, train_loss: 0.1603, step time: 0.5027\n",
      "93/388, train_loss: 0.1077, step time: 0.4949\n",
      "94/388, train_loss: 0.2211, step time: 0.5216\n",
      "95/388, train_loss: 0.1022, step time: 0.5121\n",
      "96/388, train_loss: 0.0811, step time: 0.5146\n",
      "97/388, train_loss: 0.0453, step time: 0.4967\n",
      "98/388, train_loss: 0.1072, step time: 0.4989\n",
      "99/388, train_loss: 0.2068, step time: 0.5148\n",
      "100/388, train_loss: 0.1186, step time: 0.4966\n",
      "101/388, train_loss: 0.4287, step time: 0.4875\n",
      "102/388, train_loss: 0.2828, step time: 0.4921\n",
      "103/388, train_loss: 0.0528, step time: 0.9714\n",
      "104/388, train_loss: 0.0566, step time: 0.5399\n",
      "105/388, train_loss: 0.2180, step time: 0.5113\n",
      "106/388, train_loss: 0.0640, step time: 0.5034\n",
      "107/388, train_loss: 0.1349, step time: 0.4945\n",
      "108/388, train_loss: 0.0587, step time: 0.4953\n",
      "109/388, train_loss: 0.1840, step time: 0.5198\n",
      "110/388, train_loss: 0.2430, step time: 0.5140\n",
      "111/388, train_loss: 0.1545, step time: 0.5274\n",
      "112/388, train_loss: 0.0793, step time: 0.5493\n",
      "113/388, train_loss: 0.0340, step time: 0.5259\n",
      "114/388, train_loss: 0.1030, step time: 0.5314\n",
      "115/388, train_loss: 0.1825, step time: 0.5268\n",
      "116/388, train_loss: 0.2190, step time: 0.5034\n",
      "117/388, train_loss: 0.3176, step time: 0.5506\n",
      "118/388, train_loss: 0.1402, step time: 0.5286\n",
      "119/388, train_loss: 0.1418, step time: 0.5148\n",
      "120/388, train_loss: 0.1961, step time: 0.4969\n",
      "121/388, train_loss: 0.2613, step time: 0.4919\n",
      "122/388, train_loss: 0.0640, step time: 0.5409\n",
      "123/388, train_loss: 0.1007, step time: 0.5423\n",
      "124/388, train_loss: 0.1292, step time: 0.5106\n",
      "125/388, train_loss: 0.1537, step time: 0.4991\n",
      "126/388, train_loss: 0.2173, step time: 0.4961\n",
      "127/388, train_loss: 0.2306, step time: 1.1960\n",
      "128/388, train_loss: 0.2144, step time: 0.5329\n",
      "129/388, train_loss: 0.2335, step time: 0.5093\n",
      "130/388, train_loss: 0.1879, step time: 0.4997\n",
      "131/388, train_loss: 0.4231, step time: 0.5324\n",
      "132/388, train_loss: 0.1697, step time: 0.5133\n",
      "133/388, train_loss: 0.1865, step time: 0.5090\n",
      "134/388, train_loss: 0.2068, step time: 0.4979\n",
      "135/388, train_loss: 0.1047, step time: 0.4986\n",
      "136/388, train_loss: 0.1155, step time: 0.4858\n",
      "137/388, train_loss: 0.4813, step time: 0.5006\n",
      "138/388, train_loss: 0.1502, step time: 0.5366\n",
      "139/388, train_loss: 0.1448, step time: 0.5138\n",
      "140/388, train_loss: 0.0982, step time: 0.5018\n",
      "141/388, train_loss: 0.0587, step time: 0.4981\n",
      "142/388, train_loss: 0.0477, step time: 1.0723\n",
      "143/388, train_loss: 0.2519, step time: 0.5349\n",
      "144/388, train_loss: 0.2712, step time: 0.4997\n",
      "145/388, train_loss: 0.1637, step time: 0.5099\n",
      "146/388, train_loss: 0.0863, step time: 0.5139\n",
      "147/388, train_loss: 0.0857, step time: 0.4991\n",
      "148/388, train_loss: 0.1525, step time: 0.4912\n",
      "149/388, train_loss: 0.2368, step time: 0.4806\n",
      "150/388, train_loss: 0.2909, step time: 0.5084\n",
      "151/388, train_loss: 0.1489, step time: 0.6736\n",
      "152/388, train_loss: 0.1157, step time: 0.5626\n",
      "153/388, train_loss: 0.0835, step time: 0.5160\n",
      "154/388, train_loss: 0.2720, step time: 0.4961\n",
      "155/388, train_loss: 0.1908, step time: 0.4922\n",
      "156/388, train_loss: 0.1472, step time: 0.9912\n",
      "157/388, train_loss: 0.1212, step time: 0.5412\n",
      "158/388, train_loss: 0.1936, step time: 0.5190\n",
      "159/388, train_loss: 0.1306, step time: 0.4939\n",
      "160/388, train_loss: 0.1713, step time: 0.4976\n",
      "161/388, train_loss: 0.1147, step time: 1.2370\n",
      "162/388, train_loss: 0.1544, step time: 0.5373\n",
      "163/388, train_loss: 0.0950, step time: 0.5077\n",
      "164/388, train_loss: 0.3048, step time: 0.4836\n",
      "165/388, train_loss: 0.2153, step time: 0.4863\n",
      "166/388, train_loss: 0.0835, step time: 0.5067\n",
      "167/388, train_loss: 0.4872, step time: 0.5055\n",
      "168/388, train_loss: 0.1560, step time: 0.5902\n",
      "169/388, train_loss: 0.1162, step time: 0.5353\n",
      "170/388, train_loss: 0.0985, step time: 0.5059\n",
      "171/388, train_loss: 0.0701, step time: 0.4903\n",
      "172/388, train_loss: 0.0914, step time: 0.4817\n",
      "173/388, train_loss: 0.1293, step time: 0.5150\n",
      "174/388, train_loss: 0.0982, step time: 0.5027\n",
      "175/388, train_loss: 0.2707, step time: 0.5475\n",
      "176/388, train_loss: 0.0298, step time: 0.6991\n",
      "177/388, train_loss: 0.0846, step time: 0.5319\n",
      "178/388, train_loss: 0.0998, step time: 0.5155\n",
      "179/388, train_loss: 0.0997, step time: 0.4980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/388, train_loss: 0.2828, step time: 0.4987\n",
      "181/388, train_loss: 0.1596, step time: 0.4960\n",
      "182/388, train_loss: 0.2882, step time: 0.5010\n",
      "183/388, train_loss: 0.1735, step time: 0.4844\n",
      "184/388, train_loss: 0.1620, step time: 0.5024\n",
      "185/388, train_loss: 0.0688, step time: 0.5009\n",
      "186/388, train_loss: 0.1484, step time: 0.4947\n",
      "187/388, train_loss: 0.4097, step time: 0.4930\n",
      "188/388, train_loss: 0.2379, step time: 0.4883\n",
      "189/388, train_loss: 0.0684, step time: 0.4973\n",
      "190/388, train_loss: 0.0636, step time: 0.5189\n",
      "191/388, train_loss: 0.0675, step time: 0.5120\n",
      "192/388, train_loss: 0.2875, step time: 0.5183\n",
      "193/388, train_loss: 0.1156, step time: 0.5492\n",
      "194/388, train_loss: 0.1100, step time: 0.5705\n",
      "195/388, train_loss: 0.2003, step time: 0.5122\n",
      "196/388, train_loss: 0.1245, step time: 0.4984\n",
      "197/388, train_loss: 0.1090, step time: 0.4863\n",
      "198/388, train_loss: 0.0554, step time: 0.5213\n",
      "199/388, train_loss: 0.1244, step time: 0.5161\n",
      "200/388, train_loss: 0.1921, step time: 0.5045\n",
      "201/388, train_loss: 0.2411, step time: 0.4850\n",
      "202/388, train_loss: 0.2396, step time: 0.4807\n",
      "203/388, train_loss: 0.2565, step time: 0.9463\n",
      "204/388, train_loss: 0.1395, step time: 0.5484\n",
      "205/388, train_loss: 0.1367, step time: 0.5335\n",
      "206/388, train_loss: 0.2312, step time: 0.5191\n",
      "207/388, train_loss: 0.3881, step time: 0.5101\n",
      "208/388, train_loss: 0.2009, step time: 0.5606\n",
      "209/388, train_loss: 0.0503, step time: 0.5337\n",
      "210/388, train_loss: 0.1702, step time: 0.5180\n",
      "211/388, train_loss: 0.1113, step time: 0.4923\n",
      "212/388, train_loss: 0.3426, step time: 0.5616\n",
      "213/388, train_loss: 0.2497, step time: 0.7145\n",
      "214/388, train_loss: 0.1536, step time: 0.5583\n",
      "215/388, train_loss: 0.1094, step time: 0.5206\n",
      "216/388, train_loss: 0.1290, step time: 0.5005\n",
      "217/388, train_loss: 0.1368, step time: 0.5106\n",
      "218/388, train_loss: 0.0708, step time: 0.5332\n",
      "219/388, train_loss: 0.1360, step time: 0.5263\n",
      "220/388, train_loss: 0.1692, step time: 0.5207\n",
      "221/388, train_loss: 0.1134, step time: 0.5045\n",
      "222/388, train_loss: 0.3066, step time: 0.4957\n",
      "223/388, train_loss: 0.1227, step time: 0.9660\n",
      "224/388, train_loss: 0.2643, step time: 0.5302\n",
      "225/388, train_loss: 0.5361, step time: 0.5064\n",
      "226/388, train_loss: 0.0850, step time: 0.4821\n",
      "227/388, train_loss: 0.2883, step time: 0.4901\n",
      "228/388, train_loss: 0.1148, step time: 0.4956\n",
      "229/388, train_loss: 0.0646, step time: 0.4798\n",
      "230/388, train_loss: 0.3465, step time: 0.5005\n",
      "231/388, train_loss: 0.5417, step time: 0.5866\n",
      "232/388, train_loss: 0.3810, step time: 0.5827\n",
      "233/388, train_loss: 0.0987, step time: 0.5323\n",
      "234/388, train_loss: 0.0972, step time: 0.5140\n",
      "235/388, train_loss: 0.3946, step time: 0.5081\n",
      "236/388, train_loss: 0.1492, step time: 0.5629\n",
      "237/388, train_loss: 0.3329, step time: 0.5369\n",
      "238/388, train_loss: 0.1393, step time: 0.5040\n",
      "239/388, train_loss: 0.3938, step time: 0.5364\n",
      "240/388, train_loss: 0.1339, step time: 0.6571\n",
      "241/388, train_loss: 0.0684, step time: 0.5419\n",
      "242/388, train_loss: 0.0888, step time: 0.5151\n",
      "243/388, train_loss: 0.0940, step time: 0.4975\n",
      "244/388, train_loss: 0.1572, step time: 0.4863\n",
      "245/388, train_loss: 0.4314, step time: 0.5512\n",
      "246/388, train_loss: 0.0498, step time: 0.5094\n",
      "247/388, train_loss: 0.0956, step time: 0.4788\n",
      "248/388, train_loss: 0.2388, step time: 1.0962\n",
      "249/388, train_loss: 0.2618, step time: 0.5302\n",
      "250/388, train_loss: 0.1575, step time: 0.4897\n",
      "251/388, train_loss: 0.1227, step time: 0.4788\n",
      "252/388, train_loss: 0.0837, step time: 0.4894\n",
      "253/388, train_loss: 0.5328, step time: 0.9533\n",
      "254/388, train_loss: 0.1493, step time: 0.5563\n",
      "255/388, train_loss: 0.1782, step time: 0.5201\n",
      "256/388, train_loss: 0.5383, step time: 0.4974\n",
      "257/388, train_loss: 0.3632, step time: 0.4956\n",
      "258/388, train_loss: 0.1158, step time: 0.4963\n",
      "259/388, train_loss: 0.2611, step time: 0.4955\n",
      "260/388, train_loss: 0.0899, step time: 0.5053\n",
      "261/388, train_loss: 0.3428, step time: 0.5025\n",
      "262/388, train_loss: 0.1138, step time: 0.4822\n",
      "263/388, train_loss: 0.2151, step time: 0.5028\n",
      "264/388, train_loss: 0.0987, step time: 0.5370\n",
      "265/388, train_loss: 0.0710, step time: 0.5237\n",
      "266/388, train_loss: 0.4022, step time: 0.5143\n",
      "267/388, train_loss: 0.2026, step time: 0.4951\n",
      "268/388, train_loss: 0.2014, step time: 0.4939\n",
      "269/388, train_loss: 0.1439, step time: 0.5034\n",
      "270/388, train_loss: 0.1756, step time: 0.5527\n",
      "271/388, train_loss: 0.0835, step time: 0.5713\n",
      "272/388, train_loss: 0.0780, step time: 0.5332\n",
      "273/388, train_loss: 0.2342, step time: 0.5156\n",
      "274/388, train_loss: 0.2255, step time: 0.5056\n",
      "275/388, train_loss: 0.0826, step time: 0.5000\n",
      "276/388, train_loss: 0.3841, step time: 0.5014\n",
      "277/388, train_loss: 0.1063, step time: 0.5167\n",
      "278/388, train_loss: 0.2534, step time: 0.4953\n",
      "279/388, train_loss: 0.2292, step time: 0.9737\n",
      "280/388, train_loss: 0.1078, step time: 0.5285\n",
      "281/388, train_loss: 0.3070, step time: 0.5064\n",
      "282/388, train_loss: 0.3701, step time: 0.5075\n",
      "283/388, train_loss: 0.2098, step time: 0.5688\n",
      "284/388, train_loss: 0.0813, step time: 0.5210\n",
      "285/388, train_loss: 0.1982, step time: 0.5131\n",
      "286/388, train_loss: 0.0934, step time: 0.5129\n",
      "287/388, train_loss: 0.3919, step time: 0.4969\n",
      "288/388, train_loss: 0.0607, step time: 1.0317\n",
      "289/388, train_loss: 0.1683, step time: 0.5509\n",
      "290/388, train_loss: 0.1911, step time: 0.5205\n",
      "291/388, train_loss: 0.4418, step time: 0.5031\n",
      "292/388, train_loss: 0.0677, step time: 0.4840\n",
      "293/388, train_loss: 0.0469, step time: 0.9710\n",
      "294/388, train_loss: 0.2150, step time: 0.5328\n",
      "295/388, train_loss: 0.1731, step time: 0.5007\n",
      "296/388, train_loss: 0.3791, step time: 0.4960\n",
      "297/388, train_loss: 0.0831, step time: 0.4866\n",
      "298/388, train_loss: 0.1617, step time: 0.5198\n",
      "299/388, train_loss: 0.0858, step time: 0.5167\n",
      "300/388, train_loss: 0.0805, step time: 0.5130\n",
      "301/388, train_loss: 0.1133, step time: 0.5242\n",
      "302/388, train_loss: 0.1213, step time: 0.5036\n",
      "303/388, train_loss: 0.1045, step time: 0.4871\n",
      "304/388, train_loss: 0.2030, step time: 0.4900\n",
      "305/388, train_loss: 0.1065, step time: 0.4750\n",
      "306/388, train_loss: 0.1001, step time: 0.5089\n",
      "307/388, train_loss: 0.0905, step time: 0.4932\n",
      "308/388, train_loss: 0.1926, step time: 0.5146\n",
      "309/388, train_loss: 0.1815, step time: 0.4950\n",
      "310/388, train_loss: 0.6366, step time: 0.5501\n",
      "311/388, train_loss: 0.2365, step time: 0.5403\n",
      "312/388, train_loss: 0.1171, step time: 0.5144\n",
      "313/388, train_loss: 0.2733, step time: 0.4893\n",
      "314/388, train_loss: 0.1915, step time: 0.5091\n",
      "315/388, train_loss: 0.2790, step time: 0.5195\n",
      "316/388, train_loss: 0.1581, step time: 0.5394\n",
      "317/388, train_loss: 0.1666, step time: 0.5135\n",
      "318/388, train_loss: 0.2058, step time: 0.4917\n",
      "319/388, train_loss: 0.1400, step time: 0.4938\n",
      "320/388, train_loss: 0.0985, step time: 0.4862\n",
      "321/388, train_loss: 0.1801, step time: 0.5070\n",
      "322/388, train_loss: 0.2343, step time: 0.5396\n",
      "323/388, train_loss: 0.1884, step time: 0.5263\n",
      "324/388, train_loss: 0.2299, step time: 0.5047\n",
      "325/388, train_loss: 0.0543, step time: 0.5038\n",
      "326/388, train_loss: 0.1246, step time: 0.4968\n",
      "327/388, train_loss: 0.1039, step time: 0.4927\n",
      "328/388, train_loss: 0.4164, step time: 0.8634\n",
      "329/388, train_loss: 0.0985, step time: 0.5233\n",
      "330/388, train_loss: 0.1834, step time: 0.4804\n",
      "331/388, train_loss: 0.2576, step time: 0.4959\n",
      "332/388, train_loss: 0.1713, step time: 0.4955\n",
      "333/388, train_loss: 0.0861, step time: 0.4844\n",
      "334/388, train_loss: 0.1109, step time: 0.4997\n",
      "335/388, train_loss: 0.1666, step time: 0.4804\n",
      "336/388, train_loss: 0.0629, step time: 0.5112\n",
      "337/388, train_loss: 0.2570, step time: 0.4896\n",
      "338/388, train_loss: 0.1861, step time: 0.9306\n",
      "339/388, train_loss: 0.0443, step time: 0.5348\n",
      "340/388, train_loss: 0.2347, step time: 0.5066\n",
      "341/388, train_loss: 0.1244, step time: 0.5136\n",
      "342/388, train_loss: 0.1605, step time: 0.4979\n",
      "343/388, train_loss: 0.1150, step time: 0.4985\n",
      "344/388, train_loss: 0.2257, step time: 0.4846\n",
      "345/388, train_loss: 0.1354, step time: 0.5053\n",
      "346/388, train_loss: 0.0737, step time: 0.5296\n",
      "347/388, train_loss: 0.3031, step time: 0.5565\n",
      "348/388, train_loss: 0.0935, step time: 0.5359\n",
      "349/388, train_loss: 0.3017, step time: 0.5017\n",
      "350/388, train_loss: 0.2946, step time: 0.4986\n",
      "351/388, train_loss: 0.1515, step time: 0.5014\n",
      "352/388, train_loss: 0.4766, step time: 0.4858\n",
      "353/388, train_loss: 0.0588, step time: 0.5043\n",
      "354/388, train_loss: 0.2142, step time: 0.5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/388, train_loss: 0.1032, step time: 0.4985\n",
      "356/388, train_loss: 0.1856, step time: 0.4902\n",
      "357/388, train_loss: 0.1959, step time: 0.9405\n",
      "358/388, train_loss: 0.0715, step time: 0.5716\n",
      "359/388, train_loss: 0.1327, step time: 0.5258\n",
      "360/388, train_loss: 0.1178, step time: 0.5135\n",
      "361/388, train_loss: 0.1900, step time: 0.4925\n",
      "362/388, train_loss: 0.1227, step time: 0.4910\n",
      "363/388, train_loss: 0.1448, step time: 0.4798\n",
      "364/388, train_loss: 0.2579, step time: 0.4992\n",
      "365/388, train_loss: 0.1268, step time: 0.9911\n",
      "366/388, train_loss: 0.1090, step time: 0.5308\n",
      "367/388, train_loss: 0.0845, step time: 0.5035\n",
      "368/388, train_loss: 0.0600, step time: 0.4876\n",
      "369/388, train_loss: 0.4455, step time: 0.4969\n",
      "370/388, train_loss: 0.0919, step time: 0.5024\n",
      "371/388, train_loss: 0.3349, step time: 0.5154\n",
      "372/388, train_loss: 0.1609, step time: 0.5165\n",
      "373/388, train_loss: 0.2010, step time: 0.5055\n",
      "374/388, train_loss: 0.0326, step time: 0.4839\n",
      "375/388, train_loss: 0.1410, step time: 0.4939\n",
      "376/388, train_loss: 0.1541, step time: 0.5003\n",
      "377/388, train_loss: 0.1627, step time: 0.4813\n",
      "378/388, train_loss: 0.3319, step time: 1.0181\n",
      "379/388, train_loss: 0.2790, step time: 0.5341\n",
      "380/388, train_loss: 0.1647, step time: 0.5181\n",
      "381/388, train_loss: 0.1266, step time: 0.4928\n",
      "382/388, train_loss: 0.3420, step time: 0.5112\n",
      "383/388, train_loss: 0.1892, step time: 0.5561\n",
      "384/388, train_loss: 0.0410, step time: 0.5195\n",
      "385/388, train_loss: 0.2260, step time: 0.5012\n",
      "386/388, train_loss: 0.1759, step time: 0.4903\n",
      "387/388, train_loss: 0.1114, step time: 0.4920\n",
      "388/388, train_loss: 0.0999, step time: 0.4867\n",
      "epoch 70 average loss: 0.1797\n",
      "current epoch: 70 current mean dice: 0.7591 tc: 0.8087 wt: 0.8998 et: 0.5690\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 70 is: 302.2512\n",
      "----------\n",
      "epoch 71/300\n",
      "1/388, train_loss: 0.0540, step time: 0.4834\n",
      "2/388, train_loss: 0.2523, step time: 0.4869\n",
      "3/388, train_loss: 0.1012, step time: 0.4994\n",
      "4/388, train_loss: 0.1552, step time: 1.1285\n",
      "5/388, train_loss: 0.1660, step time: 0.5236\n",
      "6/388, train_loss: 0.2893, step time: 0.5239\n",
      "7/388, train_loss: 0.0700, step time: 0.4938\n",
      "8/388, train_loss: 0.1139, step time: 0.4907\n",
      "9/388, train_loss: 0.1746, step time: 0.5017\n",
      "10/388, train_loss: 0.2699, step time: 0.5010\n",
      "11/388, train_loss: 0.3372, step time: 0.5281\n",
      "12/388, train_loss: 0.0821, step time: 0.5319\n",
      "13/388, train_loss: 0.2440, step time: 0.5337\n",
      "14/388, train_loss: 0.0304, step time: 0.5115\n",
      "15/388, train_loss: 0.1577, step time: 0.5465\n",
      "16/388, train_loss: 0.1606, step time: 0.5278\n",
      "17/388, train_loss: 0.0659, step time: 0.5117\n",
      "18/388, train_loss: 0.3004, step time: 0.4943\n",
      "19/388, train_loss: 0.5173, step time: 0.5311\n",
      "20/388, train_loss: 0.1298, step time: 0.4947\n",
      "21/388, train_loss: 0.5061, step time: 0.5182\n",
      "22/388, train_loss: 0.1457, step time: 0.5337\n",
      "23/388, train_loss: 0.1543, step time: 0.5311\n",
      "24/388, train_loss: 0.1458, step time: 0.5260\n",
      "25/388, train_loss: 0.1536, step time: 0.5032\n",
      "26/388, train_loss: 0.0828, step time: 0.4954\n",
      "27/388, train_loss: 0.3043, step time: 0.5335\n",
      "28/388, train_loss: 0.1237, step time: 0.5309\n",
      "29/388, train_loss: 0.1839, step time: 0.5104\n",
      "30/388, train_loss: 0.2101, step time: 0.4945\n",
      "31/388, train_loss: 0.1907, step time: 0.5495\n",
      "32/388, train_loss: 0.0721, step time: 0.5300\n",
      "33/388, train_loss: 0.0971, step time: 0.5142\n",
      "34/388, train_loss: 0.1011, step time: 0.5106\n",
      "35/388, train_loss: 0.1519, step time: 1.2140\n",
      "36/388, train_loss: 0.3193, step time: 0.5355\n",
      "37/388, train_loss: 0.0546, step time: 0.5074\n",
      "38/388, train_loss: 0.0953, step time: 0.5072\n",
      "39/388, train_loss: 0.2899, step time: 0.5138\n",
      "40/388, train_loss: 0.2074, step time: 0.5909\n",
      "41/388, train_loss: 0.3036, step time: 0.5378\n",
      "42/388, train_loss: 0.1949, step time: 0.5153\n",
      "43/388, train_loss: 0.0663, step time: 0.4907\n",
      "44/388, train_loss: 0.1240, step time: 0.5156\n",
      "45/388, train_loss: 0.2520, step time: 0.5684\n",
      "46/388, train_loss: 0.1611, step time: 0.5390\n",
      "47/388, train_loss: 0.4066, step time: 0.5090\n",
      "48/388, train_loss: 0.1605, step time: 0.5160\n",
      "49/388, train_loss: 0.1792, step time: 0.5245\n",
      "50/388, train_loss: 0.2559, step time: 0.5578\n",
      "51/388, train_loss: 0.0913, step time: 0.5210\n",
      "52/388, train_loss: 0.1168, step time: 0.5104\n",
      "53/388, train_loss: 0.0530, step time: 0.5117\n",
      "54/388, train_loss: 0.3977, step time: 0.5021\n",
      "55/388, train_loss: 0.3399, step time: 0.4880\n",
      "56/388, train_loss: 0.1183, step time: 0.4956\n",
      "57/388, train_loss: 0.4662, step time: 0.4819\n",
      "58/388, train_loss: 0.2891, step time: 0.4794\n",
      "59/388, train_loss: 0.0682, step time: 0.4869\n",
      "60/388, train_loss: 0.2275, step time: 0.4821\n",
      "61/388, train_loss: 0.4465, step time: 0.4952\n",
      "62/388, train_loss: 0.3240, step time: 0.9603\n",
      "63/388, train_loss: 0.1966, step time: 0.5435\n",
      "64/388, train_loss: 0.1672, step time: 0.5173\n",
      "65/388, train_loss: 0.0730, step time: 0.4971\n",
      "66/388, train_loss: 0.0632, step time: 0.4978\n",
      "67/388, train_loss: 0.1079, step time: 0.4981\n",
      "68/388, train_loss: 0.2963, step time: 0.5039\n",
      "69/388, train_loss: 0.0973, step time: 0.4839\n",
      "70/388, train_loss: 0.1089, step time: 0.5086\n",
      "71/388, train_loss: 0.1414, step time: 0.4954\n",
      "72/388, train_loss: 0.2512, step time: 0.5522\n",
      "73/388, train_loss: 0.1153, step time: 0.5024\n",
      "74/388, train_loss: 0.1724, step time: 0.5441\n",
      "75/388, train_loss: 0.0612, step time: 0.5128\n",
      "76/388, train_loss: 0.0874, step time: 0.5000\n",
      "77/388, train_loss: 0.1011, step time: 0.4877\n",
      "78/388, train_loss: 0.1172, step time: 0.4843\n",
      "79/388, train_loss: 0.0877, step time: 0.4922\n",
      "80/388, train_loss: 0.1062, step time: 0.4883\n",
      "81/388, train_loss: 0.1731, step time: 0.5229\n",
      "82/388, train_loss: 0.0769, step time: 0.5080\n",
      "83/388, train_loss: 0.2565, step time: 0.5025\n",
      "84/388, train_loss: 0.1660, step time: 0.4932\n",
      "85/388, train_loss: 0.1105, step time: 0.4957\n",
      "86/388, train_loss: 0.1149, step time: 1.1508\n",
      "87/388, train_loss: 0.1776, step time: 0.5473\n",
      "88/388, train_loss: 0.0961, step time: 0.5077\n",
      "89/388, train_loss: 0.1311, step time: 0.4962\n",
      "90/388, train_loss: 0.2359, step time: 0.4838\n",
      "91/388, train_loss: 0.1244, step time: 0.4782\n",
      "92/388, train_loss: 0.1100, step time: 0.4736\n",
      "93/388, train_loss: 0.2164, step time: 0.4754\n",
      "94/388, train_loss: 0.1032, step time: 0.7711\n",
      "95/388, train_loss: 0.2134, step time: 0.5635\n",
      "96/388, train_loss: 0.0623, step time: 0.5230\n",
      "97/388, train_loss: 0.0725, step time: 0.5041\n",
      "98/388, train_loss: 0.0743, step time: 0.4940\n",
      "99/388, train_loss: 0.1358, step time: 0.5187\n",
      "100/388, train_loss: 0.1364, step time: 0.5042\n",
      "101/388, train_loss: 0.2018, step time: 0.4824\n",
      "102/388, train_loss: 0.1050, step time: 0.4904\n",
      "103/388, train_loss: 0.1540, step time: 0.9000\n",
      "104/388, train_loss: 0.0375, step time: 0.5541\n",
      "105/388, train_loss: 0.1024, step time: 0.5196\n",
      "106/388, train_loss: 0.5462, step time: 0.5195\n",
      "107/388, train_loss: 0.2368, step time: 0.5053\n",
      "108/388, train_loss: 0.1463, step time: 0.4839\n",
      "109/388, train_loss: 0.3427, step time: 0.4953\n",
      "110/388, train_loss: 0.2136, step time: 0.4846\n",
      "111/388, train_loss: 0.1382, step time: 0.4979\n",
      "112/388, train_loss: 0.1667, step time: 0.5180\n",
      "113/388, train_loss: 0.2097, step time: 0.5134\n",
      "114/388, train_loss: 0.2965, step time: 0.5260\n",
      "115/388, train_loss: 0.0726, step time: 0.5192\n",
      "116/388, train_loss: 0.1199, step time: 0.5059\n",
      "117/388, train_loss: 0.0912, step time: 0.5009\n",
      "118/388, train_loss: 0.0980, step time: 0.6089\n",
      "119/388, train_loss: 0.2423, step time: 0.5362\n",
      "120/388, train_loss: 0.2251, step time: 0.5013\n",
      "121/388, train_loss: 0.1422, step time: 0.5061\n",
      "122/388, train_loss: 0.1878, step time: 0.4863\n",
      "123/388, train_loss: 0.1675, step time: 0.5042\n",
      "124/388, train_loss: 0.2668, step time: 0.4854\n",
      "125/388, train_loss: 0.1538, step time: 0.4792\n",
      "126/388, train_loss: 0.0743, step time: 0.4859\n",
      "127/388, train_loss: 0.0804, step time: 0.4730\n",
      "128/388, train_loss: 0.0809, step time: 0.5310\n",
      "129/388, train_loss: 0.0542, step time: 0.5121\n",
      "130/388, train_loss: 0.0791, step time: 0.5014\n",
      "131/388, train_loss: 0.2897, step time: 1.1333\n",
      "132/388, train_loss: 0.2346, step time: 0.5435\n",
      "133/388, train_loss: 0.2314, step time: 0.4986\n",
      "134/388, train_loss: 0.0932, step time: 0.4933\n",
      "135/388, train_loss: 0.1514, step time: 0.4971\n",
      "136/388, train_loss: 0.2793, step time: 0.5064\n",
      "137/388, train_loss: 0.0442, step time: 0.5035\n",
      "138/388, train_loss: 0.1030, step time: 0.4897\n",
      "139/388, train_loss: 0.1547, step time: 0.4989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/388, train_loss: 0.1886, step time: 0.5299\n",
      "141/388, train_loss: 0.1260, step time: 0.5159\n",
      "142/388, train_loss: 0.2063, step time: 0.5066\n",
      "143/388, train_loss: 0.2301, step time: 0.5196\n",
      "144/388, train_loss: 0.2283, step time: 0.5804\n",
      "145/388, train_loss: 0.0840, step time: 0.5391\n",
      "146/388, train_loss: 0.1194, step time: 0.5150\n",
      "147/388, train_loss: 0.2373, step time: 0.5012\n",
      "148/388, train_loss: 0.2247, step time: 0.4835\n",
      "149/388, train_loss: 0.4440, step time: 0.4818\n",
      "150/388, train_loss: 0.2415, step time: 0.9804\n",
      "151/388, train_loss: 0.1286, step time: 0.5401\n",
      "152/388, train_loss: 0.0742, step time: 0.5171\n",
      "153/388, train_loss: 0.1573, step time: 0.4875\n",
      "154/388, train_loss: 0.6091, step time: 0.4860\n",
      "155/388, train_loss: 0.0485, step time: 0.4905\n",
      "156/388, train_loss: 0.1913, step time: 0.4754\n",
      "157/388, train_loss: 0.0999, step time: 0.5068\n",
      "158/388, train_loss: 0.2176, step time: 0.5021\n",
      "159/388, train_loss: 0.1402, step time: 0.5286\n",
      "160/388, train_loss: 0.2950, step time: 0.5677\n",
      "161/388, train_loss: 0.0854, step time: 0.5523\n",
      "162/388, train_loss: 0.3465, step time: 0.5252\n",
      "163/388, train_loss: 0.2299, step time: 0.5049\n",
      "164/388, train_loss: 0.2352, step time: 0.5501\n",
      "165/388, train_loss: 0.0699, step time: 0.5977\n",
      "166/388, train_loss: 0.1363, step time: 0.5306\n",
      "167/388, train_loss: 0.0922, step time: 0.5178\n",
      "168/388, train_loss: 0.0662, step time: 0.4952\n",
      "169/388, train_loss: 0.1460, step time: 0.4982\n",
      "170/388, train_loss: 0.1008, step time: 0.4802\n",
      "171/388, train_loss: 0.1469, step time: 0.4772\n",
      "172/388, train_loss: 0.1265, step time: 0.5033\n",
      "173/388, train_loss: 0.1632, step time: 0.5058\n",
      "174/388, train_loss: 0.2070, step time: 0.9051\n",
      "175/388, train_loss: 0.2181, step time: 0.5643\n",
      "176/388, train_loss: 0.2566, step time: 0.5226\n",
      "177/388, train_loss: 0.0805, step time: 0.5015\n",
      "178/388, train_loss: 0.0924, step time: 0.4884\n",
      "179/388, train_loss: 0.0654, step time: 0.5072\n",
      "180/388, train_loss: 0.1820, step time: 0.4923\n",
      "181/388, train_loss: 0.0641, step time: 0.4943\n",
      "182/388, train_loss: 0.1250, step time: 0.4821\n",
      "183/388, train_loss: 0.2194, step time: 0.5024\n",
      "184/388, train_loss: 0.0515, step time: 0.5058\n",
      "185/388, train_loss: 0.2433, step time: 0.5340\n",
      "186/388, train_loss: 0.0897, step time: 0.5251\n",
      "187/388, train_loss: 0.0654, step time: 0.5654\n",
      "188/388, train_loss: 0.1573, step time: 0.6743\n",
      "189/388, train_loss: 0.2520, step time: 0.5468\n",
      "190/388, train_loss: 0.1355, step time: 0.5258\n",
      "191/388, train_loss: 0.2305, step time: 0.5039\n",
      "192/388, train_loss: 0.1741, step time: 0.4869\n",
      "193/388, train_loss: 0.1465, step time: 1.1216\n",
      "194/388, train_loss: 0.2317, step time: 0.5320\n",
      "195/388, train_loss: 0.2588, step time: 0.5074\n",
      "196/388, train_loss: 0.1878, step time: 0.4862\n",
      "197/388, train_loss: 0.1753, step time: 0.5115\n",
      "198/388, train_loss: 0.0968, step time: 0.5178\n",
      "199/388, train_loss: 0.0702, step time: 0.5169\n",
      "200/388, train_loss: 0.1597, step time: 0.4977\n",
      "201/388, train_loss: 0.3893, step time: 0.5128\n",
      "202/388, train_loss: 0.1019, step time: 0.5195\n",
      "203/388, train_loss: 0.2038, step time: 0.5768\n",
      "204/388, train_loss: 0.1540, step time: 0.5185\n",
      "205/388, train_loss: 0.1478, step time: 0.4957\n",
      "206/388, train_loss: 0.3469, step time: 0.4965\n",
      "207/388, train_loss: 0.0989, step time: 0.5168\n",
      "208/388, train_loss: 0.2280, step time: 0.5194\n",
      "209/388, train_loss: 0.1417, step time: 0.5110\n",
      "210/388, train_loss: 0.1337, step time: 0.4919\n",
      "211/388, train_loss: 0.1428, step time: 0.5000\n",
      "212/388, train_loss: 0.1536, step time: 0.6310\n",
      "213/388, train_loss: 0.2726, step time: 0.5869\n",
      "214/388, train_loss: 0.0638, step time: 0.5331\n",
      "215/388, train_loss: 0.2405, step time: 0.5137\n",
      "216/388, train_loss: 0.2165, step time: 0.5040\n",
      "217/388, train_loss: 0.1261, step time: 0.4967\n",
      "218/388, train_loss: 0.5061, step time: 0.4828\n",
      "219/388, train_loss: 0.1238, step time: 1.1355\n",
      "220/388, train_loss: 0.2086, step time: 0.5287\n",
      "221/388, train_loss: 0.1728, step time: 0.5171\n",
      "222/388, train_loss: 0.0622, step time: 0.4956\n",
      "223/388, train_loss: 0.2036, step time: 0.5032\n",
      "224/388, train_loss: 0.1319, step time: 0.4823\n",
      "225/388, train_loss: 0.1818, step time: 0.4918\n",
      "226/388, train_loss: 0.2085, step time: 0.4895\n",
      "227/388, train_loss: 0.1724, step time: 0.5052\n",
      "228/388, train_loss: 0.1553, step time: 0.4976\n",
      "229/388, train_loss: 0.1024, step time: 0.4968\n",
      "230/388, train_loss: 0.2038, step time: 1.0816\n",
      "231/388, train_loss: 0.0329, step time: 0.5358\n",
      "232/388, train_loss: 0.1639, step time: 0.5159\n",
      "233/388, train_loss: 0.0755, step time: 0.5033\n",
      "234/388, train_loss: 0.0950, step time: 0.5038\n",
      "235/388, train_loss: 0.1009, step time: 0.4879\n",
      "236/388, train_loss: 0.3872, step time: 0.5073\n",
      "237/388, train_loss: 0.2883, step time: 0.5007\n",
      "238/388, train_loss: 0.1381, step time: 0.4987\n",
      "239/388, train_loss: 0.1184, step time: 0.4834\n",
      "240/388, train_loss: 0.1613, step time: 0.5038\n",
      "241/388, train_loss: 0.1582, step time: 0.4860\n",
      "242/388, train_loss: 0.1527, step time: 1.0083\n",
      "243/388, train_loss: 0.2073, step time: 0.5663\n",
      "244/388, train_loss: 0.2027, step time: 0.5113\n",
      "245/388, train_loss: 0.2158, step time: 0.4930\n",
      "246/388, train_loss: 0.1707, step time: 0.4947\n",
      "247/388, train_loss: 0.1927, step time: 0.4921\n",
      "248/388, train_loss: 0.0617, step time: 0.4957\n",
      "249/388, train_loss: 0.1337, step time: 0.5041\n",
      "250/388, train_loss: 0.3041, step time: 0.5214\n",
      "251/388, train_loss: 0.2361, step time: 0.6291\n",
      "252/388, train_loss: 0.0329, step time: 0.5452\n",
      "253/388, train_loss: 0.1901, step time: 0.5340\n",
      "254/388, train_loss: 0.1542, step time: 0.5278\n",
      "255/388, train_loss: 0.0936, step time: 0.5053\n",
      "256/388, train_loss: 0.0868, step time: 0.5318\n",
      "257/388, train_loss: 0.0844, step time: 0.5309\n",
      "258/388, train_loss: 0.2414, step time: 0.5489\n",
      "259/388, train_loss: 0.2748, step time: 0.7484\n",
      "260/388, train_loss: 0.1305, step time: 0.5331\n",
      "261/388, train_loss: 0.2072, step time: 0.5037\n",
      "262/388, train_loss: 0.3275, step time: 0.5205\n",
      "263/388, train_loss: 0.0859, step time: 0.6219\n",
      "264/388, train_loss: 0.0888, step time: 0.5347\n",
      "265/388, train_loss: 0.0682, step time: 0.5229\n",
      "266/388, train_loss: 0.1880, step time: 0.5048\n",
      "267/388, train_loss: 0.3058, step time: 0.4890\n",
      "268/388, train_loss: 0.1692, step time: 0.5145\n",
      "269/388, train_loss: 0.1567, step time: 0.5008\n",
      "270/388, train_loss: 0.1084, step time: 0.5038\n",
      "271/388, train_loss: 0.1407, step time: 0.4905\n",
      "272/388, train_loss: 0.2295, step time: 0.4869\n",
      "273/388, train_loss: 0.3201, step time: 0.4937\n",
      "274/388, train_loss: 0.3904, step time: 1.1270\n",
      "275/388, train_loss: 0.3665, step time: 0.5530\n",
      "276/388, train_loss: 0.0877, step time: 0.5181\n",
      "277/388, train_loss: 0.1306, step time: 0.5031\n",
      "278/388, train_loss: 0.2806, step time: 0.4884\n",
      "279/388, train_loss: 0.4564, step time: 0.4814\n",
      "280/388, train_loss: 0.0492, step time: 0.4929\n",
      "281/388, train_loss: 0.2217, step time: 0.4852\n",
      "282/388, train_loss: 0.2113, step time: 0.4994\n",
      "283/388, train_loss: 0.1776, step time: 0.4954\n",
      "284/388, train_loss: 0.1031, step time: 0.6860\n",
      "285/388, train_loss: 0.1942, step time: 0.5604\n",
      "286/388, train_loss: 0.1026, step time: 0.5330\n",
      "287/388, train_loss: 0.0920, step time: 0.5086\n",
      "288/388, train_loss: 0.1015, step time: 0.5012\n",
      "289/388, train_loss: 0.4358, step time: 0.4870\n",
      "290/388, train_loss: 0.1508, step time: 0.4765\n",
      "291/388, train_loss: 0.2582, step time: 0.5216\n",
      "292/388, train_loss: 0.2587, step time: 0.5788\n",
      "293/388, train_loss: 0.1402, step time: 0.5851\n",
      "294/388, train_loss: 0.2066, step time: 0.5493\n",
      "295/388, train_loss: 0.1296, step time: 0.5159\n",
      "296/388, train_loss: 0.0879, step time: 0.4910\n",
      "297/388, train_loss: 0.1384, step time: 0.5018\n",
      "298/388, train_loss: 0.0682, step time: 0.5020\n",
      "299/388, train_loss: 0.1306, step time: 0.4974\n",
      "300/388, train_loss: 0.1121, step time: 0.8319\n",
      "301/388, train_loss: 0.1087, step time: 0.5721\n",
      "302/388, train_loss: 0.2719, step time: 0.5310\n",
      "303/388, train_loss: 0.1800, step time: 0.4950\n",
      "304/388, train_loss: 0.3737, step time: 0.5066\n",
      "305/388, train_loss: 0.1644, step time: 0.4881\n",
      "306/388, train_loss: 0.0479, step time: 0.5065\n",
      "307/388, train_loss: 0.0797, step time: 0.5188\n",
      "308/388, train_loss: 0.1590, step time: 0.6271\n",
      "309/388, train_loss: 0.1850, step time: 0.5337\n",
      "310/388, train_loss: 0.1037, step time: 0.5239\n",
      "311/388, train_loss: 0.2023, step time: 0.5090\n",
      "312/388, train_loss: 0.1974, step time: 0.4963\n",
      "313/388, train_loss: 0.1237, step time: 0.4894\n",
      "314/388, train_loss: 0.1263, step time: 0.4982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/388, train_loss: 0.1074, step time: 0.5150\n",
      "316/388, train_loss: 0.1948, step time: 0.5234\n",
      "317/388, train_loss: 0.2202, step time: 0.5950\n",
      "318/388, train_loss: 0.1664, step time: 0.6005\n",
      "319/388, train_loss: 0.1853, step time: 0.5433\n",
      "320/388, train_loss: 0.1943, step time: 0.5041\n",
      "321/388, train_loss: 0.0962, step time: 0.7241\n",
      "322/388, train_loss: 0.1209, step time: 0.6049\n",
      "323/388, train_loss: 0.1700, step time: 0.5194\n",
      "324/388, train_loss: 0.2346, step time: 0.4946\n",
      "325/388, train_loss: 0.0677, step time: 0.4878\n",
      "326/388, train_loss: 0.0775, step time: 0.4966\n",
      "327/388, train_loss: 0.3204, step time: 0.5303\n",
      "328/388, train_loss: 0.2061, step time: 0.5042\n",
      "329/388, train_loss: 0.2843, step time: 0.5008\n",
      "330/388, train_loss: 0.1514, step time: 0.4896\n",
      "331/388, train_loss: 0.1863, step time: 0.5013\n",
      "332/388, train_loss: 0.1513, step time: 0.4911\n",
      "333/388, train_loss: 0.0777, step time: 1.0190\n",
      "334/388, train_loss: 0.1105, step time: 0.5480\n",
      "335/388, train_loss: 0.5011, step time: 0.5373\n",
      "336/388, train_loss: 0.1039, step time: 0.5363\n",
      "337/388, train_loss: 0.1235, step time: 0.5142\n",
      "338/388, train_loss: 0.1787, step time: 0.5124\n",
      "339/388, train_loss: 0.1377, step time: 0.4970\n",
      "340/388, train_loss: 0.2094, step time: 0.5275\n",
      "341/388, train_loss: 0.3193, step time: 0.5999\n",
      "342/388, train_loss: 0.1259, step time: 0.6166\n",
      "343/388, train_loss: 0.1152, step time: 0.5355\n",
      "344/388, train_loss: 0.1229, step time: 0.4951\n",
      "345/388, train_loss: 0.0919, step time: 0.5015\n",
      "346/388, train_loss: 0.1042, step time: 0.4834\n",
      "347/388, train_loss: 0.1272, step time: 0.4860\n",
      "348/388, train_loss: 0.2153, step time: 0.4962\n",
      "349/388, train_loss: 0.1949, step time: 0.6752\n",
      "350/388, train_loss: 0.1408, step time: 0.6094\n",
      "351/388, train_loss: 0.2679, step time: 0.5611\n",
      "352/388, train_loss: 0.0978, step time: 0.5277\n",
      "353/388, train_loss: 0.2563, step time: 0.5074\n",
      "354/388, train_loss: 0.1821, step time: 0.5045\n",
      "355/388, train_loss: 0.0823, step time: 0.5057\n",
      "356/388, train_loss: 0.2253, step time: 0.5906\n",
      "357/388, train_loss: 0.3194, step time: 0.5616\n",
      "358/388, train_loss: 0.1336, step time: 0.5288\n",
      "359/388, train_loss: 0.1651, step time: 0.5010\n",
      "360/388, train_loss: 0.0618, step time: 0.4952\n",
      "361/388, train_loss: 0.2525, step time: 0.4895\n",
      "362/388, train_loss: 0.0932, step time: 1.1621\n",
      "363/388, train_loss: 0.2544, step time: 0.5433\n",
      "364/388, train_loss: 0.0952, step time: 0.5164\n",
      "365/388, train_loss: 0.1194, step time: 0.5010\n",
      "366/388, train_loss: 0.0933, step time: 0.4912\n",
      "367/388, train_loss: 0.2770, step time: 0.5289\n",
      "368/388, train_loss: 0.4660, step time: 0.5634\n",
      "369/388, train_loss: 0.1898, step time: 0.6532\n",
      "370/388, train_loss: 0.0758, step time: 0.5672\n",
      "371/388, train_loss: 0.3586, step time: 0.5283\n",
      "372/388, train_loss: 0.1592, step time: 0.5041\n",
      "373/388, train_loss: 0.2716, step time: 0.5068\n",
      "374/388, train_loss: 0.0801, step time: 0.4875\n",
      "375/388, train_loss: 0.2765, step time: 0.4900\n",
      "376/388, train_loss: 0.0669, step time: 0.5150\n",
      "377/388, train_loss: 0.1678, step time: 1.1411\n",
      "378/388, train_loss: 0.0969, step time: 0.5361\n",
      "379/388, train_loss: 0.0951, step time: 0.5222\n",
      "380/388, train_loss: 0.1802, step time: 0.4977\n",
      "381/388, train_loss: 0.1404, step time: 0.4914\n",
      "382/388, train_loss: 0.4536, step time: 0.4908\n",
      "383/388, train_loss: 0.0830, step time: 0.4768\n",
      "384/388, train_loss: 0.1038, step time: 0.4678\n",
      "385/388, train_loss: 0.2014, step time: 0.5055\n",
      "386/388, train_loss: 0.2384, step time: 0.4818\n",
      "387/388, train_loss: 0.0915, step time: 0.4991\n",
      "388/388, train_loss: 0.2819, step time: 0.4970\n",
      "epoch 71 average loss: 0.1753\n",
      "current epoch: 71 current mean dice: 0.7691 tc: 0.8205 wt: 0.8943 et: 0.5925\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 71 is: 300.3955\n",
      "----------\n",
      "epoch 72/300\n",
      "1/388, train_loss: 0.1443, step time: 0.4818\n",
      "2/388, train_loss: 0.2368, step time: 0.4851\n",
      "3/388, train_loss: 0.0966, step time: 0.4875\n",
      "4/388, train_loss: 0.2307, step time: 0.5374\n",
      "5/388, train_loss: 0.1308, step time: 0.4977\n",
      "6/388, train_loss: 0.1494, step time: 0.5217\n",
      "7/388, train_loss: 0.1328, step time: 0.5297\n",
      "8/388, train_loss: 0.1754, step time: 0.4978\n",
      "9/388, train_loss: 0.1806, step time: 0.5133\n",
      "10/388, train_loss: 0.2074, step time: 0.4940\n",
      "11/388, train_loss: 0.1286, step time: 0.4985\n",
      "12/388, train_loss: 0.0676, step time: 0.9237\n",
      "13/388, train_loss: 0.1115, step time: 0.5489\n",
      "14/388, train_loss: 0.4659, step time: 0.5238\n",
      "15/388, train_loss: 0.1929, step time: 0.5015\n",
      "16/388, train_loss: 0.1317, step time: 0.5140\n",
      "17/388, train_loss: 0.1048, step time: 0.5013\n",
      "18/388, train_loss: 0.2929, step time: 1.0979\n",
      "19/388, train_loss: 0.0689, step time: 0.5521\n",
      "20/388, train_loss: 0.2755, step time: 0.5243\n",
      "21/388, train_loss: 0.3248, step time: 0.4983\n",
      "22/388, train_loss: 0.1932, step time: 0.4942\n",
      "23/388, train_loss: 0.2312, step time: 0.4917\n",
      "24/388, train_loss: 0.3245, step time: 0.4822\n",
      "25/388, train_loss: 0.0987, step time: 0.4940\n",
      "26/388, train_loss: 0.2795, step time: 0.5057\n",
      "27/388, train_loss: 0.1420, step time: 0.5478\n",
      "28/388, train_loss: 0.1700, step time: 0.6022\n",
      "29/388, train_loss: 0.1897, step time: 0.5301\n",
      "30/388, train_loss: 0.1438, step time: 0.5014\n",
      "31/388, train_loss: 0.0616, step time: 0.4887\n",
      "32/388, train_loss: 0.1323, step time: 1.1201\n",
      "33/388, train_loss: 0.1477, step time: 0.5579\n",
      "34/388, train_loss: 0.3862, step time: 0.5254\n",
      "35/388, train_loss: 0.1273, step time: 0.4970\n",
      "36/388, train_loss: 0.0989, step time: 0.4933\n",
      "37/388, train_loss: 0.1830, step time: 0.4919\n",
      "38/388, train_loss: 0.2838, step time: 0.5058\n",
      "39/388, train_loss: 0.0690, step time: 0.4858\n",
      "40/388, train_loss: 0.5215, step time: 0.4890\n",
      "41/388, train_loss: 0.4991, step time: 0.4816\n",
      "42/388, train_loss: 0.2461, step time: 0.5035\n",
      "43/388, train_loss: 0.0665, step time: 0.4885\n",
      "44/388, train_loss: 0.1226, step time: 0.5150\n",
      "45/388, train_loss: 0.2289, step time: 0.4921\n",
      "46/388, train_loss: 0.1053, step time: 0.5859\n",
      "47/388, train_loss: 0.0991, step time: 0.5585\n",
      "48/388, train_loss: 0.0628, step time: 0.5266\n",
      "49/388, train_loss: 0.1611, step time: 0.5071\n",
      "50/388, train_loss: 0.1700, step time: 0.4970\n",
      "51/388, train_loss: 0.0806, step time: 0.4980\n",
      "52/388, train_loss: 0.2614, step time: 0.4787\n",
      "53/388, train_loss: 0.3374, step time: 0.4765\n",
      "54/388, train_loss: 0.0449, step time: 0.4972\n",
      "55/388, train_loss: 0.1617, step time: 0.5095\n",
      "56/388, train_loss: 0.0930, step time: 0.5126\n",
      "57/388, train_loss: 0.1984, step time: 0.5045\n",
      "58/388, train_loss: 0.1786, step time: 0.5055\n",
      "59/388, train_loss: 0.1289, step time: 0.5000\n",
      "60/388, train_loss: 0.1077, step time: 0.9085\n",
      "61/388, train_loss: 0.0614, step time: 0.5419\n",
      "62/388, train_loss: 0.2618, step time: 0.5201\n",
      "63/388, train_loss: 0.2404, step time: 0.5652\n",
      "64/388, train_loss: 0.0844, step time: 0.5332\n",
      "65/388, train_loss: 0.3782, step time: 0.5219\n",
      "66/388, train_loss: 0.0968, step time: 0.4950\n",
      "67/388, train_loss: 0.0649, step time: 0.4947\n",
      "68/388, train_loss: 0.3590, step time: 0.4929\n",
      "69/388, train_loss: 0.1089, step time: 1.1492\n",
      "70/388, train_loss: 0.1717, step time: 0.5421\n",
      "71/388, train_loss: 0.1388, step time: 0.5012\n",
      "72/388, train_loss: 0.1503, step time: 0.4868\n",
      "73/388, train_loss: 0.1947, step time: 0.4943\n",
      "74/388, train_loss: 0.2989, step time: 0.4771\n",
      "75/388, train_loss: 0.2603, step time: 0.4823\n",
      "76/388, train_loss: 0.0847, step time: 1.1020\n",
      "77/388, train_loss: 0.2273, step time: 0.5195\n",
      "78/388, train_loss: 0.1000, step time: 0.4963\n",
      "79/388, train_loss: 0.2063, step time: 0.4920\n",
      "80/388, train_loss: 0.1407, step time: 0.5092\n",
      "81/388, train_loss: 0.2384, step time: 0.5002\n",
      "82/388, train_loss: 0.2027, step time: 0.4818\n",
      "83/388, train_loss: 0.1403, step time: 0.4963\n",
      "84/388, train_loss: 0.1219, step time: 0.4977\n",
      "85/388, train_loss: 0.2594, step time: 0.4989\n",
      "86/388, train_loss: 0.0952, step time: 0.4821\n",
      "87/388, train_loss: 0.1998, step time: 0.4829\n",
      "88/388, train_loss: 0.1590, step time: 0.5019\n",
      "89/388, train_loss: 0.1111, step time: 0.4881\n",
      "90/388, train_loss: 0.5147, step time: 0.4922\n",
      "91/388, train_loss: 0.0767, step time: 0.4878\n",
      "92/388, train_loss: 0.1674, step time: 0.4826\n",
      "93/388, train_loss: 0.1208, step time: 0.4885\n",
      "94/388, train_loss: 0.2873, step time: 0.5007\n",
      "95/388, train_loss: 0.0863, step time: 0.5326\n",
      "96/388, train_loss: 0.2594, step time: 0.5199\n",
      "97/388, train_loss: 0.1235, step time: 0.4978\n",
      "98/388, train_loss: 0.0880, step time: 0.4928\n",
      "99/388, train_loss: 0.2431, step time: 0.4849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/388, train_loss: 0.0730, step time: 0.4841\n",
      "101/388, train_loss: 0.2098, step time: 0.5549\n",
      "102/388, train_loss: 0.2606, step time: 0.5192\n",
      "103/388, train_loss: 0.1139, step time: 0.5089\n",
      "104/388, train_loss: 0.1578, step time: 0.5004\n",
      "105/388, train_loss: 0.0393, step time: 0.5666\n",
      "106/388, train_loss: 0.1112, step time: 0.5342\n",
      "107/388, train_loss: 0.1294, step time: 0.5020\n",
      "108/388, train_loss: 0.1437, step time: 0.4855\n",
      "109/388, train_loss: 0.1105, step time: 0.4874\n",
      "110/388, train_loss: 0.2410, step time: 0.4786\n",
      "111/388, train_loss: 0.1751, step time: 1.1529\n",
      "112/388, train_loss: 0.1015, step time: 0.5265\n",
      "113/388, train_loss: 0.3044, step time: 0.5092\n",
      "114/388, train_loss: 0.2147, step time: 0.4956\n",
      "115/388, train_loss: 0.0892, step time: 0.4961\n",
      "116/388, train_loss: 0.0703, step time: 0.5256\n",
      "117/388, train_loss: 0.0863, step time: 0.5110\n",
      "118/388, train_loss: 0.0938, step time: 0.5087\n",
      "119/388, train_loss: 0.0919, step time: 0.4915\n",
      "120/388, train_loss: 0.0521, step time: 0.4943\n",
      "121/388, train_loss: 0.1966, step time: 0.4970\n",
      "122/388, train_loss: 0.2278, step time: 0.4996\n",
      "123/388, train_loss: 0.1760, step time: 0.4925\n",
      "124/388, train_loss: 0.4300, step time: 0.4955\n",
      "125/388, train_loss: 0.3825, step time: 1.1418\n",
      "126/388, train_loss: 0.1283, step time: 0.5409\n",
      "127/388, train_loss: 0.0971, step time: 0.5042\n",
      "128/388, train_loss: 0.1157, step time: 0.4959\n",
      "129/388, train_loss: 0.2622, step time: 0.4953\n",
      "130/388, train_loss: 0.3396, step time: 0.4781\n",
      "131/388, train_loss: 0.0895, step time: 0.4818\n",
      "132/388, train_loss: 0.2210, step time: 0.4877\n",
      "133/388, train_loss: 0.1082, step time: 0.5153\n",
      "134/388, train_loss: 0.1851, step time: 0.5008\n",
      "135/388, train_loss: 0.0846, step time: 0.4881\n",
      "136/388, train_loss: 0.2161, step time: 0.4960\n",
      "137/388, train_loss: 0.1027, step time: 0.4884\n",
      "138/388, train_loss: 0.1964, step time: 0.4920\n",
      "139/388, train_loss: 0.1170, step time: 0.9891\n",
      "140/388, train_loss: 0.1485, step time: 0.5412\n",
      "141/388, train_loss: 0.0891, step time: 0.5082\n",
      "142/388, train_loss: 0.2539, step time: 0.4972\n",
      "143/388, train_loss: 0.3847, step time: 0.4930\n",
      "144/388, train_loss: 0.0598, step time: 0.4794\n",
      "145/388, train_loss: 0.2903, step time: 0.9915\n",
      "146/388, train_loss: 0.1674, step time: 0.5316\n",
      "147/388, train_loss: 0.1491, step time: 0.5086\n",
      "148/388, train_loss: 0.0913, step time: 0.4884\n",
      "149/388, train_loss: 0.5407, step time: 0.5334\n",
      "150/388, train_loss: 0.2517, step time: 0.5169\n",
      "151/388, train_loss: 0.1244, step time: 0.5082\n",
      "152/388, train_loss: 0.1162, step time: 0.4865\n",
      "153/388, train_loss: 0.0731, step time: 0.4819\n",
      "154/388, train_loss: 0.1166, step time: 0.4900\n",
      "155/388, train_loss: 0.3832, step time: 0.4819\n",
      "156/388, train_loss: 0.3060, step time: 0.4973\n",
      "157/388, train_loss: 0.0933, step time: 0.4855\n",
      "158/388, train_loss: 0.1509, step time: 1.1936\n",
      "159/388, train_loss: 0.1070, step time: 0.5245\n",
      "160/388, train_loss: 0.1334, step time: 0.5014\n",
      "161/388, train_loss: 0.1616, step time: 0.4830\n",
      "162/388, train_loss: 0.1207, step time: 0.4840\n",
      "163/388, train_loss: 0.2730, step time: 0.4892\n",
      "164/388, train_loss: 0.0956, step time: 0.4769\n",
      "165/388, train_loss: 0.1579, step time: 0.4832\n",
      "166/388, train_loss: 0.0405, step time: 0.5065\n",
      "167/388, train_loss: 0.2480, step time: 0.5030\n",
      "168/388, train_loss: 0.1294, step time: 0.4816\n",
      "169/388, train_loss: 0.1395, step time: 0.5090\n",
      "170/388, train_loss: 0.2858, step time: 0.4960\n",
      "171/388, train_loss: 0.1512, step time: 0.4829\n",
      "172/388, train_loss: 0.1138, step time: 0.4913\n",
      "173/388, train_loss: 0.2400, step time: 0.4849\n",
      "174/388, train_loss: 0.5903, step time: 0.5050\n",
      "175/388, train_loss: 0.0713, step time: 0.4836\n",
      "176/388, train_loss: 0.1742, step time: 0.4978\n",
      "177/388, train_loss: 0.2624, step time: 0.4836\n",
      "178/388, train_loss: 0.1280, step time: 0.5090\n",
      "179/388, train_loss: 0.2472, step time: 0.5004\n",
      "180/388, train_loss: 0.2280, step time: 0.5032\n",
      "181/388, train_loss: 0.0581, step time: 0.5088\n",
      "182/388, train_loss: 0.1998, step time: 0.4976\n",
      "183/388, train_loss: 0.1718, step time: 0.4907\n",
      "184/388, train_loss: 0.3068, step time: 1.0989\n",
      "185/388, train_loss: 0.4171, step time: 0.5428\n",
      "186/388, train_loss: 0.2524, step time: 0.5072\n",
      "187/388, train_loss: 0.2546, step time: 0.4987\n",
      "188/388, train_loss: 0.1583, step time: 0.4848\n",
      "189/388, train_loss: 0.0983, step time: 0.4832\n",
      "190/388, train_loss: 0.0497, step time: 0.9080\n",
      "191/388, train_loss: 0.1733, step time: 0.5480\n",
      "192/388, train_loss: 0.2127, step time: 0.5116\n",
      "193/388, train_loss: 0.0492, step time: 0.4932\n",
      "194/388, train_loss: 0.4201, step time: 0.4977\n",
      "195/388, train_loss: 0.1856, step time: 0.4820\n",
      "196/388, train_loss: 0.0974, step time: 0.4919\n",
      "197/388, train_loss: 0.1175, step time: 0.4848\n",
      "198/388, train_loss: 0.1581, step time: 0.4935\n",
      "199/388, train_loss: 0.0969, step time: 0.4801\n",
      "200/388, train_loss: 0.1589, step time: 0.5091\n",
      "201/388, train_loss: 0.1605, step time: 0.5051\n",
      "202/388, train_loss: 0.3116, step time: 0.5137\n",
      "203/388, train_loss: 0.2315, step time: 0.5073\n",
      "204/388, train_loss: 0.2126, step time: 0.4896\n",
      "205/388, train_loss: 0.0614, step time: 0.4869\n",
      "206/388, train_loss: 0.1363, step time: 0.4892\n",
      "207/388, train_loss: 0.1507, step time: 0.4829\n",
      "208/388, train_loss: 0.1420, step time: 1.0748\n",
      "209/388, train_loss: 0.1212, step time: 0.5438\n",
      "210/388, train_loss: 0.2407, step time: 0.5149\n",
      "211/388, train_loss: 0.2168, step time: 0.5008\n",
      "212/388, train_loss: 0.0998, step time: 0.5308\n",
      "213/388, train_loss: 0.3159, step time: 0.5063\n",
      "214/388, train_loss: 0.1282, step time: 0.4898\n",
      "215/388, train_loss: 0.0868, step time: 0.4958\n",
      "216/388, train_loss: 0.1105, step time: 0.4949\n",
      "217/388, train_loss: 0.0904, step time: 0.5038\n",
      "218/388, train_loss: 0.1016, step time: 0.4869\n",
      "219/388, train_loss: 0.1297, step time: 0.4871\n",
      "220/388, train_loss: 0.0808, step time: 0.5212\n",
      "221/388, train_loss: 0.4851, step time: 0.5181\n",
      "222/388, train_loss: 0.0806, step time: 0.4927\n",
      "223/388, train_loss: 0.1633, step time: 0.4959\n",
      "224/388, train_loss: 0.2638, step time: 0.4788\n",
      "225/388, train_loss: 0.1889, step time: 0.5029\n",
      "226/388, train_loss: 0.2194, step time: 0.5078\n",
      "227/388, train_loss: 0.0813, step time: 0.5118\n",
      "228/388, train_loss: 0.0696, step time: 0.5017\n",
      "229/388, train_loss: 0.2721, step time: 0.4972\n",
      "230/388, train_loss: 0.2867, step time: 0.4771\n",
      "231/388, train_loss: 0.0710, step time: 0.4996\n",
      "232/388, train_loss: 0.2459, step time: 0.4925\n",
      "233/388, train_loss: 0.1762, step time: 1.1192\n",
      "234/388, train_loss: 0.1213, step time: 0.5440\n",
      "235/388, train_loss: 0.1557, step time: 0.5189\n",
      "236/388, train_loss: 0.2267, step time: 0.5133\n",
      "237/388, train_loss: 0.1968, step time: 0.4904\n",
      "238/388, train_loss: 0.1811, step time: 0.4892\n",
      "239/388, train_loss: 0.1376, step time: 0.5014\n",
      "240/388, train_loss: 0.0611, step time: 0.4824\n",
      "241/388, train_loss: 0.2128, step time: 0.4917\n",
      "242/388, train_loss: 0.1661, step time: 0.5145\n",
      "243/388, train_loss: 0.0916, step time: 0.5235\n",
      "244/388, train_loss: 0.0663, step time: 0.5035\n",
      "245/388, train_loss: 0.4269, step time: 0.5100\n",
      "246/388, train_loss: 0.0512, step time: 0.5344\n",
      "247/388, train_loss: 0.1553, step time: 0.5448\n",
      "248/388, train_loss: 0.2713, step time: 0.5287\n",
      "249/388, train_loss: 0.0573, step time: 0.5520\n",
      "250/388, train_loss: 0.1222, step time: 0.5300\n",
      "251/388, train_loss: 0.2371, step time: 0.5141\n",
      "252/388, train_loss: 0.1540, step time: 0.4961\n",
      "253/388, train_loss: 0.1533, step time: 0.4978\n",
      "254/388, train_loss: 0.2117, step time: 0.4936\n",
      "255/388, train_loss: 0.1240, step time: 0.5560\n",
      "256/388, train_loss: 0.1333, step time: 0.5471\n",
      "257/388, train_loss: 0.0813, step time: 0.5178\n",
      "258/388, train_loss: 0.1973, step time: 0.5059\n",
      "259/388, train_loss: 0.2404, step time: 0.4876\n",
      "260/388, train_loss: 0.2669, step time: 0.4912\n",
      "261/388, train_loss: 0.1626, step time: 0.4866\n",
      "262/388, train_loss: 0.0647, step time: 0.4946\n",
      "263/388, train_loss: 0.1867, step time: 1.1207\n",
      "264/388, train_loss: 0.0820, step time: 0.5338\n",
      "265/388, train_loss: 0.2489, step time: 0.5083\n",
      "266/388, train_loss: 0.0985, step time: 0.4825\n",
      "267/388, train_loss: 0.0649, step time: 0.4846\n",
      "268/388, train_loss: 0.1040, step time: 0.4902\n",
      "269/388, train_loss: 0.1392, step time: 0.4756\n",
      "270/388, train_loss: 0.2521, step time: 0.5087\n",
      "271/388, train_loss: 0.0565, step time: 0.5116\n",
      "272/388, train_loss: 0.1263, step time: 0.5488\n",
      "273/388, train_loss: 0.1556, step time: 0.5231\n",
      "274/388, train_loss: 0.2011, step time: 0.5073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/388, train_loss: 0.2076, step time: 0.5389\n",
      "276/388, train_loss: 0.2811, step time: 0.5441\n",
      "277/388, train_loss: 0.1102, step time: 0.4969\n",
      "278/388, train_loss: 0.2215, step time: 0.4942\n",
      "279/388, train_loss: 0.1927, step time: 0.4760\n",
      "280/388, train_loss: 0.2707, step time: 0.5417\n",
      "281/388, train_loss: 0.2015, step time: 0.5168\n",
      "282/388, train_loss: 0.0916, step time: 0.4998\n",
      "283/388, train_loss: 0.1077, step time: 0.5515\n",
      "284/388, train_loss: 0.1895, step time: 0.6578\n",
      "285/388, train_loss: 0.1230, step time: 0.5317\n",
      "286/388, train_loss: 0.1346, step time: 0.5078\n",
      "287/388, train_loss: 0.2355, step time: 0.4932\n",
      "288/388, train_loss: 0.0766, step time: 0.4943\n",
      "289/388, train_loss: 0.1790, step time: 0.5350\n",
      "290/388, train_loss: 0.1165, step time: 0.5067\n",
      "291/388, train_loss: 0.2648, step time: 0.9255\n",
      "292/388, train_loss: 0.1625, step time: 0.5489\n",
      "293/388, train_loss: 0.0777, step time: 0.5198\n",
      "294/388, train_loss: 0.0920, step time: 0.4942\n",
      "295/388, train_loss: 0.1572, step time: 0.4892\n",
      "296/388, train_loss: 0.1637, step time: 0.4959\n",
      "297/388, train_loss: 0.3002, step time: 0.4997\n",
      "298/388, train_loss: 0.0988, step time: 0.4943\n",
      "299/388, train_loss: 0.2352, step time: 0.4998\n",
      "300/388, train_loss: 0.1887, step time: 0.4923\n",
      "301/388, train_loss: 0.0977, step time: 0.5820\n",
      "302/388, train_loss: 0.1077, step time: 0.6024\n",
      "303/388, train_loss: 0.2225, step time: 0.5458\n",
      "304/388, train_loss: 0.1201, step time: 0.5128\n",
      "305/388, train_loss: 0.0772, step time: 0.4977\n",
      "306/388, train_loss: 0.1922, step time: 0.4954\n",
      "307/388, train_loss: 0.0732, step time: 0.5485\n",
      "308/388, train_loss: 0.0657, step time: 0.5257\n",
      "309/388, train_loss: 0.0785, step time: 0.4992\n",
      "310/388, train_loss: 0.2029, step time: 0.5475\n",
      "311/388, train_loss: 0.1296, step time: 0.5160\n",
      "312/388, train_loss: 0.2661, step time: 0.4991\n",
      "313/388, train_loss: 0.1949, step time: 0.4875\n",
      "314/388, train_loss: 0.3081, step time: 1.2178\n",
      "315/388, train_loss: 0.0693, step time: 0.5261\n",
      "316/388, train_loss: 0.2464, step time: 0.5123\n",
      "317/388, train_loss: 0.1344, step time: 0.4961\n",
      "318/388, train_loss: 0.2435, step time: 0.5003\n",
      "319/388, train_loss: 0.0765, step time: 0.4817\n",
      "320/388, train_loss: 0.2157, step time: 0.4824\n",
      "321/388, train_loss: 0.1726, step time: 0.5089\n",
      "322/388, train_loss: 0.0977, step time: 0.4909\n",
      "323/388, train_loss: 0.0681, step time: 0.4894\n",
      "324/388, train_loss: 0.0930, step time: 0.5104\n",
      "325/388, train_loss: 0.1192, step time: 0.6083\n",
      "326/388, train_loss: 0.2406, step time: 0.5455\n",
      "327/388, train_loss: 0.1711, step time: 0.5269\n",
      "328/388, train_loss: 0.1644, step time: 0.4930\n",
      "329/388, train_loss: 0.0827, step time: 0.4902\n",
      "330/388, train_loss: 0.1198, step time: 0.5050\n",
      "331/388, train_loss: 0.0839, step time: 0.4990\n",
      "332/388, train_loss: 0.1315, step time: 0.4920\n",
      "333/388, train_loss: 0.0528, step time: 0.5450\n",
      "334/388, train_loss: 0.3368, step time: 0.5251\n",
      "335/388, train_loss: 0.2130, step time: 0.5770\n",
      "336/388, train_loss: 0.5961, step time: 0.5563\n",
      "337/388, train_loss: 0.1467, step time: 0.5302\n",
      "338/388, train_loss: 0.3189, step time: 0.4998\n",
      "339/388, train_loss: 0.3130, step time: 0.5012\n",
      "340/388, train_loss: 0.1708, step time: 0.4892\n",
      "341/388, train_loss: 0.1078, step time: 1.1331\n",
      "342/388, train_loss: 0.1013, step time: 0.5613\n",
      "343/388, train_loss: 0.0496, step time: 0.5242\n",
      "344/388, train_loss: 0.3243, step time: 0.5115\n",
      "345/388, train_loss: 0.4329, step time: 0.4984\n",
      "346/388, train_loss: 0.0315, step time: 0.4816\n",
      "347/388, train_loss: 0.2117, step time: 0.4827\n",
      "348/388, train_loss: 0.1267, step time: 1.1815\n",
      "349/388, train_loss: 0.3469, step time: 0.5479\n",
      "350/388, train_loss: 0.2612, step time: 0.5102\n",
      "351/388, train_loss: 0.0353, step time: 0.4988\n",
      "352/388, train_loss: 0.2285, step time: 0.5004\n",
      "353/388, train_loss: 0.0795, step time: 0.4842\n",
      "354/388, train_loss: 0.2619, step time: 0.4996\n",
      "355/388, train_loss: 0.2235, step time: 0.4956\n",
      "356/388, train_loss: 0.0705, step time: 0.4971\n",
      "357/388, train_loss: 0.2293, step time: 0.5537\n",
      "358/388, train_loss: 0.1775, step time: 0.5859\n",
      "359/388, train_loss: 0.1463, step time: 0.5590\n",
      "360/388, train_loss: 0.1040, step time: 0.5222\n",
      "361/388, train_loss: 0.0717, step time: 0.5059\n",
      "362/388, train_loss: 0.1030, step time: 0.5528\n",
      "363/388, train_loss: 0.0776, step time: 0.6644\n",
      "364/388, train_loss: 0.1964, step time: 0.5398\n",
      "365/388, train_loss: 0.1036, step time: 0.5109\n",
      "366/388, train_loss: 0.1628, step time: 0.4943\n",
      "367/388, train_loss: 0.1161, step time: 0.5330\n",
      "368/388, train_loss: 0.3848, step time: 0.5316\n",
      "369/388, train_loss: 0.1819, step time: 0.5146\n",
      "370/388, train_loss: 0.0971, step time: 0.4999\n",
      "371/388, train_loss: 0.0839, step time: 0.4978\n",
      "372/388, train_loss: 0.3766, step time: 0.4866\n",
      "373/388, train_loss: 0.2354, step time: 0.5874\n",
      "374/388, train_loss: 0.2092, step time: 0.5462\n",
      "375/388, train_loss: 0.1912, step time: 0.5214\n",
      "376/388, train_loss: 0.1181, step time: 0.4937\n",
      "377/388, train_loss: 0.2505, step time: 0.5206\n",
      "378/388, train_loss: 0.1103, step time: 0.5005\n",
      "379/388, train_loss: 0.0918, step time: 0.4982\n",
      "380/388, train_loss: 0.3168, step time: 0.4909\n",
      "381/388, train_loss: 0.1830, step time: 1.1571\n",
      "382/388, train_loss: 0.1702, step time: 0.5466\n",
      "383/388, train_loss: 0.2310, step time: 0.5144\n",
      "384/388, train_loss: 0.1918, step time: 0.4949\n",
      "385/388, train_loss: 0.1796, step time: 0.5000\n",
      "386/388, train_loss: 0.0879, step time: 0.4988\n",
      "387/388, train_loss: 0.0320, step time: 0.4848\n",
      "388/388, train_loss: 0.2312, step time: 0.5335\n",
      "epoch 72 average loss: 0.1752\n",
      "current epoch: 72 current mean dice: 0.7712 tc: 0.8182 wt: 0.9011 et: 0.5943\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 72 is: 300.7582\n",
      "----------\n",
      "epoch 73/300\n",
      "1/388, train_loss: 0.1180, step time: 0.4799\n",
      "2/388, train_loss: 0.1773, step time: 0.5052\n",
      "3/388, train_loss: 0.1963, step time: 0.4997\n",
      "4/388, train_loss: 0.0790, step time: 0.5617\n",
      "5/388, train_loss: 0.1563, step time: 0.5067\n",
      "6/388, train_loss: 0.2133, step time: 0.4981\n",
      "7/388, train_loss: 0.1567, step time: 0.5293\n",
      "8/388, train_loss: 0.1196, step time: 0.5283\n",
      "9/388, train_loss: 0.1807, step time: 0.5137\n",
      "10/388, train_loss: 0.1439, step time: 0.5370\n",
      "11/388, train_loss: 0.1344, step time: 1.0036\n",
      "12/388, train_loss: 0.1732, step time: 0.5917\n",
      "13/388, train_loss: 0.2014, step time: 0.5214\n",
      "14/388, train_loss: 0.0673, step time: 0.5094\n",
      "15/388, train_loss: 0.2003, step time: 0.5039\n",
      "16/388, train_loss: 0.0983, step time: 0.5044\n",
      "17/388, train_loss: 0.2965, step time: 0.5012\n",
      "18/388, train_loss: 0.1947, step time: 0.6123\n",
      "19/388, train_loss: 0.0718, step time: 0.5931\n",
      "20/388, train_loss: 0.1421, step time: 0.5527\n",
      "21/388, train_loss: 0.0969, step time: 0.5196\n",
      "22/388, train_loss: 0.2653, step time: 0.5080\n",
      "23/388, train_loss: 0.2066, step time: 0.4938\n",
      "24/388, train_loss: 0.3795, step time: 1.1954\n",
      "25/388, train_loss: 0.1191, step time: 0.5471\n",
      "26/388, train_loss: 0.2284, step time: 0.5135\n",
      "27/388, train_loss: 0.2063, step time: 0.4899\n",
      "28/388, train_loss: 0.4699, step time: 0.5295\n",
      "29/388, train_loss: 0.0983, step time: 0.6428\n",
      "30/388, train_loss: 0.1238, step time: 0.5712\n",
      "31/388, train_loss: 0.1151, step time: 0.5283\n",
      "32/388, train_loss: 0.1281, step time: 0.5047\n",
      "33/388, train_loss: 0.0444, step time: 0.5049\n",
      "34/388, train_loss: 0.2194, step time: 0.4833\n",
      "35/388, train_loss: 0.0284, step time: 0.5129\n",
      "36/388, train_loss: 0.0802, step time: 0.5145\n",
      "37/388, train_loss: 0.2650, step time: 0.5158\n",
      "38/388, train_loss: 0.1007, step time: 0.5158\n",
      "39/388, train_loss: 0.1912, step time: 0.4868\n",
      "40/388, train_loss: 0.2880, step time: 0.5027\n",
      "41/388, train_loss: 0.2013, step time: 0.5554\n",
      "42/388, train_loss: 0.3699, step time: 0.5319\n",
      "43/388, train_loss: 0.0645, step time: 0.5407\n",
      "44/388, train_loss: 0.0623, step time: 0.5261\n",
      "45/388, train_loss: 0.2148, step time: 0.5291\n",
      "46/388, train_loss: 0.1410, step time: 0.5203\n",
      "47/388, train_loss: 0.1585, step time: 0.5140\n",
      "48/388, train_loss: 0.2801, step time: 0.5065\n",
      "49/388, train_loss: 0.2083, step time: 0.5026\n",
      "50/388, train_loss: 0.1018, step time: 1.0292\n",
      "51/388, train_loss: 0.1369, step time: 0.5399\n",
      "52/388, train_loss: 0.2594, step time: 0.5269\n",
      "53/388, train_loss: 0.0471, step time: 0.5020\n",
      "54/388, train_loss: 0.0718, step time: 0.5068\n",
      "55/388, train_loss: 0.1399, step time: 0.4878\n",
      "56/388, train_loss: 0.2901, step time: 0.4854\n",
      "57/388, train_loss: 0.1595, step time: 1.2105\n",
      "58/388, train_loss: 0.1495, step time: 0.5753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/388, train_loss: 0.1995, step time: 0.5540\n",
      "60/388, train_loss: 0.1705, step time: 0.5018\n",
      "61/388, train_loss: 0.0657, step time: 0.5071\n",
      "62/388, train_loss: 0.2458, step time: 0.4864\n",
      "63/388, train_loss: 0.0776, step time: 0.5345\n",
      "64/388, train_loss: 0.2010, step time: 0.5584\n",
      "65/388, train_loss: 0.1185, step time: 0.5138\n",
      "66/388, train_loss: 0.0523, step time: 0.5047\n",
      "67/388, train_loss: 0.1138, step time: 0.4870\n",
      "68/388, train_loss: 0.1887, step time: 0.8073\n",
      "69/388, train_loss: 0.1240, step time: 0.5536\n",
      "70/388, train_loss: 0.1254, step time: 0.5246\n",
      "71/388, train_loss: 0.1413, step time: 0.5042\n",
      "72/388, train_loss: 0.0417, step time: 0.4988\n",
      "73/388, train_loss: 0.1176, step time: 0.4818\n",
      "74/388, train_loss: 0.0874, step time: 1.0391\n",
      "75/388, train_loss: 0.2276, step time: 0.5510\n",
      "76/388, train_loss: 0.1013, step time: 0.5210\n",
      "77/388, train_loss: 0.1092, step time: 0.5006\n",
      "78/388, train_loss: 0.1688, step time: 0.5281\n",
      "79/388, train_loss: 0.1334, step time: 0.5416\n",
      "80/388, train_loss: 0.2112, step time: 0.5220\n",
      "81/388, train_loss: 0.1051, step time: 0.5064\n",
      "82/388, train_loss: 0.2152, step time: 0.4814\n",
      "83/388, train_loss: 0.0894, step time: 0.4792\n",
      "84/388, train_loss: 0.0439, step time: 0.4931\n",
      "85/388, train_loss: 0.1542, step time: 0.4866\n",
      "86/388, train_loss: 0.0902, step time: 0.4838\n",
      "87/388, train_loss: 0.1411, step time: 0.4934\n",
      "88/388, train_loss: 0.0444, step time: 1.1538\n",
      "89/388, train_loss: 0.3143, step time: 0.5374\n",
      "90/388, train_loss: 0.2474, step time: 0.5018\n",
      "91/388, train_loss: 0.2924, step time: 0.4904\n",
      "92/388, train_loss: 0.1738, step time: 0.4933\n",
      "93/388, train_loss: 0.6226, step time: 0.4828\n",
      "94/388, train_loss: 0.1823, step time: 0.8052\n",
      "95/388, train_loss: 0.1205, step time: 0.5457\n",
      "96/388, train_loss: 0.2124, step time: 0.5044\n",
      "97/388, train_loss: 0.2557, step time: 0.4963\n",
      "98/388, train_loss: 0.1655, step time: 0.5093\n",
      "99/388, train_loss: 0.1048, step time: 0.5011\n",
      "100/388, train_loss: 0.0597, step time: 0.4862\n",
      "101/388, train_loss: 0.0966, step time: 0.4859\n",
      "102/388, train_loss: 0.1807, step time: 0.4861\n",
      "103/388, train_loss: 0.1299, step time: 0.4784\n",
      "104/388, train_loss: 0.1528, step time: 0.4743\n",
      "105/388, train_loss: 0.0997, step time: 0.5177\n",
      "106/388, train_loss: 0.1049, step time: 0.5057\n",
      "107/388, train_loss: 0.1367, step time: 0.4982\n",
      "108/388, train_loss: 0.2257, step time: 1.1888\n",
      "109/388, train_loss: 0.1285, step time: 0.5378\n",
      "110/388, train_loss: 0.4436, step time: 0.5163\n",
      "111/388, train_loss: 0.2176, step time: 0.5085\n",
      "112/388, train_loss: 0.4324, step time: 0.4865\n",
      "113/388, train_loss: 0.1084, step time: 1.0913\n",
      "114/388, train_loss: 0.1597, step time: 0.5271\n",
      "115/388, train_loss: 0.2245, step time: 0.4937\n",
      "116/388, train_loss: 0.1243, step time: 0.4868\n",
      "117/388, train_loss: 0.1090, step time: 0.4944\n",
      "118/388, train_loss: 0.1026, step time: 0.4798\n",
      "119/388, train_loss: 0.3750, step time: 0.4906\n",
      "120/388, train_loss: 0.0781, step time: 0.5098\n",
      "121/388, train_loss: 0.3128, step time: 0.4907\n",
      "122/388, train_loss: 0.1526, step time: 0.4909\n",
      "123/388, train_loss: 0.0861, step time: 0.4797\n",
      "124/388, train_loss: 0.2205, step time: 0.5079\n",
      "125/388, train_loss: 0.0846, step time: 0.5181\n",
      "126/388, train_loss: 0.1048, step time: 0.5085\n",
      "127/388, train_loss: 0.2288, step time: 0.5021\n",
      "128/388, train_loss: 0.1442, step time: 0.4867\n",
      "129/388, train_loss: 0.0951, step time: 0.5033\n",
      "130/388, train_loss: 0.1155, step time: 0.4933\n",
      "131/388, train_loss: 0.1485, step time: 0.4876\n",
      "132/388, train_loss: 0.1586, step time: 0.4735\n",
      "133/388, train_loss: 0.2923, step time: 0.5215\n",
      "134/388, train_loss: 0.3916, step time: 0.7521\n",
      "135/388, train_loss: 0.1275, step time: 0.6008\n",
      "136/388, train_loss: 0.1527, step time: 0.5454\n",
      "137/388, train_loss: 0.0896, step time: 0.5180\n",
      "138/388, train_loss: 0.4041, step time: 0.4977\n",
      "139/388, train_loss: 0.3725, step time: 0.5016\n",
      "140/388, train_loss: 0.1396, step time: 0.4866\n",
      "141/388, train_loss: 0.3601, step time: 0.5302\n",
      "142/388, train_loss: 0.0962, step time: 0.5100\n",
      "143/388, train_loss: 0.1461, step time: 0.5089\n",
      "144/388, train_loss: 0.0334, step time: 0.4875\n",
      "145/388, train_loss: 0.0892, step time: 0.4863\n",
      "146/388, train_loss: 0.2288, step time: 0.4912\n",
      "147/388, train_loss: 0.1495, step time: 0.4848\n",
      "148/388, train_loss: 0.1654, step time: 1.1670\n",
      "149/388, train_loss: 0.1135, step time: 0.5376\n",
      "150/388, train_loss: 0.5671, step time: 0.5125\n",
      "151/388, train_loss: 0.0977, step time: 0.4909\n",
      "152/388, train_loss: 0.1432, step time: 0.4905\n",
      "153/388, train_loss: 0.4053, step time: 0.4890\n",
      "154/388, train_loss: 0.0893, step time: 0.4764\n",
      "155/388, train_loss: 0.0776, step time: 0.4760\n",
      "156/388, train_loss: 0.0859, step time: 0.4914\n",
      "157/388, train_loss: 0.1178, step time: 0.4865\n",
      "158/388, train_loss: 0.4232, step time: 1.1306\n",
      "159/388, train_loss: 0.2335, step time: 0.5335\n",
      "160/388, train_loss: 0.2377, step time: 0.5140\n",
      "161/388, train_loss: 0.0832, step time: 0.4826\n",
      "162/388, train_loss: 0.0828, step time: 0.4916\n",
      "163/388, train_loss: 0.0943, step time: 0.5016\n",
      "164/388, train_loss: 0.1346, step time: 0.5039\n",
      "165/388, train_loss: 0.0920, step time: 0.4893\n",
      "166/388, train_loss: 0.3099, step time: 0.4897\n",
      "167/388, train_loss: 0.4368, step time: 0.4949\n",
      "168/388, train_loss: 0.1514, step time: 0.4829\n",
      "169/388, train_loss: 0.1963, step time: 1.1194\n",
      "170/388, train_loss: 0.2680, step time: 0.5301\n",
      "171/388, train_loss: 0.0707, step time: 0.5048\n",
      "172/388, train_loss: 0.3972, step time: 0.4968\n",
      "173/388, train_loss: 0.3892, step time: 0.4833\n",
      "174/388, train_loss: 0.1223, step time: 0.5278\n",
      "175/388, train_loss: 0.0617, step time: 0.5012\n",
      "176/388, train_loss: 0.1029, step time: 0.5044\n",
      "177/388, train_loss: 0.1737, step time: 0.4827\n",
      "178/388, train_loss: 0.2547, step time: 0.4793\n",
      "179/388, train_loss: 0.1038, step time: 0.4995\n",
      "180/388, train_loss: 0.1409, step time: 0.4828\n",
      "181/388, train_loss: 0.1498, step time: 1.0223\n",
      "182/388, train_loss: 0.2850, step time: 0.5490\n",
      "183/388, train_loss: 0.2240, step time: 0.5169\n",
      "184/388, train_loss: 0.3216, step time: 0.5001\n",
      "185/388, train_loss: 0.0967, step time: 0.4906\n",
      "186/388, train_loss: 0.1125, step time: 0.5003\n",
      "187/388, train_loss: 0.1877, step time: 0.4956\n",
      "188/388, train_loss: 0.1009, step time: 0.4984\n",
      "189/388, train_loss: 0.3001, step time: 0.4965\n",
      "190/388, train_loss: 0.2531, step time: 0.9372\n",
      "191/388, train_loss: 0.1634, step time: 0.5263\n",
      "192/388, train_loss: 0.3939, step time: 0.5054\n",
      "193/388, train_loss: 0.0610, step time: 0.4987\n",
      "194/388, train_loss: 0.2464, step time: 0.4842\n",
      "195/388, train_loss: 0.3347, step time: 0.4986\n",
      "196/388, train_loss: 0.2655, step time: 0.5355\n",
      "197/388, train_loss: 0.2738, step time: 0.5164\n",
      "198/388, train_loss: 0.3420, step time: 0.4938\n",
      "199/388, train_loss: 0.4897, step time: 0.4905\n",
      "200/388, train_loss: 0.2064, step time: 1.1615\n",
      "201/388, train_loss: 0.0363, step time: 0.5157\n",
      "202/388, train_loss: 0.0793, step time: 0.5040\n",
      "203/388, train_loss: 0.1335, step time: 0.4864\n",
      "204/388, train_loss: 0.0858, step time: 0.4878\n",
      "205/388, train_loss: 0.1734, step time: 0.4878\n",
      "206/388, train_loss: 0.4137, step time: 0.4776\n",
      "207/388, train_loss: 0.1152, step time: 0.4994\n",
      "208/388, train_loss: 0.0991, step time: 0.4893\n",
      "209/388, train_loss: 0.1782, step time: 0.4957\n",
      "210/388, train_loss: 0.2032, step time: 0.4771\n",
      "211/388, train_loss: 0.1690, step time: 0.4722\n",
      "212/388, train_loss: 0.2368, step time: 0.4761\n",
      "213/388, train_loss: 0.2753, step time: 0.5004\n",
      "214/388, train_loss: 0.2622, step time: 0.5029\n",
      "215/388, train_loss: 0.2821, step time: 1.0596\n",
      "216/388, train_loss: 0.3288, step time: 0.5328\n",
      "217/388, train_loss: 0.1939, step time: 0.5087\n",
      "218/388, train_loss: 0.0993, step time: 0.4882\n",
      "219/388, train_loss: 0.3161, step time: 0.4917\n",
      "220/388, train_loss: 0.0810, step time: 0.4934\n",
      "221/388, train_loss: 0.1124, step time: 0.5110\n",
      "222/388, train_loss: 0.2853, step time: 0.4966\n",
      "223/388, train_loss: 0.1389, step time: 0.5156\n",
      "224/388, train_loss: 0.0997, step time: 0.4986\n",
      "225/388, train_loss: 0.4021, step time: 0.4948\n",
      "226/388, train_loss: 0.5811, step time: 0.4913\n",
      "227/388, train_loss: 0.1175, step time: 1.1675\n",
      "228/388, train_loss: 0.0906, step time: 0.5224\n",
      "229/388, train_loss: 0.0924, step time: 0.5105\n",
      "230/388, train_loss: 0.1158, step time: 0.4909\n",
      "231/388, train_loss: 0.0818, step time: 0.4955\n",
      "232/388, train_loss: 0.1969, step time: 0.4911\n",
      "233/388, train_loss: 0.0808, step time: 0.4839\n",
      "234/388, train_loss: 0.0552, step time: 0.4828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/388, train_loss: 0.0992, step time: 0.4871\n",
      "236/388, train_loss: 0.0655, step time: 0.4843\n",
      "237/388, train_loss: 0.1396, step time: 0.4909\n",
      "238/388, train_loss: 0.0548, step time: 0.4789\n",
      "239/388, train_loss: 0.0978, step time: 0.4792\n",
      "240/388, train_loss: 0.2564, step time: 1.0583\n",
      "241/388, train_loss: 0.3086, step time: 0.5498\n",
      "242/388, train_loss: 0.1984, step time: 0.5080\n",
      "243/388, train_loss: 0.2455, step time: 0.5001\n",
      "244/388, train_loss: 0.2514, step time: 0.4843\n",
      "245/388, train_loss: 0.1202, step time: 0.4885\n",
      "246/388, train_loss: 0.0941, step time: 0.4817\n",
      "247/388, train_loss: 0.5106, step time: 0.4773\n",
      "248/388, train_loss: 0.1452, step time: 0.4968\n",
      "249/388, train_loss: 0.1088, step time: 0.4793\n",
      "250/388, train_loss: 0.4620, step time: 0.4808\n",
      "251/388, train_loss: 0.1049, step time: 0.4862\n",
      "252/388, train_loss: 0.3290, step time: 0.4756\n",
      "253/388, train_loss: 0.1538, step time: 0.5422\n",
      "254/388, train_loss: 0.2000, step time: 0.5064\n",
      "255/388, train_loss: 0.0877, step time: 0.5068\n",
      "256/388, train_loss: 0.1717, step time: 0.4868\n",
      "257/388, train_loss: 0.0831, step time: 0.4943\n",
      "258/388, train_loss: 0.1563, step time: 0.4942\n",
      "259/388, train_loss: 0.1281, step time: 0.5011\n",
      "260/388, train_loss: 0.2889, step time: 0.5428\n",
      "261/388, train_loss: 0.2073, step time: 0.5271\n",
      "262/388, train_loss: 0.0893, step time: 0.4949\n",
      "263/388, train_loss: 0.0652, step time: 0.4981\n",
      "264/388, train_loss: 0.1433, step time: 0.4786\n",
      "265/388, train_loss: 0.2857, step time: 0.4853\n",
      "266/388, train_loss: 0.1227, step time: 0.4999\n",
      "267/388, train_loss: 0.0997, step time: 0.5065\n",
      "268/388, train_loss: 0.1343, step time: 0.5017\n",
      "269/388, train_loss: 0.1098, step time: 0.4831\n",
      "270/388, train_loss: 0.1104, step time: 0.5125\n",
      "271/388, train_loss: 0.0971, step time: 0.4922\n",
      "272/388, train_loss: 0.1585, step time: 0.5059\n",
      "273/388, train_loss: 0.2820, step time: 0.4845\n",
      "274/388, train_loss: 0.1679, step time: 0.4952\n",
      "275/388, train_loss: 0.2655, step time: 0.5029\n",
      "276/388, train_loss: 0.1512, step time: 0.5577\n",
      "277/388, train_loss: 0.0992, step time: 0.5364\n",
      "278/388, train_loss: 0.1602, step time: 0.5231\n",
      "279/388, train_loss: 0.3983, step time: 0.5013\n",
      "280/388, train_loss: 0.0925, step time: 0.4960\n",
      "281/388, train_loss: 0.5151, step time: 0.4809\n",
      "282/388, train_loss: 0.2566, step time: 0.5082\n",
      "283/388, train_loss: 0.2163, step time: 0.9778\n",
      "284/388, train_loss: 0.2070, step time: 0.5427\n",
      "285/388, train_loss: 0.2166, step time: 0.5182\n",
      "286/388, train_loss: 0.1353, step time: 0.5048\n",
      "287/388, train_loss: 0.2275, step time: 0.4960\n",
      "288/388, train_loss: 0.4838, step time: 0.4966\n",
      "289/388, train_loss: 0.0731, step time: 0.4874\n",
      "290/388, train_loss: 0.1602, step time: 1.2363\n",
      "291/388, train_loss: 0.2543, step time: 0.5388\n",
      "292/388, train_loss: 0.1508, step time: 0.5105\n",
      "293/388, train_loss: 0.2085, step time: 0.4910\n",
      "294/388, train_loss: 0.2105, step time: 0.4968\n",
      "295/388, train_loss: 0.2628, step time: 0.4778\n",
      "296/388, train_loss: 0.0609, step time: 0.4935\n",
      "297/388, train_loss: 0.2371, step time: 0.5159\n",
      "298/388, train_loss: 0.0891, step time: 0.5513\n",
      "299/388, train_loss: 0.0511, step time: 0.5178\n",
      "300/388, train_loss: 0.1347, step time: 0.5134\n",
      "301/388, train_loss: 0.1343, step time: 0.4978\n",
      "302/388, train_loss: 0.1531, step time: 0.5008\n",
      "303/388, train_loss: 0.2462, step time: 0.4836\n",
      "304/388, train_loss: 0.1962, step time: 0.4807\n",
      "305/388, train_loss: 0.0993, step time: 0.7117\n",
      "306/388, train_loss: 0.0725, step time: 0.5582\n",
      "307/388, train_loss: 0.2022, step time: 0.5186\n",
      "308/388, train_loss: 0.1266, step time: 0.5183\n",
      "309/388, train_loss: 0.4723, step time: 0.5861\n",
      "310/388, train_loss: 0.1348, step time: 0.5289\n",
      "311/388, train_loss: 0.1607, step time: 0.5003\n",
      "312/388, train_loss: 0.0908, step time: 0.4940\n",
      "313/388, train_loss: 0.3453, step time: 0.4911\n",
      "314/388, train_loss: 0.2840, step time: 0.4920\n",
      "315/388, train_loss: 0.0955, step time: 0.4722\n",
      "316/388, train_loss: 0.1654, step time: 0.4793\n",
      "317/388, train_loss: 0.0763, step time: 0.8406\n",
      "318/388, train_loss: 0.1502, step time: 0.5676\n",
      "319/388, train_loss: 0.2334, step time: 0.5290\n",
      "320/388, train_loss: 0.2897, step time: 0.5020\n",
      "321/388, train_loss: 0.0815, step time: 0.4911\n",
      "322/388, train_loss: 0.2932, step time: 0.4957\n",
      "323/388, train_loss: 0.1193, step time: 0.4825\n",
      "324/388, train_loss: 0.1970, step time: 0.4743\n",
      "325/388, train_loss: 0.2835, step time: 0.4775\n",
      "326/388, train_loss: 0.1068, step time: 0.4744\n",
      "327/388, train_loss: 0.1229, step time: 0.7051\n",
      "328/388, train_loss: 0.1110, step time: 0.5604\n",
      "329/388, train_loss: 0.1416, step time: 0.5173\n",
      "330/388, train_loss: 0.0718, step time: 0.5020\n",
      "331/388, train_loss: 0.0730, step time: 0.4997\n",
      "332/388, train_loss: 0.2183, step time: 0.4861\n",
      "333/388, train_loss: 0.2368, step time: 1.0251\n",
      "334/388, train_loss: 0.0785, step time: 0.5561\n",
      "335/388, train_loss: 0.2711, step time: 0.5271\n",
      "336/388, train_loss: 0.2063, step time: 0.4931\n",
      "337/388, train_loss: 0.1159, step time: 0.5005\n",
      "338/388, train_loss: 0.3516, step time: 0.4840\n",
      "339/388, train_loss: 0.1566, step time: 0.4977\n",
      "340/388, train_loss: 0.1730, step time: 0.5391\n",
      "341/388, train_loss: 0.1820, step time: 0.5154\n",
      "342/388, train_loss: 0.1637, step time: 0.4905\n",
      "343/388, train_loss: 0.0508, step time: 0.4973\n",
      "344/388, train_loss: 0.1893, step time: 0.4964\n",
      "345/388, train_loss: 0.2285, step time: 0.4915\n",
      "346/388, train_loss: 0.0617, step time: 0.4777\n",
      "347/388, train_loss: 0.2034, step time: 0.4846\n",
      "348/388, train_loss: 0.0557, step time: 0.4939\n",
      "349/388, train_loss: 0.2233, step time: 0.4872\n",
      "350/388, train_loss: 0.2042, step time: 0.4977\n",
      "351/388, train_loss: 0.2846, step time: 0.4851\n",
      "352/388, train_loss: 0.1068, step time: 1.0418\n",
      "353/388, train_loss: 0.0475, step time: 0.5244\n",
      "354/388, train_loss: 0.1281, step time: 0.5032\n",
      "355/388, train_loss: 0.1274, step time: 0.4959\n",
      "356/388, train_loss: 0.1085, step time: 0.4881\n",
      "357/388, train_loss: 0.4975, step time: 0.4911\n",
      "358/388, train_loss: 0.0920, step time: 0.4883\n",
      "359/388, train_loss: 0.0990, step time: 0.4859\n",
      "360/388, train_loss: 0.1170, step time: 0.4882\n",
      "361/388, train_loss: 0.1454, step time: 0.9712\n",
      "362/388, train_loss: 0.1727, step time: 0.5366\n",
      "363/388, train_loss: 0.0869, step time: 0.5124\n",
      "364/388, train_loss: 0.1610, step time: 0.4944\n",
      "365/388, train_loss: 0.4619, step time: 0.4970\n",
      "366/388, train_loss: 0.2567, step time: 0.4871\n",
      "367/388, train_loss: 0.1328, step time: 0.4973\n",
      "368/388, train_loss: 0.2427, step time: 0.5068\n",
      "369/388, train_loss: 0.1800, step time: 0.5602\n",
      "370/388, train_loss: 0.2453, step time: 0.5303\n",
      "371/388, train_loss: 0.2720, step time: 0.5037\n",
      "372/388, train_loss: 0.3652, step time: 0.5003\n",
      "373/388, train_loss: 0.1679, step time: 0.4924\n",
      "374/388, train_loss: 0.0694, step time: 0.5004\n",
      "375/388, train_loss: 0.1034, step time: 0.4819\n",
      "376/388, train_loss: 0.1878, step time: 1.0760\n",
      "377/388, train_loss: 0.2265, step time: 0.5372\n",
      "378/388, train_loss: 0.2581, step time: 0.5035\n",
      "379/388, train_loss: 0.0891, step time: 0.4992\n",
      "380/388, train_loss: 0.0907, step time: 0.4831\n",
      "381/388, train_loss: 0.1639, step time: 0.4957\n",
      "382/388, train_loss: 0.0594, step time: 0.5139\n",
      "383/388, train_loss: 0.0924, step time: 0.4915\n",
      "384/388, train_loss: 0.0992, step time: 0.4842\n",
      "385/388, train_loss: 0.1958, step time: 0.4749\n",
      "386/388, train_loss: 0.0700, step time: 0.4694\n",
      "387/388, train_loss: 0.1437, step time: 0.8680\n",
      "388/388, train_loss: 0.3258, step time: 0.5325\n",
      "epoch 73 average loss: 0.1807\n",
      "current epoch: 73 current mean dice: 0.7682 tc: 0.8202 wt: 0.8988 et: 0.5855\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 73 is: 301.4489\n",
      "----------\n",
      "epoch 74/300\n",
      "1/388, train_loss: 0.0700, step time: 0.4717\n",
      "2/388, train_loss: 0.1019, step time: 0.4895\n",
      "3/388, train_loss: 0.1287, step time: 0.7224\n",
      "4/388, train_loss: 0.0687, step time: 0.5440\n",
      "5/388, train_loss: 0.1140, step time: 0.5196\n",
      "6/388, train_loss: 0.2742, step time: 0.4974\n",
      "7/388, train_loss: 0.4164, step time: 0.5738\n",
      "8/388, train_loss: 0.1077, step time: 0.5884\n",
      "9/388, train_loss: 0.1600, step time: 0.5211\n",
      "10/388, train_loss: 0.1990, step time: 0.4988\n",
      "11/388, train_loss: 0.0807, step time: 0.7788\n",
      "12/388, train_loss: 0.0908, step time: 0.6286\n",
      "13/388, train_loss: 0.1086, step time: 0.5491\n",
      "14/388, train_loss: 0.1020, step time: 0.5169\n",
      "15/388, train_loss: 0.1478, step time: 0.5066\n",
      "16/388, train_loss: 0.1707, step time: 0.5201\n",
      "17/388, train_loss: 0.1438, step time: 0.5221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/388, train_loss: 0.1385, step time: 0.5212\n",
      "19/388, train_loss: 0.3062, step time: 0.5092\n",
      "20/388, train_loss: 0.0673, step time: 1.2156\n",
      "21/388, train_loss: 0.1998, step time: 0.5543\n",
      "22/388, train_loss: 0.1652, step time: 0.5103\n",
      "23/388, train_loss: 0.1629, step time: 0.5055\n",
      "24/388, train_loss: 0.1201, step time: 0.4882\n",
      "25/388, train_loss: 0.2370, step time: 0.4948\n",
      "26/388, train_loss: 0.1778, step time: 0.5778\n",
      "27/388, train_loss: 0.1007, step time: 0.5295\n",
      "28/388, train_loss: 0.1149, step time: 0.4989\n",
      "29/388, train_loss: 0.0615, step time: 0.7425\n",
      "30/388, train_loss: 0.1992, step time: 0.5788\n",
      "31/388, train_loss: 0.0842, step time: 0.5212\n",
      "32/388, train_loss: 0.1330, step time: 0.4938\n",
      "33/388, train_loss: 0.1784, step time: 0.5156\n",
      "34/388, train_loss: 0.1664, step time: 0.5280\n",
      "35/388, train_loss: 0.1556, step time: 0.6154\n",
      "36/388, train_loss: 0.2106, step time: 0.5515\n",
      "37/388, train_loss: 0.0867, step time: 0.5219\n",
      "38/388, train_loss: 0.1280, step time: 0.5186\n",
      "39/388, train_loss: 0.0720, step time: 0.5450\n",
      "40/388, train_loss: 0.1034, step time: 0.5184\n",
      "41/388, train_loss: 0.1978, step time: 0.5959\n",
      "42/388, train_loss: 0.3578, step time: 0.5341\n",
      "43/388, train_loss: 0.2352, step time: 0.4984\n",
      "44/388, train_loss: 0.1445, step time: 0.4966\n",
      "45/388, train_loss: 0.1287, step time: 1.2470\n",
      "46/388, train_loss: 0.2391, step time: 0.5299\n",
      "47/388, train_loss: 0.0989, step time: 0.5218\n",
      "48/388, train_loss: 0.1337, step time: 0.5021\n",
      "49/388, train_loss: 0.1214, step time: 0.4941\n",
      "50/388, train_loss: 0.1809, step time: 0.4782\n",
      "51/388, train_loss: 0.1286, step time: 0.5015\n",
      "52/388, train_loss: 0.1985, step time: 0.6380\n",
      "53/388, train_loss: 0.0743, step time: 0.5708\n",
      "54/388, train_loss: 0.1381, step time: 0.5180\n",
      "55/388, train_loss: 0.1746, step time: 0.4935\n",
      "56/388, train_loss: 0.0577, step time: 0.9721\n",
      "57/388, train_loss: 0.0916, step time: 0.5304\n",
      "58/388, train_loss: 0.2757, step time: 0.5005\n",
      "59/388, train_loss: 0.1724, step time: 0.4894\n",
      "60/388, train_loss: 0.2786, step time: 0.5114\n",
      "61/388, train_loss: 0.1351, step time: 0.4849\n",
      "62/388, train_loss: 0.1660, step time: 0.4899\n",
      "63/388, train_loss: 0.1064, step time: 0.5011\n",
      "64/388, train_loss: 0.3077, step time: 0.8399\n",
      "65/388, train_loss: 0.0858, step time: 0.5615\n",
      "66/388, train_loss: 0.2009, step time: 0.5207\n",
      "67/388, train_loss: 0.3037, step time: 0.4905\n",
      "68/388, train_loss: 0.1603, step time: 0.6933\n",
      "69/388, train_loss: 0.0702, step time: 0.5382\n",
      "70/388, train_loss: 0.1660, step time: 0.5338\n",
      "71/388, train_loss: 0.0355, step time: 0.5077\n",
      "72/388, train_loss: 0.0910, step time: 0.4898\n",
      "73/388, train_loss: 0.0729, step time: 0.4841\n",
      "74/388, train_loss: 0.2069, step time: 0.4935\n",
      "75/388, train_loss: 0.0917, step time: 0.5017\n",
      "76/388, train_loss: 0.0966, step time: 0.4962\n",
      "77/388, train_loss: 0.2238, step time: 0.4821\n",
      "78/388, train_loss: 0.2450, step time: 0.9270\n",
      "79/388, train_loss: 0.0570, step time: 0.5720\n",
      "80/388, train_loss: 0.1285, step time: 0.5177\n",
      "81/388, train_loss: 0.2480, step time: 0.5038\n",
      "82/388, train_loss: 0.5062, step time: 0.4875\n",
      "83/388, train_loss: 0.1077, step time: 0.4958\n",
      "84/388, train_loss: 0.1443, step time: 0.4808\n",
      "85/388, train_loss: 0.2169, step time: 0.4964\n",
      "86/388, train_loss: 0.1140, step time: 0.4906\n",
      "87/388, train_loss: 0.1910, step time: 1.1212\n",
      "88/388, train_loss: 0.2296, step time: 0.5346\n",
      "89/388, train_loss: 0.2276, step time: 0.5147\n",
      "90/388, train_loss: 0.1582, step time: 0.4817\n",
      "91/388, train_loss: 0.2183, step time: 0.4890\n",
      "92/388, train_loss: 0.0712, step time: 0.4834\n",
      "93/388, train_loss: 0.0904, step time: 1.1664\n",
      "94/388, train_loss: 0.0930, step time: 0.5336\n",
      "95/388, train_loss: 0.0967, step time: 0.5046\n",
      "96/388, train_loss: 0.1764, step time: 0.4996\n",
      "97/388, train_loss: 0.2048, step time: 0.4874\n",
      "98/388, train_loss: 0.1097, step time: 0.4959\n",
      "99/388, train_loss: 0.2190, step time: 0.4981\n",
      "100/388, train_loss: 0.1320, step time: 0.4931\n",
      "101/388, train_loss: 0.3393, step time: 0.5056\n",
      "102/388, train_loss: 0.1531, step time: 0.4946\n",
      "103/388, train_loss: 0.2274, step time: 0.4959\n",
      "104/388, train_loss: 0.0655, step time: 0.9537\n",
      "105/388, train_loss: 0.1984, step time: 0.5538\n",
      "106/388, train_loss: 0.2253, step time: 0.5208\n",
      "107/388, train_loss: 0.1740, step time: 0.5056\n",
      "108/388, train_loss: 0.1788, step time: 0.4867\n",
      "109/388, train_loss: 0.2530, step time: 0.4795\n",
      "110/388, train_loss: 0.0837, step time: 0.4765\n",
      "111/388, train_loss: 0.1174, step time: 0.6572\n",
      "112/388, train_loss: 0.2368, step time: 0.5441\n",
      "113/388, train_loss: 0.3965, step time: 0.5156\n",
      "114/388, train_loss: 0.1652, step time: 0.4880\n",
      "115/388, train_loss: 0.3738, step time: 0.5143\n",
      "116/388, train_loss: 0.0506, step time: 0.5942\n",
      "117/388, train_loss: 0.1306, step time: 0.5590\n",
      "118/388, train_loss: 0.1419, step time: 0.5110\n",
      "119/388, train_loss: 0.0929, step time: 0.4881\n",
      "120/388, train_loss: 0.1284, step time: 0.4998\n",
      "121/388, train_loss: 0.0616, step time: 0.5043\n",
      "122/388, train_loss: 0.2699, step time: 0.5509\n",
      "123/388, train_loss: 0.1347, step time: 0.5187\n",
      "124/388, train_loss: 0.0907, step time: 0.6137\n",
      "125/388, train_loss: 0.0463, step time: 0.5547\n",
      "126/388, train_loss: 0.1537, step time: 0.5310\n",
      "127/388, train_loss: 0.1734, step time: 0.5107\n",
      "128/388, train_loss: 0.1284, step time: 0.4875\n",
      "129/388, train_loss: 0.1582, step time: 0.5252\n",
      "130/388, train_loss: 0.2300, step time: 0.4979\n",
      "131/388, train_loss: 0.0961, step time: 0.5226\n",
      "132/388, train_loss: 0.0845, step time: 0.5324\n",
      "133/388, train_loss: 0.2126, step time: 0.5341\n",
      "134/388, train_loss: 0.0915, step time: 0.5139\n",
      "135/388, train_loss: 0.0831, step time: 0.5027\n",
      "136/388, train_loss: 0.0702, step time: 0.4811\n",
      "137/388, train_loss: 0.1558, step time: 0.5143\n",
      "138/388, train_loss: 0.1516, step time: 0.5231\n",
      "139/388, train_loss: 0.1059, step time: 0.5094\n",
      "140/388, train_loss: 0.1224, step time: 0.5031\n",
      "141/388, train_loss: 0.0843, step time: 0.4901\n",
      "142/388, train_loss: 0.0961, step time: 0.4897\n",
      "143/388, train_loss: 0.1462, step time: 0.8001\n",
      "144/388, train_loss: 0.0666, step time: 0.5546\n",
      "145/388, train_loss: 0.2408, step time: 0.5290\n",
      "146/388, train_loss: 0.1878, step time: 0.5104\n",
      "147/388, train_loss: 0.1010, step time: 0.5470\n",
      "148/388, train_loss: 0.0689, step time: 0.5263\n",
      "149/388, train_loss: 0.1768, step time: 0.5142\n",
      "150/388, train_loss: 0.1880, step time: 0.5054\n",
      "151/388, train_loss: 0.1033, step time: 0.4915\n",
      "152/388, train_loss: 0.1340, step time: 0.4966\n",
      "153/388, train_loss: 0.2831, step time: 0.4813\n",
      "154/388, train_loss: 0.1083, step time: 1.1159\n",
      "155/388, train_loss: 0.1731, step time: 0.5428\n",
      "156/388, train_loss: 0.2183, step time: 0.5241\n",
      "157/388, train_loss: 0.1866, step time: 0.5003\n",
      "158/388, train_loss: 0.0728, step time: 0.5004\n",
      "159/388, train_loss: 0.2899, step time: 0.5051\n",
      "160/388, train_loss: 0.1420, step time: 0.5556\n",
      "161/388, train_loss: 0.1430, step time: 0.5389\n",
      "162/388, train_loss: 0.0722, step time: 0.5088\n",
      "163/388, train_loss: 0.1674, step time: 0.5369\n",
      "164/388, train_loss: 0.0981, step time: 0.5251\n",
      "165/388, train_loss: 0.2689, step time: 0.5001\n",
      "166/388, train_loss: 0.1361, step time: 0.4975\n",
      "167/388, train_loss: 0.2148, step time: 0.4936\n",
      "168/388, train_loss: 0.0915, step time: 0.4990\n",
      "169/388, train_loss: 0.0934, step time: 0.4979\n",
      "170/388, train_loss: 0.1959, step time: 0.4829\n",
      "171/388, train_loss: 0.2016, step time: 0.4922\n",
      "172/388, train_loss: 0.1521, step time: 0.4904\n",
      "173/388, train_loss: 0.0879, step time: 0.4883\n",
      "174/388, train_loss: 0.5267, step time: 0.4894\n",
      "175/388, train_loss: 0.6279, step time: 1.1092\n",
      "176/388, train_loss: 0.3303, step time: 0.5434\n",
      "177/388, train_loss: 0.2226, step time: 0.5165\n",
      "178/388, train_loss: 0.0629, step time: 0.5326\n",
      "179/388, train_loss: 0.2500, step time: 0.5187\n",
      "180/388, train_loss: 0.1344, step time: 0.5199\n",
      "181/388, train_loss: 0.2307, step time: 0.5076\n",
      "182/388, train_loss: 0.1624, step time: 0.5144\n",
      "183/388, train_loss: 0.0941, step time: 0.5564\n",
      "184/388, train_loss: 0.3160, step time: 0.5368\n",
      "185/388, train_loss: 0.0470, step time: 0.5009\n",
      "186/388, train_loss: 0.0723, step time: 0.4969\n",
      "187/388, train_loss: 0.0840, step time: 0.4830\n",
      "188/388, train_loss: 0.2075, step time: 1.0609\n",
      "189/388, train_loss: 0.4421, step time: 0.5310\n",
      "190/388, train_loss: 0.0619, step time: 0.5105\n",
      "191/388, train_loss: 0.1172, step time: 0.4871\n",
      "192/388, train_loss: 0.2334, step time: 0.4938\n",
      "193/388, train_loss: 0.0974, step time: 0.4773\n",
      "194/388, train_loss: 0.1571, step time: 0.4803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/388, train_loss: 0.0694, step time: 1.0014\n",
      "196/388, train_loss: 0.2820, step time: 0.5417\n",
      "197/388, train_loss: 0.2995, step time: 0.5301\n",
      "198/388, train_loss: 0.1214, step time: 0.4998\n",
      "199/388, train_loss: 0.2864, step time: 0.4962\n",
      "200/388, train_loss: 0.1968, step time: 0.5004\n",
      "201/388, train_loss: 0.2673, step time: 0.5168\n",
      "202/388, train_loss: 0.1155, step time: 0.5093\n",
      "203/388, train_loss: 0.3315, step time: 0.5014\n",
      "204/388, train_loss: 0.2167, step time: 0.4870\n",
      "205/388, train_loss: 0.1513, step time: 0.4945\n",
      "206/388, train_loss: 0.2066, step time: 0.4919\n",
      "207/388, train_loss: 0.1739, step time: 0.4963\n",
      "208/388, train_loss: 0.1763, step time: 0.4855\n",
      "209/388, train_loss: 0.1613, step time: 1.1535\n",
      "210/388, train_loss: 0.2723, step time: 0.5307\n",
      "211/388, train_loss: 0.2807, step time: 0.5012\n",
      "212/388, train_loss: 0.0607, step time: 0.4944\n",
      "213/388, train_loss: 0.1663, step time: 0.4880\n",
      "214/388, train_loss: 0.1229, step time: 0.4892\n",
      "215/388, train_loss: 0.1155, step time: 1.0306\n",
      "216/388, train_loss: 0.5463, step time: 0.5309\n",
      "217/388, train_loss: 0.1628, step time: 0.5039\n",
      "218/388, train_loss: 0.1502, step time: 0.4907\n",
      "219/388, train_loss: 0.1502, step time: 0.4944\n",
      "220/388, train_loss: 0.1370, step time: 0.4807\n",
      "221/388, train_loss: 0.0898, step time: 0.4779\n",
      "222/388, train_loss: 0.3817, step time: 0.4868\n",
      "223/388, train_loss: 0.2564, step time: 0.4964\n",
      "224/388, train_loss: 0.1089, step time: 1.1216\n",
      "225/388, train_loss: 0.3582, step time: 0.5550\n",
      "226/388, train_loss: 0.4523, step time: 0.5179\n",
      "227/388, train_loss: 0.1230, step time: 0.4971\n",
      "228/388, train_loss: 0.3063, step time: 0.4921\n",
      "229/388, train_loss: 0.2891, step time: 0.4792\n",
      "230/388, train_loss: 0.0466, step time: 0.4827\n",
      "231/388, train_loss: 0.1479, step time: 0.4777\n",
      "232/388, train_loss: 0.1079, step time: 0.4783\n",
      "233/388, train_loss: 0.3218, step time: 0.4791\n",
      "234/388, train_loss: 0.0678, step time: 0.5735\n",
      "235/388, train_loss: 0.1325, step time: 0.5400\n",
      "236/388, train_loss: 0.0851, step time: 0.4995\n",
      "237/388, train_loss: 0.2797, step time: 0.5013\n",
      "238/388, train_loss: 0.1839, step time: 0.4907\n",
      "239/388, train_loss: 0.0998, step time: 0.4852\n",
      "240/388, train_loss: 0.1754, step time: 0.4904\n",
      "241/388, train_loss: 0.1628, step time: 1.1490\n",
      "242/388, train_loss: 0.1055, step time: 0.5299\n",
      "243/388, train_loss: 0.1173, step time: 0.5055\n",
      "244/388, train_loss: 0.1257, step time: 0.4851\n",
      "245/388, train_loss: 0.3184, step time: 0.4855\n",
      "246/388, train_loss: 0.3248, step time: 0.4903\n",
      "247/388, train_loss: 0.0310, step time: 0.8372\n",
      "248/388, train_loss: 0.0822, step time: 0.5505\n",
      "249/388, train_loss: 0.1549, step time: 0.5131\n",
      "250/388, train_loss: 0.1848, step time: 0.4982\n",
      "251/388, train_loss: 0.0971, step time: 0.4840\n",
      "252/388, train_loss: 0.2439, step time: 0.4881\n",
      "253/388, train_loss: 0.0758, step time: 0.5036\n",
      "254/388, train_loss: 0.0905, step time: 0.4831\n",
      "255/388, train_loss: 0.0985, step time: 0.4910\n",
      "256/388, train_loss: 0.2600, step time: 0.4836\n",
      "257/388, train_loss: 0.2101, step time: 0.4831\n",
      "258/388, train_loss: 0.1072, step time: 0.5004\n",
      "259/388, train_loss: 0.3437, step time: 0.5521\n",
      "260/388, train_loss: 0.0675, step time: 0.5211\n",
      "261/388, train_loss: 0.2624, step time: 0.5134\n",
      "262/388, train_loss: 0.3125, step time: 0.4987\n",
      "263/388, train_loss: 0.1246, step time: 1.1379\n",
      "264/388, train_loss: 0.0753, step time: 0.5418\n",
      "265/388, train_loss: 0.2099, step time: 0.5015\n",
      "266/388, train_loss: 0.2742, step time: 0.4916\n",
      "267/388, train_loss: 0.1571, step time: 0.4823\n",
      "268/388, train_loss: 0.1103, step time: 0.4981\n",
      "269/388, train_loss: 0.0866, step time: 0.4920\n",
      "270/388, train_loss: 0.0437, step time: 0.4910\n",
      "271/388, train_loss: 0.1840, step time: 0.4806\n",
      "272/388, train_loss: 0.1921, step time: 0.4818\n",
      "273/388, train_loss: 0.1998, step time: 0.8910\n",
      "274/388, train_loss: 0.1024, step time: 0.5301\n",
      "275/388, train_loss: 0.1209, step time: 0.4964\n",
      "276/388, train_loss: 0.0885, step time: 0.4942\n",
      "277/388, train_loss: 0.0912, step time: 0.4913\n",
      "278/388, train_loss: 0.1412, step time: 0.5178\n",
      "279/388, train_loss: 0.2714, step time: 0.5018\n",
      "280/388, train_loss: 0.2802, step time: 0.5018\n",
      "281/388, train_loss: 0.1982, step time: 0.5299\n",
      "282/388, train_loss: 0.4031, step time: 0.5057\n",
      "283/388, train_loss: 0.0891, step time: 0.5113\n",
      "284/388, train_loss: 0.4027, step time: 0.5049\n",
      "285/388, train_loss: 0.1282, step time: 0.4927\n",
      "286/388, train_loss: 0.0662, step time: 0.5112\n",
      "287/388, train_loss: 0.2211, step time: 0.5058\n",
      "288/388, train_loss: 0.1491, step time: 0.4891\n",
      "289/388, train_loss: 0.0642, step time: 0.4873\n",
      "290/388, train_loss: 0.0788, step time: 0.4866\n",
      "291/388, train_loss: 0.1897, step time: 1.0222\n",
      "292/388, train_loss: 0.0818, step time: 0.5326\n",
      "293/388, train_loss: 0.1052, step time: 0.5162\n",
      "294/388, train_loss: 0.2768, step time: 0.4841\n",
      "295/388, train_loss: 0.2104, step time: 0.5378\n",
      "296/388, train_loss: 0.1326, step time: 0.4978\n",
      "297/388, train_loss: 0.1930, step time: 0.5037\n",
      "298/388, train_loss: 0.3255, step time: 0.4862\n",
      "299/388, train_loss: 0.1077, step time: 0.4861\n",
      "300/388, train_loss: 0.2799, step time: 0.4911\n",
      "301/388, train_loss: 0.2708, step time: 0.4941\n",
      "302/388, train_loss: 0.2119, step time: 0.6970\n",
      "303/388, train_loss: 0.1866, step time: 0.5473\n",
      "304/388, train_loss: 0.1962, step time: 0.5214\n",
      "305/388, train_loss: 0.1221, step time: 0.5032\n",
      "306/388, train_loss: 0.1124, step time: 0.4892\n",
      "307/388, train_loss: 0.2851, step time: 0.4957\n",
      "308/388, train_loss: 0.2100, step time: 0.5244\n",
      "309/388, train_loss: 0.2825, step time: 0.5110\n",
      "310/388, train_loss: 0.2619, step time: 0.5010\n",
      "311/388, train_loss: 0.3097, step time: 0.4833\n",
      "312/388, train_loss: 0.4267, step time: 0.9923\n",
      "313/388, train_loss: 0.1281, step time: 0.5247\n",
      "314/388, train_loss: 0.4687, step time: 0.5034\n",
      "315/388, train_loss: 0.1499, step time: 0.4919\n",
      "316/388, train_loss: 0.2869, step time: 0.4885\n",
      "317/388, train_loss: 0.1558, step time: 0.4927\n",
      "318/388, train_loss: 0.1345, step time: 1.0136\n",
      "319/388, train_loss: 0.2873, step time: 0.5601\n",
      "320/388, train_loss: 0.2788, step time: 0.5216\n",
      "321/388, train_loss: 0.2507, step time: 0.5008\n",
      "322/388, train_loss: 0.1086, step time: 0.4891\n",
      "323/388, train_loss: 0.2108, step time: 0.4958\n",
      "324/388, train_loss: 0.0544, step time: 0.4924\n",
      "325/388, train_loss: 0.1093, step time: 0.4949\n",
      "326/388, train_loss: 0.1357, step time: 0.4811\n",
      "327/388, train_loss: 0.0734, step time: 0.4961\n",
      "328/388, train_loss: 0.1803, step time: 0.4971\n",
      "329/388, train_loss: 0.1978, step time: 0.5084\n",
      "330/388, train_loss: 0.1515, step time: 0.5071\n",
      "331/388, train_loss: 0.2978, step time: 0.6495\n",
      "332/388, train_loss: 0.2622, step time: 0.5593\n",
      "333/388, train_loss: 0.2782, step time: 0.5223\n",
      "334/388, train_loss: 0.3306, step time: 0.4952\n",
      "335/388, train_loss: 0.3809, step time: 0.5041\n",
      "336/388, train_loss: 0.1144, step time: 0.4847\n",
      "337/388, train_loss: 0.1388, step time: 0.4858\n",
      "338/388, train_loss: 0.1192, step time: 0.8172\n",
      "339/388, train_loss: 0.0839, step time: 0.5566\n",
      "340/388, train_loss: 0.1980, step time: 0.5205\n",
      "341/388, train_loss: 0.2101, step time: 0.4991\n",
      "342/388, train_loss: 0.2230, step time: 0.5014\n",
      "343/388, train_loss: 0.2176, step time: 0.4862\n",
      "344/388, train_loss: 0.0511, step time: 0.4911\n",
      "345/388, train_loss: 0.2607, step time: 0.4970\n",
      "346/388, train_loss: 0.2348, step time: 0.4982\n",
      "347/388, train_loss: 0.2276, step time: 0.4913\n",
      "348/388, train_loss: 0.1641, step time: 0.5083\n",
      "349/388, train_loss: 0.1151, step time: 0.5035\n",
      "350/388, train_loss: 0.0669, step time: 0.4935\n",
      "351/388, train_loss: 0.0499, step time: 0.5080\n",
      "352/388, train_loss: 0.1008, step time: 0.5012\n",
      "353/388, train_loss: 0.1296, step time: 0.4832\n",
      "354/388, train_loss: 0.0995, step time: 0.5147\n",
      "355/388, train_loss: 0.1832, step time: 0.4890\n",
      "356/388, train_loss: 0.0972, step time: 0.4967\n",
      "357/388, train_loss: 0.3687, step time: 0.4856\n",
      "358/388, train_loss: 0.1829, step time: 0.5018\n",
      "359/388, train_loss: 0.1727, step time: 0.4825\n",
      "360/388, train_loss: 0.1195, step time: 0.4888\n",
      "361/388, train_loss: 0.1042, step time: 0.4769\n",
      "362/388, train_loss: 0.1942, step time: 1.1051\n",
      "363/388, train_loss: 0.1042, step time: 0.5461\n",
      "364/388, train_loss: 0.0995, step time: 0.5206\n",
      "365/388, train_loss: 0.1889, step time: 0.5071\n",
      "366/388, train_loss: 0.1805, step time: 0.4972\n",
      "367/388, train_loss: 0.2224, step time: 0.4959\n",
      "368/388, train_loss: 0.1049, step time: 0.4775\n",
      "369/388, train_loss: 0.1792, step time: 0.4924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/388, train_loss: 0.2494, step time: 0.8128\n",
      "371/388, train_loss: 0.1483, step time: 0.5379\n",
      "372/388, train_loss: 0.5580, step time: 0.5141\n",
      "373/388, train_loss: 0.2188, step time: 0.4962\n",
      "374/388, train_loss: 0.0306, step time: 0.5486\n",
      "375/388, train_loss: 0.1424, step time: 0.5173\n",
      "376/388, train_loss: 0.2809, step time: 0.4872\n",
      "377/388, train_loss: 0.0944, step time: 0.4941\n",
      "378/388, train_loss: 0.0867, step time: 0.5004\n",
      "379/388, train_loss: 0.3258, step time: 0.4769\n",
      "380/388, train_loss: 0.1783, step time: 1.1272\n",
      "381/388, train_loss: 0.3817, step time: 0.5401\n",
      "382/388, train_loss: 0.1369, step time: 0.5171\n",
      "383/388, train_loss: 0.2747, step time: 0.4932\n",
      "384/388, train_loss: 0.2133, step time: 0.4865\n",
      "385/388, train_loss: 0.0337, step time: 0.5123\n",
      "386/388, train_loss: 0.0935, step time: 0.4993\n",
      "387/388, train_loss: 0.2561, step time: 0.4874\n",
      "388/388, train_loss: 0.3861, step time: 0.4801\n",
      "epoch 74 average loss: 0.1759\n",
      "current epoch: 74 current mean dice: 0.7707 tc: 0.8204 wt: 0.8998 et: 0.5918\n",
      "best mean dice: 0.7717 at epoch: 67\n",
      "time consuming of epoch 74 is: 302.5297\n",
      "----------\n",
      "epoch 75/300\n",
      "1/388, train_loss: 0.1914, step time: 0.4765\n",
      "2/388, train_loss: 0.0968, step time: 0.4796\n",
      "3/388, train_loss: 0.0690, step time: 0.4727\n",
      "4/388, train_loss: 0.1660, step time: 0.9887\n",
      "5/388, train_loss: 0.1740, step time: 0.5399\n",
      "6/388, train_loss: 0.1414, step time: 0.5316\n",
      "7/388, train_loss: 0.1478, step time: 0.5579\n",
      "8/388, train_loss: 0.1878, step time: 0.5126\n",
      "9/388, train_loss: 0.2663, step time: 0.4944\n",
      "10/388, train_loss: 0.1821, step time: 0.5136\n",
      "11/388, train_loss: 0.2645, step time: 0.5916\n",
      "12/388, train_loss: 0.3494, step time: 0.6352\n",
      "13/388, train_loss: 0.2108, step time: 0.6191\n",
      "14/388, train_loss: 0.2714, step time: 0.5312\n",
      "15/388, train_loss: 0.0902, step time: 0.5262\n",
      "16/388, train_loss: 0.0966, step time: 0.5134\n",
      "17/388, train_loss: 0.2657, step time: 0.5141\n",
      "18/388, train_loss: 0.0775, step time: 0.4941\n",
      "19/388, train_loss: 0.4816, step time: 0.5793\n",
      "20/388, train_loss: 0.1528, step time: 0.6062\n",
      "21/388, train_loss: 0.0599, step time: 0.5274\n",
      "22/388, train_loss: 0.3330, step time: 0.4993\n",
      "23/388, train_loss: 0.2620, step time: 0.5025\n",
      "24/388, train_loss: 0.1952, step time: 0.4896\n",
      "25/388, train_loss: 0.1406, step time: 0.7627\n",
      "26/388, train_loss: 0.0603, step time: 0.5591\n",
      "27/388, train_loss: 0.3039, step time: 0.5358\n",
      "28/388, train_loss: 0.1552, step time: 0.5131\n",
      "29/388, train_loss: 0.0620, step time: 0.5669\n",
      "30/388, train_loss: 0.1291, step time: 0.5430\n",
      "31/388, train_loss: 0.0950, step time: 0.5108\n",
      "32/388, train_loss: 0.3346, step time: 0.5124\n",
      "33/388, train_loss: 0.5271, step time: 0.5129\n",
      "34/388, train_loss: 0.0828, step time: 0.5061\n",
      "35/388, train_loss: 0.1670, step time: 0.4986\n",
      "36/388, train_loss: 0.1508, step time: 0.5762\n",
      "37/388, train_loss: 0.0883, step time: 0.5971\n",
      "38/388, train_loss: 0.1372, step time: 0.5218\n",
      "39/388, train_loss: 0.1503, step time: 0.4970\n",
      "40/388, train_loss: 0.2134, step time: 0.4990\n",
      "41/388, train_loss: 0.1215, step time: 0.5525\n",
      "42/388, train_loss: 0.2476, step time: 0.5442\n",
      "43/388, train_loss: 0.0801, step time: 0.5361\n",
      "44/388, train_loss: 0.1665, step time: 0.7358\n",
      "45/388, train_loss: 0.4941, step time: 0.6608\n",
      "46/388, train_loss: 0.0556, step time: 0.5519\n",
      "47/388, train_loss: 0.1083, step time: 0.5106\n",
      "48/388, train_loss: 0.2132, step time: 0.5116\n",
      "49/388, train_loss: 0.2528, step time: 0.5287\n",
      "50/388, train_loss: 0.3330, step time: 0.5318\n",
      "51/388, train_loss: 0.1123, step time: 0.6252\n",
      "52/388, train_loss: 0.1415, step time: 0.5569\n",
      "53/388, train_loss: 0.2516, step time: 0.5356\n",
      "54/388, train_loss: 0.0838, step time: 0.5171\n",
      "55/388, train_loss: 0.0671, step time: 0.5318\n",
      "56/388, train_loss: 0.0831, step time: 0.5174\n",
      "57/388, train_loss: 0.2143, step time: 0.4980\n",
      "58/388, train_loss: 0.3791, step time: 0.5129\n",
      "59/388, train_loss: 0.3105, step time: 0.5003\n",
      "60/388, train_loss: 0.4508, step time: 0.5101\n",
      "61/388, train_loss: 0.1723, step time: 1.1897\n",
      "62/388, train_loss: 0.0649, step time: 0.5414\n",
      "63/388, train_loss: 0.1076, step time: 0.5188\n",
      "64/388, train_loss: 0.0814, step time: 0.5015\n",
      "65/388, train_loss: 0.1041, step time: 0.4987\n",
      "66/388, train_loss: 0.2627, step time: 0.4935\n",
      "67/388, train_loss: 0.1429, step time: 1.1592\n",
      "68/388, train_loss: 0.1425, step time: 0.5398\n",
      "69/388, train_loss: 0.3466, step time: 0.5141\n",
      "70/388, train_loss: 0.1125, step time: 0.5098\n",
      "71/388, train_loss: 0.0708, step time: 0.4987\n",
      "72/388, train_loss: 0.1013, step time: 0.5008\n",
      "73/388, train_loss: 0.2604, step time: 0.4897\n",
      "74/388, train_loss: 0.1076, step time: 0.5252\n",
      "75/388, train_loss: 0.2689, step time: 0.4994\n",
      "76/388, train_loss: 0.0714, step time: 0.5332\n",
      "77/388, train_loss: 0.1514, step time: 0.7054\n",
      "78/388, train_loss: 0.1019, step time: 0.5676\n",
      "79/388, train_loss: 0.2692, step time: 0.5376\n",
      "80/388, train_loss: 0.0740, step time: 0.5120\n",
      "81/388, train_loss: 0.0887, step time: 0.5910\n",
      "82/388, train_loss: 0.1707, step time: 0.5665\n",
      "83/388, train_loss: 0.3628, step time: 0.5437\n",
      "84/388, train_loss: 0.2233, step time: 0.4998\n",
      "85/388, train_loss: 0.0593, step time: 0.4963\n",
      "86/388, train_loss: 0.1842, step time: 0.5007\n",
      "87/388, train_loss: 0.1675, step time: 1.0602\n",
      "88/388, train_loss: 0.1796, step time: 0.5366\n",
      "89/388, train_loss: 0.0710, step time: 0.5132\n",
      "90/388, train_loss: 0.1054, step time: 0.5060\n",
      "91/388, train_loss: 0.1373, step time: 0.5058\n",
      "92/388, train_loss: 0.0813, step time: 0.4913\n",
      "93/388, train_loss: 0.2426, step time: 0.8518\n",
      "94/388, train_loss: 0.0767, step time: 0.5690\n",
      "95/388, train_loss: 0.0474, step time: 0.5404\n",
      "96/388, train_loss: 0.2318, step time: 0.5139\n",
      "97/388, train_loss: 0.1085, step time: 0.4987\n",
      "98/388, train_loss: 0.1383, step time: 0.4801\n",
      "99/388, train_loss: 0.1130, step time: 1.0855\n",
      "100/388, train_loss: 0.1289, step time: 0.5303\n",
      "101/388, train_loss: 0.2423, step time: 0.5156\n",
      "102/388, train_loss: 0.1354, step time: 0.5094\n",
      "103/388, train_loss: 0.0872, step time: 0.5022\n",
      "104/388, train_loss: 0.2120, step time: 0.5017\n",
      "105/388, train_loss: 0.3680, step time: 0.5070\n",
      "106/388, train_loss: 0.1687, step time: 0.4947\n",
      "107/388, train_loss: 0.1212, step time: 0.4874\n",
      "108/388, train_loss: 0.1116, step time: 0.5161\n",
      "109/388, train_loss: 0.0686, step time: 0.5162\n",
      "110/388, train_loss: 0.3069, step time: 0.5198\n",
      "111/388, train_loss: 0.3191, step time: 0.4994\n",
      "112/388, train_loss: 0.3149, step time: 0.5091\n",
      "113/388, train_loss: 0.1732, step time: 0.5103\n",
      "114/388, train_loss: 0.2632, step time: 0.5703\n",
      "115/388, train_loss: 0.1392, step time: 0.5806\n",
      "116/388, train_loss: 0.0936, step time: 0.5410\n",
      "117/388, train_loss: 0.1763, step time: 0.5193\n",
      "118/388, train_loss: 0.1883, step time: 0.5102\n",
      "119/388, train_loss: 0.2746, step time: 0.5038\n",
      "120/388, train_loss: 0.1064, step time: 1.0314\n",
      "121/388, train_loss: 0.1030, step time: 0.5341\n",
      "122/388, train_loss: 0.4382, step time: 0.5167\n",
      "123/388, train_loss: 0.1012, step time: 0.5007\n",
      "124/388, train_loss: 0.1178, step time: 0.5688\n",
      "125/388, train_loss: 0.1857, step time: 0.5376\n",
      "126/388, train_loss: 0.2945, step time: 0.5112\n",
      "127/388, train_loss: 0.1439, step time: 0.5266\n",
      "128/388, train_loss: 0.1344, step time: 0.5337\n",
      "129/388, train_loss: 0.1390, step time: 0.5164\n",
      "130/388, train_loss: 0.1201, step time: 0.5601\n",
      "131/388, train_loss: 0.1894, step time: 0.5261\n",
      "132/388, train_loss: 0.1330, step time: 0.5161\n",
      "133/388, train_loss: 0.2986, step time: 1.2446\n",
      "134/388, train_loss: 0.1339, step time: 0.5274\n",
      "135/388, train_loss: 0.1450, step time: 0.5137\n",
      "136/388, train_loss: 0.2175, step time: 0.4942\n",
      "137/388, train_loss: 0.1007, step time: 0.4954\n",
      "138/388, train_loss: 0.1061, step time: 0.4799\n",
      "139/388, train_loss: 0.0532, step time: 0.4744\n",
      "140/388, train_loss: 0.0631, step time: 1.0271\n",
      "141/388, train_loss: 0.2442, step time: 0.5263\n",
      "142/388, train_loss: 0.1618, step time: 0.5090\n",
      "143/388, train_loss: 0.0942, step time: 0.4945\n",
      "144/388, train_loss: 0.1916, step time: 0.4968\n",
      "145/388, train_loss: 0.0918, step time: 0.4785\n",
      "146/388, train_loss: 0.4424, step time: 0.5059\n",
      "147/388, train_loss: 0.2329, step time: 0.5117\n",
      "148/388, train_loss: 0.2352, step time: 0.5778\n",
      "149/388, train_loss: 0.0822, step time: 0.5321\n",
      "150/388, train_loss: 0.2197, step time: 0.5068\n",
      "151/388, train_loss: 0.0767, step time: 0.5069\n",
      "152/388, train_loss: 0.1376, step time: 1.2427\n",
      "153/388, train_loss: 0.1954, step time: 0.5426\n",
      "154/388, train_loss: 0.4033, step time: 0.5131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/388, train_loss: 0.1685, step time: 0.4948\n",
      "156/388, train_loss: 0.1437, step time: 0.4951\n",
      "157/388, train_loss: 0.1448, step time: 0.4966\n",
      "158/388, train_loss: 0.0721, step time: 0.4790\n",
      "159/388, train_loss: 0.3758, step time: 0.4826\n",
      "160/388, train_loss: 0.1124, step time: 0.9909\n",
      "161/388, train_loss: 0.1030, step time: 0.5414\n",
      "162/388, train_loss: 0.0961, step time: 0.5191\n",
      "163/388, train_loss: 0.1636, step time: 0.5097\n",
      "164/388, train_loss: 0.1308, step time: 0.4943\n",
      "165/388, train_loss: 0.0877, step time: 0.4843\n",
      "166/388, train_loss: 0.1647, step time: 0.4919\n",
      "167/388, train_loss: 0.2054, step time: 0.8019\n",
      "168/388, train_loss: 0.2549, step time: 0.5395\n",
      "169/388, train_loss: 0.0966, step time: 0.5027\n",
      "170/388, train_loss: 0.0886, step time: 0.4977\n",
      "171/388, train_loss: 0.1113, step time: 0.4907\n",
      "172/388, train_loss: 0.2886, step time: 0.4871\n",
      "173/388, train_loss: 0.1509, step time: 0.5008\n",
      "174/388, train_loss: 0.2490, step time: 0.4907\n",
      "175/388, train_loss: 0.5812, step time: 0.5008\n",
      "176/388, train_loss: 0.2576, step time: 0.5005\n",
      "177/388, train_loss: 0.2759, step time: 0.5589\n",
      "178/388, train_loss: 0.1362, step time: 0.5546\n",
      "179/388, train_loss: 0.1861, step time: 0.5239\n",
      "180/388, train_loss: 0.1849, step time: 0.4972\n",
      "181/388, train_loss: 0.2943, step time: 0.5062\n",
      "182/388, train_loss: 0.1065, step time: 0.4925\n",
      "183/388, train_loss: 0.1682, step time: 0.4845\n",
      "184/388, train_loss: 0.2013, step time: 1.1600\n",
      "185/388, train_loss: 0.1494, step time: 0.5545\n",
      "186/388, train_loss: 0.1429, step time: 0.5180\n",
      "187/388, train_loss: 0.0861, step time: 0.4965\n",
      "188/388, train_loss: 0.0587, step time: 0.5012\n",
      "189/388, train_loss: 0.1906, step time: 0.4848\n",
      "190/388, train_loss: 0.0864, step time: 0.4895\n",
      "191/388, train_loss: 0.1128, step time: 0.4947\n",
      "192/388, train_loss: 0.2128, step time: 0.9310\n",
      "193/388, train_loss: 0.1710, step time: 0.5411\n",
      "194/388, train_loss: 0.0943, step time: 0.5115\n",
      "195/388, train_loss: 0.0852, step time: 0.4951\n",
      "196/388, train_loss: 0.2881, step time: 0.4960\n",
      "197/388, train_loss: 0.1032, step time: 0.4916\n",
      "198/388, train_loss: 0.1557, step time: 0.4818\n",
      "199/388, train_loss: 0.1286, step time: 1.2338\n",
      "200/388, train_loss: 0.0766, step time: 0.5311\n",
      "201/388, train_loss: 0.3947, step time: 0.4997\n",
      "202/388, train_loss: 0.1452, step time: 0.4950\n",
      "203/388, train_loss: 0.2195, step time: 0.4802\n",
      "204/388, train_loss: 0.2608, step time: 0.4851\n",
      "205/388, train_loss: 0.1550, step time: 0.4725\n",
      "206/388, train_loss: 0.0870, step time: 0.4785\n",
      "207/388, train_loss: 0.0821, step time: 0.4875\n",
      "208/388, train_loss: 0.0748, step time: 1.0456\n",
      "209/388, train_loss: 0.2235, step time: 0.5594\n",
      "210/388, train_loss: 0.2086, step time: 0.5199\n",
      "211/388, train_loss: 0.2245, step time: 0.5079\n",
      "212/388, train_loss: 0.1608, step time: 0.4983\n",
      "213/388, train_loss: 0.2341, step time: 0.4807\n",
      "214/388, train_loss: 0.1227, step time: 0.4935\n",
      "215/388, train_loss: 0.0976, step time: 0.5454\n",
      "216/388, train_loss: 0.0621, step time: 0.5284\n",
      "217/388, train_loss: 0.1959, step time: 0.5046\n",
      "218/388, train_loss: 0.0905, step time: 0.4910\n",
      "219/388, train_loss: 0.2553, step time: 0.4871\n",
      "220/388, train_loss: 0.1534, step time: 0.5042\n",
      "221/388, train_loss: 0.1756, step time: 0.5463\n",
      "222/388, train_loss: 0.1548, step time: 0.6066\n",
      "223/388, train_loss: 0.0882, step time: 0.5436\n",
      "224/388, train_loss: 0.1410, step time: 0.5192\n",
      "225/388, train_loss: 0.1847, step time: 0.5447\n",
      "226/388, train_loss: 0.2562, step time: 0.5262\n",
      "227/388, train_loss: 0.4614, step time: 0.5040\n",
      "228/388, train_loss: 0.1123, step time: 0.5173\n",
      "229/388, train_loss: 0.1807, step time: 0.4984\n",
      "230/388, train_loss: 0.2363, step time: 0.4920\n",
      "231/388, train_loss: 0.1724, step time: 0.4948\n",
      "232/388, train_loss: 0.0996, step time: 0.4940\n",
      "233/388, train_loss: 0.1253, step time: 0.4949\n",
      "234/388, train_loss: 0.1361, step time: 0.4810\n",
      "235/388, train_loss: 0.3117, step time: 0.4809\n",
      "236/388, train_loss: 0.1416, step time: 1.1622\n",
      "237/388, train_loss: 0.2086, step time: 0.5533\n",
      "238/388, train_loss: 0.3704, step time: 0.5077\n",
      "239/388, train_loss: 0.1551, step time: 0.5018\n",
      "240/388, train_loss: 0.1399, step time: 0.4862\n",
      "241/388, train_loss: 0.1754, step time: 0.4863\n",
      "242/388, train_loss: 0.2839, step time: 0.4935\n",
      "243/388, train_loss: 0.0971, step time: 0.4822\n",
      "244/388, train_loss: 0.3134, step time: 0.9446\n",
      "245/388, train_loss: 0.2285, step time: 0.5435\n",
      "246/388, train_loss: 0.0444, step time: 0.5135\n",
      "247/388, train_loss: 0.1901, step time: 0.5030\n",
      "248/388, train_loss: 0.1614, step time: 0.4986\n",
      "249/388, train_loss: 0.1426, step time: 0.4856\n",
      "250/388, train_loss: 0.3333, step time: 1.2403\n",
      "251/388, train_loss: 0.4896, step time: 0.5350\n",
      "252/388, train_loss: 0.0552, step time: 0.5063\n",
      "253/388, train_loss: 0.2336, step time: 0.5013\n",
      "254/388, train_loss: 0.3908, step time: 0.4956\n",
      "255/388, train_loss: 0.4353, step time: 0.4859\n",
      "256/388, train_loss: 0.3796, step time: 0.5045\n",
      "257/388, train_loss: 0.1878, step time: 0.4921\n",
      "258/388, train_loss: 0.0981, step time: 0.5285\n",
      "259/388, train_loss: 0.1783, step time: 0.5077\n",
      "260/388, train_loss: 0.1006, step time: 0.5009\n",
      "261/388, train_loss: 0.1938, step time: 0.4879\n",
      "262/388, train_loss: 0.1164, step time: 0.4849\n",
      "263/388, train_loss: 0.2544, step time: 0.4907\n",
      "264/388, train_loss: 0.1425, step time: 1.1629\n",
      "265/388, train_loss: 0.0459, step time: 0.5227\n",
      "266/388, train_loss: 0.3926, step time: 0.4952\n",
      "267/388, train_loss: 0.2026, step time: 0.4940\n",
      "268/388, train_loss: 0.1808, step time: 0.4833\n",
      "269/388, train_loss: 0.2001, step time: 0.4899\n",
      "270/388, train_loss: 0.1394, step time: 0.4880\n",
      "271/388, train_loss: 0.0818, step time: 0.4813\n",
      "272/388, train_loss: 0.1302, step time: 0.4728\n",
      "273/388, train_loss: 0.1886, step time: 0.4749\n",
      "274/388, train_loss: 0.1131, step time: 0.9654\n",
      "275/388, train_loss: 0.0630, step time: 0.5246\n",
      "276/388, train_loss: 0.0837, step time: 0.5032\n",
      "277/388, train_loss: 0.3778, step time: 0.5032\n",
      "278/388, train_loss: 0.0862, step time: 0.4955\n",
      "279/388, train_loss: 0.1003, step time: 0.4938\n",
      "280/388, train_loss: 0.0815, step time: 0.4752\n",
      "281/388, train_loss: 0.2224, step time: 0.4811\n",
      "282/388, train_loss: 0.5377, step time: 0.5122\n",
      "283/388, train_loss: 0.1681, step time: 0.5945\n",
      "284/388, train_loss: 0.1634, step time: 0.5593\n",
      "285/388, train_loss: 0.0957, step time: 0.5237\n",
      "286/388, train_loss: 0.0850, step time: 0.5028\n",
      "287/388, train_loss: 0.0592, step time: 0.4963\n",
      "288/388, train_loss: 0.0831, step time: 0.4944\n",
      "289/388, train_loss: 0.2889, step time: 0.4870\n",
      "290/388, train_loss: 0.2012, step time: 0.4939\n",
      "291/388, train_loss: 0.1856, step time: 0.4882\n",
      "292/388, train_loss: 0.1174, step time: 0.5268\n",
      "293/388, train_loss: 0.1288, step time: 0.5617\n",
      "294/388, train_loss: 0.0868, step time: 0.5336\n",
      "295/388, train_loss: 0.1539, step time: 0.5124\n",
      "296/388, train_loss: 0.1461, step time: 0.4866\n",
      "297/388, train_loss: 0.0918, step time: 1.0961\n",
      "298/388, train_loss: 0.2038, step time: 0.5205\n",
      "299/388, train_loss: 0.1072, step time: 0.5001\n",
      "300/388, train_loss: 0.1619, step time: 0.4819\n",
      "301/388, train_loss: 0.2111, step time: 0.4829\n",
      "302/388, train_loss: 0.1809, step time: 0.4859\n",
      "303/388, train_loss: 0.0579, step time: 0.4761\n",
      "304/388, train_loss: 0.1951, step time: 0.4965\n",
      "305/388, train_loss: 0.2524, step time: 0.4811\n",
      "306/388, train_loss: 0.0951, step time: 0.9989\n",
      "307/388, train_loss: 0.2782, step time: 0.5358\n",
      "308/388, train_loss: 0.1547, step time: 0.5021\n",
      "309/388, train_loss: 0.2610, step time: 0.4911\n",
      "310/388, train_loss: 0.1665, step time: 0.4899\n",
      "311/388, train_loss: 0.1799, step time: 0.4935\n",
      "312/388, train_loss: 0.1175, step time: 0.4792\n",
      "313/388, train_loss: 0.2649, step time: 1.0680\n",
      "314/388, train_loss: 0.1354, step time: 0.5376\n",
      "315/388, train_loss: 0.2432, step time: 0.5034\n",
      "316/388, train_loss: 0.2411, step time: 0.5054\n",
      "317/388, train_loss: 0.0530, step time: 0.4866\n",
      "318/388, train_loss: 0.2344, step time: 0.4883\n",
      "319/388, train_loss: 0.1764, step time: 0.4963\n",
      "320/388, train_loss: 0.2807, step time: 0.4841\n",
      "321/388, train_loss: 0.2840, step time: 0.5027\n",
      "322/388, train_loss: 0.1173, step time: 0.5003\n",
      "323/388, train_loss: 0.1298, step time: 0.5182\n",
      "324/388, train_loss: 0.2641, step time: 0.4958\n",
      "325/388, train_loss: 0.2325, step time: 0.4999\n",
      "326/388, train_loss: 0.2024, step time: 0.4818\n",
      "327/388, train_loss: 0.2132, step time: 0.4800\n",
      "328/388, train_loss: 0.1491, step time: 0.4910\n",
      "329/388, train_loss: 0.5293, step time: 0.4889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/388, train_loss: 0.0696, step time: 0.4817\n",
      "331/388, train_loss: 0.1965, step time: 0.9547\n",
      "332/388, train_loss: 0.1372, step time: 0.5481\n",
      "333/388, train_loss: 0.2048, step time: 0.5239\n",
      "334/388, train_loss: 0.2058, step time: 0.5024\n",
      "335/388, train_loss: 0.2272, step time: 0.4997\n",
      "336/388, train_loss: 0.3882, step time: 0.4872\n",
      "337/388, train_loss: 0.3307, step time: 0.4874\n",
      "338/388, train_loss: 0.1053, step time: 0.4824\n",
      "339/388, train_loss: 0.2145, step time: 0.4868\n",
      "340/388, train_loss: 0.1522, step time: 0.4952\n",
      "341/388, train_loss: 0.2421, step time: 0.4908\n",
      "342/388, train_loss: 0.0548, step time: 0.4848\n",
      "343/388, train_loss: 0.3405, step time: 1.1873\n",
      "344/388, train_loss: 0.1557, step time: 0.5427\n",
      "345/388, train_loss: 0.2408, step time: 0.5164\n",
      "346/388, train_loss: 0.0321, step time: 0.4967\n",
      "347/388, train_loss: 0.0987, step time: 0.4857\n",
      "348/388, train_loss: 0.0312, step time: 0.4896\n",
      "349/388, train_loss: 0.1195, step time: 0.4878\n",
      "350/388, train_loss: 0.1550, step time: 0.5060\n",
      "351/388, train_loss: 0.1143, step time: 0.4863\n",
      "352/388, train_loss: 0.3691, step time: 0.4925\n",
      "353/388, train_loss: 0.2557, step time: 0.4972\n",
      "354/388, train_loss: 0.1634, step time: 0.4890\n",
      "355/388, train_loss: 0.1220, step time: 0.5091\n",
      "356/388, train_loss: 0.3264, step time: 0.5107\n",
      "357/388, train_loss: 0.4185, step time: 0.5368\n",
      "358/388, train_loss: 0.2456, step time: 0.5184\n",
      "359/388, train_loss: 0.1452, step time: 0.5378\n",
      "360/388, train_loss: 0.1167, step time: 0.5197\n",
      "361/388, train_loss: 0.0893, step time: 0.5089\n",
      "362/388, train_loss: 0.1064, step time: 0.4910\n",
      "363/388, train_loss: 0.0964, step time: 0.9208\n",
      "364/388, train_loss: 0.1103, step time: 0.5272\n",
      "365/388, train_loss: 0.2133, step time: 0.4996\n",
      "366/388, train_loss: 0.4418, step time: 0.4980\n",
      "367/388, train_loss: 0.1306, step time: 0.4944\n",
      "368/388, train_loss: 0.0971, step time: 0.4782\n",
      "369/388, train_loss: 0.0958, step time: 0.5722\n",
      "370/388, train_loss: 0.0915, step time: 0.5460\n",
      "371/388, train_loss: 0.1333, step time: 0.5121\n",
      "372/388, train_loss: 0.1446, step time: 0.5052\n",
      "373/388, train_loss: 0.0343, step time: 0.4874\n",
      "374/388, train_loss: 0.1705, step time: 0.4913\n",
      "375/388, train_loss: 0.2325, step time: 1.1620\n",
      "376/388, train_loss: 0.1030, step time: 0.5312\n",
      "377/388, train_loss: 0.0501, step time: 0.5016\n",
      "378/388, train_loss: 0.1044, step time: 0.4894\n",
      "379/388, train_loss: 0.1778, step time: 0.4835\n",
      "380/388, train_loss: 0.1827, step time: 0.4911\n",
      "381/388, train_loss: 0.3326, step time: 1.0679\n",
      "382/388, train_loss: 0.1037, step time: 0.5284\n",
      "383/388, train_loss: 0.0560, step time: 0.4965\n",
      "384/388, train_loss: 0.0899, step time: 0.4863\n",
      "385/388, train_loss: 0.0980, step time: 0.4740\n",
      "386/388, train_loss: 0.0638, step time: 0.4738\n",
      "387/388, train_loss: 0.2484, step time: 0.4870\n",
      "388/388, train_loss: 0.1581, step time: 0.4848\n",
      "epoch 75 average loss: 0.1788\n",
      "saved new best metric model\n",
      "current epoch: 75 current mean dice: 0.7719 tc: 0.8245 wt: 0.9008 et: 0.5902\n",
      "best mean dice: 0.7719 at epoch: 75\n",
      "time consuming of epoch 75 is: 303.0472\n",
      "----------\n",
      "epoch 76/300\n",
      "1/388, train_loss: 0.2439, step time: 0.4741\n",
      "2/388, train_loss: 0.0871, step time: 0.4838\n",
      "3/388, train_loss: 0.1305, step time: 0.4785\n",
      "4/388, train_loss: 0.2274, step time: 0.5020\n",
      "5/388, train_loss: 0.1631, step time: 0.9687\n",
      "6/388, train_loss: 0.2661, step time: 0.5389\n",
      "7/388, train_loss: 0.2484, step time: 0.5138\n",
      "8/388, train_loss: 0.2234, step time: 0.4941\n",
      "9/388, train_loss: 0.1209, step time: 0.7858\n",
      "10/388, train_loss: 0.0869, step time: 0.5595\n",
      "11/388, train_loss: 0.0599, step time: 0.5331\n",
      "12/388, train_loss: 0.1154, step time: 0.5141\n",
      "13/388, train_loss: 0.2254, step time: 0.4994\n",
      "14/388, train_loss: 0.0722, step time: 0.4882\n",
      "15/388, train_loss: 0.0523, step time: 0.4997\n",
      "16/388, train_loss: 0.1893, step time: 0.5355\n",
      "17/388, train_loss: 0.0957, step time: 0.5087\n",
      "18/388, train_loss: 0.0415, step time: 0.4855\n",
      "19/388, train_loss: 0.1301, step time: 1.1479\n",
      "20/388, train_loss: 0.2744, step time: 0.5356\n",
      "21/388, train_loss: 0.2330, step time: 0.5115\n",
      "22/388, train_loss: 0.0892, step time: 0.5037\n",
      "23/388, train_loss: 0.1893, step time: 0.4928\n",
      "24/388, train_loss: 0.1879, step time: 0.5009\n",
      "25/388, train_loss: 0.6064, step time: 0.4848\n",
      "26/388, train_loss: 0.3559, step time: 0.5453\n",
      "27/388, train_loss: 0.1909, step time: 0.5482\n",
      "28/388, train_loss: 0.1393, step time: 0.5687\n",
      "29/388, train_loss: 0.1363, step time: 0.5408\n",
      "30/388, train_loss: 0.2882, step time: 0.5078\n",
      "31/388, train_loss: 0.2009, step time: 0.4864\n",
      "32/388, train_loss: 0.1832, step time: 1.2408\n",
      "33/388, train_loss: 0.0728, step time: 0.5454\n",
      "34/388, train_loss: 0.4240, step time: 0.5121\n",
      "35/388, train_loss: 0.1358, step time: 0.5124\n",
      "36/388, train_loss: 0.1710, step time: 0.5199\n",
      "37/388, train_loss: 0.1580, step time: 0.5054\n",
      "38/388, train_loss: 0.0935, step time: 0.5021\n",
      "39/388, train_loss: 0.1071, step time: 0.4865\n",
      "40/388, train_loss: 0.1090, step time: 0.7398\n",
      "41/388, train_loss: 0.2220, step time: 0.5617\n",
      "42/388, train_loss: 0.1086, step time: 0.5297\n",
      "43/388, train_loss: 0.1955, step time: 0.5136\n",
      "44/388, train_loss: 0.1612, step time: 0.5038\n",
      "45/388, train_loss: 0.0879, step time: 0.4814\n",
      "46/388, train_loss: 0.1109, step time: 0.4777\n",
      "47/388, train_loss: 0.1600, step time: 0.4771\n",
      "48/388, train_loss: 0.1339, step time: 0.4749\n",
      "49/388, train_loss: 0.1635, step time: 0.4835\n",
      "50/388, train_loss: 0.0556, step time: 0.4854\n",
      "51/388, train_loss: 0.5179, step time: 1.0066\n",
      "52/388, train_loss: 0.0656, step time: 0.5390\n",
      "53/388, train_loss: 0.3042, step time: 0.5149\n",
      "54/388, train_loss: 0.0924, step time: 0.4914\n",
      "55/388, train_loss: 0.1852, step time: 0.5014\n",
      "56/388, train_loss: 0.2285, step time: 0.4952\n",
      "57/388, train_loss: 0.2257, step time: 0.5026\n",
      "58/388, train_loss: 0.1183, step time: 0.6036\n",
      "59/388, train_loss: 0.0314, step time: 0.5529\n",
      "60/388, train_loss: 0.1373, step time: 0.5223\n",
      "61/388, train_loss: 0.0778, step time: 0.5056\n",
      "62/388, train_loss: 0.0606, step time: 0.4980\n",
      "63/388, train_loss: 0.1038, step time: 0.5045\n",
      "64/388, train_loss: 0.1063, step time: 0.6174\n",
      "65/388, train_loss: 0.1550, step time: 0.5600\n",
      "66/388, train_loss: 0.1293, step time: 0.5358\n",
      "67/388, train_loss: 0.1797, step time: 0.5088\n",
      "68/388, train_loss: 0.0719, step time: 0.4817\n",
      "69/388, train_loss: 0.2813, step time: 1.0350\n",
      "70/388, train_loss: 0.0649, step time: 0.5313\n",
      "71/388, train_loss: 0.2867, step time: 0.5198\n",
      "72/388, train_loss: 0.2305, step time: 0.4944\n",
      "73/388, train_loss: 0.3708, step time: 0.4976\n",
      "74/388, train_loss: 0.1512, step time: 0.4944\n",
      "75/388, train_loss: 0.1017, step time: 0.4785\n",
      "76/388, train_loss: 0.1436, step time: 0.4892\n",
      "77/388, train_loss: 0.1448, step time: 0.4904\n",
      "78/388, train_loss: 0.1818, step time: 0.4841\n",
      "79/388, train_loss: 0.2410, step time: 0.8961\n",
      "80/388, train_loss: 0.1623, step time: 0.5575\n",
      "81/388, train_loss: 0.2801, step time: 0.5192\n",
      "82/388, train_loss: 0.1627, step time: 0.5048\n",
      "83/388, train_loss: 0.1010, step time: 0.5245\n",
      "84/388, train_loss: 0.0456, step time: 0.5132\n",
      "85/388, train_loss: 0.1276, step time: 0.5051\n",
      "86/388, train_loss: 0.1850, step time: 0.5607\n",
      "87/388, train_loss: 0.1557, step time: 0.5636\n",
      "88/388, train_loss: 0.2200, step time: 0.5289\n",
      "89/388, train_loss: 0.1257, step time: 0.5165\n",
      "90/388, train_loss: 0.2043, step time: 0.5302\n",
      "91/388, train_loss: 0.2148, step time: 0.5399\n",
      "92/388, train_loss: 0.1165, step time: 0.5172\n",
      "93/388, train_loss: 0.1043, step time: 0.4877\n",
      "94/388, train_loss: 0.1149, step time: 0.4759\n",
      "95/388, train_loss: 0.1308, step time: 0.9587\n",
      "96/388, train_loss: 0.0998, step time: 0.5429\n",
      "97/388, train_loss: 0.0838, step time: 0.5159\n",
      "98/388, train_loss: 0.1293, step time: 0.4994\n",
      "99/388, train_loss: 0.1031, step time: 0.4981\n",
      "100/388, train_loss: 0.1203, step time: 0.4865\n",
      "101/388, train_loss: 0.1593, step time: 1.1656\n",
      "102/388, train_loss: 0.2368, step time: 0.5330\n",
      "103/388, train_loss: 0.1528, step time: 0.5034\n",
      "104/388, train_loss: 0.1348, step time: 0.4907\n",
      "105/388, train_loss: 0.1345, step time: 0.4823\n",
      "106/388, train_loss: 0.3271, step time: 0.4845\n",
      "107/388, train_loss: 0.0633, step time: 0.4826\n",
      "108/388, train_loss: 0.3836, step time: 0.4964\n",
      "109/388, train_loss: 0.3920, step time: 0.4889\n",
      "110/388, train_loss: 0.1543, step time: 0.9493\n",
      "111/388, train_loss: 0.0954, step time: 0.5391\n",
      "112/388, train_loss: 0.0876, step time: 0.5089\n",
      "113/388, train_loss: 0.1095, step time: 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/388, train_loss: 0.0941, step time: 0.5007\n",
      "115/388, train_loss: 0.1589, step time: 0.5045\n",
      "116/388, train_loss: 0.1919, step time: 0.4845\n",
      "117/388, train_loss: 0.1087, step time: 0.4991\n",
      "118/388, train_loss: 0.0868, step time: 1.1106\n",
      "119/388, train_loss: 0.2360, step time: 0.5481\n",
      "120/388, train_loss: 0.0920, step time: 0.5231\n",
      "121/388, train_loss: 0.2019, step time: 0.5038\n",
      "122/388, train_loss: 0.0845, step time: 0.4951\n",
      "123/388, train_loss: 0.1673, step time: 0.4891\n",
      "124/388, train_loss: 0.1251, step time: 0.5083\n",
      "125/388, train_loss: 0.2560, step time: 0.4931\n",
      "126/388, train_loss: 0.0943, step time: 0.4900\n",
      "127/388, train_loss: 0.2126, step time: 0.4934\n",
      "128/388, train_loss: 0.3034, step time: 0.4862\n",
      "129/388, train_loss: 0.2019, step time: 0.5036\n",
      "130/388, train_loss: 0.0829, step time: 0.4833\n",
      "131/388, train_loss: 0.4688, step time: 0.4838\n",
      "132/388, train_loss: 0.2226, step time: 0.5786\n",
      "133/388, train_loss: 0.2695, step time: 0.5526\n",
      "134/388, train_loss: 0.0853, step time: 0.5244\n",
      "135/388, train_loss: 0.1441, step time: 0.5567\n",
      "136/388, train_loss: 0.1554, step time: 0.5402\n",
      "137/388, train_loss: 0.0870, step time: 0.5163\n",
      "138/388, train_loss: 0.2532, step time: 0.5009\n",
      "139/388, train_loss: 0.0799, step time: 1.2457\n",
      "140/388, train_loss: 0.3299, step time: 0.5344\n",
      "141/388, train_loss: 0.1559, step time: 0.5062\n",
      "142/388, train_loss: 0.1052, step time: 0.4876\n",
      "143/388, train_loss: 0.0862, step time: 0.4956\n",
      "144/388, train_loss: 0.2412, step time: 0.4845\n",
      "145/388, train_loss: 0.1520, step time: 0.4748\n",
      "146/388, train_loss: 0.1087, step time: 0.4800\n",
      "147/388, train_loss: 0.3079, step time: 0.9607\n",
      "148/388, train_loss: 0.0769, step time: 0.5413\n",
      "149/388, train_loss: 0.0912, step time: 0.5101\n",
      "150/388, train_loss: 0.0676, step time: 0.4917\n",
      "151/388, train_loss: 0.2222, step time: 0.5003\n",
      "152/388, train_loss: 0.2747, step time: 0.4830\n",
      "153/388, train_loss: 0.1055, step time: 1.0176\n",
      "154/388, train_loss: 0.0800, step time: 0.5358\n",
      "155/388, train_loss: 0.1190, step time: 0.5147\n",
      "156/388, train_loss: 0.2245, step time: 0.4965\n",
      "157/388, train_loss: 0.2398, step time: 0.4924\n",
      "158/388, train_loss: 0.1042, step time: 0.4928\n",
      "159/388, train_loss: 0.3340, step time: 0.4774\n",
      "160/388, train_loss: 0.4235, step time: 0.4853\n",
      "161/388, train_loss: 0.1017, step time: 0.4735\n",
      "162/388, train_loss: 0.1886, step time: 0.5573\n",
      "163/388, train_loss: 0.1480, step time: 0.5037\n",
      "164/388, train_loss: 0.1040, step time: 0.4993\n",
      "165/388, train_loss: 0.0668, step time: 0.9734\n",
      "166/388, train_loss: 0.1554, step time: 0.5352\n",
      "167/388, train_loss: 0.2929, step time: 0.5177\n",
      "168/388, train_loss: 0.0919, step time: 0.4982\n",
      "169/388, train_loss: 0.1029, step time: 0.4842\n",
      "170/388, train_loss: 0.2256, step time: 0.4921\n",
      "171/388, train_loss: 0.1461, step time: 0.9524\n",
      "172/388, train_loss: 0.1239, step time: 0.5414\n",
      "173/388, train_loss: 0.2341, step time: 0.5152\n",
      "174/388, train_loss: 0.0619, step time: 0.4971\n",
      "175/388, train_loss: 0.0897, step time: 0.5000\n",
      "176/388, train_loss: 0.1749, step time: 0.4862\n",
      "177/388, train_loss: 0.1130, step time: 1.0584\n",
      "178/388, train_loss: 0.1688, step time: 0.5615\n",
      "179/388, train_loss: 0.2972, step time: 0.5304\n",
      "180/388, train_loss: 0.1236, step time: 0.5076\n",
      "181/388, train_loss: 0.3004, step time: 0.4958\n",
      "182/388, train_loss: 0.0883, step time: 0.4912\n",
      "183/388, train_loss: 0.1245, step time: 0.4945\n",
      "184/388, train_loss: 0.1196, step time: 0.5070\n",
      "185/388, train_loss: 0.1935, step time: 0.5038\n",
      "186/388, train_loss: 0.1613, step time: 0.4827\n",
      "187/388, train_loss: 0.4479, step time: 0.4839\n",
      "188/388, train_loss: 0.0635, step time: 0.5166\n",
      "189/388, train_loss: 0.1268, step time: 0.5034\n",
      "190/388, train_loss: 0.0988, step time: 0.9631\n",
      "191/388, train_loss: 0.1676, step time: 0.5319\n",
      "192/388, train_loss: 0.1973, step time: 0.5071\n",
      "193/388, train_loss: 0.1957, step time: 0.5040\n",
      "194/388, train_loss: 0.0296, step time: 0.4903\n",
      "195/388, train_loss: 0.2798, step time: 0.4855\n",
      "196/388, train_loss: 0.0733, step time: 0.5193\n",
      "197/388, train_loss: 0.1111, step time: 0.5235\n",
      "198/388, train_loss: 0.3013, step time: 0.4942\n",
      "199/388, train_loss: 0.3592, step time: 0.5027\n",
      "200/388, train_loss: 0.2462, step time: 0.6039\n",
      "201/388, train_loss: 0.1364, step time: 0.5607\n",
      "202/388, train_loss: 0.0503, step time: 0.5492\n",
      "203/388, train_loss: 0.0853, step time: 0.5174\n",
      "204/388, train_loss: 0.1698, step time: 0.4930\n",
      "205/388, train_loss: 0.7264, step time: 0.4856\n",
      "206/388, train_loss: 0.1119, step time: 1.1162\n",
      "207/388, train_loss: 0.2433, step time: 0.5312\n",
      "208/388, train_loss: 0.0697, step time: 0.5069\n",
      "209/388, train_loss: 0.0696, step time: 0.4953\n",
      "210/388, train_loss: 0.0749, step time: 0.4796\n",
      "211/388, train_loss: 0.1651, step time: 0.5302\n",
      "212/388, train_loss: 0.2258, step time: 0.5106\n",
      "213/388, train_loss: 0.0589, step time: 0.4997\n",
      "214/388, train_loss: 0.0744, step time: 0.5321\n",
      "215/388, train_loss: 0.0478, step time: 0.5066\n",
      "216/388, train_loss: 0.2340, step time: 0.4934\n",
      "217/388, train_loss: 0.1122, step time: 0.9815\n",
      "218/388, train_loss: 0.1187, step time: 0.5536\n",
      "219/388, train_loss: 0.1562, step time: 0.5205\n",
      "220/388, train_loss: 0.2329, step time: 0.4868\n",
      "221/388, train_loss: 0.1408, step time: 1.1105\n",
      "222/388, train_loss: 0.0935, step time: 0.5384\n",
      "223/388, train_loss: 0.0763, step time: 0.5066\n",
      "224/388, train_loss: 0.0878, step time: 0.4899\n",
      "225/388, train_loss: 0.1672, step time: 0.4930\n",
      "226/388, train_loss: 0.2937, step time: 0.5026\n",
      "227/388, train_loss: 0.0511, step time: 0.4974\n",
      "228/388, train_loss: 0.0862, step time: 0.4840\n",
      "229/388, train_loss: 0.0824, step time: 0.5675\n",
      "230/388, train_loss: 0.2130, step time: 0.5384\n",
      "231/388, train_loss: 0.1514, step time: 0.5177\n",
      "232/388, train_loss: 0.0680, step time: 0.5667\n",
      "233/388, train_loss: 0.1021, step time: 0.5348\n",
      "234/388, train_loss: 0.1543, step time: 0.5021\n",
      "235/388, train_loss: 0.2336, step time: 0.4991\n",
      "236/388, train_loss: 0.0918, step time: 0.4862\n",
      "237/388, train_loss: 0.1956, step time: 0.8831\n",
      "238/388, train_loss: 0.3414, step time: 0.5569\n",
      "239/388, train_loss: 0.1438, step time: 0.5272\n",
      "240/388, train_loss: 0.3509, step time: 0.5055\n",
      "241/388, train_loss: 0.2298, step time: 0.4871\n",
      "242/388, train_loss: 0.0611, step time: 0.4956\n",
      "243/388, train_loss: 0.1209, step time: 0.5033\n",
      "244/388, train_loss: 0.4542, step time: 1.0719\n",
      "245/388, train_loss: 0.1160, step time: 0.5520\n",
      "246/388, train_loss: 0.1039, step time: 0.5140\n",
      "247/388, train_loss: 0.1429, step time: 0.4930\n",
      "248/388, train_loss: 0.1079, step time: 0.4887\n",
      "249/388, train_loss: 0.2333, step time: 0.4948\n",
      "250/388, train_loss: 0.0705, step time: 0.5169\n",
      "251/388, train_loss: 0.0895, step time: 0.5027\n",
      "252/388, train_loss: 0.0882, step time: 0.4880\n",
      "253/388, train_loss: 0.2114, step time: 0.9370\n",
      "254/388, train_loss: 0.2263, step time: 0.5523\n",
      "255/388, train_loss: 0.2309, step time: 0.5209\n",
      "256/388, train_loss: 0.0686, step time: 0.5044\n",
      "257/388, train_loss: 0.0710, step time: 0.4913\n",
      "258/388, train_loss: 0.1681, step time: 0.4978\n",
      "259/388, train_loss: 0.1145, step time: 0.4855\n",
      "260/388, train_loss: 0.1156, step time: 0.4960\n",
      "261/388, train_loss: 0.1242, step time: 0.4862\n",
      "262/388, train_loss: 0.2488, step time: 1.1950\n",
      "263/388, train_loss: 0.3141, step time: 0.5334\n",
      "264/388, train_loss: 0.1221, step time: 0.5112\n",
      "265/388, train_loss: 0.3687, step time: 0.4939\n",
      "266/388, train_loss: 0.3953, step time: 0.4921\n",
      "267/388, train_loss: 0.2079, step time: 0.4912\n",
      "268/388, train_loss: 0.3182, step time: 1.1329\n",
      "269/388, train_loss: 0.2182, step time: 0.5260\n",
      "270/388, train_loss: 0.0714, step time: 0.4996\n",
      "271/388, train_loss: 0.0518, step time: 0.4874\n",
      "272/388, train_loss: 0.4020, step time: 0.4981\n",
      "273/388, train_loss: 0.1141, step time: 0.4858\n",
      "274/388, train_loss: 0.2459, step time: 1.1181\n",
      "275/388, train_loss: 0.2557, step time: 0.5456\n",
      "276/388, train_loss: 0.0792, step time: 0.5152\n",
      "277/388, train_loss: 0.3164, step time: 0.4978\n",
      "278/388, train_loss: 0.2744, step time: 0.4904\n",
      "279/388, train_loss: 0.2594, step time: 0.4914\n",
      "280/388, train_loss: 0.2130, step time: 0.4818\n",
      "281/388, train_loss: 0.5378, step time: 0.4922\n",
      "282/388, train_loss: 0.3288, step time: 0.4878\n",
      "283/388, train_loss: 0.1370, step time: 0.5984\n",
      "284/388, train_loss: 0.1891, step time: 0.5526\n",
      "285/388, train_loss: 0.2684, step time: 0.5218\n",
      "286/388, train_loss: 0.5008, step time: 0.5004\n",
      "287/388, train_loss: 0.2731, step time: 0.4900\n",
      "288/388, train_loss: 0.1804, step time: 0.4829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/388, train_loss: 0.2910, step time: 1.1613\n",
      "290/388, train_loss: 0.1149, step time: 0.5463\n",
      "291/388, train_loss: 0.2975, step time: 0.5165\n",
      "292/388, train_loss: 0.1774, step time: 0.4982\n",
      "293/388, train_loss: 0.1471, step time: 0.4867\n",
      "294/388, train_loss: 0.2407, step time: 0.5111\n",
      "295/388, train_loss: 0.1008, step time: 0.4908\n",
      "296/388, train_loss: 0.1303, step time: 0.4935\n",
      "297/388, train_loss: 0.0851, step time: 0.4837\n",
      "298/388, train_loss: 0.1187, step time: 0.4931\n",
      "299/388, train_loss: 0.2397, step time: 0.4845\n",
      "300/388, train_loss: 0.2316, step time: 0.4977\n",
      "301/388, train_loss: 0.0355, step time: 0.4828\n",
      "302/388, train_loss: 0.1511, step time: 0.5072\n",
      "303/388, train_loss: 0.2045, step time: 0.5073\n",
      "304/388, train_loss: 0.4291, step time: 0.4986\n",
      "305/388, train_loss: 0.1466, step time: 0.4811\n",
      "306/388, train_loss: 0.0514, step time: 0.4970\n",
      "307/388, train_loss: 0.1964, step time: 0.5073\n",
      "308/388, train_loss: 0.2264, step time: 0.5715\n",
      "309/388, train_loss: 0.2197, step time: 0.5223\n",
      "310/388, train_loss: 0.3469, step time: 0.5644\n",
      "311/388, train_loss: 0.1547, step time: 0.5334\n",
      "312/388, train_loss: 0.1610, step time: 0.5085\n",
      "313/388, train_loss: 0.1933, step time: 0.4880\n",
      "314/388, train_loss: 0.1292, step time: 0.4942\n",
      "315/388, train_loss: 0.2361, step time: 0.4933\n",
      "316/388, train_loss: 0.0984, step time: 0.5188\n",
      "317/388, train_loss: 0.1169, step time: 0.5053\n",
      "318/388, train_loss: 0.1029, step time: 0.5644\n",
      "319/388, train_loss: 0.2235, step time: 0.5290\n",
      "320/388, train_loss: 0.1415, step time: 0.5041\n",
      "321/388, train_loss: 0.0828, step time: 0.5020\n",
      "322/388, train_loss: 0.2187, step time: 0.4952\n",
      "323/388, train_loss: 0.2749, step time: 1.0243\n",
      "324/388, train_loss: 0.2613, step time: 0.5372\n",
      "325/388, train_loss: 0.2088, step time: 0.5092\n",
      "326/388, train_loss: 0.1747, step time: 0.4845\n",
      "327/388, train_loss: 0.2022, step time: 0.4973\n",
      "328/388, train_loss: 0.0831, step time: 0.4755\n",
      "329/388, train_loss: 0.1930, step time: 0.4915\n",
      "330/388, train_loss: 0.1669, step time: 0.5053\n",
      "331/388, train_loss: 0.0896, step time: 0.5214\n",
      "332/388, train_loss: 0.1989, step time: 0.4996\n",
      "333/388, train_loss: 0.1605, step time: 0.5158\n",
      "334/388, train_loss: 0.1088, step time: 0.4921\n",
      "335/388, train_loss: 0.1459, step time: 0.5074\n",
      "336/388, train_loss: 0.1221, step time: 0.5866\n",
      "337/388, train_loss: 0.1811, step time: 0.5365\n",
      "338/388, train_loss: 0.1223, step time: 0.5051\n",
      "339/388, train_loss: 0.0910, step time: 0.4863\n",
      "340/388, train_loss: 0.0786, step time: 1.1739\n",
      "341/388, train_loss: 0.2640, step time: 0.5490\n",
      "342/388, train_loss: 0.0910, step time: 0.5119\n",
      "343/388, train_loss: 0.4058, step time: 0.4865\n",
      "344/388, train_loss: 0.2029, step time: 0.4951\n",
      "345/388, train_loss: 0.1327, step time: 0.4818\n",
      "346/388, train_loss: 0.0907, step time: 0.4840\n",
      "347/388, train_loss: 0.2305, step time: 0.4910\n",
      "348/388, train_loss: 0.2404, step time: 0.4735\n",
      "349/388, train_loss: 0.1999, step time: 0.5206\n",
      "350/388, train_loss: 0.2152, step time: 0.5232\n",
      "351/388, train_loss: 0.1639, step time: 0.5005\n",
      "352/388, train_loss: 0.1978, step time: 0.4869\n",
      "353/388, train_loss: 0.3188, step time: 0.4924\n",
      "354/388, train_loss: 0.1923, step time: 0.5141\n",
      "355/388, train_loss: 0.0601, step time: 0.5092\n",
      "356/388, train_loss: 0.1327, step time: 0.4985\n",
      "357/388, train_loss: 0.0738, step time: 0.5142\n",
      "358/388, train_loss: 0.2667, step time: 0.4915\n",
      "359/388, train_loss: 0.2235, step time: 0.4967\n",
      "360/388, train_loss: 0.1558, step time: 0.4883\n",
      "361/388, train_loss: 0.0896, step time: 1.2084\n",
      "362/388, train_loss: 0.2023, step time: 0.5446\n",
      "363/388, train_loss: 0.2479, step time: 0.5066\n",
      "364/388, train_loss: 0.0969, step time: 0.5082\n",
      "365/388, train_loss: 0.0587, step time: 0.4962\n",
      "366/388, train_loss: 0.1332, step time: 0.4939\n",
      "367/388, train_loss: 0.2834, step time: 0.9851\n",
      "368/388, train_loss: 0.1137, step time: 0.5357\n",
      "369/388, train_loss: 0.1115, step time: 0.5075\n",
      "370/388, train_loss: 0.3060, step time: 0.4838\n",
      "371/388, train_loss: 0.1316, step time: 0.4919\n",
      "372/388, train_loss: 0.0964, step time: 0.4790\n",
      "373/388, train_loss: 0.1005, step time: 0.5082\n",
      "374/388, train_loss: 0.1238, step time: 0.4876\n",
      "375/388, train_loss: 0.1595, step time: 0.4919\n",
      "376/388, train_loss: 0.0550, step time: 0.4830\n",
      "377/388, train_loss: 0.3818, step time: 0.4980\n",
      "378/388, train_loss: 0.2157, step time: 0.5517\n",
      "379/388, train_loss: 0.1983, step time: 0.5380\n",
      "380/388, train_loss: 0.0754, step time: 0.5143\n",
      "381/388, train_loss: 0.1133, step time: 0.4893\n",
      "382/388, train_loss: 0.1365, step time: 0.4727\n",
      "383/388, train_loss: 0.1671, step time: 1.1114\n",
      "384/388, train_loss: 0.1173, step time: 0.5360\n",
      "385/388, train_loss: 0.3734, step time: 0.5056\n",
      "386/388, train_loss: 0.1931, step time: 0.4985\n",
      "387/388, train_loss: 0.1966, step time: 0.4829\n",
      "388/388, train_loss: 0.3253, step time: 0.4739\n",
      "epoch 76 average loss: 0.1748\n",
      "saved new best metric model\n",
      "current epoch: 76 current mean dice: 0.7748 tc: 0.8243 wt: 0.9000 et: 0.6003\n",
      "best mean dice: 0.7748 at epoch: 76\n",
      "time consuming of epoch 76 is: 303.9161\n",
      "----------\n",
      "epoch 77/300\n",
      "1/388, train_loss: 0.1872, step time: 0.4763\n",
      "2/388, train_loss: 0.0513, step time: 0.4919\n",
      "3/388, train_loss: 0.1536, step time: 0.4822\n",
      "4/388, train_loss: 0.0724, step time: 0.9615\n",
      "5/388, train_loss: 0.1363, step time: 0.5297\n",
      "6/388, train_loss: 0.1070, step time: 0.5362\n",
      "7/388, train_loss: 0.2631, step time: 0.5028\n",
      "8/388, train_loss: 0.1020, step time: 0.4858\n",
      "9/388, train_loss: 0.1599, step time: 0.4849\n",
      "10/388, train_loss: 0.2733, step time: 0.5295\n",
      "11/388, train_loss: 0.2255, step time: 0.5175\n",
      "12/388, train_loss: 0.4640, step time: 0.5005\n",
      "13/388, train_loss: 0.1660, step time: 0.5051\n",
      "14/388, train_loss: 0.3024, step time: 0.4887\n",
      "15/388, train_loss: 0.0949, step time: 0.9230\n",
      "16/388, train_loss: 0.3601, step time: 0.5584\n",
      "17/388, train_loss: 0.3280, step time: 0.5147\n",
      "18/388, train_loss: 0.1647, step time: 0.5041\n",
      "19/388, train_loss: 0.0861, step time: 0.4933\n",
      "20/388, train_loss: 0.3513, step time: 0.5048\n",
      "21/388, train_loss: 0.1639, step time: 0.5439\n",
      "22/388, train_loss: 0.3903, step time: 0.5262\n",
      "23/388, train_loss: 0.1606, step time: 0.5583\n",
      "24/388, train_loss: 0.3640, step time: 0.5342\n",
      "25/388, train_loss: 0.0716, step time: 0.5055\n",
      "26/388, train_loss: 0.0763, step time: 0.4876\n",
      "27/388, train_loss: 0.2423, step time: 0.4980\n",
      "28/388, train_loss: 0.0867, step time: 0.5103\n",
      "29/388, train_loss: 0.0452, step time: 0.5872\n",
      "30/388, train_loss: 0.0451, step time: 0.5387\n",
      "31/388, train_loss: 0.2723, step time: 0.5138\n",
      "32/388, train_loss: 0.1013, step time: 0.5049\n",
      "33/388, train_loss: 0.0756, step time: 0.4862\n",
      "34/388, train_loss: 0.1970, step time: 0.4855\n",
      "35/388, train_loss: 0.2089, step time: 0.5204\n",
      "36/388, train_loss: 0.1482, step time: 0.5201\n",
      "37/388, train_loss: 0.0970, step time: 0.5058\n",
      "38/388, train_loss: 0.2656, step time: 0.5191\n",
      "39/388, train_loss: 0.0889, step time: 0.5047\n",
      "40/388, train_loss: 0.1697, step time: 0.5106\n",
      "41/388, train_loss: 0.0679, step time: 0.5673\n",
      "42/388, train_loss: 0.2523, step time: 0.5343\n",
      "43/388, train_loss: 0.1022, step time: 0.4941\n",
      "44/388, train_loss: 0.0398, step time: 0.4866\n",
      "45/388, train_loss: 0.2052, step time: 0.5191\n",
      "46/388, train_loss: 0.1155, step time: 0.4896\n",
      "47/388, train_loss: 0.1074, step time: 1.1574\n",
      "48/388, train_loss: 0.1430, step time: 0.5359\n",
      "49/388, train_loss: 0.2470, step time: 0.5160\n",
      "50/388, train_loss: 0.1089, step time: 0.4873\n",
      "51/388, train_loss: 0.2858, step time: 0.5308\n",
      "52/388, train_loss: 0.1582, step time: 0.4974\n",
      "53/388, train_loss: 0.1870, step time: 0.5112\n",
      "54/388, train_loss: 0.1636, step time: 0.4996\n",
      "55/388, train_loss: 0.1455, step time: 0.5296\n",
      "56/388, train_loss: 0.1839, step time: 0.4956\n",
      "57/388, train_loss: 0.0928, step time: 0.7350\n",
      "58/388, train_loss: 0.1624, step time: 0.5716\n",
      "59/388, train_loss: 0.1326, step time: 0.5347\n",
      "60/388, train_loss: 0.0538, step time: 0.4927\n",
      "61/388, train_loss: 0.1615, step time: 0.4929\n",
      "62/388, train_loss: 0.0653, step time: 0.4824\n",
      "63/388, train_loss: 0.2008, step time: 0.4746\n",
      "64/388, train_loss: 0.0942, step time: 0.4883\n",
      "65/388, train_loss: 0.2695, step time: 0.5645\n",
      "66/388, train_loss: 0.1124, step time: 0.5688\n",
      "67/388, train_loss: 0.4079, step time: 0.5665\n",
      "68/388, train_loss: 0.1063, step time: 0.5276\n",
      "69/388, train_loss: 0.2130, step time: 0.5284\n",
      "70/388, train_loss: 0.0682, step time: 0.5073\n",
      "71/388, train_loss: 0.2684, step time: 0.4942\n",
      "72/388, train_loss: 0.0834, step time: 0.8952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/388, train_loss: 0.1089, step time: 0.5349\n",
      "74/388, train_loss: 0.1180, step time: 0.5015\n",
      "75/388, train_loss: 0.1849, step time: 0.5619\n",
      "76/388, train_loss: 0.1737, step time: 0.5525\n",
      "77/388, train_loss: 0.1945, step time: 0.5258\n",
      "78/388, train_loss: 0.4705, step time: 0.5024\n",
      "79/388, train_loss: 0.2012, step time: 0.4971\n",
      "80/388, train_loss: 0.3505, step time: 0.4798\n",
      "81/388, train_loss: 0.0696, step time: 0.4874\n",
      "82/388, train_loss: 0.1513, step time: 0.4853\n",
      "83/388, train_loss: 0.2612, step time: 0.4816\n",
      "84/388, train_loss: 0.1829, step time: 1.2261\n",
      "85/388, train_loss: 0.1945, step time: 0.5496\n",
      "86/388, train_loss: 0.3425, step time: 0.5131\n",
      "87/388, train_loss: 0.0907, step time: 0.5079\n",
      "88/388, train_loss: 0.1147, step time: 0.4959\n",
      "89/388, train_loss: 0.1744, step time: 0.4897\n",
      "90/388, train_loss: 0.2258, step time: 1.1797\n",
      "91/388, train_loss: 0.2170, step time: 0.5479\n",
      "92/388, train_loss: 0.1801, step time: 0.5190\n",
      "93/388, train_loss: 0.0873, step time: 0.5008\n",
      "94/388, train_loss: 0.3197, step time: 0.4974\n",
      "95/388, train_loss: 0.1274, step time: 0.4807\n",
      "96/388, train_loss: 0.2128, step time: 0.5043\n",
      "97/388, train_loss: 0.1756, step time: 0.5259\n",
      "98/388, train_loss: 0.4814, step time: 0.5099\n",
      "99/388, train_loss: 0.1264, step time: 0.4962\n",
      "100/388, train_loss: 0.1411, step time: 0.8301\n",
      "101/388, train_loss: 0.1632, step time: 0.5363\n",
      "102/388, train_loss: 0.1503, step time: 0.5102\n",
      "103/388, train_loss: 0.0956, step time: 0.4976\n",
      "104/388, train_loss: 0.1058, step time: 0.4964\n",
      "105/388, train_loss: 0.1178, step time: 0.4846\n",
      "106/388, train_loss: 0.1435, step time: 0.4852\n",
      "107/388, train_loss: 0.2856, step time: 0.4885\n",
      "108/388, train_loss: 0.1483, step time: 0.5062\n",
      "109/388, train_loss: 0.1908, step time: 0.5452\n",
      "110/388, train_loss: 0.1248, step time: 0.5053\n",
      "111/388, train_loss: 0.1138, step time: 0.5032\n",
      "112/388, train_loss: 0.1798, step time: 0.4880\n",
      "113/388, train_loss: 0.1368, step time: 0.4894\n",
      "114/388, train_loss: 0.1911, step time: 0.4898\n",
      "115/388, train_loss: 0.2628, step time: 0.9595\n",
      "116/388, train_loss: 0.1569, step time: 0.5394\n",
      "117/388, train_loss: 0.1624, step time: 0.5196\n",
      "118/388, train_loss: 0.0955, step time: 0.4918\n",
      "119/388, train_loss: 0.1278, step time: 0.4878\n",
      "120/388, train_loss: 0.1376, step time: 0.4896\n",
      "121/388, train_loss: 0.2637, step time: 0.4783\n",
      "122/388, train_loss: 0.2516, step time: 0.4771\n",
      "123/388, train_loss: 0.3353, step time: 0.9916\n",
      "124/388, train_loss: 0.2289, step time: 0.5514\n",
      "125/388, train_loss: 0.1091, step time: 0.5139\n",
      "126/388, train_loss: 0.0870, step time: 0.5021\n",
      "127/388, train_loss: 0.4501, step time: 0.5040\n",
      "128/388, train_loss: 0.2113, step time: 0.4869\n",
      "129/388, train_loss: 0.1888, step time: 0.4876\n",
      "130/388, train_loss: 0.1380, step time: 0.4950\n",
      "131/388, train_loss: 0.0908, step time: 0.6497\n",
      "132/388, train_loss: 0.0805, step time: 0.5543\n",
      "133/388, train_loss: 0.1403, step time: 0.5160\n",
      "134/388, train_loss: 0.1258, step time: 0.4896\n",
      "135/388, train_loss: 0.0826, step time: 0.4818\n",
      "136/388, train_loss: 0.3249, step time: 0.4923\n",
      "137/388, train_loss: 0.2030, step time: 0.4786\n",
      "138/388, train_loss: 0.1530, step time: 1.1205\n",
      "139/388, train_loss: 0.1381, step time: 0.5363\n",
      "140/388, train_loss: 0.2410, step time: 0.5125\n",
      "141/388, train_loss: 0.0720, step time: 0.5011\n",
      "142/388, train_loss: 0.0632, step time: 0.5010\n",
      "143/388, train_loss: 0.1022, step time: 0.4863\n",
      "144/388, train_loss: 0.2060, step time: 1.1350\n",
      "145/388, train_loss: 0.0494, step time: 0.5393\n",
      "146/388, train_loss: 0.3513, step time: 0.5091\n",
      "147/388, train_loss: 0.0647, step time: 0.4909\n",
      "148/388, train_loss: 0.2465, step time: 0.4877\n",
      "149/388, train_loss: 0.1297, step time: 0.4931\n",
      "150/388, train_loss: 0.0645, step time: 0.4763\n",
      "151/388, train_loss: 0.0603, step time: 0.4798\n",
      "152/388, train_loss: 0.0923, step time: 0.4880\n",
      "153/388, train_loss: 0.1190, step time: 0.4738\n",
      "154/388, train_loss: 0.1202, step time: 0.5068\n",
      "155/388, train_loss: 0.0821, step time: 0.4886\n",
      "156/388, train_loss: 0.0682, step time: 0.4914\n",
      "157/388, train_loss: 0.1647, step time: 0.4900\n",
      "158/388, train_loss: 0.0424, step time: 0.5069\n",
      "159/388, train_loss: 0.1376, step time: 0.5092\n",
      "160/388, train_loss: 0.0950, step time: 0.5264\n",
      "161/388, train_loss: 0.1807, step time: 0.5017\n",
      "162/388, train_loss: 0.1267, step time: 0.4978\n",
      "163/388, train_loss: 0.5409, step time: 1.2252\n",
      "164/388, train_loss: 0.0795, step time: 0.5342\n",
      "165/388, train_loss: 0.1163, step time: 0.4985\n",
      "166/388, train_loss: 0.1707, step time: 0.4954\n",
      "167/388, train_loss: 0.1969, step time: 0.4977\n",
      "168/388, train_loss: 0.5897, step time: 0.4883\n",
      "169/388, train_loss: 0.1491, step time: 1.1160\n",
      "170/388, train_loss: 0.1043, step time: 0.5403\n",
      "171/388, train_loss: 0.1841, step time: 0.5069\n",
      "172/388, train_loss: 0.0456, step time: 0.4889\n",
      "173/388, train_loss: 0.1204, step time: 0.4877\n",
      "174/388, train_loss: 0.2040, step time: 0.4888\n",
      "175/388, train_loss: 0.2501, step time: 0.7451\n",
      "176/388, train_loss: 0.0616, step time: 0.5538\n",
      "177/388, train_loss: 0.0965, step time: 0.5235\n",
      "178/388, train_loss: 0.3871, step time: 0.4949\n",
      "179/388, train_loss: 0.1392, step time: 0.4927\n",
      "180/388, train_loss: 0.2406, step time: 0.4838\n",
      "181/388, train_loss: 0.3008, step time: 0.4870\n",
      "182/388, train_loss: 0.2018, step time: 0.4988\n",
      "183/388, train_loss: 0.0848, step time: 0.4828\n",
      "184/388, train_loss: 0.1380, step time: 1.1824\n",
      "185/388, train_loss: 0.1180, step time: 0.5237\n",
      "186/388, train_loss: 0.1385, step time: 0.5039\n",
      "187/388, train_loss: 0.0648, step time: 0.4895\n",
      "188/388, train_loss: 0.0814, step time: 0.4946\n",
      "189/388, train_loss: 0.2576, step time: 0.4898\n",
      "190/388, train_loss: 0.1775, step time: 1.1393\n",
      "191/388, train_loss: 0.0953, step time: 0.5249\n",
      "192/388, train_loss: 0.5513, step time: 0.5062\n",
      "193/388, train_loss: 0.0713, step time: 0.4947\n",
      "194/388, train_loss: 0.2031, step time: 0.4835\n",
      "195/388, train_loss: 0.1020, step time: 0.5004\n",
      "196/388, train_loss: 0.3884, step time: 0.4975\n",
      "197/388, train_loss: 0.2478, step time: 0.4827\n",
      "198/388, train_loss: 0.3967, step time: 0.5045\n",
      "199/388, train_loss: 0.1243, step time: 0.4934\n",
      "200/388, train_loss: 0.0899, step time: 0.4979\n",
      "201/388, train_loss: 0.1907, step time: 0.4856\n",
      "202/388, train_loss: 0.1478, step time: 1.0592\n",
      "203/388, train_loss: 0.1888, step time: 0.5262\n",
      "204/388, train_loss: 0.0919, step time: 0.5017\n",
      "205/388, train_loss: 0.1396, step time: 0.4895\n",
      "206/388, train_loss: 0.1294, step time: 0.5505\n",
      "207/388, train_loss: 0.0660, step time: 0.5312\n",
      "208/388, train_loss: 0.0741, step time: 0.5036\n",
      "209/388, train_loss: 0.2404, step time: 0.5029\n",
      "210/388, train_loss: 0.3236, step time: 0.5534\n",
      "211/388, train_loss: 0.3175, step time: 0.5195\n",
      "212/388, train_loss: 0.1986, step time: 0.5086\n",
      "213/388, train_loss: 0.2169, step time: 0.4944\n",
      "214/388, train_loss: 0.1079, step time: 0.5135\n",
      "215/388, train_loss: 0.1011, step time: 0.4981\n",
      "216/388, train_loss: 0.1845, step time: 0.4955\n",
      "217/388, train_loss: 0.1395, step time: 0.5094\n",
      "218/388, train_loss: 0.2305, step time: 0.5036\n",
      "219/388, train_loss: 0.1068, step time: 0.4952\n",
      "220/388, train_loss: 0.2463, step time: 0.4854\n",
      "221/388, train_loss: 0.4068, step time: 0.4928\n",
      "222/388, train_loss: 0.1985, step time: 0.4932\n",
      "223/388, train_loss: 0.1562, step time: 0.5075\n",
      "224/388, train_loss: 0.4798, step time: 0.4947\n",
      "225/388, train_loss: 0.0932, step time: 0.5738\n",
      "226/388, train_loss: 0.2435, step time: 0.5719\n",
      "227/388, train_loss: 0.1251, step time: 0.5284\n",
      "228/388, train_loss: 0.1415, step time: 0.4957\n",
      "229/388, train_loss: 0.1249, step time: 0.5065\n",
      "230/388, train_loss: 0.2561, step time: 0.4924\n",
      "231/388, train_loss: 0.2131, step time: 0.5756\n",
      "232/388, train_loss: 0.1055, step time: 0.5859\n",
      "233/388, train_loss: 0.0920, step time: 0.5451\n",
      "234/388, train_loss: 0.1659, step time: 0.5268\n",
      "235/388, train_loss: 0.0794, step time: 0.5117\n",
      "236/388, train_loss: 0.0835, step time: 0.4932\n",
      "237/388, train_loss: 0.0317, step time: 0.5375\n",
      "238/388, train_loss: 0.1175, step time: 0.5121\n",
      "239/388, train_loss: 0.2334, step time: 0.5060\n",
      "240/388, train_loss: 0.0895, step time: 0.5480\n",
      "241/388, train_loss: 0.1935, step time: 0.5074\n",
      "242/388, train_loss: 0.1224, step time: 0.4971\n",
      "243/388, train_loss: 0.1384, step time: 0.5048\n",
      "244/388, train_loss: 0.4300, step time: 0.5051\n",
      "245/388, train_loss: 0.2689, step time: 0.5040\n",
      "246/388, train_loss: 0.2954, step time: 0.4816\n",
      "247/388, train_loss: 0.0748, step time: 0.6120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/388, train_loss: 0.2560, step time: 0.5431\n",
      "249/388, train_loss: 0.2280, step time: 0.5022\n",
      "250/388, train_loss: 0.1592, step time: 0.5056\n",
      "251/388, train_loss: 0.0515, step time: 0.4878\n",
      "252/388, train_loss: 0.3284, step time: 0.5734\n",
      "253/388, train_loss: 0.0803, step time: 0.6061\n",
      "254/388, train_loss: 0.1984, step time: 0.5476\n",
      "255/388, train_loss: 0.1428, step time: 0.4976\n",
      "256/388, train_loss: 0.1171, step time: 0.5200\n",
      "257/388, train_loss: 0.1741, step time: 0.5956\n",
      "258/388, train_loss: 0.0996, step time: 0.5433\n",
      "259/388, train_loss: 0.1703, step time: 0.5070\n",
      "260/388, train_loss: 0.1014, step time: 0.5072\n",
      "261/388, train_loss: 0.4790, step time: 0.5180\n",
      "262/388, train_loss: 0.5539, step time: 0.5346\n",
      "263/388, train_loss: 0.1209, step time: 0.5040\n",
      "264/388, train_loss: 0.0927, step time: 0.5000\n",
      "265/388, train_loss: 0.1987, step time: 0.4901\n",
      "266/388, train_loss: 0.2135, step time: 0.5693\n",
      "267/388, train_loss: 0.1169, step time: 0.5469\n",
      "268/388, train_loss: 0.2114, step time: 1.0362\n",
      "269/388, train_loss: 0.1155, step time: 0.5341\n",
      "270/388, train_loss: 0.1300, step time: 0.5116\n",
      "271/388, train_loss: 0.3267, step time: 0.4873\n",
      "272/388, train_loss: 0.1003, step time: 0.4863\n",
      "273/388, train_loss: 0.2440, step time: 0.9936\n",
      "274/388, train_loss: 0.1162, step time: 0.5449\n",
      "275/388, train_loss: 0.1002, step time: 0.5138\n",
      "276/388, train_loss: 0.2607, step time: 0.4996\n",
      "277/388, train_loss: 0.0412, step time: 0.5002\n",
      "278/388, train_loss: 0.3433, step time: 0.4828\n",
      "279/388, train_loss: 0.5425, step time: 0.5317\n",
      "280/388, train_loss: 0.2091, step time: 0.4938\n",
      "281/388, train_loss: 0.3765, step time: 0.5020\n",
      "282/388, train_loss: 0.2183, step time: 0.5176\n",
      "283/388, train_loss: 0.1179, step time: 0.5261\n",
      "284/388, train_loss: 0.1496, step time: 0.4961\n",
      "285/388, train_loss: 0.1712, step time: 0.5361\n",
      "286/388, train_loss: 0.1353, step time: 0.6604\n",
      "287/388, train_loss: 0.2570, step time: 0.5507\n",
      "288/388, train_loss: 0.0312, step time: 0.5077\n",
      "289/388, train_loss: 0.2212, step time: 0.5034\n",
      "290/388, train_loss: 0.1485, step time: 0.4857\n",
      "291/388, train_loss: 0.0684, step time: 0.5011\n",
      "292/388, train_loss: 0.2561, step time: 0.9733\n",
      "293/388, train_loss: 0.1875, step time: 0.5506\n",
      "294/388, train_loss: 0.1991, step time: 0.5192\n",
      "295/388, train_loss: 0.1333, step time: 0.4989\n",
      "296/388, train_loss: 0.0956, step time: 0.4993\n",
      "297/388, train_loss: 0.3061, step time: 0.5083\n",
      "298/388, train_loss: 0.0883, step time: 0.5341\n",
      "299/388, train_loss: 0.2598, step time: 0.5899\n",
      "300/388, train_loss: 0.1929, step time: 0.5443\n",
      "301/388, train_loss: 0.2793, step time: 0.5252\n",
      "302/388, train_loss: 0.1214, step time: 0.5133\n",
      "303/388, train_loss: 0.2774, step time: 0.5049\n",
      "304/388, train_loss: 0.2033, step time: 0.5266\n",
      "305/388, train_loss: 0.0741, step time: 0.5074\n",
      "306/388, train_loss: 0.2794, step time: 0.4904\n",
      "307/388, train_loss: 0.1296, step time: 0.5405\n",
      "308/388, train_loss: 0.2310, step time: 0.5341\n",
      "309/388, train_loss: 0.1345, step time: 0.5169\n",
      "310/388, train_loss: 0.1026, step time: 0.5093\n",
      "311/388, train_loss: 0.1126, step time: 0.5297\n",
      "312/388, train_loss: 0.1757, step time: 0.5086\n",
      "313/388, train_loss: 0.0777, step time: 0.7143\n",
      "314/388, train_loss: 0.1025, step time: 0.5592\n",
      "315/388, train_loss: 0.2151, step time: 0.5130\n",
      "316/388, train_loss: 0.2170, step time: 0.4855\n",
      "317/388, train_loss: 0.1110, step time: 0.5038\n",
      "318/388, train_loss: 0.1256, step time: 0.5085\n",
      "319/388, train_loss: 0.0706, step time: 0.6016\n",
      "320/388, train_loss: 0.1691, step time: 0.5531\n",
      "321/388, train_loss: 0.2076, step time: 0.5222\n",
      "322/388, train_loss: 0.2427, step time: 0.4975\n",
      "323/388, train_loss: 0.2670, step time: 0.4965\n",
      "324/388, train_loss: 0.0932, step time: 1.0984\n",
      "325/388, train_loss: 0.2065, step time: 0.5552\n",
      "326/388, train_loss: 0.1372, step time: 0.5169\n",
      "327/388, train_loss: 0.1290, step time: 0.4901\n",
      "328/388, train_loss: 0.0682, step time: 0.4987\n",
      "329/388, train_loss: 0.0998, step time: 0.4837\n",
      "330/388, train_loss: 0.1735, step time: 0.6321\n",
      "331/388, train_loss: 0.1568, step time: 0.5603\n",
      "332/388, train_loss: 0.4966, step time: 0.5231\n",
      "333/388, train_loss: 0.0974, step time: 0.5591\n",
      "334/388, train_loss: 0.0648, step time: 0.5230\n",
      "335/388, train_loss: 0.1295, step time: 0.5395\n",
      "336/388, train_loss: 0.1752, step time: 0.5232\n",
      "337/388, train_loss: 0.2973, step time: 0.5716\n",
      "338/388, train_loss: 0.1501, step time: 0.5383\n",
      "339/388, train_loss: 0.0592, step time: 0.5067\n",
      "340/388, train_loss: 0.1251, step time: 0.4930\n",
      "341/388, train_loss: 0.2997, step time: 1.1096\n",
      "342/388, train_loss: 0.0904, step time: 0.5366\n",
      "343/388, train_loss: 0.1854, step time: 0.5156\n",
      "344/388, train_loss: 0.1186, step time: 0.4937\n",
      "345/388, train_loss: 0.0927, step time: 0.4940\n",
      "346/388, train_loss: 0.1492, step time: 0.5196\n",
      "347/388, train_loss: 0.1226, step time: 0.5047\n",
      "348/388, train_loss: 0.0759, step time: 0.4856\n",
      "349/388, train_loss: 0.2608, step time: 0.4873\n",
      "350/388, train_loss: 0.0737, step time: 0.4940\n",
      "351/388, train_loss: 0.2749, step time: 0.5024\n",
      "352/388, train_loss: 0.2131, step time: 0.5216\n",
      "353/388, train_loss: 0.2433, step time: 0.5365\n",
      "354/388, train_loss: 0.4539, step time: 0.6734\n",
      "355/388, train_loss: 0.2840, step time: 0.5634\n",
      "356/388, train_loss: 0.3378, step time: 0.5218\n",
      "357/388, train_loss: 0.1053, step time: 0.5094\n",
      "358/388, train_loss: 0.1318, step time: 0.5427\n",
      "359/388, train_loss: 0.2824, step time: 0.5178\n",
      "360/388, train_loss: 0.2570, step time: 0.4927\n",
      "361/388, train_loss: 0.1657, step time: 0.4952\n",
      "362/388, train_loss: 0.0747, step time: 1.2158\n",
      "363/388, train_loss: 0.0828, step time: 0.5165\n",
      "364/388, train_loss: 0.0943, step time: 0.4941\n",
      "365/388, train_loss: 0.1952, step time: 0.4988\n",
      "366/388, train_loss: 0.3081, step time: 0.4859\n",
      "367/388, train_loss: 0.1386, step time: 0.4831\n",
      "368/388, train_loss: 0.1368, step time: 0.4964\n",
      "369/388, train_loss: 0.0863, step time: 0.4904\n",
      "370/388, train_loss: 0.2383, step time: 0.5002\n",
      "371/388, train_loss: 0.1748, step time: 1.1650\n",
      "372/388, train_loss: 0.0675, step time: 0.5329\n",
      "373/388, train_loss: 0.1639, step time: 0.5181\n",
      "374/388, train_loss: 0.0353, step time: 0.4972\n",
      "375/388, train_loss: 0.0594, step time: 0.4914\n",
      "376/388, train_loss: 0.1734, step time: 0.5002\n",
      "377/388, train_loss: 0.0716, step time: 0.5198\n",
      "378/388, train_loss: 0.1951, step time: 0.5092\n",
      "379/388, train_loss: 0.1500, step time: 0.4864\n",
      "380/388, train_loss: 0.1114, step time: 0.5034\n",
      "381/388, train_loss: 0.1982, step time: 0.5524\n",
      "382/388, train_loss: 0.2463, step time: 0.5239\n",
      "383/388, train_loss: 0.2684, step time: 0.5097\n",
      "384/388, train_loss: 0.1045, step time: 0.4940\n",
      "385/388, train_loss: 0.2097, step time: 0.4818\n",
      "386/388, train_loss: 0.1722, step time: 0.4876\n",
      "387/388, train_loss: 0.3102, step time: 0.4723\n",
      "388/388, train_loss: 0.1270, step time: 0.4913\n",
      "epoch 77 average loss: 0.1771\n",
      "current epoch: 77 current mean dice: 0.7738 tc: 0.8234 wt: 0.9025 et: 0.5954\n",
      "best mean dice: 0.7748 at epoch: 76\n",
      "time consuming of epoch 77 is: 302.0350\n",
      "----------\n",
      "epoch 78/300\n",
      "1/388, train_loss: 0.1869, step time: 0.4695\n",
      "2/388, train_loss: 0.1147, step time: 0.4818\n",
      "3/388, train_loss: 0.1206, step time: 0.5114\n",
      "4/388, train_loss: 0.0830, step time: 0.5252\n",
      "5/388, train_loss: 0.1999, step time: 0.4999\n",
      "6/388, train_loss: 0.2877, step time: 0.4987\n",
      "7/388, train_loss: 0.3651, step time: 0.8407\n",
      "8/388, train_loss: 0.1699, step time: 0.5681\n",
      "9/388, train_loss: 0.1167, step time: 0.5227\n",
      "10/388, train_loss: 0.2208, step time: 0.5111\n",
      "11/388, train_loss: 0.0889, step time: 0.4961\n",
      "12/388, train_loss: 0.1101, step time: 0.7803\n",
      "13/388, train_loss: 0.1241, step time: 0.5406\n",
      "14/388, train_loss: 0.2795, step time: 0.5054\n",
      "15/388, train_loss: 0.2128, step time: 0.5356\n",
      "16/388, train_loss: 0.0803, step time: 0.5558\n",
      "17/388, train_loss: 0.1746, step time: 0.5238\n",
      "18/388, train_loss: 0.2928, step time: 0.5089\n",
      "19/388, train_loss: 0.1744, step time: 0.4949\n",
      "20/388, train_loss: 0.5183, step time: 0.4968\n",
      "21/388, train_loss: 0.2141, step time: 0.5500\n",
      "22/388, train_loss: 0.1920, step time: 0.5509\n",
      "23/388, train_loss: 0.1389, step time: 0.5818\n",
      "24/388, train_loss: 0.1806, step time: 0.5434\n",
      "25/388, train_loss: 0.2091, step time: 0.5151\n",
      "26/388, train_loss: 0.2444, step time: 0.4881\n",
      "27/388, train_loss: 0.1428, step time: 0.5020\n",
      "28/388, train_loss: 0.1577, step time: 0.4977\n",
      "29/388, train_loss: 0.2620, step time: 0.4912\n",
      "30/388, train_loss: 0.2047, step time: 1.1879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/388, train_loss: 0.3734, step time: 0.5211\n",
      "32/388, train_loss: 0.2036, step time: 0.5091\n",
      "33/388, train_loss: 0.0789, step time: 0.5029\n",
      "34/388, train_loss: 0.1124, step time: 0.4994\n",
      "35/388, train_loss: 0.4444, step time: 0.6386\n",
      "36/388, train_loss: 0.1034, step time: 0.5790\n",
      "37/388, train_loss: 0.2375, step time: 0.5441\n",
      "38/388, train_loss: 0.0846, step time: 0.5113\n",
      "39/388, train_loss: 0.4649, step time: 0.4978\n",
      "40/388, train_loss: 0.1396, step time: 0.4907\n",
      "41/388, train_loss: 0.1947, step time: 0.4754\n",
      "42/388, train_loss: 0.0361, step time: 0.4989\n",
      "43/388, train_loss: 0.0709, step time: 0.5106\n",
      "44/388, train_loss: 0.1050, step time: 0.5075\n",
      "45/388, train_loss: 0.0665, step time: 0.5130\n",
      "46/388, train_loss: 0.1707, step time: 0.4957\n",
      "47/388, train_loss: 0.1207, step time: 0.9594\n",
      "48/388, train_loss: 0.2287, step time: 0.5374\n",
      "49/388, train_loss: 0.3799, step time: 0.5147\n",
      "50/388, train_loss: 0.0954, step time: 0.4978\n",
      "51/388, train_loss: 0.1000, step time: 0.4973\n",
      "52/388, train_loss: 0.1229, step time: 0.4978\n",
      "53/388, train_loss: 0.2816, step time: 0.4915\n",
      "54/388, train_loss: 0.4623, step time: 0.4974\n",
      "55/388, train_loss: 0.0856, step time: 0.4847\n",
      "56/388, train_loss: 0.1341, step time: 0.9839\n",
      "57/388, train_loss: 0.1664, step time: 0.5397\n",
      "58/388, train_loss: 0.0985, step time: 0.5145\n",
      "59/388, train_loss: 0.1592, step time: 0.5007\n",
      "60/388, train_loss: 0.1102, step time: 0.4935\n",
      "61/388, train_loss: 0.0446, step time: 0.4976\n",
      "62/388, train_loss: 0.0934, step time: 0.4843\n",
      "63/388, train_loss: 0.1141, step time: 0.4765\n",
      "64/388, train_loss: 0.1409, step time: 0.6707\n",
      "65/388, train_loss: 0.1308, step time: 0.5406\n",
      "66/388, train_loss: 0.0890, step time: 0.5192\n",
      "67/388, train_loss: 0.1339, step time: 0.5146\n",
      "68/388, train_loss: 0.2248, step time: 0.5009\n",
      "69/388, train_loss: 0.1331, step time: 1.1458\n",
      "70/388, train_loss: 0.0653, step time: 0.5412\n",
      "71/388, train_loss: 0.2691, step time: 0.5173\n",
      "72/388, train_loss: 0.2073, step time: 0.5021\n",
      "73/388, train_loss: 0.1822, step time: 0.4958\n",
      "74/388, train_loss: 0.1287, step time: 0.4960\n",
      "75/388, train_loss: 0.0818, step time: 0.4772\n",
      "76/388, train_loss: 0.1691, step time: 0.4963\n",
      "77/388, train_loss: 0.0458, step time: 0.4943\n",
      "78/388, train_loss: 0.2212, step time: 0.4829\n",
      "79/388, train_loss: 0.2375, step time: 0.4940\n",
      "80/388, train_loss: 0.1200, step time: 0.4747\n",
      "81/388, train_loss: 0.0845, step time: 0.5083\n",
      "82/388, train_loss: 0.2176, step time: 0.4980\n",
      "83/388, train_loss: 0.1837, step time: 0.4976\n",
      "84/388, train_loss: 0.5011, step time: 0.5428\n",
      "85/388, train_loss: 0.2072, step time: 0.5201\n",
      "86/388, train_loss: 0.1263, step time: 0.4839\n",
      "87/388, train_loss: 0.3005, step time: 0.4744\n",
      "88/388, train_loss: 0.3560, step time: 0.7559\n",
      "89/388, train_loss: 0.3176, step time: 0.5622\n",
      "90/388, train_loss: 0.0857, step time: 0.5232\n",
      "91/388, train_loss: 0.1528, step time: 0.4966\n",
      "92/388, train_loss: 0.1439, step time: 0.6786\n",
      "93/388, train_loss: 0.1398, step time: 0.5631\n",
      "94/388, train_loss: 0.1606, step time: 0.5281\n",
      "95/388, train_loss: 0.0940, step time: 0.5169\n",
      "96/388, train_loss: 0.0829, step time: 0.4975\n",
      "97/388, train_loss: 0.2976, step time: 0.5076\n",
      "98/388, train_loss: 0.1836, step time: 0.4853\n",
      "99/388, train_loss: 0.1519, step time: 0.5597\n",
      "100/388, train_loss: 0.2381, step time: 0.5219\n",
      "101/388, train_loss: 0.0718, step time: 0.5100\n",
      "102/388, train_loss: 0.2043, step time: 0.5166\n",
      "103/388, train_loss: 0.0947, step time: 0.4884\n",
      "104/388, train_loss: 0.1052, step time: 0.5128\n",
      "105/388, train_loss: 0.2709, step time: 0.5056\n",
      "106/388, train_loss: 0.1017, step time: 0.4981\n",
      "107/388, train_loss: 0.1490, step time: 0.4822\n",
      "108/388, train_loss: 0.1939, step time: 1.1169\n",
      "109/388, train_loss: 0.3893, step time: 0.5448\n",
      "110/388, train_loss: 0.1196, step time: 0.4989\n",
      "111/388, train_loss: 0.1489, step time: 0.4996\n",
      "112/388, train_loss: 0.1500, step time: 0.4856\n",
      "113/388, train_loss: 0.0537, step time: 0.4903\n",
      "114/388, train_loss: 0.1735, step time: 0.4777\n",
      "115/388, train_loss: 0.1687, step time: 0.5004\n",
      "116/388, train_loss: 0.1076, step time: 0.4835\n",
      "117/388, train_loss: 0.0956, step time: 0.4741\n",
      "118/388, train_loss: 0.2637, step time: 1.0207\n",
      "119/388, train_loss: 0.2135, step time: 0.5309\n",
      "120/388, train_loss: 0.1594, step time: 0.5055\n",
      "121/388, train_loss: 0.1098, step time: 0.4899\n",
      "122/388, train_loss: 0.2118, step time: 0.4917\n",
      "123/388, train_loss: 0.2999, step time: 0.5026\n",
      "124/388, train_loss: 0.1359, step time: 0.4934\n",
      "125/388, train_loss: 0.1351, step time: 0.5277\n",
      "126/388, train_loss: 0.1054, step time: 0.5096\n",
      "127/388, train_loss: 0.4267, step time: 0.5014\n",
      "128/388, train_loss: 0.0483, step time: 1.1407\n",
      "129/388, train_loss: 0.1117, step time: 0.5297\n",
      "130/388, train_loss: 0.1017, step time: 0.5053\n",
      "131/388, train_loss: 0.1594, step time: 0.4831\n",
      "132/388, train_loss: 0.1765, step time: 0.4929\n",
      "133/388, train_loss: 0.1184, step time: 0.4815\n",
      "134/388, train_loss: 0.1937, step time: 0.4793\n",
      "135/388, train_loss: 0.1059, step time: 0.4877\n",
      "136/388, train_loss: 0.0471, step time: 0.4863\n",
      "137/388, train_loss: 0.3299, step time: 0.5019\n",
      "138/388, train_loss: 0.2760, step time: 0.4832\n",
      "139/388, train_loss: 0.0946, step time: 0.4897\n",
      "140/388, train_loss: 0.1268, step time: 0.4982\n",
      "141/388, train_loss: 0.1366, step time: 0.4831\n",
      "142/388, train_loss: 0.0868, step time: 0.9139\n",
      "143/388, train_loss: 0.2932, step time: 0.5323\n",
      "144/388, train_loss: 0.0342, step time: 0.5088\n",
      "145/388, train_loss: 0.2300, step time: 0.4871\n",
      "146/388, train_loss: 0.3453, step time: 0.6384\n",
      "147/388, train_loss: 0.0293, step time: 0.5626\n",
      "148/388, train_loss: 0.0957, step time: 0.5238\n",
      "149/388, train_loss: 0.0974, step time: 0.5002\n",
      "150/388, train_loss: 0.1448, step time: 0.5011\n",
      "151/388, train_loss: 0.2442, step time: 0.4895\n",
      "152/388, train_loss: 0.1930, step time: 0.4779\n",
      "153/388, train_loss: 0.0977, step time: 0.4782\n",
      "154/388, train_loss: 0.2407, step time: 0.4795\n",
      "155/388, train_loss: 0.1242, step time: 0.4881\n",
      "156/388, train_loss: 0.2225, step time: 0.5069\n",
      "157/388, train_loss: 0.0709, step time: 0.4882\n",
      "158/388, train_loss: 0.2453, step time: 1.0757\n",
      "159/388, train_loss: 0.0818, step time: 0.5361\n",
      "160/388, train_loss: 0.2732, step time: 0.5109\n",
      "161/388, train_loss: 0.1218, step time: 0.4965\n",
      "162/388, train_loss: 0.3835, step time: 0.4813\n",
      "163/388, train_loss: 0.1229, step time: 0.4856\n",
      "164/388, train_loss: 0.0751, step time: 0.4850\n",
      "165/388, train_loss: 0.1265, step time: 0.4832\n",
      "166/388, train_loss: 0.1013, step time: 0.4874\n",
      "167/388, train_loss: 0.3147, step time: 0.5046\n",
      "168/388, train_loss: 0.1408, step time: 0.4872\n",
      "169/388, train_loss: 0.2035, step time: 0.4995\n",
      "170/388, train_loss: 0.5043, step time: 0.4994\n",
      "171/388, train_loss: 0.1071, step time: 0.4915\n",
      "172/388, train_loss: 0.0767, step time: 1.1091\n",
      "173/388, train_loss: 0.1129, step time: 0.5350\n",
      "174/388, train_loss: 0.1012, step time: 0.5053\n",
      "175/388, train_loss: 0.2119, step time: 0.4906\n",
      "176/388, train_loss: 0.1391, step time: 0.4917\n",
      "177/388, train_loss: 0.1904, step time: 0.4939\n",
      "178/388, train_loss: 0.0968, step time: 0.4787\n",
      "179/388, train_loss: 0.3323, step time: 0.4824\n",
      "180/388, train_loss: 0.1511, step time: 1.0345\n",
      "181/388, train_loss: 0.2805, step time: 0.5551\n",
      "182/388, train_loss: 0.5732, step time: 0.5119\n",
      "183/388, train_loss: 0.1348, step time: 0.5005\n",
      "184/388, train_loss: 0.1621, step time: 0.4851\n",
      "185/388, train_loss: 0.1098, step time: 0.4934\n",
      "186/388, train_loss: 0.2881, step time: 0.4792\n",
      "187/388, train_loss: 0.3158, step time: 0.4778\n",
      "188/388, train_loss: 0.3367, step time: 0.4932\n",
      "189/388, train_loss: 0.2393, step time: 0.5457\n",
      "190/388, train_loss: 0.1020, step time: 0.5156\n",
      "191/388, train_loss: 0.0956, step time: 0.5012\n",
      "192/388, train_loss: 0.1905, step time: 0.4875\n",
      "193/388, train_loss: 0.3618, step time: 0.6206\n",
      "194/388, train_loss: 0.2337, step time: 0.5400\n",
      "195/388, train_loss: 0.1990, step time: 0.5298\n",
      "196/388, train_loss: 0.2071, step time: 0.5028\n",
      "197/388, train_loss: 0.2927, step time: 0.4957\n",
      "198/388, train_loss: 0.1237, step time: 0.4895\n",
      "199/388, train_loss: 0.2514, step time: 0.4889\n",
      "200/388, train_loss: 0.2828, step time: 0.4893\n",
      "201/388, train_loss: 0.2217, step time: 0.5873\n",
      "202/388, train_loss: 0.2733, step time: 0.5584\n",
      "203/388, train_loss: 0.3117, step time: 0.5298\n",
      "204/388, train_loss: 0.1733, step time: 0.5084\n",
      "205/388, train_loss: 0.0812, step time: 0.4949\n",
      "206/388, train_loss: 0.0928, step time: 0.4803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/388, train_loss: 0.1366, step time: 0.4973\n",
      "208/388, train_loss: 0.2055, step time: 0.4973\n",
      "209/388, train_loss: 0.0719, step time: 0.4846\n",
      "210/388, train_loss: 0.0661, step time: 0.4958\n",
      "211/388, train_loss: 0.1102, step time: 0.4785\n",
      "212/388, train_loss: 0.3146, step time: 0.4834\n",
      "213/388, train_loss: 0.1409, step time: 0.5006\n",
      "214/388, train_loss: 0.2188, step time: 0.4963\n",
      "215/388, train_loss: 0.2048, step time: 0.4955\n",
      "216/388, train_loss: 0.2201, step time: 1.0975\n",
      "217/388, train_loss: 0.4518, step time: 0.5368\n",
      "218/388, train_loss: 0.2101, step time: 0.4979\n",
      "219/388, train_loss: 0.1428, step time: 0.4804\n",
      "220/388, train_loss: 0.1023, step time: 1.0163\n",
      "221/388, train_loss: 0.1956, step time: 0.5186\n",
      "222/388, train_loss: 0.1066, step time: 0.5010\n",
      "223/388, train_loss: 0.5000, step time: 0.4886\n",
      "224/388, train_loss: 0.1647, step time: 0.5304\n",
      "225/388, train_loss: 0.2341, step time: 0.5176\n",
      "226/388, train_loss: 0.1350, step time: 0.5074\n",
      "227/388, train_loss: 0.2952, step time: 0.5077\n",
      "228/388, train_loss: 0.2089, step time: 0.5130\n",
      "229/388, train_loss: 0.5680, step time: 0.5801\n",
      "230/388, train_loss: 0.0994, step time: 0.5296\n",
      "231/388, train_loss: 0.0909, step time: 0.5081\n",
      "232/388, train_loss: 0.3767, step time: 0.4986\n",
      "233/388, train_loss: 0.2282, step time: 0.4827\n",
      "234/388, train_loss: 0.3713, step time: 0.5746\n",
      "235/388, train_loss: 0.0872, step time: 0.5500\n",
      "236/388, train_loss: 0.0826, step time: 0.5160\n",
      "237/388, train_loss: 0.2339, step time: 0.5048\n",
      "238/388, train_loss: 0.2231, step time: 0.4843\n",
      "239/388, train_loss: 0.1680, step time: 0.5041\n",
      "240/388, train_loss: 0.4574, step time: 0.4847\n",
      "241/388, train_loss: 0.2290, step time: 0.7309\n",
      "242/388, train_loss: 0.1178, step time: 0.5621\n",
      "243/388, train_loss: 0.1192, step time: 0.5113\n",
      "244/388, train_loss: 0.0735, step time: 0.4991\n",
      "245/388, train_loss: 0.1158, step time: 0.4824\n",
      "246/388, train_loss: 0.2257, step time: 0.4792\n",
      "247/388, train_loss: 0.1582, step time: 1.0396\n",
      "248/388, train_loss: 0.0849, step time: 0.5336\n",
      "249/388, train_loss: 0.0613, step time: 0.5037\n",
      "250/388, train_loss: 0.3025, step time: 0.4937\n",
      "251/388, train_loss: 0.0631, step time: 0.4943\n",
      "252/388, train_loss: 0.1128, step time: 0.4797\n",
      "253/388, train_loss: 0.3329, step time: 0.4809\n",
      "254/388, train_loss: 0.1463, step time: 0.4773\n",
      "255/388, train_loss: 0.0596, step time: 0.6850\n",
      "256/388, train_loss: 0.0474, step time: 0.5430\n",
      "257/388, train_loss: 0.1072, step time: 0.5115\n",
      "258/388, train_loss: 0.1964, step time: 0.4988\n",
      "259/388, train_loss: 0.0548, step time: 0.4893\n",
      "260/388, train_loss: 0.1411, step time: 0.4783\n",
      "261/388, train_loss: 0.2091, step time: 0.6307\n",
      "262/388, train_loss: 0.5540, step time: 0.5363\n",
      "263/388, train_loss: 0.2276, step time: 0.5099\n",
      "264/388, train_loss: 0.1676, step time: 0.4845\n",
      "265/388, train_loss: 0.1481, step time: 0.5064\n",
      "266/388, train_loss: 0.0991, step time: 0.5004\n",
      "267/388, train_loss: 0.2028, step time: 0.4937\n",
      "268/388, train_loss: 0.1654, step time: 0.4955\n",
      "269/388, train_loss: 0.0661, step time: 0.4928\n",
      "270/388, train_loss: 0.0982, step time: 0.5002\n",
      "271/388, train_loss: 0.2865, step time: 0.5222\n",
      "272/388, train_loss: 0.1958, step time: 0.5716\n",
      "273/388, train_loss: 0.0708, step time: 0.5335\n",
      "274/388, train_loss: 0.2430, step time: 0.5123\n",
      "275/388, train_loss: 0.0794, step time: 0.5001\n",
      "276/388, train_loss: 0.2461, step time: 0.5321\n",
      "277/388, train_loss: 0.1340, step time: 0.5135\n",
      "278/388, train_loss: 0.1477, step time: 0.5342\n",
      "279/388, train_loss: 0.1009, step time: 0.5039\n",
      "280/388, train_loss: 0.0985, step time: 0.4993\n",
      "281/388, train_loss: 0.1401, step time: 1.0964\n",
      "282/388, train_loss: 0.1059, step time: 0.5375\n",
      "283/388, train_loss: 0.1943, step time: 0.5156\n",
      "284/388, train_loss: 0.1553, step time: 0.4903\n",
      "285/388, train_loss: 0.4117, step time: 0.5087\n",
      "286/388, train_loss: 0.4344, step time: 0.5069\n",
      "287/388, train_loss: 0.0473, step time: 0.4947\n",
      "288/388, train_loss: 0.0863, step time: 0.4771\n",
      "289/388, train_loss: 0.0926, step time: 0.5027\n",
      "290/388, train_loss: 0.2965, step time: 0.4893\n",
      "291/388, train_loss: 0.0935, step time: 0.4945\n",
      "292/388, train_loss: 0.1486, step time: 0.4985\n",
      "293/388, train_loss: 0.0942, step time: 0.4921\n",
      "294/388, train_loss: 0.1138, step time: 0.4949\n",
      "295/388, train_loss: 0.2250, step time: 0.5013\n",
      "296/388, train_loss: 0.2391, step time: 0.4976\n",
      "297/388, train_loss: 0.1965, step time: 0.5114\n",
      "298/388, train_loss: 0.1202, step time: 0.4953\n",
      "299/388, train_loss: 0.3691, step time: 0.4998\n",
      "300/388, train_loss: 0.3157, step time: 0.5073\n",
      "301/388, train_loss: 0.0684, step time: 0.5261\n",
      "302/388, train_loss: 0.0677, step time: 0.5764\n",
      "303/388, train_loss: 0.2028, step time: 0.5364\n",
      "304/388, train_loss: 0.0687, step time: 0.5122\n",
      "305/388, train_loss: 0.0787, step time: 0.4933\n",
      "306/388, train_loss: 0.2715, step time: 0.5057\n",
      "307/388, train_loss: 0.0731, step time: 0.4988\n",
      "308/388, train_loss: 0.2229, step time: 0.4997\n",
      "309/388, train_loss: 0.1289, step time: 1.1179\n",
      "310/388, train_loss: 0.1436, step time: 0.5391\n",
      "311/388, train_loss: 0.0825, step time: 0.5165\n",
      "312/388, train_loss: 0.0507, step time: 0.4881\n",
      "313/388, train_loss: 0.2075, step time: 0.5008\n",
      "314/388, train_loss: 0.1546, step time: 0.5220\n",
      "315/388, train_loss: 0.1696, step time: 0.5078\n",
      "316/388, train_loss: 0.2772, step time: 0.4896\n",
      "317/388, train_loss: 0.2182, step time: 0.4885\n",
      "318/388, train_loss: 0.2171, step time: 0.4905\n",
      "319/388, train_loss: 0.1205, step time: 0.5270\n",
      "320/388, train_loss: 0.1306, step time: 0.5012\n",
      "321/388, train_loss: 0.1339, step time: 0.8038\n",
      "322/388, train_loss: 0.2889, step time: 0.5608\n",
      "323/388, train_loss: 0.1211, step time: 0.5358\n",
      "324/388, train_loss: 0.0846, step time: 0.5095\n",
      "325/388, train_loss: 0.5382, step time: 0.5049\n",
      "326/388, train_loss: 0.2696, step time: 0.4881\n",
      "327/388, train_loss: 0.0877, step time: 0.4835\n",
      "328/388, train_loss: 0.3548, step time: 0.4900\n",
      "329/388, train_loss: 0.1027, step time: 0.8874\n",
      "330/388, train_loss: 0.2819, step time: 0.5687\n",
      "331/388, train_loss: 0.1139, step time: 0.5102\n",
      "332/388, train_loss: 0.1936, step time: 0.4907\n",
      "333/388, train_loss: 0.1035, step time: 1.1866\n",
      "334/388, train_loss: 0.1717, step time: 0.5373\n",
      "335/388, train_loss: 0.2382, step time: 0.5102\n",
      "336/388, train_loss: 0.1357, step time: 0.4921\n",
      "337/388, train_loss: 0.1514, step time: 0.4944\n",
      "338/388, train_loss: 0.1656, step time: 0.4785\n",
      "339/388, train_loss: 0.1675, step time: 0.4825\n",
      "340/388, train_loss: 0.1253, step time: 0.9397\n",
      "341/388, train_loss: 0.2135, step time: 0.5423\n",
      "342/388, train_loss: 0.1583, step time: 0.5162\n",
      "343/388, train_loss: 0.1646, step time: 0.4959\n",
      "344/388, train_loss: 0.1389, step time: 0.4862\n",
      "345/388, train_loss: 0.3432, step time: 0.9000\n",
      "346/388, train_loss: 0.2226, step time: 0.5453\n",
      "347/388, train_loss: 0.2050, step time: 0.5125\n",
      "348/388, train_loss: 0.1051, step time: 0.4885\n",
      "349/388, train_loss: 0.0577, step time: 0.4949\n",
      "350/388, train_loss: 0.1230, step time: 0.4760\n",
      "351/388, train_loss: 0.1216, step time: 0.8482\n",
      "352/388, train_loss: 0.0659, step time: 0.5625\n",
      "353/388, train_loss: 0.1926, step time: 0.5232\n",
      "354/388, train_loss: 0.0331, step time: 0.4905\n",
      "355/388, train_loss: 0.1549, step time: 0.4928\n",
      "356/388, train_loss: 0.1513, step time: 0.4996\n",
      "357/388, train_loss: 0.1335, step time: 0.4866\n",
      "358/388, train_loss: 0.0521, step time: 0.4935\n",
      "359/388, train_loss: 0.0831, step time: 0.4751\n",
      "360/388, train_loss: 0.0851, step time: 0.8593\n",
      "361/388, train_loss: 0.1164, step time: 0.5530\n",
      "362/388, train_loss: 0.1885, step time: 0.5185\n",
      "363/388, train_loss: 0.1397, step time: 0.4948\n",
      "364/388, train_loss: 0.1187, step time: 0.5059\n",
      "365/388, train_loss: 0.2650, step time: 0.5004\n",
      "366/388, train_loss: 0.2170, step time: 0.4856\n",
      "367/388, train_loss: 0.2021, step time: 0.4909\n",
      "368/388, train_loss: 0.1437, step time: 0.4854\n",
      "369/388, train_loss: 0.1039, step time: 0.4792\n",
      "370/388, train_loss: 0.1334, step time: 0.4925\n",
      "371/388, train_loss: 0.3277, step time: 0.4985\n",
      "372/388, train_loss: 0.2004, step time: 0.5589\n",
      "373/388, train_loss: 0.0913, step time: 0.5190\n",
      "374/388, train_loss: 0.1293, step time: 0.4962\n",
      "375/388, train_loss: 0.0718, step time: 0.5853\n",
      "376/388, train_loss: 0.2933, step time: 0.5837\n",
      "377/388, train_loss: 0.1751, step time: 0.5364\n",
      "378/388, train_loss: 0.1004, step time: 0.5040\n",
      "379/388, train_loss: 0.0580, step time: 0.5109\n",
      "380/388, train_loss: 0.3163, step time: 0.4996\n",
      "381/388, train_loss: 0.4583, step time: 1.1983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/388, train_loss: 0.1363, step time: 0.5393\n",
      "383/388, train_loss: 0.2187, step time: 0.5067\n",
      "384/388, train_loss: 0.1819, step time: 0.4934\n",
      "385/388, train_loss: 0.2066, step time: 0.4795\n",
      "386/388, train_loss: 0.0776, step time: 0.4746\n",
      "387/388, train_loss: 0.1118, step time: 1.1311\n",
      "388/388, train_loss: 0.0961, step time: 0.5205\n",
      "epoch 78 average loss: 0.1794\n",
      "current epoch: 78 current mean dice: 0.7480 tc: 0.7873 wt: 0.8827 et: 0.5739\n",
      "best mean dice: 0.7748 at epoch: 76\n",
      "time consuming of epoch 78 is: 301.5463\n",
      "----------\n",
      "epoch 79/300\n",
      "1/388, train_loss: 0.0640, step time: 0.4715\n",
      "2/388, train_loss: 0.3773, step time: 0.4859\n",
      "3/388, train_loss: 0.1274, step time: 0.5058\n",
      "4/388, train_loss: 0.1703, step time: 0.6306\n",
      "5/388, train_loss: 0.0949, step time: 0.5333\n",
      "6/388, train_loss: 0.1191, step time: 0.5189\n",
      "7/388, train_loss: 0.2477, step time: 0.6648\n",
      "8/388, train_loss: 0.2481, step time: 0.6413\n",
      "9/388, train_loss: 0.1292, step time: 0.5384\n",
      "10/388, train_loss: 0.1028, step time: 0.5243\n",
      "11/388, train_loss: 0.4840, step time: 0.5018\n",
      "12/388, train_loss: 0.0869, step time: 0.5899\n",
      "13/388, train_loss: 0.2319, step time: 0.5135\n",
      "14/388, train_loss: 0.0547, step time: 1.0068\n",
      "15/388, train_loss: 0.1392, step time: 0.5721\n",
      "16/388, train_loss: 0.0300, step time: 0.5327\n",
      "17/388, train_loss: 0.3074, step time: 0.5200\n",
      "18/388, train_loss: 0.2518, step time: 0.5125\n",
      "19/388, train_loss: 0.0594, step time: 0.4813\n",
      "20/388, train_loss: 0.2450, step time: 0.7523\n",
      "21/388, train_loss: 0.1244, step time: 0.6065\n",
      "22/388, train_loss: 0.0686, step time: 0.5400\n",
      "23/388, train_loss: 0.1395, step time: 0.5189\n",
      "24/388, train_loss: 0.1276, step time: 0.5094\n",
      "25/388, train_loss: 0.1793, step time: 0.5542\n",
      "26/388, train_loss: 0.1034, step time: 0.5391\n",
      "27/388, train_loss: 0.1434, step time: 0.5164\n",
      "28/388, train_loss: 0.1875, step time: 0.4932\n",
      "29/388, train_loss: 0.2504, step time: 1.1759\n",
      "30/388, train_loss: 0.3281, step time: 0.5323\n",
      "31/388, train_loss: 0.1693, step time: 0.5180\n",
      "32/388, train_loss: 0.1236, step time: 0.5008\n",
      "33/388, train_loss: 0.3100, step time: 0.4995\n",
      "34/388, train_loss: 0.4387, step time: 0.5553\n",
      "35/388, train_loss: 0.3541, step time: 0.5563\n",
      "36/388, train_loss: 0.1191, step time: 0.5895\n",
      "37/388, train_loss: 0.0916, step time: 0.5496\n",
      "38/388, train_loss: 0.0511, step time: 0.5051\n",
      "39/388, train_loss: 0.0943, step time: 0.4922\n",
      "40/388, train_loss: 0.0862, step time: 0.5311\n",
      "41/388, train_loss: 0.0634, step time: 0.5091\n",
      "42/388, train_loss: 0.2589, step time: 0.6193\n",
      "43/388, train_loss: 0.2059, step time: 0.5643\n",
      "44/388, train_loss: 0.1397, step time: 0.5360\n",
      "45/388, train_loss: 0.0892, step time: 0.5021\n",
      "46/388, train_loss: 0.1165, step time: 0.4888\n",
      "47/388, train_loss: 0.1886, step time: 0.4987\n",
      "48/388, train_loss: 0.1386, step time: 0.5057\n",
      "49/388, train_loss: 0.2896, step time: 0.5386\n",
      "50/388, train_loss: 0.1575, step time: 0.4995\n",
      "51/388, train_loss: 0.0785, step time: 0.6645\n",
      "52/388, train_loss: 0.2788, step time: 0.5634\n",
      "53/388, train_loss: 0.1391, step time: 0.5388\n",
      "54/388, train_loss: 0.1788, step time: 0.5268\n",
      "55/388, train_loss: 0.3693, step time: 0.5754\n",
      "56/388, train_loss: 0.1043, step time: 0.5314\n",
      "57/388, train_loss: 0.1669, step time: 0.5098\n",
      "58/388, train_loss: 0.0831, step time: 0.5122\n",
      "59/388, train_loss: 0.2334, step time: 0.5554\n",
      "60/388, train_loss: 0.1269, step time: 0.5386\n",
      "61/388, train_loss: 0.2320, step time: 0.5142\n",
      "62/388, train_loss: 0.1382, step time: 0.5061\n",
      "63/388, train_loss: 0.1735, step time: 0.4971\n",
      "64/388, train_loss: 0.0919, step time: 0.5461\n",
      "65/388, train_loss: 0.2195, step time: 0.5420\n",
      "66/388, train_loss: 0.1447, step time: 0.5014\n",
      "67/388, train_loss: 0.2599, step time: 0.5099\n",
      "68/388, train_loss: 0.4581, step time: 0.4953\n",
      "69/388, train_loss: 0.1521, step time: 0.5107\n",
      "70/388, train_loss: 0.2905, step time: 0.5123\n",
      "71/388, train_loss: 0.0898, step time: 0.6283\n",
      "72/388, train_loss: 0.2255, step time: 0.5661\n",
      "73/388, train_loss: 0.1059, step time: 0.5262\n",
      "74/388, train_loss: 0.1502, step time: 0.4903\n",
      "75/388, train_loss: 0.4479, step time: 0.5007\n",
      "76/388, train_loss: 0.1958, step time: 0.4828\n",
      "77/388, train_loss: 0.0810, step time: 1.0760\n",
      "78/388, train_loss: 0.2444, step time: 0.5304\n",
      "79/388, train_loss: 0.0894, step time: 0.5144\n",
      "80/388, train_loss: 0.2537, step time: 0.4989\n",
      "81/388, train_loss: 0.1090, step time: 0.5007\n",
      "82/388, train_loss: 0.2040, step time: 0.5546\n",
      "83/388, train_loss: 0.1538, step time: 0.5265\n",
      "84/388, train_loss: 0.1880, step time: 0.5208\n",
      "85/388, train_loss: 0.1378, step time: 0.4941\n",
      "86/388, train_loss: 0.1096, step time: 0.5193\n",
      "87/388, train_loss: 0.1524, step time: 0.5360\n",
      "88/388, train_loss: 0.0437, step time: 0.5482\n",
      "89/388, train_loss: 0.1523, step time: 0.7007\n",
      "90/388, train_loss: 0.1129, step time: 0.5589\n",
      "91/388, train_loss: 0.0839, step time: 0.5142\n",
      "92/388, train_loss: 0.0652, step time: 0.4886\n",
      "93/388, train_loss: 0.1264, step time: 0.5118\n",
      "94/388, train_loss: 0.2758, step time: 0.5028\n",
      "95/388, train_loss: 0.2000, step time: 0.4999\n",
      "96/388, train_loss: 0.1942, step time: 0.5035\n",
      "97/388, train_loss: 0.1946, step time: 0.4824\n",
      "98/388, train_loss: 0.1926, step time: 0.8825\n",
      "99/388, train_loss: 0.0885, step time: 0.5393\n",
      "100/388, train_loss: 0.1212, step time: 0.5155\n",
      "101/388, train_loss: 0.1019, step time: 0.4956\n",
      "102/388, train_loss: 0.2776, step time: 0.5000\n",
      "103/388, train_loss: 0.1164, step time: 0.4835\n",
      "104/388, train_loss: 0.0867, step time: 0.4954\n",
      "105/388, train_loss: 0.2670, step time: 1.1159\n",
      "106/388, train_loss: 0.1416, step time: 0.5110\n",
      "107/388, train_loss: 0.2568, step time: 0.4973\n",
      "108/388, train_loss: 0.1024, step time: 0.4915\n",
      "109/388, train_loss: 0.1328, step time: 0.4757\n",
      "110/388, train_loss: 0.3176, step time: 0.6443\n",
      "111/388, train_loss: 0.3071, step time: 0.5420\n",
      "112/388, train_loss: 0.4403, step time: 0.5127\n",
      "113/388, train_loss: 0.1385, step time: 0.5021\n",
      "114/388, train_loss: 0.1876, step time: 0.4977\n",
      "115/388, train_loss: 0.1084, step time: 0.4822\n",
      "116/388, train_loss: 0.1873, step time: 0.4879\n",
      "117/388, train_loss: 0.1587, step time: 0.4737\n",
      "118/388, train_loss: 0.2366, step time: 0.4983\n",
      "119/388, train_loss: 0.0596, step time: 0.4816\n",
      "120/388, train_loss: 0.2556, step time: 1.0884\n",
      "121/388, train_loss: 0.0891, step time: 0.5494\n",
      "122/388, train_loss: 0.1624, step time: 0.5221\n",
      "123/388, train_loss: 0.2268, step time: 0.4989\n",
      "124/388, train_loss: 0.0912, step time: 0.4983\n",
      "125/388, train_loss: 0.2149, step time: 0.4830\n",
      "126/388, train_loss: 0.0931, step time: 0.4771\n",
      "127/388, train_loss: 0.1237, step time: 0.4823\n",
      "128/388, train_loss: 0.0281, step time: 0.9704\n",
      "129/388, train_loss: 0.1928, step time: 0.5289\n",
      "130/388, train_loss: 0.1671, step time: 0.4951\n",
      "131/388, train_loss: 0.2482, step time: 0.4937\n",
      "132/388, train_loss: 0.0682, step time: 0.4790\n",
      "133/388, train_loss: 0.0602, step time: 0.4745\n",
      "134/388, train_loss: 0.2585, step time: 0.4958\n",
      "135/388, train_loss: 0.1301, step time: 0.5066\n",
      "136/388, train_loss: 0.1177, step time: 0.4920\n",
      "137/388, train_loss: 0.2986, step time: 1.1504\n",
      "138/388, train_loss: 0.1084, step time: 0.5253\n",
      "139/388, train_loss: 0.1451, step time: 0.5019\n",
      "140/388, train_loss: 0.0865, step time: 0.4923\n",
      "141/388, train_loss: 0.2260, step time: 0.4799\n",
      "142/388, train_loss: 0.1588, step time: 0.5077\n",
      "143/388, train_loss: 0.0543, step time: 0.4873\n",
      "144/388, train_loss: 0.0746, step time: 0.4943\n",
      "145/388, train_loss: 0.0974, step time: 0.4942\n",
      "146/388, train_loss: 0.0773, step time: 0.4808\n",
      "147/388, train_loss: 0.0970, step time: 0.5320\n",
      "148/388, train_loss: 0.0491, step time: 0.5078\n",
      "149/388, train_loss: 0.1583, step time: 0.5088\n",
      "150/388, train_loss: 0.1285, step time: 0.4935\n",
      "151/388, train_loss: 0.2797, step time: 0.4965\n",
      "152/388, train_loss: 0.0673, step time: 0.4816\n",
      "153/388, train_loss: 0.1493, step time: 0.4858\n",
      "154/388, train_loss: 0.1216, step time: 0.4804\n",
      "155/388, train_loss: 0.2612, step time: 1.0304\n",
      "156/388, train_loss: 0.2061, step time: 0.5557\n",
      "157/388, train_loss: 0.4172, step time: 0.5053\n",
      "158/388, train_loss: 0.0647, step time: 0.4892\n",
      "159/388, train_loss: 0.0732, step time: 0.4866\n",
      "160/388, train_loss: 0.0797, step time: 0.4863\n",
      "161/388, train_loss: 0.1480, step time: 0.5106\n",
      "162/388, train_loss: 0.1110, step time: 0.5022\n",
      "163/388, train_loss: 0.0947, step time: 0.4872\n",
      "164/388, train_loss: 0.1864, step time: 0.5177\n",
      "165/388, train_loss: 0.0554, step time: 0.4965\n",
      "166/388, train_loss: 0.0885, step time: 0.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/388, train_loss: 0.1935, step time: 0.4844\n",
      "168/388, train_loss: 0.1097, step time: 0.4830\n",
      "169/388, train_loss: 0.4521, step time: 0.4900\n",
      "170/388, train_loss: 0.1490, step time: 0.4762\n",
      "171/388, train_loss: 0.2696, step time: 0.5105\n",
      "172/388, train_loss: 0.1305, step time: 0.5047\n",
      "173/388, train_loss: 0.2175, step time: 0.5023\n",
      "174/388, train_loss: 0.1464, step time: 0.5197\n",
      "175/388, train_loss: 0.1414, step time: 0.5327\n",
      "176/388, train_loss: 0.3016, step time: 0.6206\n",
      "177/388, train_loss: 0.1544, step time: 0.5778\n",
      "178/388, train_loss: 0.2215, step time: 0.5270\n",
      "179/388, train_loss: 0.2849, step time: 0.5056\n",
      "180/388, train_loss: 0.1113, step time: 0.5487\n",
      "181/388, train_loss: 0.2299, step time: 0.5375\n",
      "182/388, train_loss: 0.2332, step time: 0.5106\n",
      "183/388, train_loss: 0.2866, step time: 0.4963\n",
      "184/388, train_loss: 0.0823, step time: 0.4930\n",
      "185/388, train_loss: 0.0549, step time: 0.4819\n",
      "186/388, train_loss: 0.2009, step time: 0.5033\n",
      "187/388, train_loss: 0.0846, step time: 0.4836\n",
      "188/388, train_loss: 0.1061, step time: 1.0801\n",
      "189/388, train_loss: 0.1054, step time: 0.5342\n",
      "190/388, train_loss: 0.2318, step time: 0.5173\n",
      "191/388, train_loss: 0.2298, step time: 0.4958\n",
      "192/388, train_loss: 0.2742, step time: 0.4998\n",
      "193/388, train_loss: 0.1011, step time: 0.5590\n",
      "194/388, train_loss: 0.1059, step time: 0.5230\n",
      "195/388, train_loss: 0.2138, step time: 0.5124\n",
      "196/388, train_loss: 0.1152, step time: 0.4885\n",
      "197/388, train_loss: 0.1401, step time: 0.4848\n",
      "198/388, train_loss: 0.1520, step time: 0.4883\n",
      "199/388, train_loss: 0.2039, step time: 0.7494\n",
      "200/388, train_loss: 0.2896, step time: 0.5522\n",
      "201/388, train_loss: 0.1445, step time: 0.5205\n",
      "202/388, train_loss: 0.1662, step time: 0.5008\n",
      "203/388, train_loss: 0.0975, step time: 0.4932\n",
      "204/388, train_loss: 0.0954, step time: 0.4940\n",
      "205/388, train_loss: 0.1514, step time: 0.6784\n",
      "206/388, train_loss: 0.2048, step time: 0.5540\n",
      "207/388, train_loss: 0.0723, step time: 0.5202\n",
      "208/388, train_loss: 0.3195, step time: 0.4966\n",
      "209/388, train_loss: 0.1074, step time: 0.4845\n",
      "210/388, train_loss: 0.1295, step time: 0.4938\n",
      "211/388, train_loss: 0.0717, step time: 0.4755\n",
      "212/388, train_loss: 0.0982, step time: 0.9607\n",
      "213/388, train_loss: 0.2072, step time: 0.5556\n",
      "214/388, train_loss: 0.1627, step time: 0.4990\n",
      "215/388, train_loss: 0.2782, step time: 0.4953\n",
      "216/388, train_loss: 0.1725, step time: 0.4934\n",
      "217/388, train_loss: 0.4112, step time: 0.4861\n",
      "218/388, train_loss: 0.3154, step time: 0.4877\n",
      "219/388, train_loss: 0.3856, step time: 0.4767\n",
      "220/388, train_loss: 0.2293, step time: 0.4783\n",
      "221/388, train_loss: 0.1649, step time: 1.0380\n",
      "222/388, train_loss: 0.1919, step time: 0.5374\n",
      "223/388, train_loss: 0.0872, step time: 0.5025\n",
      "224/388, train_loss: 0.0609, step time: 0.4960\n",
      "225/388, train_loss: 0.1188, step time: 0.4843\n",
      "226/388, train_loss: 0.2186, step time: 0.5060\n",
      "227/388, train_loss: 0.1378, step time: 0.4815\n",
      "228/388, train_loss: 0.1267, step time: 0.4928\n",
      "229/388, train_loss: 0.0787, step time: 0.4755\n",
      "230/388, train_loss: 0.3139, step time: 0.4885\n",
      "231/388, train_loss: 0.1529, step time: 0.4937\n",
      "232/388, train_loss: 0.2073, step time: 0.4948\n",
      "233/388, train_loss: 0.0349, step time: 0.4982\n",
      "234/388, train_loss: 0.1295, step time: 0.4842\n",
      "235/388, train_loss: 0.2436, step time: 0.5113\n",
      "236/388, train_loss: 0.4119, step time: 0.4941\n",
      "237/388, train_loss: 0.1887, step time: 0.4902\n",
      "238/388, train_loss: 0.2857, step time: 0.4981\n",
      "239/388, train_loss: 0.3203, step time: 0.5040\n",
      "240/388, train_loss: 0.2620, step time: 0.5627\n",
      "241/388, train_loss: 0.2032, step time: 0.5587\n",
      "242/388, train_loss: 0.2624, step time: 0.5357\n",
      "243/388, train_loss: 0.1340, step time: 0.5153\n",
      "244/388, train_loss: 0.1775, step time: 0.5078\n",
      "245/388, train_loss: 0.2727, step time: 0.5018\n",
      "246/388, train_loss: 0.2800, step time: 0.4923\n",
      "247/388, train_loss: 0.1053, step time: 1.0740\n",
      "248/388, train_loss: 0.1165, step time: 0.5300\n",
      "249/388, train_loss: 0.1025, step time: 0.5120\n",
      "250/388, train_loss: 0.1081, step time: 0.4906\n",
      "251/388, train_loss: 0.0462, step time: 0.4947\n",
      "252/388, train_loss: 0.1781, step time: 0.4832\n",
      "253/388, train_loss: 0.0838, step time: 0.4789\n",
      "254/388, train_loss: 0.1242, step time: 0.4847\n",
      "255/388, train_loss: 0.0992, step time: 0.4770\n",
      "256/388, train_loss: 0.0410, step time: 0.6265\n",
      "257/388, train_loss: 0.1608, step time: 0.5282\n",
      "258/388, train_loss: 0.2556, step time: 0.5079\n",
      "259/388, train_loss: 0.2589, step time: 0.5046\n",
      "260/388, train_loss: 0.1184, step time: 0.4877\n",
      "261/388, train_loss: 0.1631, step time: 0.8706\n",
      "262/388, train_loss: 0.2231, step time: 0.5384\n",
      "263/388, train_loss: 0.0821, step time: 0.5157\n",
      "264/388, train_loss: 0.2360, step time: 0.4936\n",
      "265/388, train_loss: 0.1929, step time: 0.4952\n",
      "266/388, train_loss: 0.1747, step time: 0.4785\n",
      "267/388, train_loss: 0.2758, step time: 0.4844\n",
      "268/388, train_loss: 0.0921, step time: 0.4941\n",
      "269/388, train_loss: 0.1676, step time: 0.4826\n",
      "270/388, train_loss: 0.2732, step time: 0.4900\n",
      "271/388, train_loss: 0.1614, step time: 0.4949\n",
      "272/388, train_loss: 0.3040, step time: 0.5727\n",
      "273/388, train_loss: 0.1876, step time: 0.5470\n",
      "274/388, train_loss: 0.0649, step time: 0.5163\n",
      "275/388, train_loss: 0.2557, step time: 0.5012\n",
      "276/388, train_loss: 0.0569, step time: 0.4860\n",
      "277/388, train_loss: 0.1753, step time: 0.5045\n",
      "278/388, train_loss: 0.1320, step time: 0.5348\n",
      "279/388, train_loss: 0.1899, step time: 0.5228\n",
      "280/388, train_loss: 0.0781, step time: 0.4948\n",
      "281/388, train_loss: 0.2995, step time: 0.4908\n",
      "282/388, train_loss: 0.2826, step time: 0.5022\n",
      "283/388, train_loss: 0.5349, step time: 0.4986\n",
      "284/388, train_loss: 0.0525, step time: 0.4832\n",
      "285/388, train_loss: 0.1548, step time: 0.5268\n",
      "286/388, train_loss: 0.1899, step time: 0.5150\n",
      "287/388, train_loss: 0.2836, step time: 0.4939\n",
      "288/388, train_loss: 0.2767, step time: 0.5310\n",
      "289/388, train_loss: 0.1167, step time: 0.5030\n",
      "290/388, train_loss: 0.1087, step time: 0.5003\n",
      "291/388, train_loss: 0.4149, step time: 0.5072\n",
      "292/388, train_loss: 0.2063, step time: 0.5034\n",
      "293/388, train_loss: 0.1237, step time: 0.5140\n",
      "294/388, train_loss: 0.2364, step time: 0.5074\n",
      "295/388, train_loss: 0.1976, step time: 0.4966\n",
      "296/388, train_loss: 0.0864, step time: 0.4995\n",
      "297/388, train_loss: 0.2059, step time: 0.4806\n",
      "298/388, train_loss: 0.0906, step time: 0.9713\n",
      "299/388, train_loss: 0.0892, step time: 0.5278\n",
      "300/388, train_loss: 0.1284, step time: 0.5026\n",
      "301/388, train_loss: 0.0360, step time: 0.4867\n",
      "302/388, train_loss: 0.1045, step time: 0.4938\n",
      "303/388, train_loss: 0.1200, step time: 0.4819\n",
      "304/388, train_loss: 0.2957, step time: 0.4853\n",
      "305/388, train_loss: 0.0749, step time: 0.5169\n",
      "306/388, train_loss: 0.0661, step time: 0.5316\n",
      "307/388, train_loss: 0.4123, step time: 0.5073\n",
      "308/388, train_loss: 0.1032, step time: 0.4996\n",
      "309/388, train_loss: 0.1229, step time: 1.1210\n",
      "310/388, train_loss: 0.2240, step time: 0.5232\n",
      "311/388, train_loss: 0.2306, step time: 0.4991\n",
      "312/388, train_loss: 0.2058, step time: 0.4916\n",
      "313/388, train_loss: 0.1481, step time: 0.4837\n",
      "314/388, train_loss: 0.2509, step time: 0.4975\n",
      "315/388, train_loss: 0.0902, step time: 0.5086\n",
      "316/388, train_loss: 0.1164, step time: 0.4984\n",
      "317/388, train_loss: 0.2038, step time: 0.4800\n",
      "318/388, train_loss: 0.1086, step time: 0.4776\n",
      "319/388, train_loss: 0.0952, step time: 1.0882\n",
      "320/388, train_loss: 0.1049, step time: 0.5265\n",
      "321/388, train_loss: 0.2461, step time: 0.4983\n",
      "322/388, train_loss: 0.1562, step time: 0.4916\n",
      "323/388, train_loss: 0.3930, step time: 0.4997\n",
      "324/388, train_loss: 0.2806, step time: 0.4811\n",
      "325/388, train_loss: 0.0696, step time: 0.4760\n",
      "326/388, train_loss: 0.3730, step time: 0.5098\n",
      "327/388, train_loss: 0.0840, step time: 0.4978\n",
      "328/388, train_loss: 0.2115, step time: 0.5228\n",
      "329/388, train_loss: 0.1431, step time: 0.4983\n",
      "330/388, train_loss: 0.1277, step time: 0.4958\n",
      "331/388, train_loss: 0.0842, step time: 0.4791\n",
      "332/388, train_loss: 0.2233, step time: 0.4767\n",
      "333/388, train_loss: 0.2040, step time: 1.1026\n",
      "334/388, train_loss: 0.0715, step time: 0.5199\n",
      "335/388, train_loss: 0.2234, step time: 0.4967\n",
      "336/388, train_loss: 0.0559, step time: 0.4924\n",
      "337/388, train_loss: 0.1263, step time: 0.4843\n",
      "338/388, train_loss: 0.2735, step time: 0.6630\n",
      "339/388, train_loss: 0.5006, step time: 0.5413\n",
      "340/388, train_loss: 0.3081, step time: 0.5136\n",
      "341/388, train_loss: 0.1370, step time: 0.4932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/388, train_loss: 0.0875, step time: 0.4959\n",
      "343/388, train_loss: 0.3585, step time: 0.4843\n",
      "344/388, train_loss: 0.2171, step time: 0.4825\n",
      "345/388, train_loss: 0.1175, step time: 0.4853\n",
      "346/388, train_loss: 0.0965, step time: 0.4902\n",
      "347/388, train_loss: 0.0802, step time: 0.5316\n",
      "348/388, train_loss: 0.2526, step time: 0.6087\n",
      "349/388, train_loss: 0.1451, step time: 0.5359\n",
      "350/388, train_loss: 0.0963, step time: 0.4988\n",
      "351/388, train_loss: 0.1000, step time: 0.4975\n",
      "352/388, train_loss: 0.3431, step time: 0.4923\n",
      "353/388, train_loss: 0.1451, step time: 0.4955\n",
      "354/388, train_loss: 0.1458, step time: 0.4910\n",
      "355/388, train_loss: 0.2193, step time: 0.4782\n",
      "356/388, train_loss: 0.2026, step time: 0.4859\n",
      "357/388, train_loss: 0.1709, step time: 0.5285\n",
      "358/388, train_loss: 0.1102, step time: 0.5133\n",
      "359/388, train_loss: 0.2417, step time: 0.5046\n",
      "360/388, train_loss: 0.0912, step time: 0.4859\n",
      "361/388, train_loss: 0.0633, step time: 0.5310\n",
      "362/388, train_loss: 0.1033, step time: 0.5081\n",
      "363/388, train_loss: 0.1125, step time: 0.4947\n",
      "364/388, train_loss: 0.1369, step time: 0.4808\n",
      "365/388, train_loss: 0.0874, step time: 0.4933\n",
      "366/388, train_loss: 0.2305, step time: 0.4770\n",
      "367/388, train_loss: 0.2599, step time: 0.4872\n",
      "368/388, train_loss: 0.0994, step time: 0.4920\n",
      "369/388, train_loss: 0.1112, step time: 1.1539\n",
      "370/388, train_loss: 0.2823, step time: 0.5284\n",
      "371/388, train_loss: 0.3183, step time: 0.4927\n",
      "372/388, train_loss: 0.1353, step time: 0.5950\n",
      "373/388, train_loss: 0.1075, step time: 0.5376\n",
      "374/388, train_loss: 0.2621, step time: 0.5070\n",
      "375/388, train_loss: 0.3479, step time: 0.4975\n",
      "376/388, train_loss: 0.0995, step time: 0.5114\n",
      "377/388, train_loss: 0.2049, step time: 0.5046\n",
      "378/388, train_loss: 0.1874, step time: 0.4859\n",
      "379/388, train_loss: 0.1022, step time: 0.5070\n",
      "380/388, train_loss: 0.1247, step time: 0.5000\n",
      "381/388, train_loss: 0.5935, step time: 0.4875\n",
      "382/388, train_loss: 0.2097, step time: 0.5019\n",
      "383/388, train_loss: 0.1219, step time: 0.4954\n",
      "384/388, train_loss: 0.1423, step time: 0.5053\n",
      "385/388, train_loss: 0.1989, step time: 0.4894\n",
      "386/388, train_loss: 0.1205, step time: 0.4851\n",
      "387/388, train_loss: 0.5151, step time: 0.4821\n",
      "388/388, train_loss: 0.0541, step time: 0.4864\n",
      "epoch 79 average loss: 0.1760\n",
      "current epoch: 79 current mean dice: 0.7738 tc: 0.8229 wt: 0.9023 et: 0.5962\n",
      "best mean dice: 0.7748 at epoch: 76\n",
      "time consuming of epoch 79 is: 298.2571\n",
      "----------\n",
      "epoch 80/300\n",
      "1/388, train_loss: 0.1636, step time: 0.4684\n",
      "2/388, train_loss: 0.0926, step time: 0.4815\n",
      "3/388, train_loss: 0.1051, step time: 0.4977\n",
      "4/388, train_loss: 0.3050, step time: 0.6488\n",
      "5/388, train_loss: 0.0876, step time: 0.5560\n",
      "6/388, train_loss: 0.2218, step time: 0.5410\n",
      "7/388, train_loss: 0.1047, step time: 0.5339\n",
      "8/388, train_loss: 0.1209, step time: 0.4999\n",
      "9/388, train_loss: 0.1426, step time: 0.5320\n",
      "10/388, train_loss: 0.1356, step time: 0.5079\n",
      "11/388, train_loss: 0.1635, step time: 0.5234\n",
      "12/388, train_loss: 0.6232, step time: 0.5403\n",
      "13/388, train_loss: 0.1237, step time: 0.5096\n",
      "14/388, train_loss: 0.1043, step time: 0.5002\n",
      "15/388, train_loss: 0.0824, step time: 0.4893\n",
      "16/388, train_loss: 0.1234, step time: 0.6339\n",
      "17/388, train_loss: 0.1700, step time: 0.5569\n",
      "18/388, train_loss: 0.1679, step time: 0.6188\n",
      "19/388, train_loss: 0.2842, step time: 0.5817\n",
      "20/388, train_loss: 0.1724, step time: 0.5529\n",
      "21/388, train_loss: 0.1352, step time: 0.5225\n",
      "22/388, train_loss: 0.1057, step time: 0.5171\n",
      "23/388, train_loss: 0.0744, step time: 0.5260\n",
      "24/388, train_loss: 0.4367, step time: 0.5313\n",
      "25/388, train_loss: 0.2971, step time: 0.5069\n",
      "26/388, train_loss: 0.1439, step time: 0.4938\n",
      "27/388, train_loss: 0.0699, step time: 0.5157\n",
      "28/388, train_loss: 0.1952, step time: 0.5872\n",
      "29/388, train_loss: 0.0755, step time: 0.5687\n",
      "30/388, train_loss: 0.0906, step time: 0.5325\n",
      "31/388, train_loss: 0.4971, step time: 0.5112\n",
      "32/388, train_loss: 0.0787, step time: 0.5300\n",
      "33/388, train_loss: 0.0859, step time: 0.5178\n",
      "34/388, train_loss: 0.1604, step time: 0.5029\n",
      "35/388, train_loss: 0.4552, step time: 1.0612\n",
      "36/388, train_loss: 0.1402, step time: 0.5539\n",
      "37/388, train_loss: 0.2636, step time: 0.5153\n",
      "38/388, train_loss: 0.4054, step time: 0.5095\n",
      "39/388, train_loss: 0.1492, step time: 0.5859\n",
      "40/388, train_loss: 0.1079, step time: 0.5748\n",
      "41/388, train_loss: 0.0555, step time: 0.5399\n",
      "42/388, train_loss: 0.1171, step time: 0.5423\n",
      "43/388, train_loss: 0.0979, step time: 0.6066\n",
      "44/388, train_loss: 0.0850, step time: 0.5591\n",
      "45/388, train_loss: 0.1447, step time: 0.5249\n",
      "46/388, train_loss: 0.0743, step time: 0.5365\n",
      "47/388, train_loss: 0.0668, step time: 0.5980\n",
      "48/388, train_loss: 0.1870, step time: 0.5626\n",
      "49/388, train_loss: 0.1373, step time: 0.5336\n",
      "50/388, train_loss: 0.0871, step time: 0.5315\n",
      "51/388, train_loss: 0.1293, step time: 0.6043\n",
      "52/388, train_loss: 0.0723, step time: 0.5500\n",
      "53/388, train_loss: 0.2393, step time: 0.5331\n",
      "54/388, train_loss: 0.1865, step time: 0.5100\n",
      "55/388, train_loss: 0.1687, step time: 0.5008\n",
      "56/388, train_loss: 0.3027, step time: 0.6177\n",
      "57/388, train_loss: 0.3690, step time: 0.5686\n",
      "58/388, train_loss: 0.5054, step time: 0.5224\n",
      "59/388, train_loss: 0.1849, step time: 0.4958\n",
      "60/388, train_loss: 0.3403, step time: 1.1395\n",
      "61/388, train_loss: 0.0807, step time: 0.5377\n",
      "62/388, train_loss: 0.2093, step time: 0.5153\n",
      "63/388, train_loss: 0.1408, step time: 0.5004\n",
      "64/388, train_loss: 0.1881, step time: 0.4809\n",
      "65/388, train_loss: 0.1130, step time: 0.4762\n",
      "66/388, train_loss: 0.2711, step time: 0.5571\n",
      "67/388, train_loss: 0.3751, step time: 0.5404\n",
      "68/388, train_loss: 0.1823, step time: 0.5196\n",
      "69/388, train_loss: 0.2332, step time: 0.5109\n",
      "70/388, train_loss: 0.2138, step time: 0.5065\n",
      "71/388, train_loss: 0.0742, step time: 0.5260\n",
      "72/388, train_loss: 0.3184, step time: 0.5022\n",
      "73/388, train_loss: 0.1594, step time: 0.5024\n",
      "74/388, train_loss: 0.2496, step time: 0.5682\n",
      "75/388, train_loss: 0.1833, step time: 0.5706\n",
      "76/388, train_loss: 0.1114, step time: 0.5403\n",
      "77/388, train_loss: 0.1599, step time: 0.5012\n",
      "78/388, train_loss: 0.1383, step time: 0.4930\n",
      "79/388, train_loss: 0.0484, step time: 0.5084\n",
      "80/388, train_loss: 0.0894, step time: 0.5305\n",
      "81/388, train_loss: 0.1999, step time: 0.4913\n",
      "82/388, train_loss: 0.1286, step time: 0.5197\n",
      "83/388, train_loss: 0.1387, step time: 0.5233\n",
      "84/388, train_loss: 0.1881, step time: 0.5088\n",
      "85/388, train_loss: 0.1895, step time: 0.4866\n",
      "86/388, train_loss: 0.1325, step time: 0.4773\n",
      "87/388, train_loss: 0.0927, step time: 0.4797\n",
      "88/388, train_loss: 0.4422, step time: 0.7036\n",
      "89/388, train_loss: 0.0848, step time: 0.6091\n",
      "90/388, train_loss: 0.1295, step time: 0.5281\n",
      "91/388, train_loss: 0.2328, step time: 0.5093\n",
      "92/388, train_loss: 0.0974, step time: 0.5642\n",
      "93/388, train_loss: 0.3711, step time: 0.5381\n",
      "94/388, train_loss: 0.1626, step time: 0.5238\n",
      "95/388, train_loss: 0.1015, step time: 0.5438\n",
      "96/388, train_loss: 0.1740, step time: 0.5585\n",
      "97/388, train_loss: 0.1737, step time: 0.5141\n",
      "98/388, train_loss: 0.4817, step time: 0.5302\n",
      "99/388, train_loss: 0.0720, step time: 0.5145\n",
      "100/388, train_loss: 0.0882, step time: 0.5302\n",
      "101/388, train_loss: 0.1542, step time: 0.5911\n",
      "102/388, train_loss: 0.2805, step time: 0.5315\n",
      "103/388, train_loss: 0.1622, step time: 0.5002\n",
      "104/388, train_loss: 0.1601, step time: 0.7982\n",
      "105/388, train_loss: 0.1408, step time: 0.5582\n",
      "106/388, train_loss: 0.0821, step time: 0.5291\n",
      "107/388, train_loss: 0.0699, step time: 0.5014\n",
      "108/388, train_loss: 0.1100, step time: 0.4904\n",
      "109/388, train_loss: 0.1386, step time: 0.4814\n",
      "110/388, train_loss: 0.1383, step time: 1.0135\n",
      "111/388, train_loss: 0.2124, step time: 0.5352\n",
      "112/388, train_loss: 0.0658, step time: 0.5055\n",
      "113/388, train_loss: 0.1138, step time: 0.4964\n",
      "114/388, train_loss: 0.1843, step time: 0.5903\n",
      "115/388, train_loss: 0.1766, step time: 0.5909\n",
      "116/388, train_loss: 0.1234, step time: 0.5357\n",
      "117/388, train_loss: 0.1716, step time: 0.5045\n",
      "118/388, train_loss: 0.1643, step time: 0.5020\n",
      "119/388, train_loss: 0.2014, step time: 1.2418\n",
      "120/388, train_loss: 0.5252, step time: 0.5509\n",
      "121/388, train_loss: 0.0708, step time: 0.5235\n",
      "122/388, train_loss: 0.3132, step time: 0.4913\n",
      "123/388, train_loss: 0.0977, step time: 0.4967\n",
      "124/388, train_loss: 0.1467, step time: 0.4803\n",
      "125/388, train_loss: 0.4892, step time: 1.1268\n",
      "126/388, train_loss: 0.0306, step time: 0.5406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/388, train_loss: 0.2759, step time: 0.5165\n",
      "128/388, train_loss: 0.1128, step time: 0.5008\n",
      "129/388, train_loss: 0.1103, step time: 0.4888\n",
      "130/388, train_loss: 0.3189, step time: 0.4981\n",
      "131/388, train_loss: 0.1104, step time: 0.5402\n",
      "132/388, train_loss: 0.1982, step time: 0.5343\n",
      "133/388, train_loss: 0.2391, step time: 0.5123\n",
      "134/388, train_loss: 0.1199, step time: 0.6038\n",
      "135/388, train_loss: 0.1909, step time: 0.5368\n",
      "136/388, train_loss: 0.1374, step time: 0.5005\n",
      "137/388, train_loss: 0.0987, step time: 0.4802\n",
      "138/388, train_loss: 0.1298, step time: 0.5026\n",
      "139/388, train_loss: 0.1584, step time: 0.4967\n",
      "140/388, train_loss: 0.2349, step time: 0.4932\n",
      "141/388, train_loss: 0.1568, step time: 0.4833\n",
      "142/388, train_loss: 0.3404, step time: 0.4885\n",
      "143/388, train_loss: 0.0990, step time: 0.6193\n",
      "144/388, train_loss: 0.0555, step time: 0.5391\n",
      "145/388, train_loss: 0.2334, step time: 0.5164\n",
      "146/388, train_loss: 0.0905, step time: 0.4940\n",
      "147/388, train_loss: 0.1172, step time: 0.5004\n",
      "148/388, train_loss: 0.2193, step time: 0.4841\n",
      "149/388, train_loss: 0.0420, step time: 1.0573\n",
      "150/388, train_loss: 0.0736, step time: 0.5565\n",
      "151/388, train_loss: 0.1414, step time: 0.5230\n",
      "152/388, train_loss: 0.1051, step time: 0.5043\n",
      "153/388, train_loss: 0.0906, step time: 0.5014\n",
      "154/388, train_loss: 0.0994, step time: 0.4834\n",
      "155/388, train_loss: 0.2400, step time: 0.5233\n",
      "156/388, train_loss: 0.0585, step time: 0.4905\n",
      "157/388, train_loss: 0.0926, step time: 0.9553\n",
      "158/388, train_loss: 0.0319, step time: 0.5343\n",
      "159/388, train_loss: 0.1849, step time: 0.5056\n",
      "160/388, train_loss: 0.1777, step time: 0.5063\n",
      "161/388, train_loss: 0.1075, step time: 0.4885\n",
      "162/388, train_loss: 0.0949, step time: 1.1803\n",
      "163/388, train_loss: 0.3539, step time: 0.5367\n",
      "164/388, train_loss: 0.2765, step time: 0.5180\n",
      "165/388, train_loss: 0.3816, step time: 0.4958\n",
      "166/388, train_loss: 0.0745, step time: 0.4990\n",
      "167/388, train_loss: 0.0620, step time: 0.4821\n",
      "168/388, train_loss: 0.1448, step time: 0.4789\n",
      "169/388, train_loss: 0.0928, step time: 0.4874\n",
      "170/388, train_loss: 0.2634, step time: 0.4878\n",
      "171/388, train_loss: 0.1721, step time: 1.2260\n",
      "172/388, train_loss: 0.1234, step time: 0.5335\n",
      "173/388, train_loss: 0.1542, step time: 0.5098\n",
      "174/388, train_loss: 0.3034, step time: 0.5028\n",
      "175/388, train_loss: 0.2347, step time: 0.4937\n",
      "176/388, train_loss: 0.0924, step time: 0.4929\n",
      "177/388, train_loss: 0.3438, step time: 0.4761\n",
      "178/388, train_loss: 0.2012, step time: 0.4946\n",
      "179/388, train_loss: 0.2214, step time: 0.4976\n",
      "180/388, train_loss: 0.1139, step time: 0.9854\n",
      "181/388, train_loss: 0.0715, step time: 0.5451\n",
      "182/388, train_loss: 0.1380, step time: 0.5044\n",
      "183/388, train_loss: 0.1351, step time: 0.4885\n",
      "184/388, train_loss: 0.1620, step time: 0.4819\n",
      "185/388, train_loss: 0.0792, step time: 1.2258\n",
      "186/388, train_loss: 0.1803, step time: 0.5214\n",
      "187/388, train_loss: 0.2809, step time: 0.5029\n",
      "188/388, train_loss: 0.4566, step time: 0.4995\n",
      "189/388, train_loss: 0.0489, step time: 0.5004\n",
      "190/388, train_loss: 0.3827, step time: 0.4837\n",
      "191/388, train_loss: 0.2204, step time: 0.5294\n",
      "192/388, train_loss: 0.1629, step time: 0.5113\n",
      "193/388, train_loss: 0.0566, step time: 0.4904\n",
      "194/388, train_loss: 0.2042, step time: 0.4962\n",
      "195/388, train_loss: 0.2748, step time: 0.4791\n",
      "196/388, train_loss: 0.0996, step time: 0.5140\n",
      "197/388, train_loss: 0.1217, step time: 0.4927\n",
      "198/388, train_loss: 0.0764, step time: 0.4923\n",
      "199/388, train_loss: 0.3504, step time: 0.5002\n",
      "200/388, train_loss: 0.2196, step time: 0.4987\n",
      "201/388, train_loss: 0.2938, step time: 0.4873\n",
      "202/388, train_loss: 0.3084, step time: 0.4862\n",
      "203/388, train_loss: 0.1546, step time: 0.5671\n",
      "204/388, train_loss: 0.0630, step time: 0.5358\n",
      "205/388, train_loss: 0.1302, step time: 0.5113\n",
      "206/388, train_loss: 0.2729, step time: 0.5162\n",
      "207/388, train_loss: 0.2756, step time: 0.5971\n",
      "208/388, train_loss: 0.1766, step time: 0.5479\n",
      "209/388, train_loss: 0.3529, step time: 0.5213\n",
      "210/388, train_loss: 0.1506, step time: 0.4954\n",
      "211/388, train_loss: 0.1991, step time: 0.4960\n",
      "212/388, train_loss: 0.3276, step time: 0.4999\n",
      "213/388, train_loss: 0.1045, step time: 0.5346\n",
      "214/388, train_loss: 0.0687, step time: 0.5034\n",
      "215/388, train_loss: 0.0590, step time: 0.5010\n",
      "216/388, train_loss: 0.1051, step time: 0.6439\n",
      "217/388, train_loss: 0.1403, step time: 0.5536\n",
      "218/388, train_loss: 0.4617, step time: 0.5105\n",
      "219/388, train_loss: 0.0906, step time: 0.5286\n",
      "220/388, train_loss: 0.1752, step time: 0.5043\n",
      "221/388, train_loss: 0.1921, step time: 0.4883\n",
      "222/388, train_loss: 0.2203, step time: 0.5071\n",
      "223/388, train_loss: 0.4104, step time: 0.5018\n",
      "224/388, train_loss: 0.0825, step time: 0.8647\n",
      "225/388, train_loss: 0.0878, step time: 0.5507\n",
      "226/388, train_loss: 0.3471, step time: 0.5120\n",
      "227/388, train_loss: 0.2779, step time: 0.4912\n",
      "228/388, train_loss: 0.0962, step time: 0.5135\n",
      "229/388, train_loss: 0.1472, step time: 0.5958\n",
      "230/388, train_loss: 0.2440, step time: 0.5361\n",
      "231/388, train_loss: 0.0489, step time: 0.5093\n",
      "232/388, train_loss: 0.2047, step time: 0.4992\n",
      "233/388, train_loss: 0.2415, step time: 0.5528\n",
      "234/388, train_loss: 0.2470, step time: 0.5261\n",
      "235/388, train_loss: 0.1316, step time: 0.5342\n",
      "236/388, train_loss: 0.1306, step time: 0.5192\n",
      "237/388, train_loss: 0.1205, step time: 0.5106\n",
      "238/388, train_loss: 0.1380, step time: 0.5085\n",
      "239/388, train_loss: 0.1783, step time: 0.6219\n",
      "240/388, train_loss: 0.1875, step time: 0.5445\n",
      "241/388, train_loss: 0.1172, step time: 0.5167\n",
      "242/388, train_loss: 0.1652, step time: 0.5156\n",
      "243/388, train_loss: 0.0797, step time: 0.4861\n",
      "244/388, train_loss: 0.0301, step time: 0.4846\n",
      "245/388, train_loss: 0.1483, step time: 0.5552\n",
      "246/388, train_loss: 0.2791, step time: 0.5478\n",
      "247/388, train_loss: 0.3772, step time: 0.5099\n",
      "248/388, train_loss: 0.2116, step time: 0.5006\n",
      "249/388, train_loss: 0.1652, step time: 0.4972\n",
      "250/388, train_loss: 0.2727, step time: 0.5027\n",
      "251/388, train_loss: 0.1600, step time: 0.4784\n",
      "252/388, train_loss: 0.1178, step time: 0.4806\n",
      "253/388, train_loss: 0.1414, step time: 0.8135\n",
      "254/388, train_loss: 0.1795, step time: 0.5535\n",
      "255/388, train_loss: 0.2466, step time: 0.5237\n",
      "256/388, train_loss: 0.1022, step time: 0.5079\n",
      "257/388, train_loss: 0.0852, step time: 0.5017\n",
      "258/388, train_loss: 0.0492, step time: 0.5476\n",
      "259/388, train_loss: 0.1106, step time: 0.5052\n",
      "260/388, train_loss: 0.2314, step time: 0.4988\n",
      "261/388, train_loss: 0.1270, step time: 0.5042\n",
      "262/388, train_loss: 0.1148, step time: 0.5043\n",
      "263/388, train_loss: 0.1977, step time: 0.4985\n",
      "264/388, train_loss: 0.1802, step time: 0.4855\n",
      "265/388, train_loss: 0.2848, step time: 1.1367\n",
      "266/388, train_loss: 0.1233, step time: 0.5533\n",
      "267/388, train_loss: 0.4534, step time: 0.5093\n",
      "268/388, train_loss: 0.1840, step time: 0.4999\n",
      "269/388, train_loss: 0.2100, step time: 0.4883\n",
      "270/388, train_loss: 0.2438, step time: 0.4844\n",
      "271/388, train_loss: 0.0781, step time: 0.4920\n",
      "272/388, train_loss: 0.2018, step time: 0.4821\n",
      "273/388, train_loss: 0.0959, step time: 0.4893\n",
      "274/388, train_loss: 0.1899, step time: 0.4917\n",
      "275/388, train_loss: 0.0532, step time: 1.0017\n",
      "276/388, train_loss: 0.1050, step time: 0.5434\n",
      "277/388, train_loss: 0.1952, step time: 0.5200\n",
      "278/388, train_loss: 0.0654, step time: 0.4972\n",
      "279/388, train_loss: 0.2408, step time: 0.5047\n",
      "280/388, train_loss: 0.0688, step time: 0.4848\n",
      "281/388, train_loss: 0.0905, step time: 0.5014\n",
      "282/388, train_loss: 0.0680, step time: 0.5500\n",
      "283/388, train_loss: 0.1555, step time: 0.5238\n",
      "284/388, train_loss: 0.1756, step time: 0.4959\n",
      "285/388, train_loss: 0.1905, step time: 0.4959\n",
      "286/388, train_loss: 0.1331, step time: 0.4845\n",
      "287/388, train_loss: 0.1321, step time: 0.4903\n",
      "288/388, train_loss: 0.1045, step time: 0.9495\n",
      "289/388, train_loss: 0.0416, step time: 0.5433\n",
      "290/388, train_loss: 0.1460, step time: 0.5234\n",
      "291/388, train_loss: 0.0828, step time: 0.4956\n",
      "292/388, train_loss: 0.2714, step time: 0.4954\n",
      "293/388, train_loss: 0.1389, step time: 0.7219\n",
      "294/388, train_loss: 0.1173, step time: 0.5537\n",
      "295/388, train_loss: 0.1583, step time: 0.5161\n",
      "296/388, train_loss: 0.1354, step time: 0.5053\n",
      "297/388, train_loss: 0.0988, step time: 0.4948\n",
      "298/388, train_loss: 0.1312, step time: 0.4898\n",
      "299/388, train_loss: 0.0938, step time: 0.8401\n",
      "300/388, train_loss: 0.0775, step time: 0.5365\n",
      "301/388, train_loss: 0.2309, step time: 0.5135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/388, train_loss: 0.2577, step time: 0.4884\n",
      "303/388, train_loss: 0.1316, step time: 0.4934\n",
      "304/388, train_loss: 0.2469, step time: 0.4795\n",
      "305/388, train_loss: 0.1228, step time: 0.7226\n",
      "306/388, train_loss: 0.1663, step time: 0.5456\n",
      "307/388, train_loss: 0.2619, step time: 0.5177\n",
      "308/388, train_loss: 0.2159, step time: 0.4993\n",
      "309/388, train_loss: 0.1212, step time: 0.4969\n",
      "310/388, train_loss: 0.0498, step time: 0.4832\n",
      "311/388, train_loss: 0.1252, step time: 0.4911\n",
      "312/388, train_loss: 0.0796, step time: 0.4744\n",
      "313/388, train_loss: 0.1423, step time: 0.4742\n",
      "314/388, train_loss: 0.1598, step time: 0.4760\n",
      "315/388, train_loss: 0.1173, step time: 0.4743\n",
      "316/388, train_loss: 0.0928, step time: 0.7796\n",
      "317/388, train_loss: 0.4134, step time: 0.5495\n",
      "318/388, train_loss: 0.1509, step time: 0.5200\n",
      "319/388, train_loss: 0.2302, step time: 0.5060\n",
      "320/388, train_loss: 0.0974, step time: 0.4929\n",
      "321/388, train_loss: 0.2395, step time: 0.4987\n",
      "322/388, train_loss: 0.1316, step time: 0.4855\n",
      "323/388, train_loss: 0.1026, step time: 0.4790\n",
      "324/388, train_loss: 0.2026, step time: 0.5028\n",
      "325/388, train_loss: 0.0945, step time: 0.4988\n",
      "326/388, train_loss: 0.0827, step time: 0.4960\n",
      "327/388, train_loss: 0.1063, step time: 0.4791\n",
      "328/388, train_loss: 0.2304, step time: 0.6737\n",
      "329/388, train_loss: 0.3161, step time: 0.5728\n",
      "330/388, train_loss: 0.1603, step time: 0.5323\n",
      "331/388, train_loss: 0.1804, step time: 0.5055\n",
      "332/388, train_loss: 0.1485, step time: 0.4888\n",
      "333/388, train_loss: 0.2518, step time: 0.4916\n",
      "334/388, train_loss: 0.2275, step time: 0.5176\n",
      "335/388, train_loss: 0.1316, step time: 0.5718\n",
      "336/388, train_loss: 0.3232, step time: 0.5395\n",
      "337/388, train_loss: 0.0801, step time: 0.5021\n",
      "338/388, train_loss: 0.2030, step time: 0.4979\n",
      "339/388, train_loss: 0.4522, step time: 0.4843\n",
      "340/388, train_loss: 0.3693, step time: 0.4758\n",
      "341/388, train_loss: 0.0893, step time: 1.0781\n",
      "342/388, train_loss: 0.0685, step time: 0.5304\n",
      "343/388, train_loss: 0.3252, step time: 0.4996\n",
      "344/388, train_loss: 0.0672, step time: 0.5012\n",
      "345/388, train_loss: 0.2594, step time: 0.4868\n",
      "346/388, train_loss: 0.0908, step time: 0.4819\n",
      "347/388, train_loss: 0.1845, step time: 0.4853\n",
      "348/388, train_loss: 0.1856, step time: 0.9699\n",
      "349/388, train_loss: 0.5003, step time: 0.5399\n",
      "350/388, train_loss: 0.3485, step time: 0.5038\n",
      "351/388, train_loss: 0.2000, step time: 0.4897\n",
      "352/388, train_loss: 0.0903, step time: 0.4978\n",
      "353/388, train_loss: 0.1813, step time: 0.4790\n",
      "354/388, train_loss: 0.2588, step time: 0.4924\n",
      "355/388, train_loss: 0.1240, step time: 0.4965\n",
      "356/388, train_loss: 0.0954, step time: 0.4867\n",
      "357/388, train_loss: 0.2487, step time: 0.4943\n",
      "358/388, train_loss: 0.1048, step time: 0.4800\n",
      "359/388, train_loss: 0.3510, step time: 0.4733\n",
      "360/388, train_loss: 0.0818, step time: 0.4914\n",
      "361/388, train_loss: 0.1600, step time: 0.4961\n",
      "362/388, train_loss: 0.0982, step time: 0.5196\n",
      "363/388, train_loss: 0.0702, step time: 0.5029\n",
      "364/388, train_loss: 0.1873, step time: 0.4963\n",
      "365/388, train_loss: 0.2185, step time: 0.4978\n",
      "366/388, train_loss: 0.1923, step time: 0.5189\n",
      "367/388, train_loss: 0.0691, step time: 0.5052\n",
      "368/388, train_loss: 0.0664, step time: 0.5090\n",
      "369/388, train_loss: 0.2845, step time: 0.5089\n",
      "370/388, train_loss: 0.1546, step time: 0.4941\n",
      "371/388, train_loss: 0.2895, step time: 0.8591\n",
      "372/388, train_loss: 0.1971, step time: 0.5345\n",
      "373/388, train_loss: 0.2088, step time: 0.5096\n",
      "374/388, train_loss: 0.3140, step time: 0.5044\n",
      "375/388, train_loss: 0.0720, step time: 0.5024\n",
      "376/388, train_loss: 0.3346, step time: 0.4985\n",
      "377/388, train_loss: 0.3190, step time: 0.4935\n",
      "378/388, train_loss: 0.1197, step time: 0.4814\n",
      "379/388, train_loss: 0.1340, step time: 0.5058\n",
      "380/388, train_loss: 0.2701, step time: 0.4995\n",
      "381/388, train_loss: 0.0831, step time: 0.5074\n",
      "382/388, train_loss: 0.2222, step time: 0.4945\n",
      "383/388, train_loss: 0.1953, step time: 0.4723\n",
      "384/388, train_loss: 0.3302, step time: 0.7311\n",
      "385/388, train_loss: 0.1790, step time: 0.5276\n",
      "386/388, train_loss: 0.1555, step time: 0.5157\n",
      "387/388, train_loss: 0.2389, step time: 0.4973\n",
      "388/388, train_loss: 0.1227, step time: 0.4919\n",
      "epoch 80 average loss: 0.1774\n",
      "saved new best metric model\n",
      "current epoch: 80 current mean dice: 0.7788 tc: 0.8273 wt: 0.9062 et: 0.6028\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 80 is: 300.3813\n",
      "----------\n",
      "epoch 81/300\n",
      "1/388, train_loss: 0.2594, step time: 0.4771\n",
      "2/388, train_loss: 0.1402, step time: 0.4802\n",
      "3/388, train_loss: 0.2587, step time: 0.8395\n",
      "4/388, train_loss: 0.0818, step time: 0.5450\n",
      "5/388, train_loss: 0.0688, step time: 0.5258\n",
      "6/388, train_loss: 0.2646, step time: 0.4990\n",
      "7/388, train_loss: 0.5124, step time: 0.4973\n",
      "8/388, train_loss: 0.0697, step time: 0.5878\n",
      "9/388, train_loss: 0.0612, step time: 0.5184\n",
      "10/388, train_loss: 0.1556, step time: 0.5094\n",
      "11/388, train_loss: 0.1771, step time: 0.5905\n",
      "12/388, train_loss: 0.1289, step time: 0.5634\n",
      "13/388, train_loss: 0.0979, step time: 0.5119\n",
      "14/388, train_loss: 0.0936, step time: 0.4998\n",
      "15/388, train_loss: 0.1433, step time: 0.6550\n",
      "16/388, train_loss: 0.1367, step time: 0.5399\n",
      "17/388, train_loss: 0.0688, step time: 0.5080\n",
      "18/388, train_loss: 0.2621, step time: 0.6620\n",
      "19/388, train_loss: 0.0913, step time: 0.5806\n",
      "20/388, train_loss: 0.2973, step time: 0.5477\n",
      "21/388, train_loss: 0.2899, step time: 0.5182\n",
      "22/388, train_loss: 0.2307, step time: 0.5378\n",
      "23/388, train_loss: 0.2363, step time: 0.5005\n",
      "24/388, train_loss: 0.1510, step time: 0.5274\n",
      "25/388, train_loss: 0.3607, step time: 0.5274\n",
      "26/388, train_loss: 0.1708, step time: 0.5241\n",
      "27/388, train_loss: 0.1166, step time: 0.5411\n",
      "28/388, train_loss: 0.1649, step time: 0.5424\n",
      "29/388, train_loss: 0.0755, step time: 0.5824\n",
      "30/388, train_loss: 0.1068, step time: 0.5408\n",
      "31/388, train_loss: 0.1803, step time: 0.5111\n",
      "32/388, train_loss: 0.2489, step time: 0.6628\n",
      "33/388, train_loss: 0.0738, step time: 0.6121\n",
      "34/388, train_loss: 0.1279, step time: 0.5395\n",
      "35/388, train_loss: 0.2244, step time: 0.4920\n",
      "36/388, train_loss: 0.0934, step time: 0.5114\n",
      "37/388, train_loss: 0.1200, step time: 0.5727\n",
      "38/388, train_loss: 0.2237, step time: 0.5413\n",
      "39/388, train_loss: 0.1442, step time: 0.5150\n",
      "40/388, train_loss: 0.1183, step time: 0.4971\n",
      "41/388, train_loss: 0.0843, step time: 0.5039\n",
      "42/388, train_loss: 0.1189, step time: 0.4823\n",
      "43/388, train_loss: 0.2000, step time: 0.4905\n",
      "44/388, train_loss: 0.4150, step time: 1.1829\n",
      "45/388, train_loss: 0.4584, step time: 0.5582\n",
      "46/388, train_loss: 0.2267, step time: 0.5188\n",
      "47/388, train_loss: 0.2444, step time: 0.5201\n",
      "48/388, train_loss: 0.1057, step time: 0.5017\n",
      "49/388, train_loss: 0.0618, step time: 0.5021\n",
      "50/388, train_loss: 0.0941, step time: 0.4842\n",
      "51/388, train_loss: 0.2163, step time: 0.5254\n",
      "52/388, train_loss: 0.0460, step time: 0.5463\n",
      "53/388, train_loss: 0.1097, step time: 0.5344\n",
      "54/388, train_loss: 0.2253, step time: 0.5176\n",
      "55/388, train_loss: 0.2286, step time: 1.2051\n",
      "56/388, train_loss: 0.2102, step time: 0.5409\n",
      "57/388, train_loss: 0.1698, step time: 0.5051\n",
      "58/388, train_loss: 0.2984, step time: 0.4989\n",
      "59/388, train_loss: 0.1321, step time: 0.4872\n",
      "60/388, train_loss: 0.0931, step time: 0.5650\n",
      "61/388, train_loss: 0.0661, step time: 0.5366\n",
      "62/388, train_loss: 0.1517, step time: 0.5235\n",
      "63/388, train_loss: 0.1536, step time: 0.5164\n",
      "64/388, train_loss: 0.2418, step time: 0.4987\n",
      "65/388, train_loss: 0.1928, step time: 0.4946\n",
      "66/388, train_loss: 0.2220, step time: 1.0900\n",
      "67/388, train_loss: 0.1648, step time: 0.5293\n",
      "68/388, train_loss: 0.2153, step time: 0.5012\n",
      "69/388, train_loss: 0.0523, step time: 0.4921\n",
      "70/388, train_loss: 0.2613, step time: 0.4824\n",
      "71/388, train_loss: 0.0557, step time: 0.5540\n",
      "72/388, train_loss: 0.0891, step time: 0.5978\n",
      "73/388, train_loss: 0.1502, step time: 0.5399\n",
      "74/388, train_loss: 0.1632, step time: 0.5103\n",
      "75/388, train_loss: 0.1052, step time: 0.5445\n",
      "76/388, train_loss: 0.1977, step time: 0.5408\n",
      "77/388, train_loss: 0.1168, step time: 0.5229\n",
      "78/388, train_loss: 0.2086, step time: 0.5108\n",
      "79/388, train_loss: 0.4037, step time: 0.4929\n",
      "80/388, train_loss: 0.1064, step time: 0.9352\n",
      "81/388, train_loss: 0.4941, step time: 0.5337\n",
      "82/388, train_loss: 0.1086, step time: 0.5164\n",
      "83/388, train_loss: 0.1892, step time: 0.5033\n",
      "84/388, train_loss: 0.2367, step time: 0.5100\n",
      "85/388, train_loss: 0.1197, step time: 0.4964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/388, train_loss: 0.0869, step time: 0.6025\n",
      "87/388, train_loss: 0.1082, step time: 0.5355\n",
      "88/388, train_loss: 0.1644, step time: 0.5072\n",
      "89/388, train_loss: 0.1056, step time: 0.5006\n",
      "90/388, train_loss: 0.1260, step time: 0.4807\n",
      "91/388, train_loss: 0.0918, step time: 0.9887\n",
      "92/388, train_loss: 0.2459, step time: 0.5440\n",
      "93/388, train_loss: 0.3804, step time: 0.5094\n",
      "94/388, train_loss: 0.1297, step time: 0.4968\n",
      "95/388, train_loss: 0.1609, step time: 0.9158\n",
      "96/388, train_loss: 0.1167, step time: 0.5437\n",
      "97/388, train_loss: 0.1406, step time: 0.5172\n",
      "98/388, train_loss: 0.0961, step time: 0.4972\n",
      "99/388, train_loss: 0.1438, step time: 0.4980\n",
      "100/388, train_loss: 0.2396, step time: 0.4834\n",
      "101/388, train_loss: 0.1025, step time: 0.5163\n",
      "102/388, train_loss: 0.0961, step time: 0.4947\n",
      "103/388, train_loss: 0.1016, step time: 0.5263\n",
      "104/388, train_loss: 0.4288, step time: 0.5367\n",
      "105/388, train_loss: 0.1135, step time: 0.5265\n",
      "106/388, train_loss: 0.2253, step time: 0.5034\n",
      "107/388, train_loss: 0.1749, step time: 0.5137\n",
      "108/388, train_loss: 0.1110, step time: 0.4944\n",
      "109/388, train_loss: 0.2812, step time: 0.5164\n",
      "110/388, train_loss: 0.1142, step time: 0.6285\n",
      "111/388, train_loss: 0.1589, step time: 0.5465\n",
      "112/388, train_loss: 0.1571, step time: 0.5291\n",
      "113/388, train_loss: 0.0789, step time: 0.5605\n",
      "114/388, train_loss: 0.0705, step time: 0.5262\n",
      "115/388, train_loss: 0.0553, step time: 0.5140\n",
      "116/388, train_loss: 0.3784, step time: 0.5098\n",
      "117/388, train_loss: 0.0808, step time: 0.4927\n",
      "118/388, train_loss: 0.1088, step time: 0.4964\n",
      "119/388, train_loss: 0.1334, step time: 0.5201\n",
      "120/388, train_loss: 0.2060, step time: 0.4887\n",
      "121/388, train_loss: 0.1304, step time: 0.5097\n",
      "122/388, train_loss: 0.1260, step time: 0.4987\n",
      "123/388, train_loss: 0.0657, step time: 0.4932\n",
      "124/388, train_loss: 0.1499, step time: 0.4954\n",
      "125/388, train_loss: 0.0845, step time: 0.4986\n",
      "126/388, train_loss: 0.1601, step time: 0.4888\n",
      "127/388, train_loss: 0.1574, step time: 0.5298\n",
      "128/388, train_loss: 0.0931, step time: 0.5226\n",
      "129/388, train_loss: 0.2863, step time: 0.5700\n",
      "130/388, train_loss: 0.1927, step time: 0.5329\n",
      "131/388, train_loss: 0.0946, step time: 0.5100\n",
      "132/388, train_loss: 0.1279, step time: 1.1392\n",
      "133/388, train_loss: 0.1702, step time: 0.5454\n",
      "134/388, train_loss: 0.1142, step time: 0.5266\n",
      "135/388, train_loss: 0.0959, step time: 0.5349\n",
      "136/388, train_loss: 0.1796, step time: 0.5135\n",
      "137/388, train_loss: 0.2656, step time: 0.5102\n",
      "138/388, train_loss: 0.0911, step time: 0.4906\n",
      "139/388, train_loss: 0.3095, step time: 0.4913\n",
      "140/388, train_loss: 0.5990, step time: 1.1430\n",
      "141/388, train_loss: 0.2008, step time: 0.5380\n",
      "142/388, train_loss: 0.2761, step time: 0.5141\n",
      "143/388, train_loss: 0.2016, step time: 0.5806\n",
      "144/388, train_loss: 0.1865, step time: 0.5451\n",
      "145/388, train_loss: 0.2814, step time: 0.5204\n",
      "146/388, train_loss: 0.0965, step time: 0.5070\n",
      "147/388, train_loss: 0.1955, step time: 0.4863\n",
      "148/388, train_loss: 0.3720, step time: 0.4858\n",
      "149/388, train_loss: 0.3968, step time: 0.4848\n",
      "150/388, train_loss: 0.1963, step time: 1.1313\n",
      "151/388, train_loss: 0.3656, step time: 0.5455\n",
      "152/388, train_loss: 0.0995, step time: 0.5101\n",
      "153/388, train_loss: 0.1532, step time: 0.5005\n",
      "154/388, train_loss: 0.2778, step time: 1.1587\n",
      "155/388, train_loss: 0.0653, step time: 0.5306\n",
      "156/388, train_loss: 0.4392, step time: 0.5132\n",
      "157/388, train_loss: 0.1229, step time: 0.4939\n",
      "158/388, train_loss: 0.0848, step time: 0.4947\n",
      "159/388, train_loss: 0.0682, step time: 0.4776\n",
      "160/388, train_loss: 0.1131, step time: 0.5570\n",
      "161/388, train_loss: 0.1829, step time: 0.5225\n",
      "162/388, train_loss: 0.1149, step time: 0.4920\n",
      "163/388, train_loss: 0.2347, step time: 0.4843\n",
      "164/388, train_loss: 0.2140, step time: 1.2053\n",
      "165/388, train_loss: 0.0984, step time: 0.5286\n",
      "166/388, train_loss: 0.2059, step time: 0.5140\n",
      "167/388, train_loss: 0.1734, step time: 0.4949\n",
      "168/388, train_loss: 0.1125, step time: 0.4949\n",
      "169/388, train_loss: 0.2534, step time: 0.4827\n",
      "170/388, train_loss: 0.3591, step time: 0.4871\n",
      "171/388, train_loss: 0.1615, step time: 0.4805\n",
      "172/388, train_loss: 0.3221, step time: 1.2016\n",
      "173/388, train_loss: 0.3555, step time: 0.5296\n",
      "174/388, train_loss: 0.1645, step time: 0.4963\n",
      "175/388, train_loss: 0.1155, step time: 0.4896\n",
      "176/388, train_loss: 0.1476, step time: 0.5032\n",
      "177/388, train_loss: 0.1957, step time: 0.5020\n",
      "178/388, train_loss: 0.1622, step time: 0.4865\n",
      "179/388, train_loss: 0.0437, step time: 0.4839\n",
      "180/388, train_loss: 0.2588, step time: 0.4848\n",
      "181/388, train_loss: 0.1072, step time: 0.5344\n",
      "182/388, train_loss: 0.1196, step time: 0.5158\n",
      "183/388, train_loss: 0.1195, step time: 0.4983\n",
      "184/388, train_loss: 0.1810, step time: 0.6585\n",
      "185/388, train_loss: 0.3202, step time: 0.5561\n",
      "186/388, train_loss: 0.1395, step time: 0.5153\n",
      "187/388, train_loss: 0.0633, step time: 0.4940\n",
      "188/388, train_loss: 0.1839, step time: 0.4963\n",
      "189/388, train_loss: 0.1206, step time: 0.4850\n",
      "190/388, train_loss: 0.1263, step time: 0.4861\n",
      "191/388, train_loss: 0.5178, step time: 0.5000\n",
      "192/388, train_loss: 0.2292, step time: 0.4939\n",
      "193/388, train_loss: 0.0959, step time: 1.0257\n",
      "194/388, train_loss: 0.1524, step time: 0.5464\n",
      "195/388, train_loss: 0.1466, step time: 0.5280\n",
      "196/388, train_loss: 0.1805, step time: 0.5050\n",
      "197/388, train_loss: 0.2299, step time: 0.5046\n",
      "198/388, train_loss: 0.1305, step time: 0.5190\n",
      "199/388, train_loss: 0.2151, step time: 0.5031\n",
      "200/388, train_loss: 0.2107, step time: 0.4866\n",
      "201/388, train_loss: 0.1389, step time: 1.0456\n",
      "202/388, train_loss: 0.5798, step time: 0.5455\n",
      "203/388, train_loss: 0.1574, step time: 0.5265\n",
      "204/388, train_loss: 0.2602, step time: 0.4999\n",
      "205/388, train_loss: 0.0728, step time: 0.4915\n",
      "206/388, train_loss: 0.1355, step time: 1.0913\n",
      "207/388, train_loss: 0.2343, step time: 0.5368\n",
      "208/388, train_loss: 0.1028, step time: 0.5131\n",
      "209/388, train_loss: 0.1848, step time: 0.4854\n",
      "210/388, train_loss: 0.2049, step time: 0.4972\n",
      "211/388, train_loss: 0.1964, step time: 1.0495\n",
      "212/388, train_loss: 0.2800, step time: 0.5373\n",
      "213/388, train_loss: 0.1970, step time: 0.5096\n",
      "214/388, train_loss: 0.2178, step time: 0.5110\n",
      "215/388, train_loss: 0.0910, step time: 0.4921\n",
      "216/388, train_loss: 0.2101, step time: 0.4790\n",
      "217/388, train_loss: 0.2453, step time: 0.4914\n",
      "218/388, train_loss: 0.0881, step time: 0.4785\n",
      "219/388, train_loss: 0.2497, step time: 0.5248\n",
      "220/388, train_loss: 0.1608, step time: 0.5050\n",
      "221/388, train_loss: 0.1317, step time: 1.0933\n",
      "222/388, train_loss: 0.1609, step time: 0.5426\n",
      "223/388, train_loss: 0.1603, step time: 0.5005\n",
      "224/388, train_loss: 0.1182, step time: 0.5000\n",
      "225/388, train_loss: 0.0706, step time: 0.4823\n",
      "226/388, train_loss: 0.2258, step time: 0.4872\n",
      "227/388, train_loss: 0.2903, step time: 1.1363\n",
      "228/388, train_loss: 0.0983, step time: 0.5406\n",
      "229/388, train_loss: 0.0666, step time: 0.5097\n",
      "230/388, train_loss: 0.4178, step time: 0.4991\n",
      "231/388, train_loss: 0.1372, step time: 0.4908\n",
      "232/388, train_loss: 0.1421, step time: 0.4955\n",
      "233/388, train_loss: 0.2153, step time: 0.4854\n",
      "234/388, train_loss: 0.1313, step time: 0.6481\n",
      "235/388, train_loss: 0.2287, step time: 0.5389\n",
      "236/388, train_loss: 0.1259, step time: 0.5349\n",
      "237/388, train_loss: 0.0860, step time: 0.5157\n",
      "238/388, train_loss: 0.0729, step time: 0.4975\n",
      "239/388, train_loss: 0.2892, step time: 0.6221\n",
      "240/388, train_loss: 0.0810, step time: 0.5346\n",
      "241/388, train_loss: 0.0909, step time: 0.5089\n",
      "242/388, train_loss: 0.0302, step time: 0.4976\n",
      "243/388, train_loss: 0.1173, step time: 0.4879\n",
      "244/388, train_loss: 0.2929, step time: 0.4911\n",
      "245/388, train_loss: 0.1388, step time: 0.4821\n",
      "246/388, train_loss: 0.5483, step time: 0.4734\n",
      "247/388, train_loss: 0.2109, step time: 0.6566\n",
      "248/388, train_loss: 0.3925, step time: 0.5681\n",
      "249/388, train_loss: 0.1401, step time: 0.5414\n",
      "250/388, train_loss: 0.2555, step time: 0.4984\n",
      "251/388, train_loss: 0.0823, step time: 0.4960\n",
      "252/388, train_loss: 0.2694, step time: 0.4953\n",
      "253/388, train_loss: 0.1076, step time: 0.4958\n",
      "254/388, train_loss: 0.1202, step time: 0.4985\n",
      "255/388, train_loss: 0.1976, step time: 0.5434\n",
      "256/388, train_loss: 0.1251, step time: 0.5383\n",
      "257/388, train_loss: 0.4725, step time: 0.5248\n",
      "258/388, train_loss: 0.0919, step time: 0.4925\n",
      "259/388, train_loss: 0.2327, step time: 0.4963\n",
      "260/388, train_loss: 0.1065, step time: 0.5145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/388, train_loss: 0.1004, step time: 0.5318\n",
      "262/388, train_loss: 0.1990, step time: 0.5055\n",
      "263/388, train_loss: 0.0612, step time: 0.5105\n",
      "264/388, train_loss: 0.1163, step time: 0.5113\n",
      "265/388, train_loss: 0.0410, step time: 0.5159\n",
      "266/388, train_loss: 0.0865, step time: 0.5013\n",
      "267/388, train_loss: 0.3537, step time: 1.1914\n",
      "268/388, train_loss: 0.0909, step time: 0.5507\n",
      "269/388, train_loss: 0.1803, step time: 0.5187\n",
      "270/388, train_loss: 0.0953, step time: 0.5029\n",
      "271/388, train_loss: 0.1670, step time: 0.4919\n",
      "272/388, train_loss: 0.1695, step time: 0.4935\n",
      "273/388, train_loss: 0.3289, step time: 0.4902\n",
      "274/388, train_loss: 0.1865, step time: 0.4856\n",
      "275/388, train_loss: 0.1092, step time: 1.1116\n",
      "276/388, train_loss: 0.0782, step time: 0.5208\n",
      "277/388, train_loss: 0.2273, step time: 0.5012\n",
      "278/388, train_loss: 0.1597, step time: 0.4958\n",
      "279/388, train_loss: 0.0665, step time: 0.4882\n",
      "280/388, train_loss: 0.1246, step time: 0.4911\n",
      "281/388, train_loss: 0.0726, step time: 1.0418\n",
      "282/388, train_loss: 0.0536, step time: 0.5400\n",
      "283/388, train_loss: 0.0383, step time: 0.5019\n",
      "284/388, train_loss: 0.0942, step time: 0.4972\n",
      "285/388, train_loss: 0.2325, step time: 0.4856\n",
      "286/388, train_loss: 0.1508, step time: 0.4832\n",
      "287/388, train_loss: 0.0645, step time: 0.4893\n",
      "288/388, train_loss: 0.2076, step time: 0.4792\n",
      "289/388, train_loss: 0.5058, step time: 0.4934\n",
      "290/388, train_loss: 0.2179, step time: 0.5018\n",
      "291/388, train_loss: 0.1356, step time: 0.4976\n",
      "292/388, train_loss: 0.0665, step time: 0.4969\n",
      "293/388, train_loss: 0.0524, step time: 0.5424\n",
      "294/388, train_loss: 0.1358, step time: 0.5136\n",
      "295/388, train_loss: 0.1933, step time: 0.5003\n",
      "296/388, train_loss: 0.1237, step time: 0.4908\n",
      "297/388, train_loss: 0.0616, step time: 1.0396\n",
      "298/388, train_loss: 0.1834, step time: 0.5313\n",
      "299/388, train_loss: 0.0316, step time: 0.5017\n",
      "300/388, train_loss: 0.1050, step time: 0.4986\n",
      "301/388, train_loss: 0.0651, step time: 0.4834\n",
      "302/388, train_loss: 0.2334, step time: 0.5247\n",
      "303/388, train_loss: 0.1309, step time: 0.4995\n",
      "304/388, train_loss: 0.1167, step time: 0.5507\n",
      "305/388, train_loss: 0.0773, step time: 0.5396\n",
      "306/388, train_loss: 0.2131, step time: 0.5133\n",
      "307/388, train_loss: 0.0905, step time: 0.5456\n",
      "308/388, train_loss: 0.1644, step time: 0.6319\n",
      "309/388, train_loss: 0.0676, step time: 0.5359\n",
      "310/388, train_loss: 0.1037, step time: 0.5070\n",
      "311/388, train_loss: 0.1617, step time: 0.4969\n",
      "312/388, train_loss: 0.0912, step time: 0.4935\n",
      "313/388, train_loss: 0.1966, step time: 0.4819\n",
      "314/388, train_loss: 0.0985, step time: 0.4926\n",
      "315/388, train_loss: 0.1769, step time: 0.4949\n",
      "316/388, train_loss: 0.0805, step time: 0.4879\n",
      "317/388, train_loss: 0.0867, step time: 0.4998\n",
      "318/388, train_loss: 0.4680, step time: 0.5093\n",
      "319/388, train_loss: 0.2649, step time: 0.5925\n",
      "320/388, train_loss: 0.1585, step time: 0.5282\n",
      "321/388, train_loss: 0.1433, step time: 0.4962\n",
      "322/388, train_loss: 0.0873, step time: 0.4812\n",
      "323/388, train_loss: 0.1108, step time: 1.2129\n",
      "324/388, train_loss: 0.3632, step time: 0.5366\n",
      "325/388, train_loss: 0.2687, step time: 0.5023\n",
      "326/388, train_loss: 0.1729, step time: 0.4930\n",
      "327/388, train_loss: 0.0529, step time: 0.4967\n",
      "328/388, train_loss: 0.2651, step time: 0.4820\n",
      "329/388, train_loss: 0.1201, step time: 0.5149\n",
      "330/388, train_loss: 0.0980, step time: 0.5018\n",
      "331/388, train_loss: 0.2259, step time: 0.4930\n",
      "332/388, train_loss: 0.2809, step time: 0.5006\n",
      "333/388, train_loss: 0.1360, step time: 0.4938\n",
      "334/388, train_loss: 0.2398, step time: 0.4891\n",
      "335/388, train_loss: 0.2605, step time: 0.5012\n",
      "336/388, train_loss: 0.1283, step time: 0.5572\n",
      "337/388, train_loss: 0.2632, step time: 0.5312\n",
      "338/388, train_loss: 0.1009, step time: 0.5042\n",
      "339/388, train_loss: 0.1950, step time: 0.4802\n",
      "340/388, train_loss: 0.1691, step time: 0.5117\n",
      "341/388, train_loss: 0.2186, step time: 0.4936\n",
      "342/388, train_loss: 0.1912, step time: 0.6896\n",
      "343/388, train_loss: 0.1104, step time: 0.5620\n",
      "344/388, train_loss: 0.0353, step time: 0.5189\n",
      "345/388, train_loss: 0.0803, step time: 0.5102\n",
      "346/388, train_loss: 0.3335, step time: 0.4894\n",
      "347/388, train_loss: 0.1252, step time: 0.4765\n",
      "348/388, train_loss: 0.1556, step time: 0.5080\n",
      "349/388, train_loss: 0.3253, step time: 0.5063\n",
      "350/388, train_loss: 0.0821, step time: 0.9178\n",
      "351/388, train_loss: 0.1439, step time: 0.5604\n",
      "352/388, train_loss: 0.1845, step time: 0.5253\n",
      "353/388, train_loss: 0.0770, step time: 0.5122\n",
      "354/388, train_loss: 0.1368, step time: 0.5013\n",
      "355/388, train_loss: 0.0903, step time: 0.4949\n",
      "356/388, train_loss: 0.1341, step time: 0.4920\n",
      "357/388, train_loss: 0.3283, step time: 0.5551\n",
      "358/388, train_loss: 0.2569, step time: 0.5406\n",
      "359/388, train_loss: 0.1349, step time: 0.5084\n",
      "360/388, train_loss: 0.2224, step time: 0.4964\n",
      "361/388, train_loss: 0.2250, step time: 0.4827\n",
      "362/388, train_loss: 0.2001, step time: 0.4867\n",
      "363/388, train_loss: 0.2650, step time: 0.4784\n",
      "364/388, train_loss: 0.0860, step time: 0.4977\n",
      "365/388, train_loss: 0.1424, step time: 0.4930\n",
      "366/388, train_loss: 0.4243, step time: 0.4984\n",
      "367/388, train_loss: 0.1668, step time: 0.4944\n",
      "368/388, train_loss: 0.1584, step time: 0.9374\n",
      "369/388, train_loss: 0.1856, step time: 0.5433\n",
      "370/388, train_loss: 0.1258, step time: 0.5100\n",
      "371/388, train_loss: 0.1878, step time: 0.5202\n",
      "372/388, train_loss: 0.1331, step time: 0.5140\n",
      "373/388, train_loss: 0.2495, step time: 0.5042\n",
      "374/388, train_loss: 0.0812, step time: 0.4896\n",
      "375/388, train_loss: 0.3656, step time: 0.4874\n",
      "376/388, train_loss: 0.1005, step time: 0.5080\n",
      "377/388, train_loss: 0.0915, step time: 0.5089\n",
      "378/388, train_loss: 0.1991, step time: 0.4947\n",
      "379/388, train_loss: 0.0651, step time: 0.5982\n",
      "380/388, train_loss: 0.4614, step time: 0.5339\n",
      "381/388, train_loss: 0.1165, step time: 0.5113\n",
      "382/388, train_loss: 0.0951, step time: 0.5010\n",
      "383/388, train_loss: 0.1451, step time: 0.5010\n",
      "384/388, train_loss: 0.0843, step time: 0.4920\n",
      "385/388, train_loss: 0.1442, step time: 0.4793\n",
      "386/388, train_loss: 0.2818, step time: 0.5041\n",
      "387/388, train_loss: 0.0988, step time: 0.5124\n",
      "388/388, train_loss: 0.1444, step time: 0.5085\n",
      "epoch 81 average loss: 0.1747\n",
      "current epoch: 81 current mean dice: 0.7738 tc: 0.8253 wt: 0.9005 et: 0.5955\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 81 is: 302.3834\n",
      "----------\n",
      "epoch 82/300\n",
      "1/388, train_loss: 0.0438, step time: 0.4674\n",
      "2/388, train_loss: 0.1172, step time: 0.4832\n",
      "3/388, train_loss: 0.0624, step time: 0.7045\n",
      "4/388, train_loss: 0.0781, step time: 0.5673\n",
      "5/388, train_loss: 0.1111, step time: 0.5161\n",
      "6/388, train_loss: 0.1016, step time: 0.4863\n",
      "7/388, train_loss: 0.3366, step time: 0.7443\n",
      "8/388, train_loss: 0.1241, step time: 0.6082\n",
      "9/388, train_loss: 0.0597, step time: 0.5665\n",
      "10/388, train_loss: 0.0634, step time: 0.5361\n",
      "11/388, train_loss: 0.1000, step time: 0.5075\n",
      "12/388, train_loss: 0.1077, step time: 0.4945\n",
      "13/388, train_loss: 0.1117, step time: 0.5536\n",
      "14/388, train_loss: 0.1906, step time: 0.5167\n",
      "15/388, train_loss: 0.2025, step time: 0.4866\n",
      "16/388, train_loss: 0.0962, step time: 0.7896\n",
      "17/388, train_loss: 0.2468, step time: 0.5490\n",
      "18/388, train_loss: 0.2383, step time: 0.5207\n",
      "19/388, train_loss: 0.0784, step time: 0.4994\n",
      "20/388, train_loss: 0.0494, step time: 1.1691\n",
      "21/388, train_loss: 0.1226, step time: 0.5506\n",
      "22/388, train_loss: 0.0737, step time: 0.5212\n",
      "23/388, train_loss: 0.2201, step time: 0.4990\n",
      "24/388, train_loss: 0.1754, step time: 0.5677\n",
      "25/388, train_loss: 0.3405, step time: 0.5454\n",
      "26/388, train_loss: 0.2153, step time: 0.5158\n",
      "27/388, train_loss: 0.1313, step time: 0.5353\n",
      "28/388, train_loss: 0.0729, step time: 0.5234\n",
      "29/388, train_loss: 0.1884, step time: 0.6104\n",
      "30/388, train_loss: 0.1288, step time: 0.5443\n",
      "31/388, train_loss: 0.1385, step time: 0.5310\n",
      "32/388, train_loss: 0.1667, step time: 0.5521\n",
      "33/388, train_loss: 0.2866, step time: 0.5293\n",
      "34/388, train_loss: 0.1136, step time: 0.5144\n",
      "35/388, train_loss: 0.1011, step time: 0.4918\n",
      "36/388, train_loss: 0.0811, step time: 0.4874\n",
      "37/388, train_loss: 0.1995, step time: 0.9950\n",
      "38/388, train_loss: 0.0820, step time: 0.5366\n",
      "39/388, train_loss: 0.0896, step time: 0.5332\n",
      "40/388, train_loss: 0.2644, step time: 0.5028\n",
      "41/388, train_loss: 0.3364, step time: 0.4953\n",
      "42/388, train_loss: 0.2656, step time: 1.0493\n",
      "43/388, train_loss: 0.1721, step time: 0.5430\n",
      "44/388, train_loss: 0.2033, step time: 0.5136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/388, train_loss: 0.0879, step time: 0.4954\n",
      "46/388, train_loss: 0.1099, step time: 0.4971\n",
      "47/388, train_loss: 0.2207, step time: 0.4918\n",
      "48/388, train_loss: 0.2782, step time: 0.4758\n",
      "49/388, train_loss: 0.1918, step time: 0.4810\n",
      "50/388, train_loss: 0.0874, step time: 0.4859\n",
      "51/388, train_loss: 0.0521, step time: 1.1179\n",
      "52/388, train_loss: 0.2007, step time: 0.5273\n",
      "53/388, train_loss: 0.0636, step time: 0.4995\n",
      "54/388, train_loss: 0.1278, step time: 0.4904\n",
      "55/388, train_loss: 0.1904, step time: 0.5007\n",
      "56/388, train_loss: 0.1746, step time: 0.4897\n",
      "57/388, train_loss: 0.1201, step time: 0.5242\n",
      "58/388, train_loss: 0.0919, step time: 0.5000\n",
      "59/388, train_loss: 0.3697, step time: 0.4911\n",
      "60/388, train_loss: 0.0900, step time: 0.4923\n",
      "61/388, train_loss: 0.2112, step time: 0.4815\n",
      "62/388, train_loss: 0.1659, step time: 1.0424\n",
      "63/388, train_loss: 0.0541, step time: 0.5406\n",
      "64/388, train_loss: 0.0615, step time: 0.5194\n",
      "65/388, train_loss: 0.0654, step time: 0.5005\n",
      "66/388, train_loss: 0.2551, step time: 0.4941\n",
      "67/388, train_loss: 0.2986, step time: 0.4995\n",
      "68/388, train_loss: 0.1006, step time: 1.1368\n",
      "69/388, train_loss: 0.2804, step time: 0.5398\n",
      "70/388, train_loss: 0.1222, step time: 0.5059\n",
      "71/388, train_loss: 0.0945, step time: 0.5002\n",
      "72/388, train_loss: 0.1502, step time: 0.4850\n",
      "73/388, train_loss: 0.1675, step time: 0.4856\n",
      "74/388, train_loss: 0.0832, step time: 0.4911\n",
      "75/388, train_loss: 0.1951, step time: 0.4793\n",
      "76/388, train_loss: 0.0982, step time: 0.5057\n",
      "77/388, train_loss: 0.1483, step time: 0.4971\n",
      "78/388, train_loss: 0.1018, step time: 0.4927\n",
      "79/388, train_loss: 0.0978, step time: 0.4840\n",
      "80/388, train_loss: 0.1267, step time: 0.5706\n",
      "81/388, train_loss: 0.3223, step time: 0.5447\n",
      "82/388, train_loss: 0.2631, step time: 0.5141\n",
      "83/388, train_loss: 0.1867, step time: 0.5117\n",
      "84/388, train_loss: 0.1444, step time: 0.4916\n",
      "85/388, train_loss: 0.1839, step time: 0.4928\n",
      "86/388, train_loss: 0.2837, step time: 0.5014\n",
      "87/388, train_loss: 0.3732, step time: 0.4832\n",
      "88/388, train_loss: 0.0836, step time: 1.0881\n",
      "89/388, train_loss: 0.4625, step time: 0.5460\n",
      "90/388, train_loss: 0.1527, step time: 0.5128\n",
      "91/388, train_loss: 0.5037, step time: 0.5034\n",
      "92/388, train_loss: 0.1098, step time: 0.4874\n",
      "93/388, train_loss: 0.1156, step time: 0.5226\n",
      "94/388, train_loss: 0.1161, step time: 0.5083\n",
      "95/388, train_loss: 0.3092, step time: 0.5070\n",
      "96/388, train_loss: 0.5884, step time: 0.4872\n",
      "97/388, train_loss: 0.1482, step time: 0.4801\n",
      "98/388, train_loss: 0.2043, step time: 0.7897\n",
      "99/388, train_loss: 0.0386, step time: 0.5416\n",
      "100/388, train_loss: 0.2071, step time: 0.5050\n",
      "101/388, train_loss: 0.0978, step time: 0.4938\n",
      "102/388, train_loss: 0.0974, step time: 0.4817\n",
      "103/388, train_loss: 0.1158, step time: 0.4916\n",
      "104/388, train_loss: 0.0976, step time: 0.4992\n",
      "105/388, train_loss: 0.1050, step time: 0.4913\n",
      "106/388, train_loss: 0.3368, step time: 1.1032\n",
      "107/388, train_loss: 0.2518, step time: 0.5334\n",
      "108/388, train_loss: 0.1961, step time: 0.5048\n",
      "109/388, train_loss: 0.1206, step time: 0.4925\n",
      "110/388, train_loss: 0.1511, step time: 0.4897\n",
      "111/388, train_loss: 0.2679, step time: 0.4927\n",
      "112/388, train_loss: 0.0989, step time: 0.4776\n",
      "113/388, train_loss: 0.1326, step time: 0.4841\n",
      "114/388, train_loss: 0.1263, step time: 0.9605\n",
      "115/388, train_loss: 0.0410, step time: 0.5327\n",
      "116/388, train_loss: 0.1554, step time: 0.4983\n",
      "117/388, train_loss: 0.2018, step time: 0.4856\n",
      "118/388, train_loss: 0.2249, step time: 0.4902\n",
      "119/388, train_loss: 0.1149, step time: 0.4768\n",
      "120/388, train_loss: 0.0687, step time: 0.4825\n",
      "121/388, train_loss: 0.1272, step time: 0.5245\n",
      "122/388, train_loss: 0.0883, step time: 0.5426\n",
      "123/388, train_loss: 0.1081, step time: 0.5260\n",
      "124/388, train_loss: 0.0895, step time: 0.5096\n",
      "125/388, train_loss: 0.1942, step time: 0.5029\n",
      "126/388, train_loss: 0.1064, step time: 0.5377\n",
      "127/388, train_loss: 0.2249, step time: 0.5183\n",
      "128/388, train_loss: 0.2318, step time: 0.5050\n",
      "129/388, train_loss: 0.2040, step time: 0.4902\n",
      "130/388, train_loss: 0.1949, step time: 1.0127\n",
      "131/388, train_loss: 0.1064, step time: 0.5441\n",
      "132/388, train_loss: 0.1455, step time: 0.5240\n",
      "133/388, train_loss: 0.1179, step time: 0.5077\n",
      "134/388, train_loss: 0.2074, step time: 0.4898\n",
      "135/388, train_loss: 0.3334, step time: 0.5204\n",
      "136/388, train_loss: 0.2373, step time: 0.5652\n",
      "137/388, train_loss: 0.2128, step time: 0.5172\n",
      "138/388, train_loss: 0.1108, step time: 0.5132\n",
      "139/388, train_loss: 0.1310, step time: 0.5033\n",
      "140/388, train_loss: 0.1958, step time: 0.5063\n",
      "141/388, train_loss: 0.1585, step time: 0.4826\n",
      "142/388, train_loss: 0.7178, step time: 0.4972\n",
      "143/388, train_loss: 0.1114, step time: 0.4978\n",
      "144/388, train_loss: 0.1426, step time: 0.5191\n",
      "145/388, train_loss: 0.0748, step time: 0.5160\n",
      "146/388, train_loss: 0.1904, step time: 0.5435\n",
      "147/388, train_loss: 0.1555, step time: 0.6614\n",
      "148/388, train_loss: 0.1155, step time: 0.5606\n",
      "149/388, train_loss: 0.2079, step time: 0.5164\n",
      "150/388, train_loss: 0.2596, step time: 0.4899\n",
      "151/388, train_loss: 0.0767, step time: 0.4850\n",
      "152/388, train_loss: 0.1871, step time: 0.5195\n",
      "153/388, train_loss: 0.1863, step time: 0.5104\n",
      "154/388, train_loss: 0.3755, step time: 1.0012\n",
      "155/388, train_loss: 0.1898, step time: 0.5494\n",
      "156/388, train_loss: 0.1092, step time: 0.5009\n",
      "157/388, train_loss: 0.1732, step time: 0.4961\n",
      "158/388, train_loss: 0.0855, step time: 0.4847\n",
      "159/388, train_loss: 0.5486, step time: 0.5655\n",
      "160/388, train_loss: 0.0613, step time: 0.5403\n",
      "161/388, train_loss: 0.5036, step time: 0.5369\n",
      "162/388, train_loss: 0.2191, step time: 0.5138\n",
      "163/388, train_loss: 0.1404, step time: 0.4980\n",
      "164/388, train_loss: 0.0994, step time: 0.4879\n",
      "165/388, train_loss: 0.1883, step time: 0.5259\n",
      "166/388, train_loss: 0.1907, step time: 0.5400\n",
      "167/388, train_loss: 0.1942, step time: 0.5808\n",
      "168/388, train_loss: 0.0356, step time: 0.5452\n",
      "169/388, train_loss: 0.3161, step time: 0.5196\n",
      "170/388, train_loss: 0.2005, step time: 0.5030\n",
      "171/388, train_loss: 0.3934, step time: 0.5485\n",
      "172/388, train_loss: 0.3337, step time: 0.5126\n",
      "173/388, train_loss: 0.1565, step time: 0.5165\n",
      "174/388, train_loss: 0.1809, step time: 0.4995\n",
      "175/388, train_loss: 0.0719, step time: 1.1173\n",
      "176/388, train_loss: 0.2143, step time: 0.5365\n",
      "177/388, train_loss: 0.2713, step time: 0.5205\n",
      "178/388, train_loss: 0.1413, step time: 0.4989\n",
      "179/388, train_loss: 0.1215, step time: 0.4959\n",
      "180/388, train_loss: 0.2329, step time: 0.4863\n",
      "181/388, train_loss: 0.0884, step time: 0.5118\n",
      "182/388, train_loss: 0.1961, step time: 0.4934\n",
      "183/388, train_loss: 0.1578, step time: 0.5655\n",
      "184/388, train_loss: 0.1579, step time: 0.6144\n",
      "185/388, train_loss: 0.1975, step time: 0.5499\n",
      "186/388, train_loss: 0.0921, step time: 0.5126\n",
      "187/388, train_loss: 0.1869, step time: 0.5155\n",
      "188/388, train_loss: 0.2380, step time: 0.4976\n",
      "189/388, train_loss: 0.0813, step time: 0.4879\n",
      "190/388, train_loss: 0.2048, step time: 0.4886\n",
      "191/388, train_loss: 0.1938, step time: 0.5169\n",
      "192/388, train_loss: 0.0919, step time: 0.5080\n",
      "193/388, train_loss: 0.2290, step time: 0.5304\n",
      "194/388, train_loss: 0.0495, step time: 0.5535\n",
      "195/388, train_loss: 0.2273, step time: 0.6591\n",
      "196/388, train_loss: 0.0897, step time: 0.5381\n",
      "197/388, train_loss: 0.1290, step time: 0.5122\n",
      "198/388, train_loss: 0.2461, step time: 0.4935\n",
      "199/388, train_loss: 0.1486, step time: 0.4959\n",
      "200/388, train_loss: 0.0630, step time: 0.5282\n",
      "201/388, train_loss: 0.2974, step time: 0.5091\n",
      "202/388, train_loss: 0.1016, step time: 0.4942\n",
      "203/388, train_loss: 0.1287, step time: 0.5029\n",
      "204/388, train_loss: 0.2979, step time: 0.4800\n",
      "205/388, train_loss: 0.2282, step time: 0.5172\n",
      "206/388, train_loss: 0.2423, step time: 0.4961\n",
      "207/388, train_loss: 0.0452, step time: 0.4868\n",
      "208/388, train_loss: 0.2807, step time: 0.4984\n",
      "209/388, train_loss: 0.2671, step time: 0.7200\n",
      "210/388, train_loss: 0.1086, step time: 0.5619\n",
      "211/388, train_loss: 0.3290, step time: 0.5386\n",
      "212/388, train_loss: 0.0865, step time: 0.4992\n",
      "213/388, train_loss: 0.2126, step time: 0.5049\n",
      "214/388, train_loss: 0.1083, step time: 0.4960\n",
      "215/388, train_loss: 0.1229, step time: 0.5030\n",
      "216/388, train_loss: 0.0903, step time: 0.4892\n",
      "217/388, train_loss: 0.0581, step time: 1.2677\n",
      "218/388, train_loss: 0.5819, step time: 0.5404\n",
      "219/388, train_loss: 0.0994, step time: 0.5060\n",
      "220/388, train_loss: 0.0913, step time: 0.4980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/388, train_loss: 0.1856, step time: 0.5019\n",
      "222/388, train_loss: 0.2722, step time: 0.5483\n",
      "223/388, train_loss: 0.0971, step time: 0.5129\n",
      "224/388, train_loss: 0.1386, step time: 0.5045\n",
      "225/388, train_loss: 0.3883, step time: 0.5005\n",
      "226/388, train_loss: 0.1246, step time: 0.4803\n",
      "227/388, train_loss: 0.1128, step time: 0.9563\n",
      "228/388, train_loss: 0.0820, step time: 0.5507\n",
      "229/388, train_loss: 0.1267, step time: 0.5360\n",
      "230/388, train_loss: 0.1329, step time: 0.5070\n",
      "231/388, train_loss: 0.1750, step time: 0.4988\n",
      "232/388, train_loss: 0.1415, step time: 0.4950\n",
      "233/388, train_loss: 0.1724, step time: 1.1096\n",
      "234/388, train_loss: 0.1066, step time: 0.5544\n",
      "235/388, train_loss: 0.1739, step time: 0.5258\n",
      "236/388, train_loss: 0.1589, step time: 0.5051\n",
      "237/388, train_loss: 0.0984, step time: 0.5365\n",
      "238/388, train_loss: 0.1630, step time: 0.5053\n",
      "239/388, train_loss: 0.0777, step time: 0.4860\n",
      "240/388, train_loss: 0.3823, step time: 0.5121\n",
      "241/388, train_loss: 0.0954, step time: 0.5081\n",
      "242/388, train_loss: 0.0682, step time: 0.4984\n",
      "243/388, train_loss: 0.1863, step time: 0.5048\n",
      "244/388, train_loss: 0.2100, step time: 0.5514\n",
      "245/388, train_loss: 0.1183, step time: 0.5385\n",
      "246/388, train_loss: 0.1071, step time: 0.5287\n",
      "247/388, train_loss: 0.2404, step time: 0.5012\n",
      "248/388, train_loss: 0.4946, step time: 1.1749\n",
      "249/388, train_loss: 0.2280, step time: 0.5355\n",
      "250/388, train_loss: 0.0771, step time: 0.5124\n",
      "251/388, train_loss: 0.1416, step time: 0.4917\n",
      "252/388, train_loss: 0.1483, step time: 0.4940\n",
      "253/388, train_loss: 0.0632, step time: -2.0748\n",
      "254/388, train_loss: 0.2011, step time: 0.5389\n",
      "255/388, train_loss: 0.1464, step time: 0.5113\n",
      "256/388, train_loss: 0.0814, step time: 0.4998\n",
      "257/388, train_loss: 0.1133, step time: 0.4856\n",
      "258/388, train_loss: 0.1649, step time: 0.4968\n",
      "259/388, train_loss: 0.2081, step time: 0.5074\n",
      "260/388, train_loss: 0.1724, step time: 0.5058\n",
      "261/388, train_loss: 0.3262, step time: 0.4823\n",
      "262/388, train_loss: 0.1238, step time: 1.0302\n",
      "263/388, train_loss: 0.2614, step time: 0.5491\n",
      "264/388, train_loss: 0.0992, step time: 0.5216\n",
      "265/388, train_loss: 0.2040, step time: 0.5064\n",
      "266/388, train_loss: 0.3114, step time: 0.4945\n",
      "267/388, train_loss: 0.1589, step time: 0.4943\n",
      "268/388, train_loss: 0.0958, step time: 0.4940\n",
      "269/388, train_loss: 0.1527, step time: 0.5665\n",
      "270/388, train_loss: 0.1720, step time: 0.5434\n",
      "271/388, train_loss: 0.4216, step time: 0.5316\n",
      "272/388, train_loss: 0.2706, step time: 0.5108\n",
      "273/388, train_loss: 0.1977, step time: 0.4985\n",
      "274/388, train_loss: 0.2575, step time: 0.4992\n",
      "275/388, train_loss: 0.0709, step time: 0.4815\n",
      "276/388, train_loss: 0.0997, step time: 0.5061\n",
      "277/388, train_loss: 0.0678, step time: 0.5031\n",
      "278/388, train_loss: 0.2542, step time: 0.4915\n",
      "279/388, train_loss: 0.2568, step time: 0.5323\n",
      "280/388, train_loss: 0.2008, step time: 0.5034\n",
      "281/388, train_loss: 0.4363, step time: 0.5008\n",
      "282/388, train_loss: 0.0954, step time: 0.5106\n",
      "283/388, train_loss: 0.2285, step time: 0.5037\n",
      "284/388, train_loss: 0.2165, step time: 0.6413\n",
      "285/388, train_loss: 0.1427, step time: 0.5414\n",
      "286/388, train_loss: 0.1415, step time: 0.5255\n",
      "287/388, train_loss: 0.1177, step time: 0.5993\n",
      "288/388, train_loss: 0.1241, step time: 0.5472\n",
      "289/388, train_loss: 0.1705, step time: 0.5154\n",
      "290/388, train_loss: 0.1185, step time: 0.5141\n",
      "291/388, train_loss: 0.1941, step time: 0.6216\n",
      "292/388, train_loss: 0.0672, step time: 0.5468\n",
      "293/388, train_loss: 0.3338, step time: 0.5294\n",
      "294/388, train_loss: 0.1845, step time: 0.5037\n",
      "295/388, train_loss: 0.2004, step time: 0.5031\n",
      "296/388, train_loss: 0.3757, step time: 0.5717\n",
      "297/388, train_loss: 0.1532, step time: 0.5706\n",
      "298/388, train_loss: 0.1913, step time: 0.5380\n",
      "299/388, train_loss: 0.3728, step time: 0.5129\n",
      "300/388, train_loss: 0.1135, step time: 0.5631\n",
      "301/388, train_loss: 0.1029, step time: 0.5232\n",
      "302/388, train_loss: 0.0701, step time: 0.5093\n",
      "303/388, train_loss: 0.0860, step time: 0.4943\n",
      "304/388, train_loss: 0.1508, step time: 0.5012\n",
      "305/388, train_loss: 0.1201, step time: 0.4932\n",
      "306/388, train_loss: 0.3004, step time: 0.5030\n",
      "307/388, train_loss: 0.1217, step time: 0.5067\n",
      "308/388, train_loss: 0.0814, step time: 0.9974\n",
      "309/388, train_loss: 0.1032, step time: 0.5353\n",
      "310/388, train_loss: 0.1456, step time: 0.5187\n",
      "311/388, train_loss: 0.1880, step time: 0.5065\n",
      "312/388, train_loss: 0.0508, step time: 0.4930\n",
      "313/388, train_loss: 0.2446, step time: 0.5179\n",
      "314/388, train_loss: 0.1083, step time: 0.5144\n",
      "315/388, train_loss: 0.1465, step time: 0.4993\n",
      "316/388, train_loss: 0.4894, step time: 0.5311\n",
      "317/388, train_loss: 0.2803, step time: 0.5361\n",
      "318/388, train_loss: 0.2705, step time: 0.5139\n",
      "319/388, train_loss: 0.2610, step time: 0.5167\n",
      "320/388, train_loss: 0.0691, step time: 0.5184\n",
      "321/388, train_loss: 0.1532, step time: 0.5662\n",
      "322/388, train_loss: 0.1113, step time: 0.5361\n",
      "323/388, train_loss: 0.0904, step time: 0.5164\n",
      "324/388, train_loss: 0.0973, step time: 0.5169\n",
      "325/388, train_loss: 0.2471, step time: 0.6345\n",
      "326/388, train_loss: 0.3433, step time: 0.5852\n",
      "327/388, train_loss: 0.1103, step time: 0.5183\n",
      "328/388, train_loss: 0.2656, step time: 0.4964\n",
      "329/388, train_loss: 0.1051, step time: 1.0174\n",
      "330/388, train_loss: 0.0766, step time: 0.5480\n",
      "331/388, train_loss: 0.1192, step time: 0.5316\n",
      "332/388, train_loss: 0.1122, step time: 0.5026\n",
      "333/388, train_loss: 0.3749, step time: 0.4943\n",
      "334/388, train_loss: 0.3349, step time: 0.4949\n",
      "335/388, train_loss: 0.1409, step time: 0.4888\n",
      "336/388, train_loss: 0.1130, step time: 0.5009\n",
      "337/388, train_loss: 0.2209, step time: 0.5162\n",
      "338/388, train_loss: 0.0299, step time: 0.5172\n",
      "339/388, train_loss: 0.1684, step time: 0.4986\n",
      "340/388, train_loss: 0.0480, step time: 0.5007\n",
      "341/388, train_loss: 0.1458, step time: 0.9615\n",
      "342/388, train_loss: 0.4681, step time: 0.5406\n",
      "343/388, train_loss: 0.0898, step time: 0.5159\n",
      "344/388, train_loss: 0.1639, step time: 0.5791\n",
      "345/388, train_loss: 0.2167, step time: 0.5412\n",
      "346/388, train_loss: 0.2442, step time: 0.5086\n",
      "347/388, train_loss: 0.0576, step time: 0.5054\n",
      "348/388, train_loss: 0.0800, step time: 0.4861\n",
      "349/388, train_loss: 0.1442, step time: 0.4999\n",
      "350/388, train_loss: 0.2503, step time: 0.5020\n",
      "351/388, train_loss: 0.4611, step time: 0.4982\n",
      "352/388, train_loss: 0.4358, step time: 1.1729\n",
      "353/388, train_loss: 0.0378, step time: 0.5414\n",
      "354/388, train_loss: 0.1201, step time: 0.5068\n",
      "355/388, train_loss: 0.0763, step time: 0.4832\n",
      "356/388, train_loss: 0.0581, step time: 0.4957\n",
      "357/388, train_loss: 0.2580, step time: 0.4788\n",
      "358/388, train_loss: 0.5320, step time: 0.4822\n",
      "359/388, train_loss: 0.2152, step time: 0.7441\n",
      "360/388, train_loss: 0.3413, step time: 0.5593\n",
      "361/388, train_loss: 0.1886, step time: 0.5101\n",
      "362/388, train_loss: 0.2413, step time: 0.4844\n",
      "363/388, train_loss: 0.1084, step time: 0.4989\n",
      "364/388, train_loss: 0.2196, step time: 1.1626\n",
      "365/388, train_loss: 0.3338, step time: 0.5405\n",
      "366/388, train_loss: 0.1183, step time: 0.5098\n",
      "367/388, train_loss: 0.0758, step time: 0.4929\n",
      "368/388, train_loss: 0.1874, step time: 0.4988\n",
      "369/388, train_loss: 0.1459, step time: 0.4825\n",
      "370/388, train_loss: 0.1768, step time: 0.5443\n",
      "371/388, train_loss: 0.1632, step time: 0.5194\n",
      "372/388, train_loss: 0.1722, step time: 0.4990\n",
      "373/388, train_loss: 0.1576, step time: 0.4900\n",
      "374/388, train_loss: 0.1947, step time: 0.4982\n",
      "375/388, train_loss: 0.1006, step time: 0.4949\n",
      "376/388, train_loss: 0.1220, step time: 0.5261\n",
      "377/388, train_loss: 0.1924, step time: 0.5056\n",
      "378/388, train_loss: 0.0606, step time: 0.4901\n",
      "379/388, train_loss: 0.1239, step time: 1.0185\n",
      "380/388, train_loss: 0.1425, step time: 0.5329\n",
      "381/388, train_loss: 0.6017, step time: 0.5102\n",
      "382/388, train_loss: 0.1311, step time: 0.4926\n",
      "383/388, train_loss: 0.2544, step time: 0.4835\n",
      "384/388, train_loss: 0.1379, step time: 0.4785\n",
      "385/388, train_loss: 0.0930, step time: 0.4731\n",
      "386/388, train_loss: 0.2609, step time: 0.4807\n",
      "387/388, train_loss: 0.2660, step time: 0.5539\n",
      "388/388, train_loss: 0.1263, step time: 0.4954\n",
      "epoch 82 average loss: 0.1780\n",
      "current epoch: 82 current mean dice: 0.7702 tc: 0.8150 wt: 0.9066 et: 0.5889\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 82 is: 299.3414\n",
      "----------\n",
      "epoch 83/300\n",
      "1/388, train_loss: 0.0983, step time: 0.4688\n",
      "2/388, train_loss: 0.1676, step time: 0.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/388, train_loss: 0.0475, step time: 0.9571\n",
      "4/388, train_loss: 0.0819, step time: 0.5352\n",
      "5/388, train_loss: 0.1875, step time: 0.5184\n",
      "6/388, train_loss: 0.1384, step time: 0.6057\n",
      "7/388, train_loss: 0.1368, step time: 0.5479\n",
      "8/388, train_loss: 0.3027, step time: 0.5277\n",
      "9/388, train_loss: 0.1976, step time: 0.5083\n",
      "10/388, train_loss: 0.0983, step time: 0.9400\n",
      "11/388, train_loss: 0.0679, step time: 0.5395\n",
      "12/388, train_loss: 0.1767, step time: 0.5058\n",
      "13/388, train_loss: 0.0932, step time: 0.4857\n",
      "14/388, train_loss: 0.1487, step time: 0.5031\n",
      "15/388, train_loss: 0.3914, step time: 0.5151\n",
      "16/388, train_loss: 0.2212, step time: 0.4993\n",
      "17/388, train_loss: 0.1654, step time: 0.4869\n",
      "18/388, train_loss: 0.3890, step time: 0.4847\n",
      "19/388, train_loss: 0.2121, step time: 0.7027\n",
      "20/388, train_loss: 0.3824, step time: 0.5927\n",
      "21/388, train_loss: 0.2021, step time: 0.5259\n",
      "22/388, train_loss: 0.1046, step time: 0.4944\n",
      "23/388, train_loss: 0.1182, step time: 0.5365\n",
      "24/388, train_loss: 0.1899, step time: 0.6176\n",
      "25/388, train_loss: 0.2014, step time: 0.5252\n",
      "26/388, train_loss: 0.1747, step time: 0.5026\n",
      "27/388, train_loss: 0.0472, step time: 0.5502\n",
      "28/388, train_loss: 0.1220, step time: 0.5403\n",
      "29/388, train_loss: 0.2187, step time: 0.5217\n",
      "30/388, train_loss: 0.2036, step time: 0.5033\n",
      "31/388, train_loss: 0.0820, step time: 0.4954\n",
      "32/388, train_loss: 0.3346, step time: 0.5728\n",
      "33/388, train_loss: 0.0982, step time: 0.5177\n",
      "34/388, train_loss: 0.0900, step time: 0.4963\n",
      "35/388, train_loss: 0.1878, step time: 0.4958\n",
      "36/388, train_loss: 0.1099, step time: 0.4937\n",
      "37/388, train_loss: 0.3257, step time: 0.5052\n",
      "38/388, train_loss: 0.1794, step time: 0.4932\n",
      "39/388, train_loss: 0.1004, step time: 1.0527\n",
      "40/388, train_loss: 0.2624, step time: 0.5688\n",
      "41/388, train_loss: 0.2262, step time: 0.5273\n",
      "42/388, train_loss: 0.1971, step time: 0.5064\n",
      "43/388, train_loss: 0.1345, step time: 0.4983\n",
      "44/388, train_loss: 0.0955, step time: 0.4884\n",
      "45/388, train_loss: 0.1632, step time: 0.4931\n",
      "46/388, train_loss: 0.3954, step time: 0.5162\n",
      "47/388, train_loss: 0.1750, step time: 0.5036\n",
      "48/388, train_loss: 0.4220, step time: 0.5355\n",
      "49/388, train_loss: 0.1668, step time: 0.5231\n",
      "50/388, train_loss: 0.1423, step time: 0.5844\n",
      "51/388, train_loss: 0.1111, step time: 0.5466\n",
      "52/388, train_loss: 0.2420, step time: 0.5039\n",
      "53/388, train_loss: 0.2073, step time: 0.4947\n",
      "54/388, train_loss: 0.1290, step time: 0.4774\n",
      "55/388, train_loss: 0.1526, step time: 0.4776\n",
      "56/388, train_loss: 0.0718, step time: 1.0727\n",
      "57/388, train_loss: 0.2634, step time: 0.5559\n",
      "58/388, train_loss: 0.1515, step time: 0.5134\n",
      "59/388, train_loss: 0.1437, step time: 0.5014\n",
      "60/388, train_loss: 0.1779, step time: 0.4962\n",
      "61/388, train_loss: 0.2747, step time: 0.5362\n",
      "62/388, train_loss: 0.1445, step time: 0.5137\n",
      "63/388, train_loss: 0.0662, step time: 0.4983\n",
      "64/388, train_loss: 0.1642, step time: 0.4871\n",
      "65/388, train_loss: 0.0616, step time: 0.4883\n",
      "66/388, train_loss: 0.2498, step time: 0.4751\n",
      "67/388, train_loss: 0.1802, step time: 0.4785\n",
      "68/388, train_loss: 0.1204, step time: 0.4825\n",
      "69/388, train_loss: 0.0663, step time: 1.0081\n",
      "70/388, train_loss: 0.2560, step time: 0.5377\n",
      "71/388, train_loss: 0.1791, step time: 0.5029\n",
      "72/388, train_loss: 0.1685, step time: 0.4902\n",
      "73/388, train_loss: 0.1456, step time: 0.4970\n",
      "74/388, train_loss: 0.2684, step time: 0.4967\n",
      "75/388, train_loss: 0.2010, step time: 0.4808\n",
      "76/388, train_loss: 0.0936, step time: 0.4732\n",
      "77/388, train_loss: 0.0436, step time: 0.5227\n",
      "78/388, train_loss: 0.1628, step time: 0.5297\n",
      "79/388, train_loss: 0.1361, step time: 0.4958\n",
      "80/388, train_loss: 0.1558, step time: 0.4977\n",
      "81/388, train_loss: 0.1199, step time: 0.4831\n",
      "82/388, train_loss: 0.1704, step time: 0.7885\n",
      "83/388, train_loss: 0.3533, step time: 0.5613\n",
      "84/388, train_loss: 0.1053, step time: 0.5185\n",
      "85/388, train_loss: 0.2737, step time: 0.4962\n",
      "86/388, train_loss: 0.0648, step time: 0.4946\n",
      "87/388, train_loss: 0.1254, step time: 0.5015\n",
      "88/388, train_loss: 0.1179, step time: 0.4947\n",
      "89/388, train_loss: 0.4421, step time: 0.4996\n",
      "90/388, train_loss: 0.0369, step time: 0.4994\n",
      "91/388, train_loss: 0.0630, step time: 0.4841\n",
      "92/388, train_loss: 0.0673, step time: 0.5234\n",
      "93/388, train_loss: 0.0878, step time: 0.5173\n",
      "94/388, train_loss: 0.0997, step time: 0.4996\n",
      "95/388, train_loss: 0.1223, step time: 0.5388\n",
      "96/388, train_loss: 0.0879, step time: 0.5175\n",
      "97/388, train_loss: 0.1444, step time: 0.5074\n",
      "98/388, train_loss: 0.1275, step time: 0.4871\n",
      "99/388, train_loss: 0.2867, step time: 0.5062\n",
      "100/388, train_loss: 0.1656, step time: 0.6135\n",
      "101/388, train_loss: 0.3256, step time: 0.5473\n",
      "102/388, train_loss: 0.0922, step time: 0.5180\n",
      "103/388, train_loss: 0.2108, step time: 0.5114\n",
      "104/388, train_loss: 0.1250, step time: 0.4955\n",
      "105/388, train_loss: 0.2341, step time: 0.4945\n",
      "106/388, train_loss: 0.3940, step time: 0.5053\n",
      "107/388, train_loss: 0.1952, step time: 0.4897\n",
      "108/388, train_loss: 0.1699, step time: 0.4888\n",
      "109/388, train_loss: 0.1855, step time: 0.4811\n",
      "110/388, train_loss: 0.0704, step time: 1.0754\n",
      "111/388, train_loss: 0.2747, step time: 0.5365\n",
      "112/388, train_loss: 0.2122, step time: 0.5092\n",
      "113/388, train_loss: 0.1561, step time: 0.4872\n",
      "114/388, train_loss: 0.0676, step time: 0.4914\n",
      "115/388, train_loss: 0.0936, step time: 0.4984\n",
      "116/388, train_loss: 0.1215, step time: 0.4899\n",
      "117/388, train_loss: 0.2103, step time: 0.4821\n",
      "118/388, train_loss: 0.1477, step time: 0.4800\n",
      "119/388, train_loss: 0.1387, step time: 0.5275\n",
      "120/388, train_loss: 0.5817, step time: 0.5019\n",
      "121/388, train_loss: 0.2532, step time: 0.5071\n",
      "122/388, train_loss: 0.1479, step time: 0.5054\n",
      "123/388, train_loss: 0.1278, step time: 0.5660\n",
      "124/388, train_loss: 0.1120, step time: 0.5334\n",
      "125/388, train_loss: 0.1467, step time: 0.5057\n",
      "126/388, train_loss: 0.1272, step time: 0.5035\n",
      "127/388, train_loss: 0.2171, step time: 0.4905\n",
      "128/388, train_loss: 0.0934, step time: 0.4950\n",
      "129/388, train_loss: 0.2557, step time: 1.1369\n",
      "130/388, train_loss: 0.3805, step time: 0.5269\n",
      "131/388, train_loss: 0.0403, step time: 0.5059\n",
      "132/388, train_loss: 0.3746, step time: 0.4896\n",
      "133/388, train_loss: 0.3048, step time: 0.4995\n",
      "134/388, train_loss: 0.0842, step time: 0.4778\n",
      "135/388, train_loss: 0.3592, step time: 0.6941\n",
      "136/388, train_loss: 0.0857, step time: 0.5459\n",
      "137/388, train_loss: 0.2747, step time: 0.5236\n",
      "138/388, train_loss: 0.1612, step time: 0.4896\n",
      "139/388, train_loss: 0.1400, step time: 0.4938\n",
      "140/388, train_loss: 0.1993, step time: 0.4811\n",
      "141/388, train_loss: 0.2996, step time: 0.5017\n",
      "142/388, train_loss: 0.4667, step time: 0.4861\n",
      "143/388, train_loss: 0.1622, step time: 1.0930\n",
      "144/388, train_loss: 0.1760, step time: 0.5294\n",
      "145/388, train_loss: 0.2146, step time: 0.4988\n",
      "146/388, train_loss: 0.2291, step time: 0.4965\n",
      "147/388, train_loss: 0.2681, step time: 0.4856\n",
      "148/388, train_loss: 0.1554, step time: 0.4937\n",
      "149/388, train_loss: 0.1294, step time: 0.4910\n",
      "150/388, train_loss: 0.2423, step time: 0.4760\n",
      "151/388, train_loss: 0.2138, step time: 0.4909\n",
      "152/388, train_loss: 0.3823, step time: 1.1333\n",
      "153/388, train_loss: 0.1924, step time: 0.5387\n",
      "154/388, train_loss: 0.1679, step time: 0.5204\n",
      "155/388, train_loss: 0.2678, step time: 0.4982\n",
      "156/388, train_loss: 0.0900, step time: 0.4967\n",
      "157/388, train_loss: 0.2987, step time: 0.4834\n",
      "158/388, train_loss: 0.2354, step time: 0.4900\n",
      "159/388, train_loss: 0.2881, step time: 0.4746\n",
      "160/388, train_loss: 0.1882, step time: 0.4994\n",
      "161/388, train_loss: 0.1236, step time: 0.4824\n",
      "162/388, train_loss: 0.2785, step time: 0.7322\n",
      "163/388, train_loss: 0.1838, step time: 0.5527\n",
      "164/388, train_loss: 0.1668, step time: 0.5135\n",
      "165/388, train_loss: 0.0483, step time: 0.5090\n",
      "166/388, train_loss: 0.1770, step time: 0.4931\n",
      "167/388, train_loss: 0.1831, step time: 0.5089\n",
      "168/388, train_loss: 0.0989, step time: 0.4902\n",
      "169/388, train_loss: 0.5294, step time: 0.4843\n",
      "170/388, train_loss: 0.1540, step time: 0.5268\n",
      "171/388, train_loss: 0.0564, step time: 0.5068\n",
      "172/388, train_loss: 0.1992, step time: 0.4955\n",
      "173/388, train_loss: 0.0284, step time: 1.1458\n",
      "174/388, train_loss: 0.2629, step time: 0.5186\n",
      "175/388, train_loss: 0.1263, step time: 0.4991\n",
      "176/388, train_loss: 0.0699, step time: 0.4843\n",
      "177/388, train_loss: 0.1006, step time: 0.4934\n",
      "178/388, train_loss: 0.1236, step time: 0.5227\n",
      "179/388, train_loss: 0.2491, step time: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/388, train_loss: 0.2192, step time: 0.4972\n",
      "181/388, train_loss: 0.0590, step time: 0.4949\n",
      "182/388, train_loss: 0.0776, step time: 0.4810\n",
      "183/388, train_loss: 0.0785, step time: 0.4751\n",
      "184/388, train_loss: 0.0901, step time: 0.4826\n",
      "185/388, train_loss: 0.1015, step time: 1.0566\n",
      "186/388, train_loss: 0.0943, step time: 0.5364\n",
      "187/388, train_loss: 0.4222, step time: 0.5183\n",
      "188/388, train_loss: 0.1448, step time: 0.4989\n",
      "189/388, train_loss: 0.0608, step time: 0.4927\n",
      "190/388, train_loss: 0.1193, step time: 0.4850\n",
      "191/388, train_loss: 0.0853, step time: 0.4772\n",
      "192/388, train_loss: 0.1736, step time: 0.4753\n",
      "193/388, train_loss: 0.2209, step time: 0.4990\n",
      "194/388, train_loss: 0.0438, step time: 0.4819\n",
      "195/388, train_loss: 0.0815, step time: 0.4741\n",
      "196/388, train_loss: 0.2013, step time: 0.4948\n",
      "197/388, train_loss: 0.1524, step time: 0.4822\n",
      "198/388, train_loss: 0.1976, step time: 0.4999\n",
      "199/388, train_loss: 0.0614, step time: 0.5469\n",
      "200/388, train_loss: 0.2084, step time: 0.5418\n",
      "201/388, train_loss: 0.0735, step time: 0.5238\n",
      "202/388, train_loss: 0.0668, step time: 0.5032\n",
      "203/388, train_loss: 0.4526, step time: 0.5294\n",
      "204/388, train_loss: 0.4453, step time: 0.4982\n",
      "205/388, train_loss: 0.2223, step time: 1.0325\n",
      "206/388, train_loss: 0.0893, step time: 0.5493\n",
      "207/388, train_loss: 0.3034, step time: 0.5265\n",
      "208/388, train_loss: 0.0970, step time: 0.5113\n",
      "209/388, train_loss: 0.1897, step time: 0.4910\n",
      "210/388, train_loss: 0.1207, step time: 0.4846\n",
      "211/388, train_loss: 0.3336, step time: 0.5003\n",
      "212/388, train_loss: 0.1341, step time: 1.1709\n",
      "213/388, train_loss: 0.1476, step time: 0.5288\n",
      "214/388, train_loss: 0.0970, step time: 0.4995\n",
      "215/388, train_loss: 0.0827, step time: 0.4975\n",
      "216/388, train_loss: 0.0760, step time: 0.4854\n",
      "217/388, train_loss: 0.2025, step time: 0.4773\n",
      "218/388, train_loss: 0.2258, step time: 0.4826\n",
      "219/388, train_loss: 0.3654, step time: 0.4853\n",
      "220/388, train_loss: 0.0783, step time: 0.4953\n",
      "221/388, train_loss: 0.1971, step time: 0.4848\n",
      "222/388, train_loss: 0.1491, step time: 1.1529\n",
      "223/388, train_loss: 0.0828, step time: 0.5273\n",
      "224/388, train_loss: 0.1602, step time: 0.4958\n",
      "225/388, train_loss: 0.1435, step time: 0.4813\n",
      "226/388, train_loss: 0.2030, step time: 0.4884\n",
      "227/388, train_loss: 0.0950, step time: 0.4795\n",
      "228/388, train_loss: 0.1015, step time: 0.4863\n",
      "229/388, train_loss: 0.3862, step time: 0.4902\n",
      "230/388, train_loss: 0.0889, step time: 0.4935\n",
      "231/388, train_loss: 0.1164, step time: 0.4853\n",
      "232/388, train_loss: 0.0968, step time: 0.4972\n",
      "233/388, train_loss: 0.1503, step time: 0.5034\n",
      "234/388, train_loss: 0.0528, step time: 0.5616\n",
      "235/388, train_loss: 0.1060, step time: 0.5346\n",
      "236/388, train_loss: 0.0845, step time: 0.5293\n",
      "237/388, train_loss: 0.0768, step time: 0.5094\n",
      "238/388, train_loss: 0.2351, step time: 0.5819\n",
      "239/388, train_loss: 0.0672, step time: 0.5548\n",
      "240/388, train_loss: 0.2103, step time: 0.5144\n",
      "241/388, train_loss: 0.1077, step time: 0.5082\n",
      "242/388, train_loss: 0.2022, step time: 0.4912\n",
      "243/388, train_loss: 0.1609, step time: 0.4884\n",
      "244/388, train_loss: 0.1018, step time: 0.4901\n",
      "245/388, train_loss: 0.3073, step time: 0.4805\n",
      "246/388, train_loss: 0.0730, step time: 0.9757\n",
      "247/388, train_loss: 0.2618, step time: 0.5327\n",
      "248/388, train_loss: 0.1079, step time: 0.5091\n",
      "249/388, train_loss: 0.0962, step time: 0.4929\n",
      "250/388, train_loss: 0.0869, step time: 0.4895\n",
      "251/388, train_loss: 0.0716, step time: 0.4930\n",
      "252/388, train_loss: 0.0827, step time: 0.5014\n",
      "253/388, train_loss: 0.0345, step time: 0.4994\n",
      "254/388, train_loss: 0.0816, step time: 0.4954\n",
      "255/388, train_loss: 0.1022, step time: 0.5100\n",
      "256/388, train_loss: 0.1953, step time: 0.5048\n",
      "257/388, train_loss: 0.1037, step time: 0.4860\n",
      "258/388, train_loss: 0.1804, step time: 0.4860\n",
      "259/388, train_loss: 0.0729, step time: 0.6506\n",
      "260/388, train_loss: 0.1831, step time: 0.5461\n",
      "261/388, train_loss: 0.1435, step time: 0.5113\n",
      "262/388, train_loss: 0.0908, step time: 0.4938\n",
      "263/388, train_loss: 0.0685, step time: 0.4963\n",
      "264/388, train_loss: 0.1503, step time: 0.4945\n",
      "265/388, train_loss: 0.2263, step time: 0.5004\n",
      "266/388, train_loss: 0.1004, step time: 1.0668\n",
      "267/388, train_loss: 0.2028, step time: 0.5359\n",
      "268/388, train_loss: 0.2447, step time: 0.5142\n",
      "269/388, train_loss: 0.0976, step time: 0.4945\n",
      "270/388, train_loss: 0.0830, step time: 0.4851\n",
      "271/388, train_loss: 0.2027, step time: 0.4958\n",
      "272/388, train_loss: 0.2620, step time: 0.5189\n",
      "273/388, train_loss: 0.1523, step time: 0.5736\n",
      "274/388, train_loss: 0.1764, step time: 0.5312\n",
      "275/388, train_loss: 0.1249, step time: 0.5165\n",
      "276/388, train_loss: 0.2749, step time: 0.5128\n",
      "277/388, train_loss: 0.1761, step time: 0.4970\n",
      "278/388, train_loss: 0.2569, step time: 1.1672\n",
      "279/388, train_loss: 0.0712, step time: 0.5470\n",
      "280/388, train_loss: 0.1401, step time: 0.5131\n",
      "281/388, train_loss: 0.2600, step time: 0.5010\n",
      "282/388, train_loss: 0.2289, step time: 0.4990\n",
      "283/388, train_loss: 0.1061, step time: 0.4977\n",
      "284/388, train_loss: 0.0484, step time: 0.4994\n",
      "285/388, train_loss: 0.0437, step time: 0.4874\n",
      "286/388, train_loss: 0.1231, step time: 0.4880\n",
      "287/388, train_loss: 0.2091, step time: 0.4823\n",
      "288/388, train_loss: 0.2244, step time: 0.4817\n",
      "289/388, train_loss: 0.3007, step time: 0.4942\n",
      "290/388, train_loss: 0.1338, step time: 0.5051\n",
      "291/388, train_loss: 0.4668, step time: 0.5044\n",
      "292/388, train_loss: 0.1364, step time: 0.4876\n",
      "293/388, train_loss: 0.4024, step time: 1.2158\n",
      "294/388, train_loss: 0.2367, step time: 0.5294\n",
      "295/388, train_loss: 0.1409, step time: 0.5035\n",
      "296/388, train_loss: 0.2390, step time: 0.4884\n",
      "297/388, train_loss: 0.1232, step time: 0.4934\n",
      "298/388, train_loss: 0.0915, step time: 0.4791\n",
      "299/388, train_loss: 0.1545, step time: 0.8460\n",
      "300/388, train_loss: 0.0524, step time: 0.5404\n",
      "301/388, train_loss: 0.2079, step time: 0.5083\n",
      "302/388, train_loss: 0.0919, step time: 0.4923\n",
      "303/388, train_loss: 0.1066, step time: 0.5506\n",
      "304/388, train_loss: 0.0854, step time: 0.5386\n",
      "305/388, train_loss: 0.1342, step time: 0.5122\n",
      "306/388, train_loss: 0.3344, step time: 0.5178\n",
      "307/388, train_loss: 0.1416, step time: 0.5149\n",
      "308/388, train_loss: 0.1823, step time: 0.5009\n",
      "309/388, train_loss: 0.2721, step time: 0.4867\n",
      "310/388, train_loss: 0.1551, step time: 0.5055\n",
      "311/388, train_loss: 0.2812, step time: 0.4935\n",
      "312/388, train_loss: 0.1615, step time: 0.4926\n",
      "313/388, train_loss: 0.1083, step time: 0.4820\n",
      "314/388, train_loss: 0.1259, step time: 0.4925\n",
      "315/388, train_loss: 0.3076, step time: 0.5358\n",
      "316/388, train_loss: 0.1956, step time: 0.6685\n",
      "317/388, train_loss: 0.1535, step time: 0.5641\n",
      "318/388, train_loss: 0.2098, step time: 0.5269\n",
      "319/388, train_loss: 0.3506, step time: 0.5111\n",
      "320/388, train_loss: 0.0770, step time: 0.4953\n",
      "321/388, train_loss: 0.1351, step time: 0.5268\n",
      "322/388, train_loss: 0.3978, step time: 0.5373\n",
      "323/388, train_loss: 0.2524, step time: 0.5064\n",
      "324/388, train_loss: 0.1465, step time: 0.4939\n",
      "325/388, train_loss: 0.0680, step time: 0.4981\n",
      "326/388, train_loss: 0.0922, step time: 0.4926\n",
      "327/388, train_loss: 0.0665, step time: 0.5104\n",
      "328/388, train_loss: 0.1146, step time: 0.5026\n",
      "329/388, train_loss: 0.1549, step time: 0.5560\n",
      "330/388, train_loss: 0.1021, step time: 0.5396\n",
      "331/388, train_loss: 0.0997, step time: 0.5133\n",
      "332/388, train_loss: 0.1421, step time: 1.1953\n",
      "333/388, train_loss: 0.2149, step time: 0.5339\n",
      "334/388, train_loss: 0.0743, step time: 0.4920\n",
      "335/388, train_loss: 0.0774, step time: 0.4807\n",
      "336/388, train_loss: 0.1016, step time: 0.4971\n",
      "337/388, train_loss: 0.0568, step time: 0.4898\n",
      "338/388, train_loss: 0.0740, step time: 0.5245\n",
      "339/388, train_loss: 0.1174, step time: 0.5180\n",
      "340/388, train_loss: 0.2623, step time: 0.5633\n",
      "341/388, train_loss: 0.1657, step time: 0.6280\n",
      "342/388, train_loss: 0.1231, step time: 0.5534\n",
      "343/388, train_loss: 0.0891, step time: 0.5209\n",
      "344/388, train_loss: 0.1948, step time: 0.4966\n",
      "345/388, train_loss: 0.1130, step time: 0.5361\n",
      "346/388, train_loss: 0.0566, step time: 0.6251\n",
      "347/388, train_loss: 0.1003, step time: 0.5435\n",
      "348/388, train_loss: 0.2547, step time: 0.5224\n",
      "349/388, train_loss: 0.1017, step time: 0.5151\n",
      "350/388, train_loss: 0.2314, step time: 0.5215\n",
      "351/388, train_loss: 0.1413, step time: 0.5095\n",
      "352/388, train_loss: 0.1575, step time: 0.4848\n",
      "353/388, train_loss: 0.1218, step time: 1.1175\n",
      "354/388, train_loss: 0.1294, step time: 0.5355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/388, train_loss: 0.0913, step time: 0.4968\n",
      "356/388, train_loss: 0.2970, step time: 0.4908\n",
      "357/388, train_loss: 0.1830, step time: 0.4765\n",
      "358/388, train_loss: 0.1133, step time: 0.4984\n",
      "359/388, train_loss: 0.1456, step time: 1.0994\n",
      "360/388, train_loss: 0.2439, step time: 0.5431\n",
      "361/388, train_loss: 0.2641, step time: 0.5107\n",
      "362/388, train_loss: 0.1800, step time: 0.4849\n",
      "363/388, train_loss: 0.0647, step time: 0.4956\n",
      "364/388, train_loss: 0.2140, step time: 0.5043\n",
      "365/388, train_loss: 0.2129, step time: 0.5623\n",
      "366/388, train_loss: 0.0899, step time: 0.6089\n",
      "367/388, train_loss: 0.1030, step time: 0.5398\n",
      "368/388, train_loss: 0.1434, step time: 0.5158\n",
      "369/388, train_loss: 0.1686, step time: 0.5045\n",
      "370/388, train_loss: 0.3612, step time: 0.4972\n",
      "371/388, train_loss: 0.0461, step time: 0.4875\n",
      "372/388, train_loss: 0.1504, step time: 0.5718\n",
      "373/388, train_loss: 0.1304, step time: 0.5506\n",
      "374/388, train_loss: 0.3236, step time: 0.5303\n",
      "375/388, train_loss: 0.0841, step time: 0.5301\n",
      "376/388, train_loss: 0.3501, step time: 0.5165\n",
      "377/388, train_loss: 0.4882, step time: 0.4963\n",
      "378/388, train_loss: 0.1350, step time: 0.5014\n",
      "379/388, train_loss: 0.1589, step time: 0.4897\n",
      "380/388, train_loss: 0.1373, step time: 0.5342\n",
      "381/388, train_loss: 0.1271, step time: 0.4992\n",
      "382/388, train_loss: 0.2391, step time: 0.5318\n",
      "383/388, train_loss: 0.0650, step time: 0.6898\n",
      "384/388, train_loss: 0.1375, step time: 0.5563\n",
      "385/388, train_loss: 0.1290, step time: 0.5207\n",
      "386/388, train_loss: 0.0503, step time: 0.4940\n",
      "387/388, train_loss: 0.1437, step time: 0.4972\n",
      "388/388, train_loss: 0.1935, step time: 0.5160\n",
      "epoch 83 average loss: 0.1706\n",
      "current epoch: 83 current mean dice: 0.7783 tc: 0.8254 wt: 0.9058 et: 0.6037\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 83 is: 300.7911\n",
      "----------\n",
      "epoch 84/300\n",
      "1/388, train_loss: 0.2091, step time: 0.4729\n",
      "2/388, train_loss: 0.2219, step time: 0.4892\n",
      "3/388, train_loss: 0.2460, step time: 0.5472\n",
      "4/388, train_loss: 0.0521, step time: 0.5528\n",
      "5/388, train_loss: 0.2247, step time: 0.5183\n",
      "6/388, train_loss: 0.0887, step time: 0.5348\n",
      "7/388, train_loss: 0.1804, step time: 0.6212\n",
      "8/388, train_loss: 0.2670, step time: 0.5425\n",
      "9/388, train_loss: 0.2296, step time: 0.5046\n",
      "10/388, train_loss: 0.1454, step time: 0.5867\n",
      "11/388, train_loss: 0.0726, step time: 0.5599\n",
      "12/388, train_loss: 0.1384, step time: 0.5417\n",
      "13/388, train_loss: 0.2598, step time: 0.7312\n",
      "14/388, train_loss: 0.2622, step time: 0.6566\n",
      "15/388, train_loss: 0.1615, step time: 0.5888\n",
      "16/388, train_loss: 0.3404, step time: 0.5327\n",
      "17/388, train_loss: 0.1393, step time: 0.5218\n",
      "18/388, train_loss: 0.1539, step time: 0.6032\n",
      "19/388, train_loss: 0.3381, step time: 0.5861\n",
      "20/388, train_loss: 0.1554, step time: 0.5371\n",
      "21/388, train_loss: 0.1961, step time: 0.5210\n",
      "22/388, train_loss: 0.3137, step time: 0.5469\n",
      "23/388, train_loss: 0.0621, step time: 0.5638\n",
      "24/388, train_loss: 0.0442, step time: 0.5258\n",
      "25/388, train_loss: 0.1969, step time: 0.5032\n",
      "26/388, train_loss: 0.2119, step time: 0.5314\n",
      "27/388, train_loss: 0.0828, step time: 0.5361\n",
      "28/388, train_loss: 0.2319, step time: 0.5182\n",
      "29/388, train_loss: 0.3716, step time: 0.5059\n",
      "30/388, train_loss: 0.1627, step time: 0.5559\n",
      "31/388, train_loss: 0.1157, step time: 0.5427\n",
      "32/388, train_loss: 0.1706, step time: 0.5449\n",
      "33/388, train_loss: 0.1136, step time: 0.5229\n",
      "34/388, train_loss: 0.1365, step time: 0.7575\n",
      "35/388, train_loss: 0.1238, step time: 0.6137\n",
      "36/388, train_loss: 0.1699, step time: 0.5517\n",
      "37/388, train_loss: 0.1000, step time: 0.5193\n",
      "38/388, train_loss: 0.1394, step time: 0.5131\n",
      "39/388, train_loss: 0.1564, step time: 0.5476\n",
      "40/388, train_loss: 0.0940, step time: 0.6060\n",
      "41/388, train_loss: 0.1032, step time: 0.5449\n",
      "42/388, train_loss: 0.1537, step time: 0.5318\n",
      "43/388, train_loss: 0.1933, step time: 0.5274\n",
      "44/388, train_loss: 0.1432, step time: 0.5945\n",
      "45/388, train_loss: 0.1248, step time: 0.5388\n",
      "46/388, train_loss: 0.1398, step time: 0.5109\n",
      "47/388, train_loss: 0.1040, step time: 0.4963\n",
      "48/388, train_loss: 0.1267, step time: 0.5162\n",
      "49/388, train_loss: 0.1322, step time: 0.5901\n",
      "50/388, train_loss: 0.1156, step time: 0.5352\n",
      "51/388, train_loss: 0.3093, step time: 0.5100\n",
      "52/388, train_loss: 0.1061, step time: 0.4854\n",
      "53/388, train_loss: 0.0381, step time: 1.1622\n",
      "54/388, train_loss: 0.4282, step time: 0.5503\n",
      "55/388, train_loss: 0.0715, step time: 0.5165\n",
      "56/388, train_loss: 0.0747, step time: 0.4996\n",
      "57/388, train_loss: 0.1590, step time: 0.5058\n",
      "58/388, train_loss: 0.0959, step time: 0.4885\n",
      "59/388, train_loss: 0.0843, step time: 0.4891\n",
      "60/388, train_loss: 0.0994, step time: 0.4804\n",
      "61/388, train_loss: 0.1105, step time: 0.5538\n",
      "62/388, train_loss: 0.2555, step time: 0.5439\n",
      "63/388, train_loss: 0.1337, step time: 0.5107\n",
      "64/388, train_loss: 0.1759, step time: 0.4997\n",
      "65/388, train_loss: 0.1027, step time: 0.4989\n",
      "66/388, train_loss: 0.1210, step time: 0.5309\n",
      "67/388, train_loss: 0.2014, step time: 0.5193\n",
      "68/388, train_loss: 0.1939, step time: 0.5553\n",
      "69/388, train_loss: 0.2601, step time: 0.5288\n",
      "70/388, train_loss: 0.2047, step time: 0.5055\n",
      "71/388, train_loss: 0.1153, step time: 0.4936\n",
      "72/388, train_loss: 0.4158, step time: 0.4984\n",
      "73/388, train_loss: 0.1195, step time: 0.4925\n",
      "74/388, train_loss: 0.0921, step time: 0.4851\n",
      "75/388, train_loss: 0.1738, step time: 0.5219\n",
      "76/388, train_loss: 0.1877, step time: 0.4935\n",
      "77/388, train_loss: 0.2299, step time: 0.5393\n",
      "78/388, train_loss: 0.1221, step time: 0.5835\n",
      "79/388, train_loss: 0.1894, step time: 0.5391\n",
      "80/388, train_loss: 0.2272, step time: 0.5081\n",
      "81/388, train_loss: 0.0865, step time: 0.4868\n",
      "82/388, train_loss: 0.0631, step time: 0.5078\n",
      "83/388, train_loss: 0.2713, step time: 0.4958\n",
      "84/388, train_loss: 0.0830, step time: 0.4976\n",
      "85/388, train_loss: 0.0756, step time: 0.4824\n",
      "86/388, train_loss: 0.1501, step time: 1.1392\n",
      "87/388, train_loss: 0.1563, step time: 0.5315\n",
      "88/388, train_loss: 0.2277, step time: 0.5086\n",
      "89/388, train_loss: 0.0829, step time: 0.4952\n",
      "90/388, train_loss: 0.2015, step time: 0.4822\n",
      "91/388, train_loss: 0.2136, step time: 0.4820\n",
      "92/388, train_loss: 0.1544, step time: 0.4815\n",
      "93/388, train_loss: 0.1812, step time: 0.4882\n",
      "94/388, train_loss: 0.2206, step time: 0.5003\n",
      "95/388, train_loss: 0.1398, step time: 1.1444\n",
      "96/388, train_loss: 0.2930, step time: 0.5353\n",
      "97/388, train_loss: 0.0973, step time: 0.5062\n",
      "98/388, train_loss: 0.1200, step time: 0.5244\n",
      "99/388, train_loss: 0.3085, step time: 0.5071\n",
      "100/388, train_loss: 0.0960, step time: 0.5005\n",
      "101/388, train_loss: 0.2792, step time: 0.4828\n",
      "102/388, train_loss: 0.0704, step time: 0.4813\n",
      "103/388, train_loss: 0.0592, step time: 0.4992\n",
      "104/388, train_loss: 0.0834, step time: 0.5292\n",
      "105/388, train_loss: 0.1185, step time: 0.5154\n",
      "106/388, train_loss: 0.2089, step time: 0.5165\n",
      "107/388, train_loss: 0.3106, step time: 0.5230\n",
      "108/388, train_loss: 0.2602, step time: 0.5643\n",
      "109/388, train_loss: 0.1947, step time: 0.5409\n",
      "110/388, train_loss: 0.0835, step time: 0.5191\n",
      "111/388, train_loss: 0.4000, step time: 0.5008\n",
      "112/388, train_loss: 0.2945, step time: 0.5333\n",
      "113/388, train_loss: 0.1161, step time: 0.5098\n",
      "114/388, train_loss: 0.1392, step time: 0.5022\n",
      "115/388, train_loss: 0.0879, step time: 0.4903\n",
      "116/388, train_loss: 0.1519, step time: 0.4805\n",
      "117/388, train_loss: 0.1183, step time: 0.4819\n",
      "118/388, train_loss: 0.4598, step time: 0.4858\n",
      "119/388, train_loss: 0.1549, step time: 1.0779\n",
      "120/388, train_loss: 0.4144, step time: 0.5323\n",
      "121/388, train_loss: 0.0331, step time: 0.5044\n",
      "122/388, train_loss: 0.0689, step time: 0.4876\n",
      "123/388, train_loss: 0.0783, step time: 0.4950\n",
      "124/388, train_loss: 0.1919, step time: 0.4862\n",
      "125/388, train_loss: 0.1894, step time: 0.4788\n",
      "126/388, train_loss: 0.1910, step time: 0.4935\n",
      "127/388, train_loss: 0.1418, step time: 0.7184\n",
      "128/388, train_loss: 0.2480, step time: 0.5408\n",
      "129/388, train_loss: 0.2618, step time: 0.5177\n",
      "130/388, train_loss: 0.3761, step time: 0.4960\n",
      "131/388, train_loss: 0.4759, step time: 0.4994\n",
      "132/388, train_loss: 0.1413, step time: 0.4877\n",
      "133/388, train_loss: 0.1186, step time: 0.4909\n",
      "134/388, train_loss: 0.1358, step time: 0.4807\n",
      "135/388, train_loss: 0.2110, step time: 0.5239\n",
      "136/388, train_loss: 0.0725, step time: 0.5202\n",
      "137/388, train_loss: 0.0572, step time: 0.5060\n",
      "138/388, train_loss: 0.0824, step time: 0.4914\n",
      "139/388, train_loss: 0.1150, step time: 0.5889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/388, train_loss: 0.1979, step time: 0.5578\n",
      "141/388, train_loss: 0.1146, step time: 0.5270\n",
      "142/388, train_loss: 0.2125, step time: 0.4989\n",
      "143/388, train_loss: 0.1285, step time: 0.4980\n",
      "144/388, train_loss: 0.1547, step time: 0.4916\n",
      "145/388, train_loss: 0.1330, step time: 0.4776\n",
      "146/388, train_loss: 0.0830, step time: 0.4937\n",
      "147/388, train_loss: 0.2582, step time: 0.4930\n",
      "148/388, train_loss: 0.1127, step time: 0.4944\n",
      "149/388, train_loss: 0.2418, step time: 0.5328\n",
      "150/388, train_loss: 0.2234, step time: 0.5256\n",
      "151/388, train_loss: 0.1129, step time: 0.4970\n",
      "152/388, train_loss: 0.1100, step time: 0.5337\n",
      "153/388, train_loss: 0.2037, step time: 0.5199\n",
      "154/388, train_loss: 0.2213, step time: 0.5037\n",
      "155/388, train_loss: 0.4916, step time: 0.4914\n",
      "156/388, train_loss: 0.0645, step time: 0.4932\n",
      "157/388, train_loss: 0.2523, step time: 0.4862\n",
      "158/388, train_loss: 0.0453, step time: 0.4936\n",
      "159/388, train_loss: 0.0409, step time: 0.5767\n",
      "160/388, train_loss: 0.3828, step time: 0.5747\n",
      "161/388, train_loss: 0.0779, step time: 0.5453\n",
      "162/388, train_loss: 0.2424, step time: 0.5164\n",
      "163/388, train_loss: 0.1548, step time: 0.5072\n",
      "164/388, train_loss: 0.1381, step time: 0.5594\n",
      "165/388, train_loss: 0.3275, step time: 0.5355\n",
      "166/388, train_loss: 0.2700, step time: 0.5043\n",
      "167/388, train_loss: 0.1793, step time: 0.4996\n",
      "168/388, train_loss: 0.1159, step time: 0.4838\n",
      "169/388, train_loss: 0.3547, step time: 0.4958\n",
      "170/388, train_loss: 0.3924, step time: 0.4981\n",
      "171/388, train_loss: 0.2028, step time: 0.5324\n",
      "172/388, train_loss: 0.0965, step time: 0.5036\n",
      "173/388, train_loss: 0.2035, step time: 0.5107\n",
      "174/388, train_loss: 0.1309, step time: 0.4907\n",
      "175/388, train_loss: 0.2044, step time: 0.4899\n",
      "176/388, train_loss: 0.1904, step time: 0.5096\n",
      "177/388, train_loss: 0.2211, step time: 0.4931\n",
      "178/388, train_loss: 0.1403, step time: 0.4866\n",
      "179/388, train_loss: 0.2760, step time: 0.4914\n",
      "180/388, train_loss: 0.0960, step time: 0.4808\n",
      "181/388, train_loss: 0.1350, step time: 0.5034\n",
      "182/388, train_loss: 0.0994, step time: 0.4803\n",
      "183/388, train_loss: 0.0791, step time: 1.0837\n",
      "184/388, train_loss: 0.0637, step time: 0.5439\n",
      "185/388, train_loss: 0.1341, step time: 0.5199\n",
      "186/388, train_loss: 0.1875, step time: 0.5045\n",
      "187/388, train_loss: 0.0312, step time: 0.5501\n",
      "188/388, train_loss: 0.1964, step time: 0.5159\n",
      "189/388, train_loss: 0.2170, step time: 0.5067\n",
      "190/388, train_loss: 0.2032, step time: 0.4965\n",
      "191/388, train_loss: 0.0777, step time: 0.4896\n",
      "192/388, train_loss: 0.0434, step time: 0.4759\n",
      "193/388, train_loss: 0.5927, step time: 0.5040\n",
      "194/388, train_loss: 0.0993, step time: 0.4979\n",
      "195/388, train_loss: 0.1088, step time: 0.5004\n",
      "196/388, train_loss: 0.1631, step time: 0.4990\n",
      "197/388, train_loss: 0.1007, step time: 0.4899\n",
      "198/388, train_loss: 0.1336, step time: 0.5114\n",
      "199/388, train_loss: 0.1301, step time: 0.4874\n",
      "200/388, train_loss: 0.1261, step time: 0.5090\n",
      "201/388, train_loss: 0.0727, step time: 0.5481\n",
      "202/388, train_loss: 0.3816, step time: 0.5202\n",
      "203/388, train_loss: 0.0836, step time: 0.5080\n",
      "204/388, train_loss: 0.1555, step time: 0.5323\n",
      "205/388, train_loss: 0.1758, step time: 0.5244\n",
      "206/388, train_loss: 0.1862, step time: 0.5424\n",
      "207/388, train_loss: 0.0621, step time: 0.5702\n",
      "208/388, train_loss: 0.0716, step time: 0.5208\n",
      "209/388, train_loss: 0.0651, step time: 0.4984\n",
      "210/388, train_loss: 0.2320, step time: 0.4887\n",
      "211/388, train_loss: 0.1661, step time: 0.5185\n",
      "212/388, train_loss: 0.1215, step time: 0.4935\n",
      "213/388, train_loss: 0.2775, step time: 0.9566\n",
      "214/388, train_loss: 0.0691, step time: 0.5366\n",
      "215/388, train_loss: 0.2327, step time: 0.5107\n",
      "216/388, train_loss: 0.0424, step time: 0.4910\n",
      "217/388, train_loss: 0.3021, step time: 0.5374\n",
      "218/388, train_loss: 0.1903, step time: 0.5166\n",
      "219/388, train_loss: 0.0871, step time: 0.5039\n",
      "220/388, train_loss: 0.3012, step time: 0.4831\n",
      "221/388, train_loss: 0.1824, step time: 0.4894\n",
      "222/388, train_loss: 0.1467, step time: 0.4852\n",
      "223/388, train_loss: 0.1044, step time: 1.0401\n",
      "224/388, train_loss: 0.1588, step time: 0.5439\n",
      "225/388, train_loss: 0.0834, step time: 0.5142\n",
      "226/388, train_loss: 0.1584, step time: 0.4909\n",
      "227/388, train_loss: 0.3067, step time: 0.4992\n",
      "228/388, train_loss: 0.1968, step time: 0.5124\n",
      "229/388, train_loss: 0.2844, step time: 0.5035\n",
      "230/388, train_loss: 0.1311, step time: 0.4858\n",
      "231/388, train_loss: 0.1334, step time: 0.4932\n",
      "232/388, train_loss: 0.1238, step time: 0.4769\n",
      "233/388, train_loss: 0.3339, step time: 0.4951\n",
      "234/388, train_loss: 0.0537, step time: 0.4903\n",
      "235/388, train_loss: 0.0658, step time: 1.1107\n",
      "236/388, train_loss: 0.1212, step time: 0.5326\n",
      "237/388, train_loss: 0.2784, step time: 0.5245\n",
      "238/388, train_loss: 0.1305, step time: 0.4985\n",
      "239/388, train_loss: 0.0693, step time: 0.4866\n",
      "240/388, train_loss: 0.3042, step time: 0.4888\n",
      "241/388, train_loss: 0.1805, step time: 0.4933\n",
      "242/388, train_loss: 0.0558, step time: 0.4808\n",
      "243/388, train_loss: 0.1397, step time: 0.5083\n",
      "244/388, train_loss: 0.2419, step time: 0.4919\n",
      "245/388, train_loss: 0.1226, step time: 0.4952\n",
      "246/388, train_loss: 0.1070, step time: 0.4921\n",
      "247/388, train_loss: 0.1871, step time: 0.5447\n",
      "248/388, train_loss: 0.0627, step time: 0.5160\n",
      "249/388, train_loss: 0.0454, step time: 0.5096\n",
      "250/388, train_loss: 0.0491, step time: 0.5232\n",
      "251/388, train_loss: 0.4317, step time: 0.5111\n",
      "252/388, train_loss: 0.1594, step time: 0.5010\n",
      "253/388, train_loss: 0.1266, step time: 0.4868\n",
      "254/388, train_loss: 0.1084, step time: 0.4947\n",
      "255/388, train_loss: 0.0917, step time: 0.4885\n",
      "256/388, train_loss: 0.3656, step time: 0.4976\n",
      "257/388, train_loss: 0.1252, step time: 0.4906\n",
      "258/388, train_loss: 0.0871, step time: 1.1845\n",
      "259/388, train_loss: 0.1562, step time: 0.5294\n",
      "260/388, train_loss: 0.1872, step time: 0.5155\n",
      "261/388, train_loss: 0.1614, step time: 0.4971\n",
      "262/388, train_loss: 0.0895, step time: 0.5021\n",
      "263/388, train_loss: 0.1583, step time: 0.4850\n",
      "264/388, train_loss: 0.2678, step time: 0.4997\n",
      "265/388, train_loss: 0.1123, step time: 0.4974\n",
      "266/388, train_loss: 0.1571, step time: 0.8492\n",
      "267/388, train_loss: 0.1846, step time: 0.5396\n",
      "268/388, train_loss: 0.2852, step time: 0.5134\n",
      "269/388, train_loss: 0.5134, step time: 0.4956\n",
      "270/388, train_loss: 0.0948, step time: 0.4886\n",
      "271/388, train_loss: 0.0858, step time: 0.5014\n",
      "272/388, train_loss: 0.0709, step time: 0.4759\n",
      "273/388, train_loss: 0.1298, step time: 0.4792\n",
      "274/388, train_loss: 0.1654, step time: 0.4871\n",
      "275/388, train_loss: 0.0955, step time: 1.1116\n",
      "276/388, train_loss: 0.0943, step time: 0.5432\n",
      "277/388, train_loss: 0.0883, step time: 0.5033\n",
      "278/388, train_loss: 0.2127, step time: 0.4976\n",
      "279/388, train_loss: 0.4696, step time: 0.4962\n",
      "280/388, train_loss: 0.0914, step time: 0.4844\n",
      "281/388, train_loss: 0.1157, step time: 0.4922\n",
      "282/388, train_loss: 0.2030, step time: 0.4805\n",
      "283/388, train_loss: 0.0865, step time: 0.4739\n",
      "284/388, train_loss: 0.1946, step time: 0.4808\n",
      "285/388, train_loss: 0.2417, step time: 0.7852\n",
      "286/388, train_loss: 0.2476, step time: 0.5698\n",
      "287/388, train_loss: 0.0699, step time: 0.5307\n",
      "288/388, train_loss: 0.1743, step time: 0.5254\n",
      "289/388, train_loss: 0.1671, step time: 0.5135\n",
      "290/388, train_loss: 0.0883, step time: 0.5015\n",
      "291/388, train_loss: 0.3064, step time: 0.5022\n",
      "292/388, train_loss: 0.0904, step time: 0.4824\n",
      "293/388, train_loss: 0.1311, step time: 0.4847\n",
      "294/388, train_loss: 0.0857, step time: 0.4960\n",
      "295/388, train_loss: 0.2086, step time: 0.4896\n",
      "296/388, train_loss: 0.0929, step time: 0.5118\n",
      "297/388, train_loss: 0.1173, step time: 0.5135\n",
      "298/388, train_loss: 0.3395, step time: 0.4972\n",
      "299/388, train_loss: 0.4420, step time: 0.4999\n",
      "300/388, train_loss: 0.1654, step time: 1.0044\n",
      "301/388, train_loss: 0.2614, step time: 0.5257\n",
      "302/388, train_loss: 0.1775, step time: 0.5059\n",
      "303/388, train_loss: 0.2024, step time: 0.4883\n",
      "304/388, train_loss: 0.4155, step time: 0.4898\n",
      "305/388, train_loss: 0.1562, step time: 0.4924\n",
      "306/388, train_loss: 0.0974, step time: 0.4884\n",
      "307/388, train_loss: 0.2681, step time: 1.0924\n",
      "308/388, train_loss: 0.1070, step time: 0.5343\n",
      "309/388, train_loss: 0.0297, step time: 0.5123\n",
      "310/388, train_loss: 0.3463, step time: 0.4888\n",
      "311/388, train_loss: 0.0711, step time: 0.4939\n",
      "312/388, train_loss: 0.3684, step time: 0.4944\n",
      "313/388, train_loss: 0.0906, step time: 0.4794\n",
      "314/388, train_loss: 0.1895, step time: 0.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/388, train_loss: 0.3320, step time: 0.5130\n",
      "316/388, train_loss: 0.2166, step time: 0.5046\n",
      "317/388, train_loss: 0.2552, step time: 0.4985\n",
      "318/388, train_loss: 0.3058, step time: 0.4992\n",
      "319/388, train_loss: 0.1925, step time: 0.8551\n",
      "320/388, train_loss: 0.1070, step time: 0.5588\n",
      "321/388, train_loss: 0.1571, step time: 0.5163\n",
      "322/388, train_loss: 0.1239, step time: 0.5012\n",
      "323/388, train_loss: 0.2661, step time: 0.4946\n",
      "324/388, train_loss: 0.0846, step time: 0.5287\n",
      "325/388, train_loss: 0.2041, step time: 0.5127\n",
      "326/388, train_loss: 0.1518, step time: 0.4984\n",
      "327/388, train_loss: 0.0988, step time: 0.4931\n",
      "328/388, train_loss: 0.1190, step time: 0.4863\n",
      "329/388, train_loss: 0.5018, step time: 0.4826\n",
      "330/388, train_loss: 0.0549, step time: 0.4903\n",
      "331/388, train_loss: 0.1896, step time: 0.4841\n",
      "332/388, train_loss: 0.0854, step time: 0.4966\n",
      "333/388, train_loss: 0.1923, step time: 0.5631\n",
      "334/388, train_loss: 0.0902, step time: 0.5305\n",
      "335/388, train_loss: 0.1225, step time: 0.5102\n",
      "336/388, train_loss: 0.1037, step time: 0.5261\n",
      "337/388, train_loss: 0.2803, step time: 0.5119\n",
      "338/388, train_loss: 0.0974, step time: 0.5046\n",
      "339/388, train_loss: 0.3492, step time: 0.4889\n",
      "340/388, train_loss: 0.2730, step time: 0.5096\n",
      "341/388, train_loss: 0.1542, step time: 0.4954\n",
      "342/388, train_loss: 0.1152, step time: 0.5042\n",
      "343/388, train_loss: 0.3630, step time: 0.5043\n",
      "344/388, train_loss: 0.2708, step time: 0.5017\n",
      "345/388, train_loss: 0.1903, step time: 0.4910\n",
      "346/388, train_loss: 0.1983, step time: 0.5014\n",
      "347/388, train_loss: 0.1659, step time: 0.4832\n",
      "348/388, train_loss: 0.3987, step time: 0.4933\n",
      "349/388, train_loss: 0.0964, step time: 0.5003\n",
      "350/388, train_loss: 0.1372, step time: 0.5342\n",
      "351/388, train_loss: 0.0526, step time: 0.5314\n",
      "352/388, train_loss: 0.1256, step time: 0.5100\n",
      "353/388, train_loss: 0.1726, step time: 0.5145\n",
      "354/388, train_loss: 0.1528, step time: 0.5288\n",
      "355/388, train_loss: 0.0576, step time: 0.5510\n",
      "356/388, train_loss: 0.1033, step time: 0.5305\n",
      "357/388, train_loss: 0.1735, step time: 0.5145\n",
      "358/388, train_loss: 0.1232, step time: 0.5114\n",
      "359/388, train_loss: 0.1035, step time: 0.5008\n",
      "360/388, train_loss: 0.0982, step time: 0.4991\n",
      "361/388, train_loss: 0.2483, step time: 0.5625\n",
      "362/388, train_loss: 0.1253, step time: 0.5685\n",
      "363/388, train_loss: 0.2667, step time: 0.5266\n",
      "364/388, train_loss: 0.1031, step time: 0.5195\n",
      "365/388, train_loss: 0.1051, step time: 0.5250\n",
      "366/388, train_loss: 0.0953, step time: 0.5110\n",
      "367/388, train_loss: 0.1907, step time: 0.4890\n",
      "368/388, train_loss: 0.0657, step time: 0.4971\n",
      "369/388, train_loss: 0.0791, step time: 0.4938\n",
      "370/388, train_loss: 0.0816, step time: 0.4894\n",
      "371/388, train_loss: 0.4034, step time: 0.4875\n",
      "372/388, train_loss: 0.2582, step time: 1.0418\n",
      "373/388, train_loss: 0.1571, step time: 0.5600\n",
      "374/388, train_loss: 0.0920, step time: 0.5296\n",
      "375/388, train_loss: 0.2053, step time: 0.5064\n",
      "376/388, train_loss: 0.2602, step time: 0.5028\n",
      "377/388, train_loss: 0.0924, step time: 0.9649\n",
      "378/388, train_loss: 0.0877, step time: 0.5385\n",
      "379/388, train_loss: 0.1903, step time: 0.5016\n",
      "380/388, train_loss: 0.1078, step time: 0.4956\n",
      "381/388, train_loss: 0.1082, step time: 0.5057\n",
      "382/388, train_loss: 0.1331, step time: 0.4834\n",
      "383/388, train_loss: 0.1306, step time: 0.4971\n",
      "384/388, train_loss: 0.0888, step time: 0.4819\n",
      "385/388, train_loss: 0.1572, step time: 0.4997\n",
      "386/388, train_loss: 0.1837, step time: 0.4995\n",
      "387/388, train_loss: 0.1418, step time: 0.4756\n",
      "388/388, train_loss: 0.1204, step time: 0.4976\n",
      "epoch 84 average loss: 0.1719\n",
      "current epoch: 84 current mean dice: 0.7677 tc: 0.8112 wt: 0.9046 et: 0.5872\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 84 is: 298.8275\n",
      "----------\n",
      "epoch 85/300\n",
      "1/388, train_loss: 0.1946, step time: 0.4794\n",
      "2/388, train_loss: 0.0773, step time: 0.4810\n",
      "3/388, train_loss: 0.0793, step time: 0.4869\n",
      "4/388, train_loss: 0.2734, step time: 0.5217\n",
      "5/388, train_loss: 0.0906, step time: 0.4895\n",
      "6/388, train_loss: 0.5028, step time: 0.5379\n",
      "7/388, train_loss: 0.2326, step time: 0.7342\n",
      "8/388, train_loss: 0.1264, step time: 0.6025\n",
      "9/388, train_loss: 0.1268, step time: 0.5145\n",
      "10/388, train_loss: 0.1386, step time: 0.5031\n",
      "11/388, train_loss: 0.3080, step time: 0.9418\n",
      "12/388, train_loss: 0.1396, step time: 0.5655\n",
      "13/388, train_loss: 0.0820, step time: 0.5217\n",
      "14/388, train_loss: 0.3726, step time: 0.4910\n",
      "15/388, train_loss: 0.1625, step time: 0.5822\n",
      "16/388, train_loss: 0.1406, step time: 0.5376\n",
      "17/388, train_loss: 0.2362, step time: 0.5406\n",
      "18/388, train_loss: 0.1854, step time: 0.6610\n",
      "19/388, train_loss: 0.0883, step time: 0.5873\n",
      "20/388, train_loss: 0.1806, step time: 0.5399\n",
      "21/388, train_loss: 0.0764, step time: 0.5095\n",
      "22/388, train_loss: 0.1038, step time: 0.4960\n",
      "23/388, train_loss: 0.3293, step time: 0.5144\n",
      "24/388, train_loss: 0.1296, step time: 0.5134\n",
      "25/388, train_loss: 0.0831, step time: 0.5079\n",
      "26/388, train_loss: 0.2967, step time: 0.5616\n",
      "27/388, train_loss: 0.1037, step time: 0.5100\n",
      "28/388, train_loss: 0.0464, step time: 0.4977\n",
      "29/388, train_loss: 0.0934, step time: 0.5070\n",
      "30/388, train_loss: 0.1010, step time: 0.5019\n",
      "31/388, train_loss: 0.0838, step time: 0.5263\n",
      "32/388, train_loss: 0.2664, step time: 0.5929\n",
      "33/388, train_loss: 0.1610, step time: 0.6025\n",
      "34/388, train_loss: 0.1176, step time: 0.5596\n",
      "35/388, train_loss: 0.0546, step time: 0.5349\n",
      "36/388, train_loss: 0.0924, step time: 0.6137\n",
      "37/388, train_loss: 0.1566, step time: 0.5532\n",
      "38/388, train_loss: 0.1297, step time: 0.5245\n",
      "39/388, train_loss: 0.0620, step time: 0.5210\n",
      "40/388, train_loss: 0.0631, step time: 0.5158\n",
      "41/388, train_loss: 0.2948, step time: 0.5736\n",
      "42/388, train_loss: 0.1024, step time: 0.5254\n",
      "43/388, train_loss: 0.1066, step time: 0.9757\n",
      "44/388, train_loss: 0.1601, step time: 0.5449\n",
      "45/388, train_loss: 0.0912, step time: 0.5043\n",
      "46/388, train_loss: 0.0928, step time: 0.4981\n",
      "47/388, train_loss: 0.0476, step time: 0.5185\n",
      "48/388, train_loss: 0.1203, step time: 0.5277\n",
      "49/388, train_loss: 0.2530, step time: 0.5224\n",
      "50/388, train_loss: 0.0898, step time: 0.4950\n",
      "51/388, train_loss: 0.2337, step time: 0.5003\n",
      "52/388, train_loss: 0.1736, step time: 0.5275\n",
      "53/388, train_loss: 0.1708, step time: 0.5183\n",
      "54/388, train_loss: 0.1014, step time: 0.5093\n",
      "55/388, train_loss: 0.1475, step time: 0.5057\n",
      "56/388, train_loss: 0.2782, step time: 0.4809\n",
      "57/388, train_loss: 0.1931, step time: 0.4993\n",
      "58/388, train_loss: 0.0460, step time: 0.4950\n",
      "59/388, train_loss: 0.1033, step time: 0.8574\n",
      "60/388, train_loss: 0.2707, step time: 0.5421\n",
      "61/388, train_loss: 0.2144, step time: 0.4968\n",
      "62/388, train_loss: 0.1989, step time: 0.7740\n",
      "63/388, train_loss: 0.1386, step time: 0.5506\n",
      "64/388, train_loss: 0.0818, step time: 0.5137\n",
      "65/388, train_loss: 0.0553, step time: 0.4961\n",
      "66/388, train_loss: 0.1371, step time: 0.5007\n",
      "67/388, train_loss: 0.0914, step time: 1.0773\n",
      "68/388, train_loss: 0.1330, step time: 0.5292\n",
      "69/388, train_loss: 0.0913, step time: 0.5067\n",
      "70/388, train_loss: 0.2793, step time: 0.5260\n",
      "71/388, train_loss: 0.1196, step time: 0.5157\n",
      "72/388, train_loss: 0.2526, step time: 0.5619\n",
      "73/388, train_loss: 0.1336, step time: 0.6768\n",
      "74/388, train_loss: 0.1741, step time: 0.5483\n",
      "75/388, train_loss: 0.0833, step time: 0.5006\n",
      "76/388, train_loss: 0.1931, step time: 0.5457\n",
      "77/388, train_loss: 0.2231, step time: 0.5082\n",
      "78/388, train_loss: 0.2804, step time: 0.5038\n",
      "79/388, train_loss: 0.3590, step time: 0.5024\n",
      "80/388, train_loss: 0.1429, step time: 0.5185\n",
      "81/388, train_loss: 0.3001, step time: 0.5044\n",
      "82/388, train_loss: 0.1526, step time: 0.5083\n",
      "83/388, train_loss: 0.2064, step time: 0.4976\n",
      "84/388, train_loss: 0.1456, step time: 0.9046\n",
      "85/388, train_loss: 0.1426, step time: 0.5516\n",
      "86/388, train_loss: 0.1432, step time: 0.5328\n",
      "87/388, train_loss: 0.2318, step time: 0.5060\n",
      "88/388, train_loss: 0.1720, step time: 0.5537\n",
      "89/388, train_loss: 0.0864, step time: 0.5234\n",
      "90/388, train_loss: 0.2202, step time: 0.5061\n",
      "91/388, train_loss: 0.0640, step time: 0.5126\n",
      "92/388, train_loss: 0.0974, step time: 0.4893\n",
      "93/388, train_loss: 0.0769, step time: 0.4952\n",
      "94/388, train_loss: 0.0888, step time: 0.4868\n",
      "95/388, train_loss: 0.0767, step time: 0.4956\n",
      "96/388, train_loss: 0.3077, step time: 0.4927\n",
      "97/388, train_loss: 0.0816, step time: 0.4832\n",
      "98/388, train_loss: 0.1067, step time: 0.4800\n",
      "99/388, train_loss: 0.3899, step time: 0.9657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/388, train_loss: 0.0651, step time: 0.5450\n",
      "101/388, train_loss: 0.1429, step time: 0.5081\n",
      "102/388, train_loss: 0.1499, step time: 0.4978\n",
      "103/388, train_loss: 0.2764, step time: 0.4995\n",
      "104/388, train_loss: 0.1041, step time: 0.5102\n",
      "105/388, train_loss: 0.1668, step time: 0.5110\n",
      "106/388, train_loss: 0.2812, step time: 0.5858\n",
      "107/388, train_loss: 0.1983, step time: 0.5832\n",
      "108/388, train_loss: 0.1106, step time: 0.5401\n",
      "109/388, train_loss: 0.0440, step time: 0.5221\n",
      "110/388, train_loss: 0.0684, step time: 0.4954\n",
      "111/388, train_loss: 0.1683, step time: 0.5221\n",
      "112/388, train_loss: 0.5597, step time: 0.6260\n",
      "113/388, train_loss: 0.0974, step time: 0.5350\n",
      "114/388, train_loss: 0.1302, step time: 0.5348\n",
      "115/388, train_loss: 0.1162, step time: 0.5291\n",
      "116/388, train_loss: 0.0381, step time: 0.5068\n",
      "117/388, train_loss: 0.2605, step time: 0.5207\n",
      "118/388, train_loss: 0.1474, step time: 0.5100\n",
      "119/388, train_loss: 0.0815, step time: 0.4885\n",
      "120/388, train_loss: 0.0507, step time: 0.4909\n",
      "121/388, train_loss: 0.2889, step time: 1.1339\n",
      "122/388, train_loss: 0.0947, step time: 0.5029\n",
      "123/388, train_loss: 0.1351, step time: 0.4898\n",
      "124/388, train_loss: 0.1330, step time: 0.4904\n",
      "125/388, train_loss: 0.2455, step time: 0.4801\n",
      "126/388, train_loss: 0.1547, step time: 0.4758\n",
      "127/388, train_loss: 0.4151, step time: 0.4776\n",
      "128/388, train_loss: 0.2894, step time: 0.4907\n",
      "129/388, train_loss: 0.0790, step time: 1.1228\n",
      "130/388, train_loss: 0.1929, step time: 0.5455\n",
      "131/388, train_loss: 0.0668, step time: 0.5014\n",
      "132/388, train_loss: 0.1923, step time: 0.4976\n",
      "133/388, train_loss: 0.0584, step time: 0.4840\n",
      "134/388, train_loss: 0.3599, step time: 1.1065\n",
      "135/388, train_loss: 0.1111, step time: 0.5170\n",
      "136/388, train_loss: 0.1075, step time: 0.4927\n",
      "137/388, train_loss: 0.1072, step time: 0.4863\n",
      "138/388, train_loss: 0.2304, step time: 0.4891\n",
      "139/388, train_loss: 0.1874, step time: 0.4820\n",
      "140/388, train_loss: 0.2920, step time: 0.4779\n",
      "141/388, train_loss: 0.1353, step time: 0.4780\n",
      "142/388, train_loss: 0.0348, step time: 1.1608\n",
      "143/388, train_loss: 0.3256, step time: 0.5414\n",
      "144/388, train_loss: 0.2161, step time: 0.5072\n",
      "145/388, train_loss: 0.0437, step time: 0.4967\n",
      "146/388, train_loss: 0.5600, step time: 0.4841\n",
      "147/388, train_loss: 0.1171, step time: 0.5231\n",
      "148/388, train_loss: 0.1426, step time: 0.5096\n",
      "149/388, train_loss: 0.2485, step time: 0.4982\n",
      "150/388, train_loss: 0.2549, step time: 0.4884\n",
      "151/388, train_loss: 0.0815, step time: 0.5090\n",
      "152/388, train_loss: 0.1518, step time: 0.4933\n",
      "153/388, train_loss: 0.2553, step time: 0.5017\n",
      "154/388, train_loss: 0.2454, step time: 0.4956\n",
      "155/388, train_loss: 0.1461, step time: 0.4956\n",
      "156/388, train_loss: 0.2100, step time: 1.1428\n",
      "157/388, train_loss: 0.3274, step time: 0.5470\n",
      "158/388, train_loss: 0.2016, step time: 0.5056\n",
      "159/388, train_loss: 0.1043, step time: 0.5029\n",
      "160/388, train_loss: 0.0627, step time: 0.4872\n",
      "161/388, train_loss: 0.1059, step time: 0.4822\n",
      "162/388, train_loss: 0.1213, step time: 0.4860\n",
      "163/388, train_loss: 0.1079, step time: 0.4736\n",
      "164/388, train_loss: 0.2227, step time: 0.4842\n",
      "165/388, train_loss: 0.1604, step time: 0.4873\n",
      "166/388, train_loss: 0.2088, step time: 0.5342\n",
      "167/388, train_loss: 0.5347, step time: 0.5008\n",
      "168/388, train_loss: 0.1026, step time: 0.4944\n",
      "169/388, train_loss: 0.2329, step time: 0.4779\n",
      "170/388, train_loss: 0.1354, step time: 0.4757\n",
      "171/388, train_loss: 0.1199, step time: 0.4780\n",
      "172/388, train_loss: 0.2154, step time: 0.9564\n",
      "173/388, train_loss: 0.4330, step time: 0.5466\n",
      "174/388, train_loss: 0.1692, step time: 0.5145\n",
      "175/388, train_loss: 0.1799, step time: 0.5024\n",
      "176/388, train_loss: 0.4057, step time: 0.5396\n",
      "177/388, train_loss: 0.0681, step time: 0.5145\n",
      "178/388, train_loss: 0.1032, step time: 0.4956\n",
      "179/388, train_loss: 0.2236, step time: 0.4962\n",
      "180/388, train_loss: 0.3822, step time: 0.4825\n",
      "181/388, train_loss: 0.1809, step time: 0.4921\n",
      "182/388, train_loss: 0.1204, step time: 0.6274\n",
      "183/388, train_loss: 0.0915, step time: 0.5426\n",
      "184/388, train_loss: 0.2435, step time: 0.5193\n",
      "185/388, train_loss: 0.2418, step time: 0.5051\n",
      "186/388, train_loss: 0.1066, step time: 0.5009\n",
      "187/388, train_loss: 0.1417, step time: 0.4989\n",
      "188/388, train_loss: 0.1398, step time: 0.4908\n",
      "189/388, train_loss: 0.2405, step time: 0.5303\n",
      "190/388, train_loss: 0.0745, step time: 0.5424\n",
      "191/388, train_loss: 0.1929, step time: 0.5119\n",
      "192/388, train_loss: 0.2668, step time: 0.4862\n",
      "193/388, train_loss: 0.1976, step time: 0.5057\n",
      "194/388, train_loss: 0.2641, step time: 0.4975\n",
      "195/388, train_loss: 0.1368, step time: 0.4952\n",
      "196/388, train_loss: 0.0834, step time: 0.4806\n",
      "197/388, train_loss: 0.2033, step time: 0.5193\n",
      "198/388, train_loss: 0.1176, step time: 0.5084\n",
      "199/388, train_loss: 0.0775, step time: 0.5092\n",
      "200/388, train_loss: 0.1908, step time: 0.4926\n",
      "201/388, train_loss: 0.0518, step time: 0.4950\n",
      "202/388, train_loss: 0.1180, step time: 0.4814\n",
      "203/388, train_loss: 0.0908, step time: 0.5402\n",
      "204/388, train_loss: 0.0686, step time: 0.5339\n",
      "205/388, train_loss: 0.2055, step time: 0.4986\n",
      "206/388, train_loss: 0.0641, step time: 1.0888\n",
      "207/388, train_loss: 0.1655, step time: 0.5441\n",
      "208/388, train_loss: 0.1693, step time: 0.5156\n",
      "209/388, train_loss: 0.3072, step time: 0.5006\n",
      "210/388, train_loss: 0.2927, step time: 1.0390\n",
      "211/388, train_loss: 0.1394, step time: 0.5278\n",
      "212/388, train_loss: 0.1358, step time: 0.5091\n",
      "213/388, train_loss: 0.2560, step time: 0.4895\n",
      "214/388, train_loss: 0.1089, step time: 0.4916\n",
      "215/388, train_loss: 0.1133, step time: 0.4857\n",
      "216/388, train_loss: 0.0951, step time: 0.5775\n",
      "217/388, train_loss: 0.1447, step time: 0.5495\n",
      "218/388, train_loss: 0.0885, step time: 0.5161\n",
      "219/388, train_loss: 0.1193, step time: 0.5135\n",
      "220/388, train_loss: 0.1034, step time: 0.4944\n",
      "221/388, train_loss: 0.3434, step time: 0.4956\n",
      "222/388, train_loss: 0.0900, step time: 1.1667\n",
      "223/388, train_loss: 0.2012, step time: 0.5293\n",
      "224/388, train_loss: 0.2343, step time: 0.5080\n",
      "225/388, train_loss: 0.1933, step time: 0.4961\n",
      "226/388, train_loss: 0.0934, step time: 0.4839\n",
      "227/388, train_loss: 0.1499, step time: 0.4880\n",
      "228/388, train_loss: 0.1603, step time: 0.4724\n",
      "229/388, train_loss: 0.1285, step time: 0.8885\n",
      "230/388, train_loss: 0.1160, step time: 0.5364\n",
      "231/388, train_loss: 0.1656, step time: 0.5171\n",
      "232/388, train_loss: 0.0821, step time: 0.4960\n",
      "233/388, train_loss: 0.1124, step time: 0.4826\n",
      "234/388, train_loss: 0.1331, step time: 0.4896\n",
      "235/388, train_loss: 0.0393, step time: 0.4877\n",
      "236/388, train_loss: 0.3053, step time: 0.4743\n",
      "237/388, train_loss: 0.2486, step time: 0.4799\n",
      "238/388, train_loss: 0.1417, step time: 0.4764\n",
      "239/388, train_loss: 0.0730, step time: 0.5267\n",
      "240/388, train_loss: 0.1073, step time: 0.4976\n",
      "241/388, train_loss: 0.1228, step time: 0.4976\n",
      "242/388, train_loss: 0.1194, step time: 0.4970\n",
      "243/388, train_loss: 0.0578, step time: 0.5220\n",
      "244/388, train_loss: 0.1220, step time: 0.4982\n",
      "245/388, train_loss: 0.2135, step time: 0.5170\n",
      "246/388, train_loss: 0.1766, step time: 0.5249\n",
      "247/388, train_loss: 0.0979, step time: 0.5051\n",
      "248/388, train_loss: 0.1443, step time: 0.4974\n",
      "249/388, train_loss: 0.2333, step time: 0.4884\n",
      "250/388, train_loss: 0.1040, step time: 0.4991\n",
      "251/388, train_loss: 0.4170, step time: 0.5492\n",
      "252/388, train_loss: 0.2494, step time: 0.5691\n",
      "253/388, train_loss: 0.2423, step time: 0.5315\n",
      "254/388, train_loss: 0.2171, step time: 0.4976\n",
      "255/388, train_loss: 0.0938, step time: 0.4939\n",
      "256/388, train_loss: 0.0887, step time: 0.4844\n",
      "257/388, train_loss: 0.1952, step time: 0.5015\n",
      "258/388, train_loss: 0.0617, step time: 0.4851\n",
      "259/388, train_loss: 0.0975, step time: 1.0488\n",
      "260/388, train_loss: 0.1617, step time: 0.5500\n",
      "261/388, train_loss: 0.1652, step time: 0.5189\n",
      "262/388, train_loss: 0.1513, step time: 0.4977\n",
      "263/388, train_loss: 0.2003, step time: 0.5053\n",
      "264/388, train_loss: 0.1946, step time: 0.4851\n",
      "265/388, train_loss: 0.2729, step time: 0.5185\n",
      "266/388, train_loss: 0.3885, step time: 0.5516\n",
      "267/388, train_loss: 0.1322, step time: 0.5272\n",
      "268/388, train_loss: 0.0951, step time: 0.5096\n",
      "269/388, train_loss: 0.2998, step time: 0.4915\n",
      "270/388, train_loss: 0.1402, step time: 0.4824\n",
      "271/388, train_loss: 0.0997, step time: 1.2093\n",
      "272/388, train_loss: 0.1013, step time: 0.5271\n",
      "273/388, train_loss: 0.2255, step time: 0.5075\n",
      "274/388, train_loss: 0.1893, step time: 0.4899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/388, train_loss: 0.2918, step time: 0.4931\n",
      "276/388, train_loss: 0.1095, step time: 0.4822\n",
      "277/388, train_loss: 0.1597, step time: 0.4910\n",
      "278/388, train_loss: 0.4743, step time: 0.4800\n",
      "279/388, train_loss: 0.2404, step time: 0.4822\n",
      "280/388, train_loss: 0.2214, step time: 0.6470\n",
      "281/388, train_loss: 0.2359, step time: 0.5359\n",
      "282/388, train_loss: 0.0825, step time: 0.5076\n",
      "283/388, train_loss: 0.1465, step time: 0.5055\n",
      "284/388, train_loss: 0.1764, step time: 0.4865\n",
      "285/388, train_loss: 0.0935, step time: 0.4827\n",
      "286/388, train_loss: 0.0908, step time: 0.4928\n",
      "287/388, train_loss: 0.1186, step time: 0.5025\n",
      "288/388, train_loss: 0.1130, step time: 0.4895\n",
      "289/388, train_loss: 0.2787, step time: 0.5023\n",
      "290/388, train_loss: 0.2410, step time: 0.5030\n",
      "291/388, train_loss: 0.2158, step time: 0.5736\n",
      "292/388, train_loss: 0.0896, step time: 0.5519\n",
      "293/388, train_loss: 0.1221, step time: 0.5350\n",
      "294/388, train_loss: 0.3645, step time: 0.5163\n",
      "295/388, train_loss: 0.0958, step time: 0.4906\n",
      "296/388, train_loss: 0.0995, step time: 0.4987\n",
      "297/388, train_loss: 0.1197, step time: 0.5032\n",
      "298/388, train_loss: 0.0765, step time: 0.4932\n",
      "299/388, train_loss: 0.1806, step time: 0.4883\n",
      "300/388, train_loss: 0.1384, step time: 0.5158\n",
      "301/388, train_loss: 0.2083, step time: 0.4998\n",
      "302/388, train_loss: 0.0910, step time: 0.4993\n",
      "303/388, train_loss: 0.1786, step time: 0.4848\n",
      "304/388, train_loss: 0.1046, step time: 0.8795\n",
      "305/388, train_loss: 0.0779, step time: 0.5428\n",
      "306/388, train_loss: 0.0767, step time: 0.5326\n",
      "307/388, train_loss: 0.2948, step time: 0.5146\n",
      "308/388, train_loss: 0.2243, step time: 0.5007\n",
      "309/388, train_loss: 0.0867, step time: 0.4877\n",
      "310/388, train_loss: 0.1927, step time: 0.4980\n",
      "311/388, train_loss: 0.1941, step time: 0.4940\n",
      "312/388, train_loss: 0.0455, step time: 0.5009\n",
      "313/388, train_loss: 0.1545, step time: 0.4810\n",
      "314/388, train_loss: 0.1291, step time: 0.5151\n",
      "315/388, train_loss: 0.2136, step time: 0.5110\n",
      "316/388, train_loss: 0.1754, step time: 0.5321\n",
      "317/388, train_loss: 0.0919, step time: 0.5409\n",
      "318/388, train_loss: 0.2538, step time: 0.6260\n",
      "319/388, train_loss: 0.3014, step time: 0.5511\n",
      "320/388, train_loss: 0.1619, step time: 0.5200\n",
      "321/388, train_loss: 0.2441, step time: 0.5171\n",
      "322/388, train_loss: 0.4063, step time: 0.5138\n",
      "323/388, train_loss: 0.4908, step time: 0.5048\n",
      "324/388, train_loss: 0.0934, step time: 0.5501\n",
      "325/388, train_loss: 0.1178, step time: 0.5193\n",
      "326/388, train_loss: 0.1376, step time: 0.5111\n",
      "327/388, train_loss: 0.0595, step time: 0.4915\n",
      "328/388, train_loss: 0.3153, step time: 0.4881\n",
      "329/388, train_loss: 0.2649, step time: 1.2029\n",
      "330/388, train_loss: 0.3674, step time: 0.5379\n",
      "331/388, train_loss: 0.1923, step time: 0.5054\n",
      "332/388, train_loss: 0.0909, step time: 0.5065\n",
      "333/388, train_loss: 0.1780, step time: 0.4886\n",
      "334/388, train_loss: 0.1136, step time: 0.4764\n",
      "335/388, train_loss: 0.3657, step time: 0.9819\n",
      "336/388, train_loss: 0.0686, step time: 0.5388\n",
      "337/388, train_loss: 0.2347, step time: 0.5087\n",
      "338/388, train_loss: 0.1501, step time: 0.4992\n",
      "339/388, train_loss: 0.2321, step time: 0.4817\n",
      "340/388, train_loss: 0.1633, step time: 0.4867\n",
      "341/388, train_loss: 0.3246, step time: 0.4942\n",
      "342/388, train_loss: 0.5000, step time: 0.5349\n",
      "343/388, train_loss: 0.1049, step time: 0.5115\n",
      "344/388, train_loss: 0.2143, step time: 0.5005\n",
      "345/388, train_loss: 0.1717, step time: 0.4818\n",
      "346/388, train_loss: 0.0597, step time: 0.7911\n",
      "347/388, train_loss: 0.0739, step time: 0.5453\n",
      "348/388, train_loss: 0.0571, step time: 0.5075\n",
      "349/388, train_loss: 0.2055, step time: 0.4952\n",
      "350/388, train_loss: 0.1582, step time: 0.5039\n",
      "351/388, train_loss: 0.1906, step time: 0.4966\n",
      "352/388, train_loss: 0.0883, step time: 0.4934\n",
      "353/388, train_loss: 0.1020, step time: 0.5240\n",
      "354/388, train_loss: 0.2925, step time: 0.5188\n",
      "355/388, train_loss: 0.2232, step time: 0.5026\n",
      "356/388, train_loss: 0.1569, step time: 0.4853\n",
      "357/388, train_loss: 0.1152, step time: 1.0722\n",
      "358/388, train_loss: 0.1765, step time: 0.5365\n",
      "359/388, train_loss: 0.1455, step time: 0.5186\n",
      "360/388, train_loss: 0.3617, step time: 0.5008\n",
      "361/388, train_loss: 0.1554, step time: 0.4896\n",
      "362/388, train_loss: 0.1949, step time: 0.4855\n",
      "363/388, train_loss: 0.2190, step time: 0.4991\n",
      "364/388, train_loss: 0.4883, step time: 0.9907\n",
      "365/388, train_loss: 0.3296, step time: 0.5400\n",
      "366/388, train_loss: 0.1488, step time: 0.5192\n",
      "367/388, train_loss: 0.0715, step time: 0.4937\n",
      "368/388, train_loss: 0.0294, step time: 0.4976\n",
      "369/388, train_loss: 0.1788, step time: 0.4830\n",
      "370/388, train_loss: 0.1680, step time: 0.4854\n",
      "371/388, train_loss: 0.0616, step time: 1.0320\n",
      "372/388, train_loss: 0.1960, step time: 0.5567\n",
      "373/388, train_loss: 0.0882, step time: 0.5284\n",
      "374/388, train_loss: 0.2332, step time: 0.4906\n",
      "375/388, train_loss: 0.1325, step time: 0.4970\n",
      "376/388, train_loss: 0.3914, step time: 0.4897\n",
      "377/388, train_loss: 0.1627, step time: 0.4812\n",
      "378/388, train_loss: 0.1144, step time: 0.5007\n",
      "379/388, train_loss: 0.2712, step time: 0.4884\n",
      "380/388, train_loss: 0.3756, step time: 0.4900\n",
      "381/388, train_loss: 0.1285, step time: 0.4982\n",
      "382/388, train_loss: 0.1239, step time: 0.4761\n",
      "383/388, train_loss: 0.1410, step time: 1.1436\n",
      "384/388, train_loss: 0.1497, step time: 0.5392\n",
      "385/388, train_loss: 0.2634, step time: 0.5321\n",
      "386/388, train_loss: 0.1120, step time: 0.5089\n",
      "387/388, train_loss: 0.0874, step time: 0.4939\n",
      "388/388, train_loss: 0.2088, step time: 0.4850\n",
      "epoch 85 average loss: 0.1730\n",
      "current epoch: 85 current mean dice: 0.7762 tc: 0.8238 wt: 0.9055 et: 0.5993\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 85 is: 302.7262\n",
      "----------\n",
      "epoch 86/300\n",
      "1/388, train_loss: 0.1174, step time: 0.4708\n",
      "2/388, train_loss: 0.3057, step time: 0.4909\n",
      "3/388, train_loss: 0.2823, step time: 0.5320\n",
      "4/388, train_loss: 0.0967, step time: 1.1716\n",
      "5/388, train_loss: 0.2111, step time: 0.5352\n",
      "6/388, train_loss: 0.0877, step time: 0.5170\n",
      "7/388, train_loss: 0.1708, step time: 0.4887\n",
      "8/388, train_loss: 0.1858, step time: 0.6358\n",
      "9/388, train_loss: 0.1645, step time: 0.5488\n",
      "10/388, train_loss: 0.3895, step time: 0.5123\n",
      "11/388, train_loss: 0.1612, step time: 0.5027\n",
      "12/388, train_loss: 0.3281, step time: 0.4931\n",
      "13/388, train_loss: 0.1435, step time: 0.5194\n",
      "14/388, train_loss: 0.2350, step time: 0.5532\n",
      "15/388, train_loss: 0.1349, step time: 0.6794\n",
      "16/388, train_loss: 0.0834, step time: 0.5941\n",
      "17/388, train_loss: 0.2002, step time: 0.5490\n",
      "18/388, train_loss: 0.0962, step time: 0.5232\n",
      "19/388, train_loss: 0.0802, step time: 0.5104\n",
      "20/388, train_loss: 0.1807, step time: 0.5934\n",
      "21/388, train_loss: 0.1558, step time: 0.5634\n",
      "22/388, train_loss: 0.1364, step time: 0.5561\n",
      "23/388, train_loss: 0.2603, step time: 0.5393\n",
      "24/388, train_loss: 0.3812, step time: 0.5283\n",
      "25/388, train_loss: 0.2612, step time: 0.5455\n",
      "26/388, train_loss: 0.0469, step time: 0.6405\n",
      "27/388, train_loss: 0.0667, step time: 0.5517\n",
      "28/388, train_loss: 0.1267, step time: 0.5130\n",
      "29/388, train_loss: 0.0894, step time: 1.0082\n",
      "30/388, train_loss: 0.2173, step time: 0.5601\n",
      "31/388, train_loss: 0.2101, step time: 0.5273\n",
      "32/388, train_loss: 0.0783, step time: 0.5106\n",
      "33/388, train_loss: 0.0803, step time: 0.4943\n",
      "34/388, train_loss: 0.1357, step time: 0.5191\n",
      "35/388, train_loss: 0.0838, step time: 0.5100\n",
      "36/388, train_loss: 0.4248, step time: 0.9055\n",
      "37/388, train_loss: 0.3037, step time: 0.5507\n",
      "38/388, train_loss: 0.2283, step time: 0.5160\n",
      "39/388, train_loss: 0.3275, step time: 0.4995\n",
      "40/388, train_loss: 0.2434, step time: 0.9102\n",
      "41/388, train_loss: 0.0896, step time: 0.5585\n",
      "42/388, train_loss: 0.2113, step time: 0.5311\n",
      "43/388, train_loss: 0.0699, step time: 0.5066\n",
      "44/388, train_loss: 0.0940, step time: 0.4997\n",
      "45/388, train_loss: 0.0626, step time: 0.5166\n",
      "46/388, train_loss: 0.1984, step time: 0.4957\n",
      "47/388, train_loss: 0.2582, step time: 0.7478\n",
      "48/388, train_loss: 0.1165, step time: 0.5808\n",
      "49/388, train_loss: 0.0974, step time: 0.5624\n",
      "50/388, train_loss: 0.2101, step time: 0.5223\n",
      "51/388, train_loss: 0.1022, step time: 0.5128\n",
      "52/388, train_loss: 0.1034, step time: 0.5003\n",
      "53/388, train_loss: 0.1439, step time: 0.5068\n",
      "54/388, train_loss: 0.1776, step time: 0.5227\n",
      "55/388, train_loss: 0.2441, step time: 0.4995\n",
      "56/388, train_loss: 0.1757, step time: 0.9753\n",
      "57/388, train_loss: 0.1873, step time: 0.5373\n",
      "58/388, train_loss: 0.1464, step time: 0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/388, train_loss: 0.0980, step time: 0.4978\n",
      "60/388, train_loss: 0.1127, step time: 0.5166\n",
      "61/388, train_loss: 0.2105, step time: 0.5158\n",
      "62/388, train_loss: 0.2365, step time: 0.4948\n",
      "63/388, train_loss: 0.0996, step time: 0.4995\n",
      "64/388, train_loss: 0.3421, step time: 0.4869\n",
      "65/388, train_loss: 0.1393, step time: 0.5546\n",
      "66/388, train_loss: 0.4993, step time: 0.5212\n",
      "67/388, train_loss: 0.0966, step time: 0.5077\n",
      "68/388, train_loss: 0.1286, step time: 0.5296\n",
      "69/388, train_loss: 0.2098, step time: 0.5245\n",
      "70/388, train_loss: 0.2754, step time: 0.5031\n",
      "71/388, train_loss: 0.1503, step time: 1.0491\n",
      "72/388, train_loss: 0.0963, step time: 0.5496\n",
      "73/388, train_loss: 0.4087, step time: 0.5237\n",
      "74/388, train_loss: 0.1647, step time: 0.5066\n",
      "75/388, train_loss: 0.1324, step time: 0.5001\n",
      "76/388, train_loss: 0.1189, step time: 0.4999\n",
      "77/388, train_loss: 0.2415, step time: 0.7959\n",
      "78/388, train_loss: 0.0562, step time: 0.5815\n",
      "79/388, train_loss: 0.2855, step time: 0.5331\n",
      "80/388, train_loss: 0.0876, step time: 0.5004\n",
      "81/388, train_loss: 0.1876, step time: 0.5478\n",
      "82/388, train_loss: 0.0984, step time: 0.5255\n",
      "83/388, train_loss: 0.0348, step time: 0.5035\n",
      "84/388, train_loss: 0.3262, step time: 0.4921\n",
      "85/388, train_loss: 0.2225, step time: 0.5042\n",
      "86/388, train_loss: 0.2700, step time: 0.5556\n",
      "87/388, train_loss: 0.1387, step time: 0.5151\n",
      "88/388, train_loss: 0.3175, step time: 0.5088\n",
      "89/388, train_loss: 0.0813, step time: 0.5080\n",
      "90/388, train_loss: 0.0817, step time: 0.5074\n",
      "91/388, train_loss: 0.1963, step time: 0.7323\n",
      "92/388, train_loss: 0.0847, step time: 0.5734\n",
      "93/388, train_loss: 0.1074, step time: 0.5416\n",
      "94/388, train_loss: 0.2445, step time: 0.5231\n",
      "95/388, train_loss: 0.0988, step time: 0.5546\n",
      "96/388, train_loss: 0.0976, step time: 0.5115\n",
      "97/388, train_loss: 0.1383, step time: 0.5098\n",
      "98/388, train_loss: 0.6936, step time: 0.4906\n",
      "99/388, train_loss: 0.1461, step time: 0.7524\n",
      "100/388, train_loss: 0.0626, step time: 0.5784\n",
      "101/388, train_loss: 0.2657, step time: 0.5531\n",
      "102/388, train_loss: 0.1228, step time: 0.5310\n",
      "103/388, train_loss: 0.2613, step time: 0.5125\n",
      "104/388, train_loss: 0.0304, step time: 0.5438\n",
      "105/388, train_loss: 0.1239, step time: 0.5343\n",
      "106/388, train_loss: 0.0731, step time: 0.5060\n",
      "107/388, train_loss: 0.2168, step time: 0.5259\n",
      "108/388, train_loss: 0.2183, step time: 0.5787\n",
      "109/388, train_loss: 0.0874, step time: 0.5319\n",
      "110/388, train_loss: 0.1454, step time: 0.5240\n",
      "111/388, train_loss: 0.1313, step time: 0.4966\n",
      "112/388, train_loss: 0.1480, step time: 1.1067\n",
      "113/388, train_loss: 0.2023, step time: 0.5384\n",
      "114/388, train_loss: 0.2381, step time: 0.5137\n",
      "115/388, train_loss: 0.2062, step time: 0.4982\n",
      "116/388, train_loss: 0.1528, step time: 0.4885\n",
      "117/388, train_loss: 0.1062, step time: 0.5008\n",
      "118/388, train_loss: 0.6176, step time: 0.5548\n",
      "119/388, train_loss: 0.1667, step time: 0.5046\n",
      "120/388, train_loss: 0.0531, step time: 0.5127\n",
      "121/388, train_loss: 0.0972, step time: 0.4973\n",
      "122/388, train_loss: 0.1072, step time: 0.5737\n",
      "123/388, train_loss: 0.0673, step time: 0.6011\n",
      "124/388, train_loss: 0.1642, step time: 0.5442\n",
      "125/388, train_loss: 0.1188, step time: 0.5101\n",
      "126/388, train_loss: 0.1919, step time: 0.5001\n",
      "127/388, train_loss: 0.1651, step time: 0.5183\n",
      "128/388, train_loss: 0.2434, step time: 0.5043\n",
      "129/388, train_loss: 0.0666, step time: 0.4897\n",
      "130/388, train_loss: 0.1404, step time: 0.5081\n",
      "131/388, train_loss: 0.0567, step time: 0.5105\n",
      "132/388, train_loss: 0.0786, step time: 0.5036\n",
      "133/388, train_loss: 0.2122, step time: 0.5570\n",
      "134/388, train_loss: 0.1217, step time: 0.5391\n",
      "135/388, train_loss: 0.2765, step time: 0.5231\n",
      "136/388, train_loss: 0.1696, step time: 0.4991\n",
      "137/388, train_loss: 0.3125, step time: 0.4957\n",
      "138/388, train_loss: 0.0732, step time: 1.0982\n",
      "139/388, train_loss: 0.2310, step time: 0.5448\n",
      "140/388, train_loss: 0.1477, step time: 0.5193\n",
      "141/388, train_loss: 0.2632, step time: 0.4921\n",
      "142/388, train_loss: 0.1295, step time: 0.4914\n",
      "143/388, train_loss: 0.1001, step time: 0.4809\n",
      "144/388, train_loss: 0.1515, step time: 0.5232\n",
      "145/388, train_loss: 0.1234, step time: 0.5012\n",
      "146/388, train_loss: 0.1510, step time: 0.4995\n",
      "147/388, train_loss: 0.2384, step time: 0.5193\n",
      "148/388, train_loss: 0.1204, step time: 0.5037\n",
      "149/388, train_loss: 0.2266, step time: 0.4868\n",
      "150/388, train_loss: 0.1512, step time: 1.1223\n",
      "151/388, train_loss: 0.1944, step time: 0.5488\n",
      "152/388, train_loss: 0.2453, step time: 0.5238\n",
      "153/388, train_loss: 0.3498, step time: 0.6059\n",
      "154/388, train_loss: 0.2692, step time: 0.5296\n",
      "155/388, train_loss: 0.1104, step time: 0.5001\n",
      "156/388, train_loss: 0.1943, step time: 0.4905\n",
      "157/388, train_loss: 0.0785, step time: 0.5038\n",
      "158/388, train_loss: 0.1742, step time: 0.4856\n",
      "159/388, train_loss: 0.2179, step time: 0.9957\n",
      "160/388, train_loss: 0.0545, step time: 0.5262\n",
      "161/388, train_loss: 0.1892, step time: 0.4992\n",
      "162/388, train_loss: 0.1124, step time: 0.4846\n",
      "163/388, train_loss: 0.1566, step time: 0.4965\n",
      "164/388, train_loss: 0.0739, step time: 0.4785\n",
      "165/388, train_loss: 0.1949, step time: 0.4908\n",
      "166/388, train_loss: 0.2109, step time: 0.5334\n",
      "167/388, train_loss: 0.2212, step time: 0.6210\n",
      "168/388, train_loss: 0.1977, step time: 0.5315\n",
      "169/388, train_loss: 0.1210, step time: 0.5163\n",
      "170/388, train_loss: 0.1125, step time: 0.4985\n",
      "171/388, train_loss: 0.0309, step time: 0.4980\n",
      "172/388, train_loss: 0.0908, step time: 0.5092\n",
      "173/388, train_loss: 0.0884, step time: 0.5162\n",
      "174/388, train_loss: 0.0727, step time: 0.4896\n",
      "175/388, train_loss: 0.0459, step time: 0.4849\n",
      "176/388, train_loss: 0.2790, step time: 1.2407\n",
      "177/388, train_loss: 0.0819, step time: 0.5358\n",
      "178/388, train_loss: 0.0757, step time: 0.4946\n",
      "179/388, train_loss: 0.2517, step time: 0.4850\n",
      "180/388, train_loss: 0.1030, step time: 0.5101\n",
      "181/388, train_loss: 0.1153, step time: 0.5008\n",
      "182/388, train_loss: 0.2361, step time: 0.4985\n",
      "183/388, train_loss: 0.1816, step time: 0.9635\n",
      "184/388, train_loss: 0.2196, step time: 0.5295\n",
      "185/388, train_loss: 0.1201, step time: 0.5113\n",
      "186/388, train_loss: 0.1209, step time: 0.4843\n",
      "187/388, train_loss: 0.0835, step time: 0.4958\n",
      "188/388, train_loss: 0.1704, step time: 0.4973\n",
      "189/388, train_loss: 0.0651, step time: 0.5099\n",
      "190/388, train_loss: 0.1788, step time: 0.5256\n",
      "191/388, train_loss: 0.0871, step time: 0.5740\n",
      "192/388, train_loss: 0.0968, step time: 0.5282\n",
      "193/388, train_loss: 0.2051, step time: 0.5066\n",
      "194/388, train_loss: 0.0585, step time: 0.4996\n",
      "195/388, train_loss: 0.0344, step time: 0.4854\n",
      "196/388, train_loss: 0.1305, step time: 0.8002\n",
      "197/388, train_loss: 0.1339, step time: 0.5561\n",
      "198/388, train_loss: 0.2054, step time: 0.5186\n",
      "199/388, train_loss: 0.5182, step time: 0.4918\n",
      "200/388, train_loss: 0.1630, step time: 0.4954\n",
      "201/388, train_loss: 0.0814, step time: 0.4836\n",
      "202/388, train_loss: 0.4335, step time: 0.4805\n",
      "203/388, train_loss: 0.2369, step time: 0.5221\n",
      "204/388, train_loss: 0.1065, step time: 0.5030\n",
      "205/388, train_loss: 0.1253, step time: 0.5246\n",
      "206/388, train_loss: 0.1887, step time: 0.5192\n",
      "207/388, train_loss: 0.5151, step time: 0.5157\n",
      "208/388, train_loss: 0.1179, step time: 0.5423\n",
      "209/388, train_loss: 0.1126, step time: 0.5211\n",
      "210/388, train_loss: 0.5920, step time: 0.5110\n",
      "211/388, train_loss: 0.2438, step time: 0.4867\n",
      "212/388, train_loss: 0.4261, step time: 1.1485\n",
      "213/388, train_loss: 0.1227, step time: 0.5358\n",
      "214/388, train_loss: 0.2890, step time: 0.5199\n",
      "215/388, train_loss: 0.0470, step time: 0.4889\n",
      "216/388, train_loss: 0.1649, step time: 0.4881\n",
      "217/388, train_loss: 0.0877, step time: 0.4940\n",
      "218/388, train_loss: 0.2966, step time: 0.4787\n",
      "219/388, train_loss: 0.0812, step time: 0.4877\n",
      "220/388, train_loss: 0.2299, step time: 0.4950\n",
      "221/388, train_loss: 0.1464, step time: 0.4873\n",
      "222/388, train_loss: 0.1394, step time: 0.4756\n",
      "223/388, train_loss: 0.1235, step time: 0.4891\n",
      "224/388, train_loss: 0.1415, step time: 0.5007\n",
      "225/388, train_loss: 0.0619, step time: 0.5500\n",
      "226/388, train_loss: 0.1560, step time: 0.6731\n",
      "227/388, train_loss: 0.2052, step time: 0.5428\n",
      "228/388, train_loss: 0.0881, step time: 0.5116\n",
      "229/388, train_loss: 0.0739, step time: 0.4926\n",
      "230/388, train_loss: 0.0489, step time: 0.5112\n",
      "231/388, train_loss: 0.0908, step time: 0.4958\n",
      "232/388, train_loss: 0.1024, step time: 0.4873\n",
      "233/388, train_loss: 0.1026, step time: 0.4893\n",
      "234/388, train_loss: 0.0636, step time: 0.4758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/388, train_loss: 0.0849, step time: 0.4781\n",
      "236/388, train_loss: 0.2007, step time: 0.4755\n",
      "237/388, train_loss: 0.0909, step time: 0.5172\n",
      "238/388, train_loss: 0.1164, step time: 0.5162\n",
      "239/388, train_loss: 0.2491, step time: 0.5164\n",
      "240/388, train_loss: 0.0939, step time: 0.5589\n",
      "241/388, train_loss: 0.2647, step time: 0.5385\n",
      "242/388, train_loss: 0.4285, step time: 0.5135\n",
      "243/388, train_loss: 0.2544, step time: 0.5059\n",
      "244/388, train_loss: 0.0726, step time: 0.5016\n",
      "245/388, train_loss: 0.5469, step time: 0.5144\n",
      "246/388, train_loss: 0.1344, step time: 0.4960\n",
      "247/388, train_loss: 0.0828, step time: 0.4957\n",
      "248/388, train_loss: 0.1133, step time: 0.4961\n",
      "249/388, train_loss: 0.0828, step time: 0.5079\n",
      "250/388, train_loss: 0.1009, step time: 0.5035\n",
      "251/388, train_loss: 0.1751, step time: 0.4927\n",
      "252/388, train_loss: 0.1535, step time: 0.4923\n",
      "253/388, train_loss: 0.3822, step time: 0.4867\n",
      "254/388, train_loss: 0.1730, step time: 0.4771\n",
      "255/388, train_loss: 0.1019, step time: 0.9373\n",
      "256/388, train_loss: 0.1556, step time: 0.5469\n",
      "257/388, train_loss: 0.0462, step time: 0.5213\n",
      "258/388, train_loss: 0.2499, step time: 0.4913\n",
      "259/388, train_loss: 0.1624, step time: 0.4838\n",
      "260/388, train_loss: 0.1696, step time: 0.5045\n",
      "261/388, train_loss: 0.0906, step time: 0.5076\n",
      "262/388, train_loss: 0.2764, step time: 0.4997\n",
      "263/388, train_loss: 0.1091, step time: 0.4825\n",
      "264/388, train_loss: 0.1680, step time: 0.5186\n",
      "265/388, train_loss: 0.3466, step time: 0.4890\n",
      "266/388, train_loss: 0.1749, step time: 0.4807\n",
      "267/388, train_loss: 0.1251, step time: 0.4872\n",
      "268/388, train_loss: 0.2659, step time: 0.4863\n",
      "269/388, train_loss: 0.3147, step time: 0.5002\n",
      "270/388, train_loss: 0.1217, step time: 0.4929\n",
      "271/388, train_loss: 0.1641, step time: 0.4894\n",
      "272/388, train_loss: 0.1701, step time: 0.5058\n",
      "273/388, train_loss: 0.1330, step time: 0.5010\n",
      "274/388, train_loss: 0.1398, step time: 0.4935\n",
      "275/388, train_loss: 0.1561, step time: 0.4842\n",
      "276/388, train_loss: 0.0881, step time: 0.4947\n",
      "277/388, train_loss: 0.1843, step time: 0.4963\n",
      "278/388, train_loss: 0.2086, step time: 1.1968\n",
      "279/388, train_loss: 0.1352, step time: 0.5572\n",
      "280/388, train_loss: 0.2706, step time: 0.5218\n",
      "281/388, train_loss: 0.2034, step time: 0.5053\n",
      "282/388, train_loss: 0.2118, step time: 0.4973\n",
      "283/388, train_loss: 0.2147, step time: 0.4853\n",
      "284/388, train_loss: 0.2046, step time: 0.4815\n",
      "285/388, train_loss: 0.2093, step time: 1.1553\n",
      "286/388, train_loss: 0.0824, step time: 0.5278\n",
      "287/388, train_loss: 0.2088, step time: 0.5094\n",
      "288/388, train_loss: 0.0995, step time: 0.4905\n",
      "289/388, train_loss: 0.1027, step time: 0.4891\n",
      "290/388, train_loss: 0.2982, step time: 0.4883\n",
      "291/388, train_loss: 0.0894, step time: 0.7870\n",
      "292/388, train_loss: 0.1563, step time: 0.5517\n",
      "293/388, train_loss: 0.2902, step time: 0.5260\n",
      "294/388, train_loss: 0.1611, step time: 0.5056\n",
      "295/388, train_loss: 0.1513, step time: 0.4987\n",
      "296/388, train_loss: 0.1904, step time: 0.4793\n",
      "297/388, train_loss: 0.0921, step time: 0.4925\n",
      "298/388, train_loss: 0.0931, step time: 0.5135\n",
      "299/388, train_loss: 0.2484, step time: 0.4964\n",
      "300/388, train_loss: 0.1448, step time: 0.4832\n",
      "301/388, train_loss: 0.1679, step time: 0.5084\n",
      "302/388, train_loss: 0.1986, step time: 0.4930\n",
      "303/388, train_loss: 0.2839, step time: 0.5057\n",
      "304/388, train_loss: 0.1206, step time: 0.5181\n",
      "305/388, train_loss: 0.2246, step time: 0.5082\n",
      "306/388, train_loss: 0.0925, step time: 0.4981\n",
      "307/388, train_loss: 0.2878, step time: 0.5054\n",
      "308/388, train_loss: 0.1334, step time: 0.5891\n",
      "309/388, train_loss: 0.1978, step time: 0.5260\n",
      "310/388, train_loss: 0.2110, step time: 0.4927\n",
      "311/388, train_loss: 0.1884, step time: 0.5491\n",
      "312/388, train_loss: 0.1845, step time: 0.5303\n",
      "313/388, train_loss: 0.0887, step time: 0.4970\n",
      "314/388, train_loss: 0.0779, step time: 0.4889\n",
      "315/388, train_loss: 0.2821, step time: 0.4918\n",
      "316/388, train_loss: 0.2294, step time: 0.4790\n",
      "317/388, train_loss: 0.0716, step time: 0.5521\n",
      "318/388, train_loss: 0.0917, step time: 0.5345\n",
      "319/388, train_loss: 0.2630, step time: 0.4965\n",
      "320/388, train_loss: 0.2901, step time: 0.4807\n",
      "321/388, train_loss: 0.0589, step time: 1.0803\n",
      "322/388, train_loss: 0.1786, step time: 0.5286\n",
      "323/388, train_loss: 0.1357, step time: 0.5032\n",
      "324/388, train_loss: 0.0650, step time: 0.4979\n",
      "325/388, train_loss: 0.1558, step time: 0.4814\n",
      "326/388, train_loss: 0.0760, step time: 0.4829\n",
      "327/388, train_loss: 0.1477, step time: 0.4989\n",
      "328/388, train_loss: 0.1572, step time: 0.4995\n",
      "329/388, train_loss: 0.0918, step time: 0.4994\n",
      "330/388, train_loss: 0.2595, step time: 0.4867\n",
      "331/388, train_loss: 0.2068, step time: 0.4919\n",
      "332/388, train_loss: 0.2022, step time: 0.4801\n",
      "333/388, train_loss: 0.0732, step time: 0.9735\n",
      "334/388, train_loss: 0.1001, step time: 0.5299\n",
      "335/388, train_loss: 0.0683, step time: 0.5090\n",
      "336/388, train_loss: 0.0969, step time: 0.4922\n",
      "337/388, train_loss: 0.3203, step time: 0.4869\n",
      "338/388, train_loss: 0.0446, step time: 0.4920\n",
      "339/388, train_loss: 0.3272, step time: 0.4767\n",
      "340/388, train_loss: 0.0698, step time: 0.5902\n",
      "341/388, train_loss: 0.1264, step time: 0.5423\n",
      "342/388, train_loss: 0.0967, step time: 0.5028\n",
      "343/388, train_loss: 0.1699, step time: 0.5071\n",
      "344/388, train_loss: 0.1809, step time: 0.6006\n",
      "345/388, train_loss: 0.2157, step time: 0.5470\n",
      "346/388, train_loss: 0.1459, step time: 0.5122\n",
      "347/388, train_loss: 0.1229, step time: 0.5056\n",
      "348/388, train_loss: 0.2388, step time: 0.4879\n",
      "349/388, train_loss: 0.0910, step time: 0.4839\n",
      "350/388, train_loss: 0.0845, step time: 0.5213\n",
      "351/388, train_loss: 0.3491, step time: 0.5119\n",
      "352/388, train_loss: 0.1895, step time: 0.5013\n",
      "353/388, train_loss: 0.2638, step time: 0.4908\n",
      "354/388, train_loss: 0.1203, step time: 0.4923\n",
      "355/388, train_loss: 0.3483, step time: 0.4791\n",
      "356/388, train_loss: 0.1243, step time: 0.4836\n",
      "357/388, train_loss: 0.1012, step time: 0.4827\n",
      "358/388, train_loss: 0.1122, step time: 0.4741\n",
      "359/388, train_loss: 0.0719, step time: 0.4829\n",
      "360/388, train_loss: 0.1905, step time: 0.5012\n",
      "361/388, train_loss: 0.1101, step time: 0.5367\n",
      "362/388, train_loss: 0.4115, step time: 0.5258\n",
      "363/388, train_loss: 0.3591, step time: 0.5107\n",
      "364/388, train_loss: 0.1616, step time: 1.1736\n",
      "365/388, train_loss: 0.2660, step time: 0.5373\n",
      "366/388, train_loss: 0.1309, step time: 0.4946\n",
      "367/388, train_loss: 0.2226, step time: 0.4925\n",
      "368/388, train_loss: 0.1788, step time: 0.4769\n",
      "369/388, train_loss: 0.1966, step time: 0.4757\n",
      "370/388, train_loss: 0.1143, step time: 1.0016\n",
      "371/388, train_loss: 0.0755, step time: 0.5283\n",
      "372/388, train_loss: 0.1142, step time: 0.5105\n",
      "373/388, train_loss: 0.1362, step time: 0.4877\n",
      "374/388, train_loss: 0.1454, step time: 0.4929\n",
      "375/388, train_loss: 0.0688, step time: 0.4876\n",
      "376/388, train_loss: 0.0915, step time: 0.4904\n",
      "377/388, train_loss: 0.1497, step time: 0.4863\n",
      "378/388, train_loss: 0.1880, step time: 1.0153\n",
      "379/388, train_loss: 0.0757, step time: 0.5421\n",
      "380/388, train_loss: 0.1628, step time: 0.5141\n",
      "381/388, train_loss: 0.0654, step time: 0.4839\n",
      "382/388, train_loss: 0.4386, step time: 0.4813\n",
      "383/388, train_loss: 0.0967, step time: 0.4988\n",
      "384/388, train_loss: 0.1123, step time: 0.4809\n",
      "385/388, train_loss: 0.0459, step time: 0.5087\n",
      "386/388, train_loss: 0.0678, step time: 0.4843\n",
      "387/388, train_loss: 0.2808, step time: 0.4797\n",
      "388/388, train_loss: 0.1928, step time: 0.4807\n",
      "epoch 86 average loss: 0.1710\n",
      "current epoch: 86 current mean dice: 0.7745 tc: 0.8211 wt: 0.9084 et: 0.5939\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 86 is: 301.0522\n",
      "----------\n",
      "epoch 87/300\n",
      "1/388, train_loss: 0.1620, step time: 0.4800\n",
      "2/388, train_loss: 0.1054, step time: 0.4860\n",
      "3/388, train_loss: 0.1535, step time: 1.2550\n",
      "4/388, train_loss: 0.2546, step time: 0.5652\n",
      "5/388, train_loss: 0.2514, step time: 0.6181\n",
      "6/388, train_loss: 0.1154, step time: 0.5553\n",
      "7/388, train_loss: 0.1023, step time: 0.5322\n",
      "8/388, train_loss: 0.1945, step time: 0.5486\n",
      "9/388, train_loss: 0.0824, step time: 0.5787\n",
      "10/388, train_loss: 0.0928, step time: 0.5361\n",
      "11/388, train_loss: 0.0499, step time: 0.5579\n",
      "12/388, train_loss: 0.2450, step time: 0.5419\n",
      "13/388, train_loss: 0.0762, step time: 0.5155\n",
      "14/388, train_loss: 0.1461, step time: 0.5174\n",
      "15/388, train_loss: 0.1034, step time: 0.5563\n",
      "16/388, train_loss: 0.1420, step time: 0.5330\n",
      "17/388, train_loss: 0.0458, step time: 0.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/388, train_loss: 0.1006, step time: 0.5715\n",
      "19/388, train_loss: 0.0695, step time: 0.5364\n",
      "20/388, train_loss: 0.1430, step time: 0.5057\n",
      "21/388, train_loss: 0.1870, step time: 0.5324\n",
      "22/388, train_loss: 0.0864, step time: 0.5280\n",
      "23/388, train_loss: 0.1133, step time: 0.6184\n",
      "24/388, train_loss: 0.0855, step time: 0.5895\n",
      "25/388, train_loss: 0.1224, step time: 0.5424\n",
      "26/388, train_loss: 0.4691, step time: 0.5111\n",
      "27/388, train_loss: 0.1218, step time: 0.6427\n",
      "28/388, train_loss: 0.1993, step time: 0.5621\n",
      "29/388, train_loss: 0.2146, step time: 0.5112\n",
      "30/388, train_loss: 0.2079, step time: 0.4963\n",
      "31/388, train_loss: 0.0268, step time: 0.9702\n",
      "32/388, train_loss: 0.1564, step time: 0.5512\n",
      "33/388, train_loss: 0.1037, step time: 0.5109\n",
      "34/388, train_loss: 0.1038, step time: 0.4822\n",
      "35/388, train_loss: 0.1973, step time: 0.4786\n",
      "36/388, train_loss: 0.1626, step time: 0.5351\n",
      "37/388, train_loss: 0.2122, step time: 0.5016\n",
      "38/388, train_loss: 0.1422, step time: 0.9994\n",
      "39/388, train_loss: 0.2478, step time: 0.5505\n",
      "40/388, train_loss: 0.0450, step time: 0.5244\n",
      "41/388, train_loss: 0.0799, step time: 0.5121\n",
      "42/388, train_loss: 0.1041, step time: 0.5013\n",
      "43/388, train_loss: 0.1056, step time: 0.4938\n",
      "44/388, train_loss: 0.2169, step time: 1.1968\n",
      "45/388, train_loss: 0.0679, step time: 0.5337\n",
      "46/388, train_loss: 0.2355, step time: 0.5037\n",
      "47/388, train_loss: 0.2072, step time: 0.4938\n",
      "48/388, train_loss: 0.2199, step time: 0.4931\n",
      "49/388, train_loss: 0.3308, step time: 0.5430\n",
      "50/388, train_loss: 0.2854, step time: 0.5311\n",
      "51/388, train_loss: 0.1015, step time: 0.5121\n",
      "52/388, train_loss: 0.3125, step time: 0.4960\n",
      "53/388, train_loss: 0.0739, step time: 0.5184\n",
      "54/388, train_loss: 0.1018, step time: 0.5022\n",
      "55/388, train_loss: 0.1045, step time: 0.5416\n",
      "56/388, train_loss: 0.1099, step time: 0.5174\n",
      "57/388, train_loss: 0.2558, step time: 0.4969\n",
      "58/388, train_loss: 0.2564, step time: 1.1789\n",
      "59/388, train_loss: 0.4958, step time: 0.5283\n",
      "60/388, train_loss: 0.1441, step time: 0.5143\n",
      "61/388, train_loss: 0.0781, step time: 0.4859\n",
      "62/388, train_loss: 0.1869, step time: 0.4868\n",
      "63/388, train_loss: 0.0984, step time: 0.8351\n",
      "64/388, train_loss: 0.0617, step time: 0.5549\n",
      "65/388, train_loss: 0.2920, step time: 0.5277\n",
      "66/388, train_loss: 0.1175, step time: 0.4917\n",
      "67/388, train_loss: 0.4952, step time: 0.5062\n",
      "68/388, train_loss: 0.1936, step time: 0.4936\n",
      "69/388, train_loss: 0.1009, step time: 0.4910\n",
      "70/388, train_loss: 0.1324, step time: 0.4781\n",
      "71/388, train_loss: 0.2897, step time: 0.5077\n",
      "72/388, train_loss: 0.2917, step time: 0.5127\n",
      "73/388, train_loss: 0.2020, step time: 0.6186\n",
      "74/388, train_loss: 0.1398, step time: 0.5669\n",
      "75/388, train_loss: 0.0893, step time: 0.5612\n",
      "76/388, train_loss: 0.2095, step time: 0.5337\n",
      "77/388, train_loss: 0.2847, step time: 0.5085\n",
      "78/388, train_loss: 0.0962, step time: 0.4996\n",
      "79/388, train_loss: 0.1984, step time: 0.5036\n",
      "80/388, train_loss: 0.1517, step time: 0.4927\n",
      "81/388, train_loss: 0.2094, step time: 0.5160\n",
      "82/388, train_loss: 0.1549, step time: 0.5041\n",
      "83/388, train_loss: 0.1828, step time: 0.4972\n",
      "84/388, train_loss: 0.5783, step time: 0.5029\n",
      "85/388, train_loss: 0.0937, step time: 1.1796\n",
      "86/388, train_loss: 0.1670, step time: 0.5447\n",
      "87/388, train_loss: 0.1591, step time: 0.5144\n",
      "88/388, train_loss: 0.1405, step time: 0.5206\n",
      "89/388, train_loss: 0.5130, step time: 0.6029\n",
      "90/388, train_loss: 0.3171, step time: 0.5303\n",
      "91/388, train_loss: 0.1040, step time: 0.5175\n",
      "92/388, train_loss: 0.2366, step time: 0.4948\n",
      "93/388, train_loss: 0.1763, step time: 0.4982\n",
      "94/388, train_loss: 0.2647, step time: 1.0766\n",
      "95/388, train_loss: 0.1002, step time: 0.5404\n",
      "96/388, train_loss: 0.1058, step time: 0.5102\n",
      "97/388, train_loss: 0.1083, step time: 0.5004\n",
      "98/388, train_loss: 0.1045, step time: 0.4904\n",
      "99/388, train_loss: 0.4695, step time: 0.7075\n",
      "100/388, train_loss: 0.0813, step time: 0.5436\n",
      "101/388, train_loss: 0.4553, step time: 0.5146\n",
      "102/388, train_loss: 0.0589, step time: 0.5048\n",
      "103/388, train_loss: 0.1164, step time: 0.4882\n",
      "104/388, train_loss: 0.0693, step time: 0.4857\n",
      "105/388, train_loss: 0.1616, step time: 0.4884\n",
      "106/388, train_loss: 0.1833, step time: 0.4772\n",
      "107/388, train_loss: 0.2639, step time: 0.4905\n",
      "108/388, train_loss: 0.1378, step time: 1.1347\n",
      "109/388, train_loss: 0.2801, step time: 0.5363\n",
      "110/388, train_loss: 0.0822, step time: 0.4948\n",
      "111/388, train_loss: 0.3116, step time: 0.4813\n",
      "112/388, train_loss: 0.1992, step time: 0.4853\n",
      "113/388, train_loss: 0.1270, step time: 0.5074\n",
      "114/388, train_loss: 0.1686, step time: 0.5590\n",
      "115/388, train_loss: 0.3883, step time: 0.5175\n",
      "116/388, train_loss: 0.1241, step time: 0.4988\n",
      "117/388, train_loss: 0.1053, step time: 0.4808\n",
      "118/388, train_loss: 0.1031, step time: 0.5027\n",
      "119/388, train_loss: 0.1831, step time: 0.4852\n",
      "120/388, train_loss: 0.1873, step time: 1.1180\n",
      "121/388, train_loss: 0.2219, step time: 0.5464\n",
      "122/388, train_loss: 0.1873, step time: 0.5152\n",
      "123/388, train_loss: 0.1462, step time: 0.5009\n",
      "124/388, train_loss: 0.0624, step time: 0.4865\n",
      "125/388, train_loss: 0.0909, step time: 0.5325\n",
      "126/388, train_loss: 0.1010, step time: 0.5153\n",
      "127/388, train_loss: 0.2688, step time: 0.5200\n",
      "128/388, train_loss: 0.0818, step time: 0.4911\n",
      "129/388, train_loss: 0.1207, step time: 0.4829\n",
      "130/388, train_loss: 0.1905, step time: 0.5198\n",
      "131/388, train_loss: 0.2086, step time: 0.5886\n",
      "132/388, train_loss: 0.1143, step time: 0.5443\n",
      "133/388, train_loss: 0.0729, step time: 0.5264\n",
      "134/388, train_loss: 0.1343, step time: 0.4989\n",
      "135/388, train_loss: 0.1063, step time: 0.4889\n",
      "136/388, train_loss: 0.1224, step time: 0.4852\n",
      "137/388, train_loss: 0.0702, step time: 0.5121\n",
      "138/388, train_loss: 0.1461, step time: 0.4929\n",
      "139/388, train_loss: 0.1047, step time: 0.5090\n",
      "140/388, train_loss: 0.0876, step time: 0.5169\n",
      "141/388, train_loss: 0.3168, step time: 0.5075\n",
      "142/388, train_loss: 0.2042, step time: 0.5629\n",
      "143/388, train_loss: 0.0723, step time: 0.5641\n",
      "144/388, train_loss: 0.0637, step time: 0.6926\n",
      "145/388, train_loss: 0.1192, step time: 0.5600\n",
      "146/388, train_loss: 0.0834, step time: 0.5194\n",
      "147/388, train_loss: 0.0861, step time: 0.5005\n",
      "148/388, train_loss: 0.1481, step time: 0.5004\n",
      "149/388, train_loss: 0.0879, step time: 0.4930\n",
      "150/388, train_loss: 0.2198, step time: 0.5254\n",
      "151/388, train_loss: 0.1274, step time: 0.5047\n",
      "152/388, train_loss: 0.1941, step time: 0.4908\n",
      "153/388, train_loss: 0.0824, step time: 0.5172\n",
      "154/388, train_loss: 0.2528, step time: 0.6376\n",
      "155/388, train_loss: 0.2396, step time: 0.5595\n",
      "156/388, train_loss: 0.0482, step time: 0.5330\n",
      "157/388, train_loss: 0.2383, step time: 0.5114\n",
      "158/388, train_loss: 0.0954, step time: 0.5837\n",
      "159/388, train_loss: 0.1665, step time: 0.5621\n",
      "160/388, train_loss: 0.0921, step time: 0.5353\n",
      "161/388, train_loss: 0.1998, step time: 0.5043\n",
      "162/388, train_loss: 0.1281, step time: 1.2828\n",
      "163/388, train_loss: 0.0308, step time: 0.5362\n",
      "164/388, train_loss: 0.1069, step time: 0.5081\n",
      "165/388, train_loss: 0.2435, step time: 0.4946\n",
      "166/388, train_loss: 0.1248, step time: 0.4855\n",
      "167/388, train_loss: 0.2420, step time: 0.4887\n",
      "168/388, train_loss: 0.0677, step time: 0.5007\n",
      "169/388, train_loss: 0.1152, step time: 0.5024\n",
      "170/388, train_loss: 0.0915, step time: 0.5044\n",
      "171/388, train_loss: 0.1656, step time: 0.5689\n",
      "172/388, train_loss: 0.2182, step time: 0.5627\n",
      "173/388, train_loss: 0.0998, step time: 0.5385\n",
      "174/388, train_loss: 0.1400, step time: 0.5082\n",
      "175/388, train_loss: 0.5595, step time: 0.4992\n",
      "176/388, train_loss: 0.1263, step time: 0.4939\n",
      "177/388, train_loss: 0.1558, step time: 0.5300\n",
      "178/388, train_loss: 0.0720, step time: 0.5013\n",
      "179/388, train_loss: 0.3206, step time: 1.1171\n",
      "180/388, train_loss: 0.1733, step time: 0.5321\n",
      "181/388, train_loss: 0.2273, step time: 0.5053\n",
      "182/388, train_loss: 0.1845, step time: 0.4993\n",
      "183/388, train_loss: 0.1184, step time: 0.4884\n",
      "184/388, train_loss: 0.4020, step time: 1.0891\n",
      "185/388, train_loss: 0.2129, step time: 0.5662\n",
      "186/388, train_loss: 0.1105, step time: 0.5161\n",
      "187/388, train_loss: 0.1284, step time: 0.5027\n",
      "188/388, train_loss: 0.1159, step time: 0.4905\n",
      "189/388, train_loss: 0.0541, step time: 0.4967\n",
      "190/388, train_loss: 0.0361, step time: 0.4813\n",
      "191/388, train_loss: 0.2313, step time: 0.4831\n",
      "192/388, train_loss: 0.1914, step time: 0.4889\n",
      "193/388, train_loss: 0.0654, step time: 1.1408\n",
      "194/388, train_loss: 0.1480, step time: 0.5468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/388, train_loss: 0.1577, step time: 0.5136\n",
      "196/388, train_loss: 0.3727, step time: 0.5049\n",
      "197/388, train_loss: 0.1304, step time: 0.4957\n",
      "198/388, train_loss: 0.1026, step time: 0.4990\n",
      "199/388, train_loss: 0.1040, step time: 0.4813\n",
      "200/388, train_loss: 0.2217, step time: 0.4801\n",
      "201/388, train_loss: 0.1494, step time: 0.5113\n",
      "202/388, train_loss: 0.3760, step time: 0.5100\n",
      "203/388, train_loss: 0.1512, step time: 0.5001\n",
      "204/388, train_loss: 0.0757, step time: 0.4940\n",
      "205/388, train_loss: 0.1291, step time: 0.4780\n",
      "206/388, train_loss: 0.1474, step time: 0.5053\n",
      "207/388, train_loss: 0.1055, step time: 0.5127\n",
      "208/388, train_loss: 0.1362, step time: 0.5319\n",
      "209/388, train_loss: 0.2443, step time: 0.5260\n",
      "210/388, train_loss: 0.1616, step time: 0.5579\n",
      "211/388, train_loss: 0.2168, step time: 0.5385\n",
      "212/388, train_loss: 0.2049, step time: 0.5023\n",
      "213/388, train_loss: 0.1725, step time: 0.5001\n",
      "214/388, train_loss: 0.4526, step time: 0.4833\n",
      "215/388, train_loss: 0.2853, step time: 1.1531\n",
      "216/388, train_loss: 0.2671, step time: 0.5334\n",
      "217/388, train_loss: 0.0788, step time: 0.5170\n",
      "218/388, train_loss: 0.0892, step time: 0.5051\n",
      "219/388, train_loss: 0.2735, step time: 0.4902\n",
      "220/388, train_loss: 0.1459, step time: 0.5019\n",
      "221/388, train_loss: 0.2031, step time: 0.4875\n",
      "222/388, train_loss: 0.1212, step time: 1.0266\n",
      "223/388, train_loss: 0.2836, step time: 0.5391\n",
      "224/388, train_loss: 0.0845, step time: 0.5166\n",
      "225/388, train_loss: 0.1243, step time: 0.5009\n",
      "226/388, train_loss: 0.2406, step time: 0.5815\n",
      "227/388, train_loss: 0.0662, step time: 0.5353\n",
      "228/388, train_loss: 0.1170, step time: 0.5231\n",
      "229/388, train_loss: 0.1985, step time: 0.5013\n",
      "230/388, train_loss: 0.3251, step time: 0.4996\n",
      "231/388, train_loss: 0.1592, step time: 0.4791\n",
      "232/388, train_loss: 0.0686, step time: 0.4985\n",
      "233/388, train_loss: 0.1046, step time: 1.0739\n",
      "234/388, train_loss: 0.0770, step time: 0.5504\n",
      "235/388, train_loss: 0.2395, step time: 0.5194\n",
      "236/388, train_loss: 0.2449, step time: 0.5036\n",
      "237/388, train_loss: 0.1251, step time: 0.5046\n",
      "238/388, train_loss: 0.0982, step time: 0.4985\n",
      "239/388, train_loss: 0.0957, step time: 0.4809\n",
      "240/388, train_loss: 0.0734, step time: 0.4884\n",
      "241/388, train_loss: 0.1859, step time: 0.4839\n",
      "242/388, train_loss: 0.3475, step time: 0.8315\n",
      "243/388, train_loss: 0.3716, step time: 0.5683\n",
      "244/388, train_loss: 0.1143, step time: 0.5166\n",
      "245/388, train_loss: 0.1066, step time: 0.4907\n",
      "246/388, train_loss: 0.3958, step time: 0.4983\n",
      "247/388, train_loss: 0.2174, step time: 0.8572\n",
      "248/388, train_loss: 0.0676, step time: 0.5308\n",
      "249/388, train_loss: 0.3350, step time: 0.5075\n",
      "250/388, train_loss: 0.1927, step time: 0.4924\n",
      "251/388, train_loss: 0.1143, step time: 0.4869\n",
      "252/388, train_loss: 0.4404, step time: 0.4979\n",
      "253/388, train_loss: 0.2115, step time: 0.5039\n",
      "254/388, train_loss: 0.0939, step time: 0.4986\n",
      "255/388, train_loss: 0.2361, step time: 0.9212\n",
      "256/388, train_loss: 0.0983, step time: 0.5497\n",
      "257/388, train_loss: 0.0791, step time: 0.5128\n",
      "258/388, train_loss: 0.2351, step time: 0.4911\n",
      "259/388, train_loss: 0.2357, step time: 0.4984\n",
      "260/388, train_loss: 0.1440, step time: 0.5047\n",
      "261/388, train_loss: 0.1098, step time: 0.5481\n",
      "262/388, train_loss: 0.1178, step time: 0.5180\n",
      "263/388, train_loss: 0.1817, step time: 0.5015\n",
      "264/388, train_loss: 0.1654, step time: 0.4992\n",
      "265/388, train_loss: 0.1035, step time: 0.4954\n",
      "266/388, train_loss: 0.1471, step time: 0.5336\n",
      "267/388, train_loss: 0.1987, step time: 0.6633\n",
      "268/388, train_loss: 0.1526, step time: 0.5326\n",
      "269/388, train_loss: 0.1575, step time: 0.5031\n",
      "270/388, train_loss: 0.1520, step time: 0.4985\n",
      "271/388, train_loss: 0.2158, step time: 0.4807\n",
      "272/388, train_loss: 0.4869, step time: 0.4806\n",
      "273/388, train_loss: 0.1920, step time: 0.9597\n",
      "274/388, train_loss: 0.3535, step time: 0.5574\n",
      "275/388, train_loss: 0.0812, step time: 0.5264\n",
      "276/388, train_loss: 0.1498, step time: 0.5034\n",
      "277/388, train_loss: 0.2457, step time: 0.4963\n",
      "278/388, train_loss: 0.3062, step time: 0.4907\n",
      "279/388, train_loss: 0.2442, step time: 0.4990\n",
      "280/388, train_loss: 0.0926, step time: 0.4846\n",
      "281/388, train_loss: 0.0571, step time: 0.4855\n",
      "282/388, train_loss: 0.1222, step time: 0.4812\n",
      "283/388, train_loss: 0.2047, step time: 0.4700\n",
      "284/388, train_loss: 0.0817, step time: 0.6455\n",
      "285/388, train_loss: 0.2921, step time: 0.5639\n",
      "286/388, train_loss: 0.0759, step time: 0.5241\n",
      "287/388, train_loss: 0.1885, step time: 0.5080\n",
      "288/388, train_loss: 0.2113, step time: 0.4890\n",
      "289/388, train_loss: 0.1336, step time: 0.4862\n",
      "290/388, train_loss: 0.2334, step time: 0.8919\n",
      "291/388, train_loss: 0.1068, step time: 0.5528\n",
      "292/388, train_loss: 0.0712, step time: 0.5186\n",
      "293/388, train_loss: 0.1017, step time: 0.4926\n",
      "294/388, train_loss: 0.0479, step time: 0.4907\n",
      "295/388, train_loss: 0.2050, step time: 0.4979\n",
      "296/388, train_loss: 0.2669, step time: 0.4794\n",
      "297/388, train_loss: 0.0826, step time: 1.0044\n",
      "298/388, train_loss: 0.0929, step time: 0.5579\n",
      "299/388, train_loss: 0.1859, step time: 0.5188\n",
      "300/388, train_loss: 0.1146, step time: 0.4860\n",
      "301/388, train_loss: 0.1582, step time: 0.4786\n",
      "302/388, train_loss: 0.3139, step time: 0.4851\n",
      "303/388, train_loss: 0.3634, step time: 0.5245\n",
      "304/388, train_loss: 0.1006, step time: 0.5037\n",
      "305/388, train_loss: 0.2231, step time: 0.4915\n",
      "306/388, train_loss: 0.0905, step time: 0.4962\n",
      "307/388, train_loss: 0.0871, step time: 0.4820\n",
      "308/388, train_loss: 0.3070, step time: 0.5046\n",
      "309/388, train_loss: 0.1183, step time: 0.5368\n",
      "310/388, train_loss: 0.2757, step time: 0.5203\n",
      "311/388, train_loss: 0.4474, step time: 0.5010\n",
      "312/388, train_loss: 0.3110, step time: 0.4998\n",
      "313/388, train_loss: 0.1865, step time: 0.4936\n",
      "314/388, train_loss: 0.0503, step time: 0.4977\n",
      "315/388, train_loss: 0.0938, step time: 0.4822\n",
      "316/388, train_loss: 0.1216, step time: 0.4987\n",
      "317/388, train_loss: 0.2713, step time: 0.4909\n",
      "318/388, train_loss: 0.4494, step time: 0.4853\n",
      "319/388, train_loss: 0.2504, step time: 1.1207\n",
      "320/388, train_loss: 0.0984, step time: 0.5374\n",
      "321/388, train_loss: 0.0822, step time: 0.5089\n",
      "322/388, train_loss: 0.1905, step time: 0.4877\n",
      "323/388, train_loss: 0.0810, step time: 0.4867\n",
      "324/388, train_loss: 0.0832, step time: 0.4905\n",
      "325/388, train_loss: 0.2789, step time: 0.4909\n",
      "326/388, train_loss: 0.1900, step time: 0.5005\n",
      "327/388, train_loss: 0.0979, step time: 0.4824\n",
      "328/388, train_loss: 0.1256, step time: 0.9955\n",
      "329/388, train_loss: 0.2559, step time: 0.5373\n",
      "330/388, train_loss: 0.3657, step time: 0.5152\n",
      "331/388, train_loss: 0.1867, step time: 0.4946\n",
      "332/388, train_loss: 0.3306, step time: 0.5107\n",
      "333/388, train_loss: 0.1386, step time: 0.4959\n",
      "334/388, train_loss: 0.1661, step time: 0.4840\n",
      "335/388, train_loss: 0.2039, step time: 0.4853\n",
      "336/388, train_loss: 0.1355, step time: 0.4744\n",
      "337/388, train_loss: 0.1114, step time: 0.5882\n",
      "338/388, train_loss: 0.1732, step time: 0.5558\n",
      "339/388, train_loss: 0.2532, step time: 0.5339\n",
      "340/388, train_loss: 0.3875, step time: 0.4984\n",
      "341/388, train_loss: 0.1435, step time: 0.5268\n",
      "342/388, train_loss: 0.1413, step time: 0.5078\n",
      "343/388, train_loss: 0.1888, step time: 0.4889\n",
      "344/388, train_loss: 0.0831, step time: 0.5213\n",
      "345/388, train_loss: 0.2085, step time: 0.5627\n",
      "346/388, train_loss: 0.1958, step time: 0.5332\n",
      "347/388, train_loss: 0.0957, step time: 0.5118\n",
      "348/388, train_loss: 0.2547, step time: 0.4949\n",
      "349/388, train_loss: 0.1210, step time: 0.4964\n",
      "350/388, train_loss: 0.1481, step time: 0.4962\n",
      "351/388, train_loss: 0.1391, step time: 0.4808\n",
      "352/388, train_loss: 0.1272, step time: 0.4996\n",
      "353/388, train_loss: 0.0646, step time: 0.4919\n",
      "354/388, train_loss: 0.2058, step time: 0.5081\n",
      "355/388, train_loss: 0.1273, step time: 0.5358\n",
      "356/388, train_loss: 0.0693, step time: 0.5118\n",
      "357/388, train_loss: 0.1451, step time: 0.4968\n",
      "358/388, train_loss: 0.2779, step time: 0.4995\n",
      "359/388, train_loss: 0.1486, step time: 0.4884\n",
      "360/388, train_loss: 0.2105, step time: 0.5372\n",
      "361/388, train_loss: 0.0621, step time: 0.5270\n",
      "362/388, train_loss: 0.0614, step time: 0.5019\n",
      "363/388, train_loss: 0.0665, step time: 0.4965\n",
      "364/388, train_loss: 0.0661, step time: 0.4811\n",
      "365/388, train_loss: 0.1500, step time: 0.4917\n",
      "366/388, train_loss: 0.0291, step time: 0.4868\n",
      "367/388, train_loss: 0.1056, step time: 0.8111\n",
      "368/388, train_loss: 0.2402, step time: 0.5452\n",
      "369/388, train_loss: 0.2873, step time: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/388, train_loss: 0.1583, step time: 0.5037\n",
      "371/388, train_loss: 0.1273, step time: 0.4944\n",
      "372/388, train_loss: 0.0490, step time: 0.4889\n",
      "373/388, train_loss: 0.1059, step time: 0.4780\n",
      "374/388, train_loss: 0.1739, step time: 0.5058\n",
      "375/388, train_loss: 0.1205, step time: 0.4891\n",
      "376/388, train_loss: 0.2135, step time: 0.5163\n",
      "377/388, train_loss: 0.4323, step time: 0.4935\n",
      "378/388, train_loss: 0.2448, step time: 0.5302\n",
      "379/388, train_loss: 0.0502, step time: 0.5027\n",
      "380/388, train_loss: 0.1681, step time: 0.5036\n",
      "381/388, train_loss: 0.1460, step time: 0.4890\n",
      "382/388, train_loss: 0.1022, step time: 0.4857\n",
      "383/388, train_loss: 0.1258, step time: 1.1847\n",
      "384/388, train_loss: 0.2921, step time: 0.5190\n",
      "385/388, train_loss: 0.2878, step time: 0.4897\n",
      "386/388, train_loss: 0.0750, step time: 0.4792\n",
      "387/388, train_loss: 0.2082, step time: 0.4752\n",
      "388/388, train_loss: 0.1754, step time: 0.4950\n",
      "epoch 87 average loss: 0.1728\n",
      "current epoch: 87 current mean dice: 0.7677 tc: 0.8169 wt: 0.9060 et: 0.5802\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 87 is: 301.5795\n",
      "----------\n",
      "epoch 88/300\n",
      "1/388, train_loss: 0.0887, step time: 0.4715\n",
      "2/388, train_loss: 0.0662, step time: 0.4838\n",
      "3/388, train_loss: 0.3915, step time: 0.7920\n",
      "4/388, train_loss: 0.2030, step time: 0.5830\n",
      "5/388, train_loss: 0.1256, step time: 0.5421\n",
      "6/388, train_loss: 0.3511, step time: 0.4942\n",
      "7/388, train_loss: 0.2819, step time: 0.5257\n",
      "8/388, train_loss: 0.2180, step time: 0.5320\n",
      "9/388, train_loss: 0.1009, step time: 0.6428\n",
      "10/388, train_loss: 0.1238, step time: 0.5735\n",
      "11/388, train_loss: 0.1286, step time: 0.5250\n",
      "12/388, train_loss: 0.1492, step time: 0.5141\n",
      "13/388, train_loss: 0.1250, step time: 0.5001\n",
      "14/388, train_loss: 0.1274, step time: 0.4922\n",
      "15/388, train_loss: 0.2869, step time: 1.0643\n",
      "16/388, train_loss: 0.0636, step time: 0.5517\n",
      "17/388, train_loss: 0.2423, step time: 0.5302\n",
      "18/388, train_loss: 0.2330, step time: 0.4983\n",
      "19/388, train_loss: 0.0745, step time: 0.4912\n",
      "20/388, train_loss: 0.1027, step time: 0.4809\n",
      "21/388, train_loss: 0.5143, step time: 0.5292\n",
      "22/388, train_loss: 0.0845, step time: 0.5349\n",
      "23/388, train_loss: 0.1424, step time: 0.5177\n",
      "24/388, train_loss: 0.1175, step time: 0.5946\n",
      "25/388, train_loss: 0.3488, step time: 0.6129\n",
      "26/388, train_loss: 0.1262, step time: 0.5436\n",
      "27/388, train_loss: 0.3250, step time: 0.5205\n",
      "28/388, train_loss: 0.1428, step time: 0.5101\n",
      "29/388, train_loss: 0.1105, step time: 0.5749\n",
      "30/388, train_loss: 0.0896, step time: 0.5778\n",
      "31/388, train_loss: 0.0496, step time: 0.5503\n",
      "32/388, train_loss: 0.2402, step time: 0.5188\n",
      "33/388, train_loss: 0.1461, step time: 0.5036\n",
      "34/388, train_loss: 0.1812, step time: 0.5152\n",
      "35/388, train_loss: 0.0425, step time: 0.5735\n",
      "36/388, train_loss: 0.0728, step time: 0.5106\n",
      "37/388, train_loss: 0.1351, step time: 0.5016\n",
      "38/388, train_loss: 0.0738, step time: 0.4965\n",
      "39/388, train_loss: 0.1079, step time: 0.5339\n",
      "40/388, train_loss: 0.0817, step time: 0.5280\n",
      "41/388, train_loss: 0.1872, step time: 0.5004\n",
      "42/388, train_loss: 0.0995, step time: 0.4980\n",
      "43/388, train_loss: 0.4491, step time: 0.5089\n",
      "44/388, train_loss: 0.3473, step time: 0.5679\n",
      "45/388, train_loss: 0.1786, step time: 0.5400\n",
      "46/388, train_loss: 0.1823, step time: 0.5069\n",
      "47/388, train_loss: 0.0827, step time: 0.5046\n",
      "48/388, train_loss: 0.2495, step time: 0.5309\n",
      "49/388, train_loss: 0.1335, step time: 0.7042\n",
      "50/388, train_loss: 0.0597, step time: 0.5339\n",
      "51/388, train_loss: 0.1838, step time: 0.4901\n",
      "52/388, train_loss: 0.0679, step time: 0.4906\n",
      "53/388, train_loss: 0.0606, step time: 0.4884\n",
      "54/388, train_loss: 0.1448, step time: 1.0647\n",
      "55/388, train_loss: 0.1920, step time: 0.5535\n",
      "56/388, train_loss: 0.2552, step time: 0.5213\n",
      "57/388, train_loss: 0.1141, step time: 0.5006\n",
      "58/388, train_loss: 0.1157, step time: 0.4973\n",
      "59/388, train_loss: 0.2745, step time: 0.5014\n",
      "60/388, train_loss: 0.3871, step time: 0.5250\n",
      "61/388, train_loss: 0.1839, step time: 0.4984\n",
      "62/388, train_loss: 0.1293, step time: 0.4865\n",
      "63/388, train_loss: 0.1669, step time: 0.4888\n",
      "64/388, train_loss: 0.2002, step time: 0.5051\n",
      "65/388, train_loss: 0.1261, step time: 0.5026\n",
      "66/388, train_loss: 0.1596, step time: 0.4873\n",
      "67/388, train_loss: 0.3420, step time: 0.5282\n",
      "68/388, train_loss: 0.1474, step time: 0.5105\n",
      "69/388, train_loss: 0.4454, step time: 0.5220\n",
      "70/388, train_loss: 0.3450, step time: 0.5121\n",
      "71/388, train_loss: 0.0852, step time: 0.5036\n",
      "72/388, train_loss: 0.0828, step time: 0.5698\n",
      "73/388, train_loss: 0.3276, step time: 0.5320\n",
      "74/388, train_loss: 0.2452, step time: 0.5111\n",
      "75/388, train_loss: 0.1957, step time: 0.5419\n",
      "76/388, train_loss: 0.1327, step time: 0.5404\n",
      "77/388, train_loss: 0.1519, step time: 0.5185\n",
      "78/388, train_loss: 0.1008, step time: 0.4983\n",
      "79/388, train_loss: 0.0885, step time: 0.5765\n",
      "80/388, train_loss: 0.2704, step time: 0.5380\n",
      "81/388, train_loss: 0.1934, step time: 0.5175\n",
      "82/388, train_loss: 0.1042, step time: 0.4845\n",
      "83/388, train_loss: 0.0823, step time: 0.5166\n",
      "84/388, train_loss: 0.2575, step time: 0.4983\n",
      "85/388, train_loss: 0.1355, step time: 0.5561\n",
      "86/388, train_loss: 0.0632, step time: 0.5216\n",
      "87/388, train_loss: 0.1742, step time: 0.5042\n",
      "88/388, train_loss: 0.4913, step time: 0.5253\n",
      "89/388, train_loss: 0.2083, step time: 0.5217\n",
      "90/388, train_loss: 0.3032, step time: 0.5486\n",
      "91/388, train_loss: 0.0947, step time: 0.6776\n",
      "92/388, train_loss: 0.2209, step time: 0.5611\n",
      "93/388, train_loss: 0.1193, step time: 0.5225\n",
      "94/388, train_loss: 0.2417, step time: 0.5185\n",
      "95/388, train_loss: 0.2453, step time: 0.5152\n",
      "96/388, train_loss: 0.0593, step time: 0.4941\n",
      "97/388, train_loss: 0.2277, step time: 0.5035\n",
      "98/388, train_loss: 0.1522, step time: 1.1856\n",
      "99/388, train_loss: 0.1934, step time: 0.5397\n",
      "100/388, train_loss: 0.2473, step time: 0.5038\n",
      "101/388, train_loss: 0.3238, step time: 0.4950\n",
      "102/388, train_loss: 0.1476, step time: 0.4938\n",
      "103/388, train_loss: 0.0976, step time: 0.4850\n",
      "104/388, train_loss: 0.1240, step time: 0.4906\n",
      "105/388, train_loss: 0.1806, step time: 0.4805\n",
      "106/388, train_loss: 0.1394, step time: 0.4784\n",
      "107/388, train_loss: 0.0833, step time: 0.9514\n",
      "108/388, train_loss: 0.0920, step time: 0.5585\n",
      "109/388, train_loss: 0.2723, step time: 0.5239\n",
      "110/388, train_loss: 0.0848, step time: 0.5025\n",
      "111/388, train_loss: 0.1237, step time: 0.4986\n",
      "112/388, train_loss: 0.0693, step time: 0.4975\n",
      "113/388, train_loss: 0.0966, step time: 0.4829\n",
      "114/388, train_loss: 0.0824, step time: 0.5050\n",
      "115/388, train_loss: 0.2370, step time: 0.4924\n",
      "116/388, train_loss: 0.0868, step time: 0.5073\n",
      "117/388, train_loss: 0.1778, step time: 0.5115\n",
      "118/388, train_loss: 0.3763, step time: 0.4979\n",
      "119/388, train_loss: 0.2726, step time: 0.5131\n",
      "120/388, train_loss: 0.1969, step time: 0.4907\n",
      "121/388, train_loss: 0.2125, step time: 1.0564\n",
      "122/388, train_loss: 0.2547, step time: 0.5393\n",
      "123/388, train_loss: 0.0853, step time: 0.5161\n",
      "124/388, train_loss: 0.2137, step time: 0.5470\n",
      "125/388, train_loss: 0.2686, step time: 0.5192\n",
      "126/388, train_loss: 0.1119, step time: 0.4957\n",
      "127/388, train_loss: 0.2004, step time: 0.4953\n",
      "128/388, train_loss: 0.2775, step time: 0.5033\n",
      "129/388, train_loss: 0.1498, step time: 0.5658\n",
      "130/388, train_loss: 0.2092, step time: 0.5404\n",
      "131/388, train_loss: 0.2220, step time: 0.5034\n",
      "132/388, train_loss: 0.0911, step time: 1.0785\n",
      "133/388, train_loss: 0.2332, step time: 0.5312\n",
      "134/388, train_loss: 0.1436, step time: 0.5005\n",
      "135/388, train_loss: 0.1986, step time: 0.4909\n",
      "136/388, train_loss: 0.2033, step time: 1.2527\n",
      "137/388, train_loss: 0.2371, step time: 0.5322\n",
      "138/388, train_loss: 0.0880, step time: 0.5014\n",
      "139/388, train_loss: 0.5692, step time: 0.4984\n",
      "140/388, train_loss: 0.1907, step time: 0.4865\n",
      "141/388, train_loss: 0.1258, step time: 0.5064\n",
      "142/388, train_loss: 0.3195, step time: 0.4857\n",
      "143/388, train_loss: 0.2143, step time: 0.5033\n",
      "144/388, train_loss: 0.0552, step time: 0.4848\n",
      "145/388, train_loss: 0.1576, step time: 0.4832\n",
      "146/388, train_loss: 0.1897, step time: 0.4992\n",
      "147/388, train_loss: 0.2438, step time: 0.5160\n",
      "148/388, train_loss: 0.2750, step time: 0.5211\n",
      "149/388, train_loss: 0.1104, step time: 0.5269\n",
      "150/388, train_loss: 0.1775, step time: 0.5057\n",
      "151/388, train_loss: 0.0740, step time: 0.5042\n",
      "152/388, train_loss: 0.0981, step time: 0.5080\n",
      "153/388, train_loss: 0.1335, step time: 0.4955\n",
      "154/388, train_loss: 0.1532, step time: 0.5020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/388, train_loss: 0.0836, step time: 0.5572\n",
      "156/388, train_loss: 0.0491, step time: 0.5652\n",
      "157/388, train_loss: 0.0652, step time: 0.5257\n",
      "158/388, train_loss: 0.0901, step time: 0.5051\n",
      "159/388, train_loss: 0.1208, step time: 0.4946\n",
      "160/388, train_loss: 0.2119, step time: 0.5319\n",
      "161/388, train_loss: 0.2636, step time: 0.5322\n",
      "162/388, train_loss: 0.2992, step time: 0.5653\n",
      "163/388, train_loss: 0.0875, step time: 0.5353\n",
      "164/388, train_loss: 0.4745, step time: 0.5144\n",
      "165/388, train_loss: 0.1740, step time: 0.5099\n",
      "166/388, train_loss: 0.1070, step time: 0.5762\n",
      "167/388, train_loss: 0.1355, step time: 0.5558\n",
      "168/388, train_loss: 0.0984, step time: 0.5214\n",
      "169/388, train_loss: 0.0877, step time: 0.5033\n",
      "170/388, train_loss: 0.1584, step time: 0.5031\n",
      "171/388, train_loss: 0.1110, step time: 0.4865\n",
      "172/388, train_loss: 0.1935, step time: 1.1476\n",
      "173/388, train_loss: 0.2153, step time: 0.5369\n",
      "174/388, train_loss: 0.1668, step time: 0.4985\n",
      "175/388, train_loss: 0.1007, step time: 0.4957\n",
      "176/388, train_loss: 0.2280, step time: 0.4856\n",
      "177/388, train_loss: 0.3320, step time: 0.4944\n",
      "178/388, train_loss: 0.1189, step time: 0.5052\n",
      "179/388, train_loss: 0.0779, step time: 0.5047\n",
      "180/388, train_loss: 0.2214, step time: 0.4923\n",
      "181/388, train_loss: 0.1310, step time: 0.5026\n",
      "182/388, train_loss: 0.1187, step time: 0.4839\n",
      "183/388, train_loss: 0.1420, step time: 0.4809\n",
      "184/388, train_loss: 0.0981, step time: 0.5091\n",
      "185/388, train_loss: 0.1741, step time: 0.5041\n",
      "186/388, train_loss: 0.2937, step time: 1.1449\n",
      "187/388, train_loss: 0.0708, step time: 0.5346\n",
      "188/388, train_loss: 0.2102, step time: 0.5160\n",
      "189/388, train_loss: 0.2204, step time: 0.4968\n",
      "190/388, train_loss: 0.2232, step time: 0.4868\n",
      "191/388, train_loss: 0.1654, step time: 0.4961\n",
      "192/388, train_loss: 0.1302, step time: 0.6396\n",
      "193/388, train_loss: 0.1325, step time: 0.5257\n",
      "194/388, train_loss: 0.1308, step time: 0.5114\n",
      "195/388, train_loss: 0.2697, step time: 0.5236\n",
      "196/388, train_loss: 0.1600, step time: 0.5206\n",
      "197/388, train_loss: 0.1032, step time: 0.4965\n",
      "198/388, train_loss: 0.0497, step time: 0.4977\n",
      "199/388, train_loss: 0.1613, step time: 0.4819\n",
      "200/388, train_loss: 0.1849, step time: 0.4809\n",
      "201/388, train_loss: 0.0985, step time: 0.4823\n",
      "202/388, train_loss: 0.1134, step time: 1.0135\n",
      "203/388, train_loss: 0.4182, step time: 0.5493\n",
      "204/388, train_loss: 0.0832, step time: 0.5234\n",
      "205/388, train_loss: 0.1048, step time: 0.5223\n",
      "206/388, train_loss: 0.0616, step time: 0.5029\n",
      "207/388, train_loss: 0.0878, step time: 0.4913\n",
      "208/388, train_loss: 0.2214, step time: 0.5278\n",
      "209/388, train_loss: 0.0949, step time: 0.5121\n",
      "210/388, train_loss: 0.0940, step time: 0.4990\n",
      "211/388, train_loss: 0.3111, step time: 0.5019\n",
      "212/388, train_loss: 0.1448, step time: 0.4957\n",
      "213/388, train_loss: 0.0597, step time: 0.9472\n",
      "214/388, train_loss: 0.1577, step time: 0.5372\n",
      "215/388, train_loss: 0.2031, step time: 0.5182\n",
      "216/388, train_loss: 0.0660, step time: 0.4983\n",
      "217/388, train_loss: 0.1810, step time: 0.4949\n",
      "218/388, train_loss: 0.2566, step time: 0.4989\n",
      "219/388, train_loss: 0.0818, step time: 1.0168\n",
      "220/388, train_loss: 0.1775, step time: 0.5416\n",
      "221/388, train_loss: 0.1987, step time: 0.5124\n",
      "222/388, train_loss: 0.2698, step time: 0.5091\n",
      "223/388, train_loss: 0.1560, step time: 0.5075\n",
      "224/388, train_loss: 0.0983, step time: 0.5335\n",
      "225/388, train_loss: 0.1728, step time: 0.5250\n",
      "226/388, train_loss: 0.2598, step time: 0.4998\n",
      "227/388, train_loss: 0.3633, step time: 0.5256\n",
      "228/388, train_loss: 0.1042, step time: 0.5568\n",
      "229/388, train_loss: 0.0770, step time: 0.5190\n",
      "230/388, train_loss: 0.1761, step time: 0.5085\n",
      "231/388, train_loss: 0.1587, step time: 0.5036\n",
      "232/388, train_loss: 0.0890, step time: 0.4946\n",
      "233/388, train_loss: 0.2042, step time: 0.5235\n",
      "234/388, train_loss: 0.2523, step time: 0.5135\n",
      "235/388, train_loss: 0.0950, step time: 0.4989\n",
      "236/388, train_loss: 0.0620, step time: 0.4960\n",
      "237/388, train_loss: 0.2377, step time: 0.4912\n",
      "238/388, train_loss: 0.1287, step time: 1.1797\n",
      "239/388, train_loss: 0.1125, step time: 0.5450\n",
      "240/388, train_loss: 0.1415, step time: 0.5141\n",
      "241/388, train_loss: 0.1704, step time: 0.4933\n",
      "242/388, train_loss: 0.2968, step time: 0.4969\n",
      "243/388, train_loss: 0.1960, step time: 0.4839\n",
      "244/388, train_loss: 0.2103, step time: 0.4832\n",
      "245/388, train_loss: 0.0826, step time: 1.0547\n",
      "246/388, train_loss: 0.2296, step time: 0.5317\n",
      "247/388, train_loss: 0.1741, step time: 0.5064\n",
      "248/388, train_loss: 0.2260, step time: 0.4980\n",
      "249/388, train_loss: 0.0440, step time: 0.4946\n",
      "250/388, train_loss: 0.1765, step time: 0.4802\n",
      "251/388, train_loss: 0.1221, step time: 0.9505\n",
      "252/388, train_loss: 0.1239, step time: 0.5578\n",
      "253/388, train_loss: 0.1186, step time: 0.5172\n",
      "254/388, train_loss: 0.1063, step time: 0.5022\n",
      "255/388, train_loss: 0.2577, step time: 0.4930\n",
      "256/388, train_loss: 0.0879, step time: 0.4996\n",
      "257/388, train_loss: 0.1639, step time: 0.4817\n",
      "258/388, train_loss: 0.4279, step time: 1.1338\n",
      "259/388, train_loss: 0.0922, step time: 0.5306\n",
      "260/388, train_loss: 0.3754, step time: 0.5119\n",
      "261/388, train_loss: 0.2314, step time: 0.5034\n",
      "262/388, train_loss: 0.1548, step time: 0.4819\n",
      "263/388, train_loss: 0.0730, step time: 0.4783\n",
      "264/388, train_loss: 0.3193, step time: 0.4954\n",
      "265/388, train_loss: 0.1417, step time: 0.4937\n",
      "266/388, train_loss: 0.1116, step time: 0.4965\n",
      "267/388, train_loss: 0.2670, step time: 0.4801\n",
      "268/388, train_loss: 0.2345, step time: 0.5491\n",
      "269/388, train_loss: 0.2597, step time: 0.5274\n",
      "270/388, train_loss: 0.2338, step time: 0.5015\n",
      "271/388, train_loss: 0.0904, step time: 0.5507\n",
      "272/388, train_loss: 0.2391, step time: 0.5540\n",
      "273/388, train_loss: 0.1389, step time: 0.5213\n",
      "274/388, train_loss: 0.1841, step time: 0.5062\n",
      "275/388, train_loss: 0.2460, step time: 0.4864\n",
      "276/388, train_loss: 0.3778, step time: 0.5263\n",
      "277/388, train_loss: 0.1030, step time: 0.5239\n",
      "278/388, train_loss: 0.0660, step time: 0.5371\n",
      "279/388, train_loss: 0.1711, step time: 0.5088\n",
      "280/388, train_loss: 0.3873, step time: 1.0949\n",
      "281/388, train_loss: 0.3881, step time: 0.5387\n",
      "282/388, train_loss: 0.0929, step time: 0.5162\n",
      "283/388, train_loss: 0.0793, step time: 0.4874\n",
      "284/388, train_loss: 0.0415, step time: 0.5140\n",
      "285/388, train_loss: 0.1698, step time: 0.4937\n",
      "286/388, train_loss: 0.1900, step time: 0.4909\n",
      "287/388, train_loss: 0.1600, step time: 1.1216\n",
      "288/388, train_loss: 0.3625, step time: 0.5324\n",
      "289/388, train_loss: 0.1760, step time: 0.5068\n",
      "290/388, train_loss: 0.2807, step time: 0.4909\n",
      "291/388, train_loss: 0.0930, step time: 0.4994\n",
      "292/388, train_loss: 0.2130, step time: 0.4797\n",
      "293/388, train_loss: 0.0771, step time: 0.5072\n",
      "294/388, train_loss: 0.1238, step time: 0.5044\n",
      "295/388, train_loss: 0.0865, step time: 0.5055\n",
      "296/388, train_loss: 0.1933, step time: 0.5071\n",
      "297/388, train_loss: 0.0729, step time: 0.5347\n",
      "298/388, train_loss: 0.2523, step time: 0.5217\n",
      "299/388, train_loss: 0.0997, step time: 0.5047\n",
      "300/388, train_loss: 0.1822, step time: 0.5034\n",
      "301/388, train_loss: 0.2118, step time: 0.5041\n",
      "302/388, train_loss: 0.1588, step time: 0.4882\n",
      "303/388, train_loss: 0.0624, step time: 0.4968\n",
      "304/388, train_loss: 0.1396, step time: 0.4966\n",
      "305/388, train_loss: 0.1127, step time: 0.5085\n",
      "306/388, train_loss: 0.1729, step time: 0.4940\n",
      "307/388, train_loss: 0.1917, step time: 0.5068\n",
      "308/388, train_loss: 0.1119, step time: 0.5012\n",
      "309/388, train_loss: 0.6466, step time: 0.6540\n",
      "310/388, train_loss: 0.1343, step time: 0.5285\n",
      "311/388, train_loss: 0.1417, step time: 0.5374\n",
      "312/388, train_loss: 0.2371, step time: 0.4948\n",
      "313/388, train_loss: 0.2703, step time: 0.8935\n",
      "314/388, train_loss: 0.1734, step time: 0.5534\n",
      "315/388, train_loss: 0.1196, step time: 0.5208\n",
      "316/388, train_loss: 0.2860, step time: 0.4992\n",
      "317/388, train_loss: 0.2898, step time: 0.4953\n",
      "318/388, train_loss: 0.1285, step time: 0.4805\n",
      "319/388, train_loss: 0.0499, step time: 0.5087\n",
      "320/388, train_loss: 0.1778, step time: 0.4967\n",
      "321/388, train_loss: 0.3030, step time: 1.1257\n",
      "322/388, train_loss: 0.3143, step time: 0.5390\n",
      "323/388, train_loss: 0.2018, step time: 0.5197\n",
      "324/388, train_loss: 0.1808, step time: 0.5025\n",
      "325/388, train_loss: 0.4118, step time: 0.5287\n",
      "326/388, train_loss: 0.1016, step time: 0.4962\n",
      "327/388, train_loss: 0.0306, step time: 0.4992\n",
      "328/388, train_loss: 0.0381, step time: 0.4877\n",
      "329/388, train_loss: 0.0974, step time: 0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/388, train_loss: 0.0966, step time: 0.4929\n",
      "331/388, train_loss: 0.1279, step time: 0.4967\n",
      "332/388, train_loss: 0.1026, step time: 0.4973\n",
      "333/388, train_loss: 0.3239, step time: 0.4822\n",
      "334/388, train_loss: 0.0798, step time: 0.5132\n",
      "335/388, train_loss: 0.1973, step time: 0.5053\n",
      "336/388, train_loss: 0.1058, step time: 0.4964\n",
      "337/388, train_loss: 0.1069, step time: 0.4949\n",
      "338/388, train_loss: 0.3327, step time: 0.5334\n",
      "339/388, train_loss: 0.1420, step time: 0.5152\n",
      "340/388, train_loss: 0.2272, step time: 0.5297\n",
      "341/388, train_loss: 0.0635, step time: 0.5154\n",
      "342/388, train_loss: 0.1470, step time: 0.5044\n",
      "343/388, train_loss: 0.5327, step time: 1.2568\n",
      "344/388, train_loss: 0.0567, step time: 0.5397\n",
      "345/388, train_loss: 0.2266, step time: 0.5210\n",
      "346/388, train_loss: 0.0911, step time: 0.4892\n",
      "347/388, train_loss: 0.1287, step time: 0.4849\n",
      "348/388, train_loss: 0.5265, step time: 0.5295\n",
      "349/388, train_loss: 0.0988, step time: 0.5202\n",
      "350/388, train_loss: 0.1167, step time: 0.4910\n",
      "351/388, train_loss: 0.2151, step time: 0.4822\n",
      "352/388, train_loss: 0.0534, step time: 0.4924\n",
      "353/388, train_loss: 0.0523, step time: 0.5520\n",
      "354/388, train_loss: 0.1692, step time: 0.5416\n",
      "355/388, train_loss: 0.1018, step time: 0.5108\n",
      "356/388, train_loss: 0.1916, step time: 0.4880\n",
      "357/388, train_loss: 0.4279, step time: 0.5157\n",
      "358/388, train_loss: 0.1268, step time: 0.5885\n",
      "359/388, train_loss: 0.0320, step time: 0.5378\n",
      "360/388, train_loss: 0.1468, step time: 0.5060\n",
      "361/388, train_loss: 0.2607, step time: 0.4902\n",
      "362/388, train_loss: 0.1470, step time: 1.1228\n",
      "363/388, train_loss: 0.5620, step time: 0.5419\n",
      "364/388, train_loss: 0.1638, step time: 0.5176\n",
      "365/388, train_loss: 0.1694, step time: 0.4926\n",
      "366/388, train_loss: 0.2016, step time: 0.4851\n",
      "367/388, train_loss: 0.0809, step time: 0.5112\n",
      "368/388, train_loss: 0.2929, step time: 0.5024\n",
      "369/388, train_loss: 0.0823, step time: 0.4937\n",
      "370/388, train_loss: 0.0587, step time: 0.4781\n",
      "371/388, train_loss: 0.0832, step time: 0.4788\n",
      "372/388, train_loss: 0.0794, step time: 0.9522\n",
      "373/388, train_loss: 0.2290, step time: 0.5408\n",
      "374/388, train_loss: 0.2027, step time: 0.5278\n",
      "375/388, train_loss: 0.1454, step time: 0.4976\n",
      "376/388, train_loss: 0.0332, step time: 0.5012\n",
      "377/388, train_loss: 0.1227, step time: 0.4899\n",
      "378/388, train_loss: 0.2377, step time: 0.4974\n",
      "379/388, train_loss: 0.0901, step time: 0.4787\n",
      "380/388, train_loss: 0.1098, step time: 0.5166\n",
      "381/388, train_loss: 0.1202, step time: 0.5416\n",
      "382/388, train_loss: 0.1067, step time: 0.5242\n",
      "383/388, train_loss: 0.0446, step time: 0.4996\n",
      "384/388, train_loss: 0.1623, step time: 0.4835\n",
      "385/388, train_loss: 0.1141, step time: 0.5186\n",
      "386/388, train_loss: 0.3124, step time: 0.5240\n",
      "387/388, train_loss: 0.2388, step time: 0.5177\n",
      "388/388, train_loss: 0.2417, step time: 0.5783\n",
      "epoch 88 average loss: 0.1760\n",
      "current epoch: 88 current mean dice: 0.7680 tc: 0.8148 wt: 0.8985 et: 0.5906\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 88 is: 301.4982\n",
      "----------\n",
      "epoch 89/300\n",
      "1/388, train_loss: 0.2645, step time: 0.4761\n",
      "2/388, train_loss: 0.2222, step time: 0.4817\n",
      "3/388, train_loss: 0.0533, step time: 0.5732\n",
      "4/388, train_loss: 0.1703, step time: 0.5486\n",
      "5/388, train_loss: 0.0911, step time: 0.5082\n",
      "6/388, train_loss: 0.3166, step time: 0.5489\n",
      "7/388, train_loss: 0.1784, step time: 0.5639\n",
      "8/388, train_loss: 0.0945, step time: 0.5259\n",
      "9/388, train_loss: 0.3479, step time: 0.5306\n",
      "10/388, train_loss: 0.2023, step time: 0.5477\n",
      "11/388, train_loss: 0.1224, step time: 0.5271\n",
      "12/388, train_loss: 0.1683, step time: 0.6179\n",
      "13/388, train_loss: 0.1981, step time: 0.5611\n",
      "14/388, train_loss: 0.2020, step time: 0.5256\n",
      "15/388, train_loss: 0.1030, step time: 0.5037\n",
      "16/388, train_loss: 0.3506, step time: 0.5037\n",
      "17/388, train_loss: 0.1497, step time: 1.1272\n",
      "18/388, train_loss: 0.1545, step time: 0.5362\n",
      "19/388, train_loss: 0.0699, step time: 0.5152\n",
      "20/388, train_loss: 0.1532, step time: 0.4993\n",
      "21/388, train_loss: 0.0638, step time: 0.4981\n",
      "22/388, train_loss: 0.2008, step time: 0.4854\n",
      "23/388, train_loss: 0.3615, step time: 0.5335\n",
      "24/388, train_loss: 0.2426, step time: 0.5258\n",
      "25/388, train_loss: 0.0824, step time: 0.5262\n",
      "26/388, train_loss: 0.1172, step time: 0.5198\n",
      "27/388, train_loss: 0.1124, step time: 0.5393\n",
      "28/388, train_loss: 0.2038, step time: 0.5452\n",
      "29/388, train_loss: 0.2282, step time: 0.5356\n",
      "30/388, train_loss: 0.1480, step time: 0.5203\n",
      "31/388, train_loss: 0.2913, step time: 0.5419\n",
      "32/388, train_loss: 0.1943, step time: 0.5277\n",
      "33/388, train_loss: 0.3290, step time: 0.5089\n",
      "34/388, train_loss: 0.1214, step time: 0.5089\n",
      "35/388, train_loss: 0.1640, step time: 0.4945\n",
      "36/388, train_loss: 0.1257, step time: 0.5753\n",
      "37/388, train_loss: 0.1063, step time: 0.5716\n",
      "38/388, train_loss: 0.0646, step time: 0.5472\n",
      "39/388, train_loss: 0.2831, step time: 0.5169\n",
      "40/388, train_loss: 0.1073, step time: 0.5276\n",
      "41/388, train_loss: 0.1231, step time: 0.5249\n",
      "42/388, train_loss: 0.0837, step time: 0.5336\n",
      "43/388, train_loss: 0.0728, step time: 0.5050\n",
      "44/388, train_loss: 0.1872, step time: 0.5056\n",
      "45/388, train_loss: 0.0877, step time: 0.4874\n",
      "46/388, train_loss: 0.1047, step time: 0.4994\n",
      "47/388, train_loss: 0.1812, step time: 0.4987\n",
      "48/388, train_loss: 0.4307, step time: 0.4961\n",
      "49/388, train_loss: 0.0853, step time: 0.5296\n",
      "50/388, train_loss: 0.1782, step time: 0.5278\n",
      "51/388, train_loss: 0.2162, step time: 0.5152\n",
      "52/388, train_loss: 0.0798, step time: 0.5061\n",
      "53/388, train_loss: 0.1174, step time: 0.5107\n",
      "54/388, train_loss: 0.2053, step time: 0.4981\n",
      "55/388, train_loss: 0.0413, step time: 0.5042\n",
      "56/388, train_loss: 0.1025, step time: 0.5215\n",
      "57/388, train_loss: 0.1167, step time: 0.5067\n",
      "58/388, train_loss: 0.1417, step time: 1.0065\n",
      "59/388, train_loss: 0.2048, step time: 0.5586\n",
      "60/388, train_loss: 0.2943, step time: 0.5384\n",
      "61/388, train_loss: 0.0933, step time: 0.6523\n",
      "62/388, train_loss: 0.0823, step time: 0.5489\n",
      "63/388, train_loss: 0.0783, step time: 0.5226\n",
      "64/388, train_loss: 0.0996, step time: 0.5133\n",
      "65/388, train_loss: 0.0475, step time: 0.4953\n",
      "66/388, train_loss: 0.3520, step time: 0.4993\n",
      "67/388, train_loss: 0.2192, step time: 0.4888\n",
      "68/388, train_loss: 0.3598, step time: 0.5037\n",
      "69/388, train_loss: 0.1660, step time: 0.4937\n",
      "70/388, train_loss: 0.0781, step time: 0.4862\n",
      "71/388, train_loss: 0.2607, step time: 0.4944\n",
      "72/388, train_loss: 0.5964, step time: 0.5096\n",
      "73/388, train_loss: 0.0720, step time: 0.5000\n",
      "74/388, train_loss: 0.1261, step time: 0.4858\n",
      "75/388, train_loss: 0.2057, step time: 1.2104\n",
      "76/388, train_loss: 0.1668, step time: 0.5285\n",
      "77/388, train_loss: 0.2316, step time: 0.5030\n",
      "78/388, train_loss: 0.2241, step time: 0.4909\n",
      "79/388, train_loss: 0.3088, step time: 0.4988\n",
      "80/388, train_loss: 0.1026, step time: 0.4795\n",
      "81/388, train_loss: 0.0914, step time: 0.4766\n",
      "82/388, train_loss: 0.1604, step time: 0.5222\n",
      "83/388, train_loss: 0.1098, step time: 0.5231\n",
      "84/388, train_loss: 0.1670, step time: 0.5120\n",
      "85/388, train_loss: 0.2415, step time: 0.5045\n",
      "86/388, train_loss: 0.1793, step time: 0.4978\n",
      "87/388, train_loss: 0.0700, step time: 0.4880\n",
      "88/388, train_loss: 0.1141, step time: 0.4852\n",
      "89/388, train_loss: 0.2263, step time: 0.4744\n",
      "90/388, train_loss: 0.1174, step time: 0.8097\n",
      "91/388, train_loss: 0.2083, step time: 0.5691\n",
      "92/388, train_loss: 0.2202, step time: 0.5349\n",
      "93/388, train_loss: 0.0992, step time: 0.5004\n",
      "94/388, train_loss: 0.1355, step time: 0.4974\n",
      "95/388, train_loss: 0.1256, step time: 0.5299\n",
      "96/388, train_loss: 0.1299, step time: 0.5072\n",
      "97/388, train_loss: 0.0838, step time: 0.5136\n",
      "98/388, train_loss: 0.2319, step time: 0.4926\n",
      "99/388, train_loss: 0.0445, step time: 0.4910\n",
      "100/388, train_loss: 0.4850, step time: 0.4890\n",
      "101/388, train_loss: 0.1126, step time: 0.5202\n",
      "102/388, train_loss: 0.1786, step time: 0.5010\n",
      "103/388, train_loss: 0.1635, step time: 0.6049\n",
      "104/388, train_loss: 0.0981, step time: 0.5764\n",
      "105/388, train_loss: 0.2481, step time: 0.5295\n",
      "106/388, train_loss: 0.0867, step time: 0.5088\n",
      "107/388, train_loss: 0.0974, step time: 0.5021\n",
      "108/388, train_loss: 0.3158, step time: 0.4869\n",
      "109/388, train_loss: 0.2148, step time: 0.5202\n",
      "110/388, train_loss: 0.2088, step time: 0.5011\n",
      "111/388, train_loss: 0.3659, step time: 0.4995\n",
      "112/388, train_loss: 0.2011, step time: 0.4871\n",
      "113/388, train_loss: 0.2974, step time: 0.4824\n",
      "114/388, train_loss: 0.1837, step time: 0.6103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/388, train_loss: 0.2316, step time: 0.5543\n",
      "116/388, train_loss: 0.1668, step time: 0.5160\n",
      "117/388, train_loss: 0.0831, step time: 0.4938\n",
      "118/388, train_loss: 0.0739, step time: 0.4896\n",
      "119/388, train_loss: 0.1385, step time: 0.4947\n",
      "120/388, train_loss: 0.1682, step time: 0.4780\n",
      "121/388, train_loss: 0.2656, step time: 0.5046\n",
      "122/388, train_loss: 0.0947, step time: 0.4825\n",
      "123/388, train_loss: 0.0753, step time: 0.4970\n",
      "124/388, train_loss: 0.1371, step time: 0.5048\n",
      "125/388, train_loss: 0.1588, step time: 0.4949\n",
      "126/388, train_loss: 0.2296, step time: 0.4935\n",
      "127/388, train_loss: 0.1963, step time: 0.9882\n",
      "128/388, train_loss: 0.0838, step time: 0.5344\n",
      "129/388, train_loss: 0.1446, step time: 0.5107\n",
      "130/388, train_loss: 0.2773, step time: 0.4871\n",
      "131/388, train_loss: 0.0897, step time: 0.5250\n",
      "132/388, train_loss: 0.1243, step time: 0.5004\n",
      "133/388, train_loss: 0.0594, step time: 0.4944\n",
      "134/388, train_loss: 0.0846, step time: 0.4822\n",
      "135/388, train_loss: 0.1314, step time: 0.4864\n",
      "136/388, train_loss: 0.1208, step time: 0.7787\n",
      "137/388, train_loss: 0.0946, step time: 0.5343\n",
      "138/388, train_loss: 0.0481, step time: 0.5051\n",
      "139/388, train_loss: 0.1932, step time: 0.4931\n",
      "140/388, train_loss: 0.0709, step time: 0.4774\n",
      "141/388, train_loss: 0.3038, step time: 0.4816\n",
      "142/388, train_loss: 0.1933, step time: 0.4889\n",
      "143/388, train_loss: 0.2711, step time: 0.4865\n",
      "144/388, train_loss: 0.2569, step time: 0.4899\n",
      "145/388, train_loss: 0.1990, step time: 1.1609\n",
      "146/388, train_loss: 0.1259, step time: 0.5469\n",
      "147/388, train_loss: 0.0921, step time: 0.5103\n",
      "148/388, train_loss: 0.1020, step time: 0.4971\n",
      "149/388, train_loss: 0.1583, step time: 0.5026\n",
      "150/388, train_loss: 0.1498, step time: 0.4881\n",
      "151/388, train_loss: 0.2229, step time: 0.4942\n",
      "152/388, train_loss: 0.1608, step time: 0.4929\n",
      "153/388, train_loss: 0.1972, step time: 0.4828\n",
      "154/388, train_loss: 0.3427, step time: 1.0345\n",
      "155/388, train_loss: 0.1833, step time: 0.5270\n",
      "156/388, train_loss: 0.1000, step time: 0.5005\n",
      "157/388, train_loss: 0.0382, step time: 0.4869\n",
      "158/388, train_loss: 0.1001, step time: 0.4918\n",
      "159/388, train_loss: 0.1055, step time: 0.4766\n",
      "160/388, train_loss: 0.0311, step time: 0.8393\n",
      "161/388, train_loss: 0.1365, step time: 0.5398\n",
      "162/388, train_loss: 0.1132, step time: 0.5147\n",
      "163/388, train_loss: 0.0732, step time: 0.4862\n",
      "164/388, train_loss: 0.1165, step time: 0.4941\n",
      "165/388, train_loss: 0.1245, step time: 0.4819\n",
      "166/388, train_loss: 0.0851, step time: 0.4844\n",
      "167/388, train_loss: 0.1923, step time: 0.5021\n",
      "168/388, train_loss: 0.1706, step time: 0.4969\n",
      "169/388, train_loss: 0.4292, step time: 0.4881\n",
      "170/388, train_loss: 0.2870, step time: 0.4943\n",
      "171/388, train_loss: 0.0454, step time: 0.7289\n",
      "172/388, train_loss: 0.1244, step time: 0.5387\n",
      "173/388, train_loss: 0.2298, step time: 0.5135\n",
      "174/388, train_loss: 0.1389, step time: 0.4950\n",
      "175/388, train_loss: 0.1621, step time: 0.4988\n",
      "176/388, train_loss: 0.1557, step time: 0.4875\n",
      "177/388, train_loss: 0.2460, step time: 0.4826\n",
      "178/388, train_loss: 0.0931, step time: 0.5929\n",
      "179/388, train_loss: 0.1703, step time: 0.5556\n",
      "180/388, train_loss: 0.0786, step time: 0.5148\n",
      "181/388, train_loss: 0.2558, step time: 0.4949\n",
      "182/388, train_loss: 0.3810, step time: 0.5030\n",
      "183/388, train_loss: 0.0706, step time: 0.5007\n",
      "184/388, train_loss: 0.0924, step time: 0.4933\n",
      "185/388, train_loss: 0.0377, step time: 0.4781\n",
      "186/388, train_loss: 0.1640, step time: 0.4981\n",
      "187/388, train_loss: 0.3143, step time: 0.5243\n",
      "188/388, train_loss: 0.2696, step time: 0.5194\n",
      "189/388, train_loss: 0.2815, step time: 0.5400\n",
      "190/388, train_loss: 0.0655, step time: 0.5739\n",
      "191/388, train_loss: 0.0852, step time: 0.5265\n",
      "192/388, train_loss: 0.2746, step time: 0.5050\n",
      "193/388, train_loss: 0.1739, step time: 0.5229\n",
      "194/388, train_loss: 0.2294, step time: 0.5081\n",
      "195/388, train_loss: 0.1010, step time: 0.4866\n",
      "196/388, train_loss: 0.0442, step time: 0.4994\n",
      "197/388, train_loss: 0.1497, step time: 0.4845\n",
      "198/388, train_loss: 0.1694, step time: 0.4918\n",
      "199/388, train_loss: 0.5060, step time: 0.4942\n",
      "200/388, train_loss: 0.1667, step time: 0.4798\n",
      "201/388, train_loss: 0.1323, step time: 0.5054\n",
      "202/388, train_loss: 0.2346, step time: 0.4913\n",
      "203/388, train_loss: 0.1634, step time: 0.4946\n",
      "204/388, train_loss: 0.1100, step time: 0.4976\n",
      "205/388, train_loss: 0.1198, step time: 0.4879\n",
      "206/388, train_loss: 0.1694, step time: 0.4806\n",
      "207/388, train_loss: 0.1961, step time: 0.5363\n",
      "208/388, train_loss: 0.1691, step time: 0.5084\n",
      "209/388, train_loss: 0.1806, step time: 0.5848\n",
      "210/388, train_loss: 0.1546, step time: 0.5485\n",
      "211/388, train_loss: 0.0778, step time: 0.5336\n",
      "212/388, train_loss: 0.1097, step time: 0.5121\n",
      "213/388, train_loss: 0.1231, step time: 0.5625\n",
      "214/388, train_loss: 0.1951, step time: 0.5641\n",
      "215/388, train_loss: 0.1179, step time: 0.5305\n",
      "216/388, train_loss: 0.2692, step time: 0.5027\n",
      "217/388, train_loss: 0.0578, step time: 0.9904\n",
      "218/388, train_loss: 0.3586, step time: 0.5335\n",
      "219/388, train_loss: 0.0688, step time: 0.5065\n",
      "220/388, train_loss: 0.0835, step time: 0.4900\n",
      "221/388, train_loss: 0.1393, step time: 0.5349\n",
      "222/388, train_loss: 0.1028, step time: 0.5276\n",
      "223/388, train_loss: 0.2032, step time: 0.5090\n",
      "224/388, train_loss: 0.0652, step time: 0.4925\n",
      "225/388, train_loss: 0.5151, step time: 0.5045\n",
      "226/388, train_loss: 0.1714, step time: 0.4826\n",
      "227/388, train_loss: 0.4879, step time: 0.4973\n",
      "228/388, train_loss: 0.0485, step time: 0.5750\n",
      "229/388, train_loss: 0.2492, step time: 0.6572\n",
      "230/388, train_loss: 0.0888, step time: 0.5606\n",
      "231/388, train_loss: 0.1348, step time: 0.5254\n",
      "232/388, train_loss: 0.2217, step time: 0.5017\n",
      "233/388, train_loss: 0.1579, step time: 0.5016\n",
      "234/388, train_loss: 0.0471, step time: 0.4827\n",
      "235/388, train_loss: 0.3316, step time: 0.5075\n",
      "236/388, train_loss: 0.1639, step time: 0.4952\n",
      "237/388, train_loss: 0.1024, step time: 0.4929\n",
      "238/388, train_loss: 0.1093, step time: 0.5031\n",
      "239/388, train_loss: 0.1416, step time: 0.4915\n",
      "240/388, train_loss: 0.1427, step time: 0.4984\n",
      "241/388, train_loss: 0.2893, step time: 1.2302\n",
      "242/388, train_loss: 0.0866, step time: 0.5254\n",
      "243/388, train_loss: 0.2317, step time: 0.4978\n",
      "244/388, train_loss: 0.3507, step time: 0.4972\n",
      "245/388, train_loss: 0.0696, step time: 0.4839\n",
      "246/388, train_loss: 0.1007, step time: 0.4850\n",
      "247/388, train_loss: 0.0995, step time: 0.4757\n",
      "248/388, train_loss: 0.0968, step time: 0.4771\n",
      "249/388, train_loss: 0.1329, step time: 0.4887\n",
      "250/388, train_loss: 0.1652, step time: 1.1534\n",
      "251/388, train_loss: 0.1588, step time: 0.5467\n",
      "252/388, train_loss: 0.1430, step time: 0.5163\n",
      "253/388, train_loss: 0.1837, step time: 0.5053\n",
      "254/388, train_loss: 0.1475, step time: 0.4992\n",
      "255/388, train_loss: 0.1141, step time: 0.4983\n",
      "256/388, train_loss: 0.2236, step time: 0.5064\n",
      "257/388, train_loss: 0.1809, step time: 0.4956\n",
      "258/388, train_loss: 0.1430, step time: 0.4787\n",
      "259/388, train_loss: 0.1415, step time: 0.4834\n",
      "260/388, train_loss: 0.1418, step time: 1.0017\n",
      "261/388, train_loss: 0.1453, step time: 0.5495\n",
      "262/388, train_loss: 0.1520, step time: 0.5161\n",
      "263/388, train_loss: 0.2471, step time: 0.5068\n",
      "264/388, train_loss: 0.1319, step time: 0.4914\n",
      "265/388, train_loss: 0.1708, step time: 0.4938\n",
      "266/388, train_loss: 0.2799, step time: 0.4837\n",
      "267/388, train_loss: 0.0800, step time: 0.4793\n",
      "268/388, train_loss: 0.1163, step time: 1.0118\n",
      "269/388, train_loss: 0.2265, step time: 0.5435\n",
      "270/388, train_loss: 0.1924, step time: 0.5077\n",
      "271/388, train_loss: 0.1253, step time: 0.4929\n",
      "272/388, train_loss: 0.0824, step time: 0.4952\n",
      "273/388, train_loss: 0.1096, step time: 0.4845\n",
      "274/388, train_loss: 0.0843, step time: 0.4890\n",
      "275/388, train_loss: 0.1226, step time: 0.4791\n",
      "276/388, train_loss: 0.0871, step time: 0.4895\n",
      "277/388, train_loss: 0.1726, step time: 0.4775\n",
      "278/388, train_loss: 0.2987, step time: 0.4722\n",
      "279/388, train_loss: 0.0538, step time: 0.8796\n",
      "280/388, train_loss: 0.4081, step time: 0.5263\n",
      "281/388, train_loss: 0.1735, step time: 0.4946\n",
      "282/388, train_loss: 0.3483, step time: 0.4899\n",
      "283/388, train_loss: 0.0919, step time: 0.5060\n",
      "284/388, train_loss: 0.1239, step time: 0.4854\n",
      "285/388, train_loss: 0.2023, step time: 0.4882\n",
      "286/388, train_loss: 0.1069, step time: 0.7856\n",
      "287/388, train_loss: 0.0762, step time: 0.5667\n",
      "288/388, train_loss: 0.2479, step time: 0.5241\n",
      "289/388, train_loss: 0.0703, step time: 0.5043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/388, train_loss: 0.2627, step time: 0.5146\n",
      "291/388, train_loss: 0.2417, step time: 0.5484\n",
      "292/388, train_loss: 0.1073, step time: 0.5162\n",
      "293/388, train_loss: 0.0650, step time: 0.5027\n",
      "294/388, train_loss: 0.1426, step time: 0.4868\n",
      "295/388, train_loss: 0.3815, step time: 0.9699\n",
      "296/388, train_loss: 0.1374, step time: 0.5240\n",
      "297/388, train_loss: 0.1142, step time: 0.4893\n",
      "298/388, train_loss: 0.2521, step time: 0.4917\n",
      "299/388, train_loss: 0.0763, step time: 0.4886\n",
      "300/388, train_loss: 0.0859, step time: 0.4901\n",
      "301/388, train_loss: 0.3019, step time: 0.5137\n",
      "302/388, train_loss: 0.0986, step time: 0.5420\n",
      "303/388, train_loss: 0.2151, step time: 0.5068\n",
      "304/388, train_loss: 0.0717, step time: 0.4954\n",
      "305/388, train_loss: 0.1975, step time: 0.4819\n",
      "306/388, train_loss: 0.2375, step time: 1.1862\n",
      "307/388, train_loss: 0.0578, step time: 0.5565\n",
      "308/388, train_loss: 0.2166, step time: 0.5157\n",
      "309/388, train_loss: 0.1065, step time: 0.4845\n",
      "310/388, train_loss: 0.0996, step time: 0.4791\n",
      "311/388, train_loss: 0.0678, step time: 0.4905\n",
      "312/388, train_loss: 0.1592, step time: 0.4910\n",
      "313/388, train_loss: 0.0810, step time: 0.5171\n",
      "314/388, train_loss: 0.2165, step time: 0.5445\n",
      "315/388, train_loss: 0.2152, step time: 0.5228\n",
      "316/388, train_loss: 0.1066, step time: 0.5074\n",
      "317/388, train_loss: 0.2071, step time: 0.4858\n",
      "318/388, train_loss: 0.1027, step time: 0.5166\n",
      "319/388, train_loss: 0.0933, step time: 0.5027\n",
      "320/388, train_loss: 0.2972, step time: 0.4894\n",
      "321/388, train_loss: 0.3418, step time: 0.5216\n",
      "322/388, train_loss: 0.3939, step time: 0.4939\n",
      "323/388, train_loss: 0.0681, step time: 1.1232\n",
      "324/388, train_loss: 0.2373, step time: 0.5365\n",
      "325/388, train_loss: 0.1142, step time: 0.5139\n",
      "326/388, train_loss: 0.4096, step time: 0.4852\n",
      "327/388, train_loss: 0.1971, step time: 1.0281\n",
      "328/388, train_loss: 0.1005, step time: 0.5474\n",
      "329/388, train_loss: 0.3706, step time: 0.5185\n",
      "330/388, train_loss: 0.1365, step time: 0.5006\n",
      "331/388, train_loss: 0.2396, step time: 0.4992\n",
      "332/388, train_loss: 0.1608, step time: 0.4847\n",
      "333/388, train_loss: 0.0678, step time: 1.1101\n",
      "334/388, train_loss: 0.0956, step time: 0.5349\n",
      "335/388, train_loss: 0.1980, step time: 0.5045\n",
      "336/388, train_loss: 0.1488, step time: 0.4970\n",
      "337/388, train_loss: 0.4074, step time: 0.4820\n",
      "338/388, train_loss: 0.0689, step time: 0.4844\n",
      "339/388, train_loss: 0.2395, step time: 0.8104\n",
      "340/388, train_loss: 0.1244, step time: 0.5367\n",
      "341/388, train_loss: 0.0750, step time: 0.5165\n",
      "342/388, train_loss: 0.2066, step time: 0.4943\n",
      "343/388, train_loss: 0.2238, step time: 0.4997\n",
      "344/388, train_loss: 0.1831, step time: 0.4896\n",
      "345/388, train_loss: 0.0959, step time: 0.4922\n",
      "346/388, train_loss: 0.2194, step time: 0.4889\n",
      "347/388, train_loss: 0.2348, step time: 0.4954\n",
      "348/388, train_loss: 0.0859, step time: 0.6529\n",
      "349/388, train_loss: 0.1020, step time: 0.5543\n",
      "350/388, train_loss: 0.2450, step time: 0.5331\n",
      "351/388, train_loss: 0.2273, step time: 0.5153\n",
      "352/388, train_loss: 0.2793, step time: 0.5021\n",
      "353/388, train_loss: 0.1559, step time: 0.4857\n",
      "354/388, train_loss: 0.0807, step time: 0.5071\n",
      "355/388, train_loss: 0.1982, step time: 0.4934\n",
      "356/388, train_loss: 0.1617, step time: 0.4970\n",
      "357/388, train_loss: 0.1026, step time: 0.4935\n",
      "358/388, train_loss: 0.2314, step time: 0.4944\n",
      "359/388, train_loss: 0.0727, step time: 0.4766\n",
      "360/388, train_loss: 0.2529, step time: 0.5018\n",
      "361/388, train_loss: 0.1612, step time: 0.5433\n",
      "362/388, train_loss: 0.2277, step time: 0.6171\n",
      "363/388, train_loss: 0.1205, step time: 0.5577\n",
      "364/388, train_loss: 0.2121, step time: 0.5336\n",
      "365/388, train_loss: 0.0678, step time: 0.5310\n",
      "366/388, train_loss: 0.1001, step time: 0.5145\n",
      "367/388, train_loss: 0.2394, step time: 0.5107\n",
      "368/388, train_loss: 0.1409, step time: 0.4895\n",
      "369/388, train_loss: 0.2495, step time: 0.5366\n",
      "370/388, train_loss: 0.0770, step time: 0.5405\n",
      "371/388, train_loss: 0.1475, step time: 0.5166\n",
      "372/388, train_loss: 0.5065, step time: 0.4921\n",
      "373/388, train_loss: 0.2631, step time: 1.0512\n",
      "374/388, train_loss: 0.1626, step time: 0.5462\n",
      "375/388, train_loss: 0.1736, step time: 0.5234\n",
      "376/388, train_loss: 0.1671, step time: 0.5214\n",
      "377/388, train_loss: 0.1020, step time: 0.5193\n",
      "378/388, train_loss: 0.0500, step time: 0.4968\n",
      "379/388, train_loss: 0.3943, step time: 0.5105\n",
      "380/388, train_loss: 0.1488, step time: 0.4967\n",
      "381/388, train_loss: 0.0345, step time: 0.4948\n",
      "382/388, train_loss: 0.4973, step time: 0.4764\n",
      "383/388, train_loss: 0.1120, step time: 0.4870\n",
      "384/388, train_loss: 0.2103, step time: 0.4808\n",
      "385/388, train_loss: 0.1505, step time: 0.6718\n",
      "386/388, train_loss: 0.2662, step time: 0.5401\n",
      "387/388, train_loss: 0.0974, step time: 0.5083\n",
      "388/388, train_loss: 0.3723, step time: 0.5102\n",
      "epoch 89 average loss: 0.1718\n",
      "current epoch: 89 current mean dice: 0.7775 tc: 0.8258 wt: 0.9047 et: 0.6019\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 89 is: 300.0434\n",
      "----------\n",
      "epoch 90/300\n",
      "1/388, train_loss: 0.2245, step time: 0.4842\n",
      "2/388, train_loss: 0.2138, step time: 0.4870\n",
      "3/388, train_loss: 0.2159, step time: 0.4871\n",
      "4/388, train_loss: 0.1477, step time: 0.5556\n",
      "5/388, train_loss: 0.3099, step time: 0.6536\n",
      "6/388, train_loss: 0.1044, step time: 0.6639\n",
      "7/388, train_loss: 0.1553, step time: 0.5732\n",
      "8/388, train_loss: 0.0810, step time: 0.5296\n",
      "9/388, train_loss: 0.1807, step time: 0.5149\n",
      "10/388, train_loss: 0.2678, step time: 0.7386\n",
      "11/388, train_loss: 0.1216, step time: 0.5844\n",
      "12/388, train_loss: 0.0610, step time: 0.5679\n",
      "13/388, train_loss: 0.1469, step time: 0.5278\n",
      "14/388, train_loss: 0.1274, step time: 0.4997\n",
      "15/388, train_loss: 0.1803, step time: 0.4858\n",
      "16/388, train_loss: 0.1546, step time: 0.4787\n",
      "17/388, train_loss: 0.1359, step time: 0.5844\n",
      "18/388, train_loss: 0.1307, step time: 0.5791\n",
      "19/388, train_loss: 0.1264, step time: 0.5312\n",
      "20/388, train_loss: 0.0731, step time: 0.4949\n",
      "21/388, train_loss: 0.1424, step time: 0.5185\n",
      "22/388, train_loss: 0.0918, step time: 0.5927\n",
      "23/388, train_loss: 0.1854, step time: 0.5478\n",
      "24/388, train_loss: 0.0896, step time: 0.5188\n",
      "25/388, train_loss: 0.1487, step time: 0.5014\n",
      "26/388, train_loss: 0.1955, step time: 1.1627\n",
      "27/388, train_loss: 0.1658, step time: 0.5671\n",
      "28/388, train_loss: 0.1091, step time: 0.5103\n",
      "29/388, train_loss: 0.1876, step time: 0.4974\n",
      "30/388, train_loss: 0.2462, step time: 0.4940\n",
      "31/388, train_loss: 0.3291, step time: 0.4894\n",
      "32/388, train_loss: 0.0919, step time: 0.5060\n",
      "33/388, train_loss: 0.1740, step time: 0.4967\n",
      "34/388, train_loss: 0.0760, step time: 0.4981\n",
      "35/388, train_loss: 0.3133, step time: 1.0121\n",
      "36/388, train_loss: 0.3145, step time: 0.5376\n",
      "37/388, train_loss: 0.0840, step time: 0.5145\n",
      "38/388, train_loss: 0.2927, step time: 0.4950\n",
      "39/388, train_loss: 0.1221, step time: 0.5107\n",
      "40/388, train_loss: 0.1143, step time: 0.4952\n",
      "41/388, train_loss: 0.1388, step time: 0.4951\n",
      "42/388, train_loss: 0.1734, step time: 0.5024\n",
      "43/388, train_loss: 0.2224, step time: 0.5241\n",
      "44/388, train_loss: 0.1961, step time: 0.5158\n",
      "45/388, train_loss: 0.0625, step time: 0.5123\n",
      "46/388, train_loss: 0.1040, step time: 0.5013\n",
      "47/388, train_loss: 0.2023, step time: 0.5311\n",
      "48/388, train_loss: 0.3502, step time: 0.5260\n",
      "49/388, train_loss: 0.2726, step time: 0.5032\n",
      "50/388, train_loss: 0.1214, step time: 0.5694\n",
      "51/388, train_loss: 0.0926, step time: 0.6009\n",
      "52/388, train_loss: 0.1433, step time: 0.5435\n",
      "53/388, train_loss: 0.0817, step time: 0.5057\n",
      "54/388, train_loss: 0.1874, step time: 0.5224\n",
      "55/388, train_loss: 0.1337, step time: 0.6422\n",
      "56/388, train_loss: 0.1029, step time: 0.5449\n",
      "57/388, train_loss: 0.1016, step time: 0.5209\n",
      "58/388, train_loss: 0.2615, step time: 0.5084\n",
      "59/388, train_loss: 0.2062, step time: 1.0605\n",
      "60/388, train_loss: 0.3685, step time: 0.5368\n",
      "61/388, train_loss: 0.0831, step time: 0.5111\n",
      "62/388, train_loss: 0.0745, step time: 0.4968\n",
      "63/388, train_loss: 0.2220, step time: 0.4869\n",
      "64/388, train_loss: 0.1163, step time: 0.4844\n",
      "65/388, train_loss: 0.2598, step time: 0.4741\n",
      "66/388, train_loss: 0.1711, step time: 0.4729\n",
      "67/388, train_loss: 0.2144, step time: 0.9013\n",
      "68/388, train_loss: 0.1204, step time: 0.5733\n",
      "69/388, train_loss: 0.0746, step time: 0.5193\n",
      "70/388, train_loss: 0.2810, step time: 0.5010\n",
      "71/388, train_loss: 0.1840, step time: 0.5610\n",
      "72/388, train_loss: 0.1644, step time: 0.5227\n",
      "73/388, train_loss: 0.2139, step time: 0.5051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/388, train_loss: 0.1594, step time: 0.4979\n",
      "75/388, train_loss: 0.0668, step time: 0.4897\n",
      "76/388, train_loss: 0.1582, step time: 0.4928\n",
      "77/388, train_loss: 0.2003, step time: 0.5111\n",
      "78/388, train_loss: 0.1362, step time: 0.4971\n",
      "79/388, train_loss: 0.1592, step time: 0.7988\n",
      "80/388, train_loss: 0.2191, step time: 0.5314\n",
      "81/388, train_loss: 0.0954, step time: 0.5128\n",
      "82/388, train_loss: 0.1982, step time: 0.5000\n",
      "83/388, train_loss: 0.1461, step time: 0.4870\n",
      "84/388, train_loss: 0.1345, step time: 0.4994\n",
      "85/388, train_loss: 0.0720, step time: 0.4881\n",
      "86/388, train_loss: 0.0937, step time: 0.4936\n",
      "87/388, train_loss: 0.0691, step time: 0.4873\n",
      "88/388, train_loss: 0.1896, step time: 0.5297\n",
      "89/388, train_loss: 0.0816, step time: 0.5071\n",
      "90/388, train_loss: 0.0811, step time: 0.5038\n",
      "91/388, train_loss: 0.0622, step time: 0.5018\n",
      "92/388, train_loss: 0.0968, step time: 0.5057\n",
      "93/388, train_loss: 0.1709, step time: 0.4975\n",
      "94/388, train_loss: 0.1056, step time: 0.6646\n",
      "95/388, train_loss: 0.2558, step time: 0.5845\n",
      "96/388, train_loss: 0.2270, step time: 0.5245\n",
      "97/388, train_loss: 0.0857, step time: 0.5049\n",
      "98/388, train_loss: 0.4852, step time: 0.5008\n",
      "99/388, train_loss: 0.1332, step time: 0.4849\n",
      "100/388, train_loss: 0.3078, step time: 0.4935\n",
      "101/388, train_loss: 0.1064, step time: 0.9932\n",
      "102/388, train_loss: 0.0925, step time: 0.5387\n",
      "103/388, train_loss: 0.2995, step time: 0.5162\n",
      "104/388, train_loss: 0.1296, step time: 0.4938\n",
      "105/388, train_loss: 0.2787, step time: 0.4992\n",
      "106/388, train_loss: 0.1686, step time: 0.4815\n",
      "107/388, train_loss: 0.1833, step time: 0.4993\n",
      "108/388, train_loss: 0.3018, step time: 0.4758\n",
      "109/388, train_loss: 0.2596, step time: 0.4783\n",
      "110/388, train_loss: 0.0920, step time: 0.4767\n",
      "111/388, train_loss: 0.0877, step time: 0.5629\n",
      "112/388, train_loss: 0.2063, step time: 0.5335\n",
      "113/388, train_loss: 0.2484, step time: 0.5021\n",
      "114/388, train_loss: 0.2200, step time: 0.5005\n",
      "115/388, train_loss: 0.5026, step time: 0.4818\n",
      "116/388, train_loss: 0.1068, step time: 0.6804\n",
      "117/388, train_loss: 0.0331, step time: 0.5419\n",
      "118/388, train_loss: 0.0891, step time: 0.5112\n",
      "119/388, train_loss: 0.2734, step time: 0.5117\n",
      "120/388, train_loss: 0.1562, step time: 0.4897\n",
      "121/388, train_loss: 0.2099, step time: 0.4881\n",
      "122/388, train_loss: 0.1465, step time: 0.4775\n",
      "123/388, train_loss: 0.1999, step time: 0.5041\n",
      "124/388, train_loss: 0.2120, step time: 0.4965\n",
      "125/388, train_loss: 0.2218, step time: 0.4928\n",
      "126/388, train_loss: 0.1190, step time: 0.4914\n",
      "127/388, train_loss: 0.0819, step time: 0.9180\n",
      "128/388, train_loss: 0.1115, step time: 0.5402\n",
      "129/388, train_loss: 0.0982, step time: 0.5063\n",
      "130/388, train_loss: 0.1391, step time: 0.5006\n",
      "131/388, train_loss: 0.0693, step time: 0.4815\n",
      "132/388, train_loss: 0.0662, step time: 1.0538\n",
      "133/388, train_loss: 0.0965, step time: 0.5501\n",
      "134/388, train_loss: 0.4215, step time: 0.5303\n",
      "135/388, train_loss: 0.3090, step time: 0.5071\n",
      "136/388, train_loss: 0.1610, step time: 0.4997\n",
      "137/388, train_loss: 0.1163, step time: 0.4839\n",
      "138/388, train_loss: 0.0858, step time: 0.4972\n",
      "139/388, train_loss: 0.2571, step time: 0.5046\n",
      "140/388, train_loss: 0.0835, step time: 0.4853\n",
      "141/388, train_loss: 0.1274, step time: 0.4856\n",
      "142/388, train_loss: 0.0864, step time: 0.4887\n",
      "143/388, train_loss: 0.2511, step time: 0.4888\n",
      "144/388, train_loss: 0.1886, step time: 0.4986\n",
      "145/388, train_loss: 0.1267, step time: 0.4868\n",
      "146/388, train_loss: 0.1449, step time: 1.2346\n",
      "147/388, train_loss: 0.0834, step time: 0.5235\n",
      "148/388, train_loss: 0.1185, step time: 0.4957\n",
      "149/388, train_loss: 0.2693, step time: 0.4863\n",
      "150/388, train_loss: 0.2394, step time: 0.4938\n",
      "151/388, train_loss: 0.0912, step time: 0.4795\n",
      "152/388, train_loss: 0.2313, step time: 0.4789\n",
      "153/388, train_loss: 0.1550, step time: 0.7389\n",
      "154/388, train_loss: 0.1196, step time: 0.5577\n",
      "155/388, train_loss: 0.2353, step time: 0.5385\n",
      "156/388, train_loss: 0.1354, step time: 0.5070\n",
      "157/388, train_loss: 0.1513, step time: 0.5055\n",
      "158/388, train_loss: 0.1348, step time: 0.4967\n",
      "159/388, train_loss: 0.1423, step time: 0.5998\n",
      "160/388, train_loss: 0.0932, step time: 0.5498\n",
      "161/388, train_loss: 0.1075, step time: 0.5153\n",
      "162/388, train_loss: 0.0509, step time: 0.4967\n",
      "163/388, train_loss: 0.0804, step time: 0.5066\n",
      "164/388, train_loss: 0.0452, step time: 0.5360\n",
      "165/388, train_loss: 0.2575, step time: 0.5214\n",
      "166/388, train_loss: 0.1669, step time: 0.5072\n",
      "167/388, train_loss: 0.2210, step time: 0.5080\n",
      "168/388, train_loss: 0.1531, step time: 0.4878\n",
      "169/388, train_loss: 0.2263, step time: 1.1184\n",
      "170/388, train_loss: 0.2899, step time: 0.5262\n",
      "171/388, train_loss: 0.1172, step time: 0.4982\n",
      "172/388, train_loss: 0.1620, step time: 0.4929\n",
      "173/388, train_loss: 0.1615, step time: 0.4825\n",
      "174/388, train_loss: 0.1041, step time: 0.4756\n",
      "175/388, train_loss: 0.1885, step time: 0.5044\n",
      "176/388, train_loss: 0.0418, step time: 0.5155\n",
      "177/388, train_loss: 0.1548, step time: 0.5013\n",
      "178/388, train_loss: 0.2925, step time: 0.5009\n",
      "179/388, train_loss: 0.2856, step time: 0.4847\n",
      "180/388, train_loss: 0.2357, step time: 1.1684\n",
      "181/388, train_loss: 0.2122, step time: 0.5231\n",
      "182/388, train_loss: 0.1914, step time: 0.5019\n",
      "183/388, train_loss: 0.1639, step time: 0.4959\n",
      "184/388, train_loss: 0.2233, step time: 0.5467\n",
      "185/388, train_loss: 0.1252, step time: 0.5355\n",
      "186/388, train_loss: 0.1237, step time: 0.5088\n",
      "187/388, train_loss: 0.2092, step time: 0.4834\n",
      "188/388, train_loss: 0.2100, step time: 0.4896\n",
      "189/388, train_loss: 0.1966, step time: 0.4995\n",
      "190/388, train_loss: 0.1636, step time: 0.4831\n",
      "191/388, train_loss: 0.1650, step time: 0.4943\n",
      "192/388, train_loss: 0.1664, step time: 0.7328\n",
      "193/388, train_loss: 0.0547, step time: 0.5437\n",
      "194/388, train_loss: 0.0871, step time: 0.4986\n",
      "195/388, train_loss: 0.1048, step time: 0.4946\n",
      "196/388, train_loss: 0.0822, step time: 0.4842\n",
      "197/388, train_loss: 0.3382, step time: 0.4837\n",
      "198/388, train_loss: 0.1902, step time: 1.0063\n",
      "199/388, train_loss: 0.2267, step time: 0.5330\n",
      "200/388, train_loss: 0.3796, step time: 0.5122\n",
      "201/388, train_loss: 0.1049, step time: 0.4908\n",
      "202/388, train_loss: 0.1884, step time: 0.4804\n",
      "203/388, train_loss: 0.1600, step time: 0.4810\n",
      "204/388, train_loss: 0.1268, step time: 0.5210\n",
      "205/388, train_loss: 0.1517, step time: 0.5337\n",
      "206/388, train_loss: 0.0999, step time: 0.5168\n",
      "207/388, train_loss: 0.1197, step time: 0.4994\n",
      "208/388, train_loss: 0.0772, step time: 0.4861\n",
      "209/388, train_loss: 0.3455, step time: 0.4845\n",
      "210/388, train_loss: 0.0873, step time: 0.9474\n",
      "211/388, train_loss: 0.0916, step time: 0.5340\n",
      "212/388, train_loss: 0.1172, step time: 0.5172\n",
      "213/388, train_loss: 0.1280, step time: 0.4956\n",
      "214/388, train_loss: 0.0632, step time: 0.4993\n",
      "215/388, train_loss: 0.0854, step time: 0.4836\n",
      "216/388, train_loss: 0.3583, step time: 0.4829\n",
      "217/388, train_loss: 0.2267, step time: 0.4858\n",
      "218/388, train_loss: 0.1111, step time: 0.4734\n",
      "219/388, train_loss: 0.1198, step time: 1.0244\n",
      "220/388, train_loss: 0.1964, step time: 0.5304\n",
      "221/388, train_loss: 0.0948, step time: 0.4986\n",
      "222/388, train_loss: 0.1612, step time: 0.4827\n",
      "223/388, train_loss: 0.0794, step time: 0.5149\n",
      "224/388, train_loss: 0.0909, step time: 0.4984\n",
      "225/388, train_loss: 0.0744, step time: 0.4991\n",
      "226/388, train_loss: 0.1707, step time: 0.4866\n",
      "227/388, train_loss: 0.2161, step time: 0.7258\n",
      "228/388, train_loss: 0.1950, step time: 0.5460\n",
      "229/388, train_loss: 0.1493, step time: 0.5082\n",
      "230/388, train_loss: 0.2328, step time: 0.4919\n",
      "231/388, train_loss: 0.0649, step time: 0.4959\n",
      "232/388, train_loss: 0.2373, step time: 0.4779\n",
      "233/388, train_loss: 0.0857, step time: 0.6068\n",
      "234/388, train_loss: 0.1040, step time: 0.5467\n",
      "235/388, train_loss: 0.1457, step time: 0.5196\n",
      "236/388, train_loss: 0.1078, step time: 0.4969\n",
      "237/388, train_loss: 0.4109, step time: 0.4924\n",
      "238/388, train_loss: 0.2646, step time: 0.4995\n",
      "239/388, train_loss: 0.4562, step time: 0.4891\n",
      "240/388, train_loss: 0.0465, step time: 1.1263\n",
      "241/388, train_loss: 0.2351, step time: 0.5327\n",
      "242/388, train_loss: 0.0620, step time: 0.5023\n",
      "243/388, train_loss: 0.0909, step time: 0.4890\n",
      "244/388, train_loss: 0.1493, step time: 0.4913\n",
      "245/388, train_loss: 0.0760, step time: 0.4934\n",
      "246/388, train_loss: 0.1034, step time: 0.4802\n",
      "247/388, train_loss: 0.1898, step time: 0.4957\n",
      "248/388, train_loss: 0.3450, step time: 0.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/388, train_loss: 0.0928, step time: 1.0274\n",
      "250/388, train_loss: 0.1205, step time: 0.5374\n",
      "251/388, train_loss: 0.0894, step time: 0.5252\n",
      "252/388, train_loss: 0.1148, step time: 0.4982\n",
      "253/388, train_loss: 0.0893, step time: 0.4994\n",
      "254/388, train_loss: 0.0549, step time: 0.4888\n",
      "255/388, train_loss: 0.3362, step time: 0.4881\n",
      "256/388, train_loss: 0.2133, step time: 0.4803\n",
      "257/388, train_loss: 0.0678, step time: 0.4737\n",
      "258/388, train_loss: 0.1967, step time: 0.5261\n",
      "259/388, train_loss: 0.0693, step time: 0.5235\n",
      "260/388, train_loss: 0.2607, step time: 0.5113\n",
      "261/388, train_loss: 0.1472, step time: 0.4954\n",
      "262/388, train_loss: 0.0903, step time: 0.4877\n",
      "263/388, train_loss: 0.6263, step time: 0.4797\n",
      "264/388, train_loss: 0.2326, step time: 0.5272\n",
      "265/388, train_loss: 0.2428, step time: 0.5099\n",
      "266/388, train_loss: 0.1179, step time: 0.5055\n",
      "267/388, train_loss: 0.2775, step time: 0.4882\n",
      "268/388, train_loss: 0.3762, step time: 1.1810\n",
      "269/388, train_loss: 0.3541, step time: 0.5369\n",
      "270/388, train_loss: 0.1266, step time: 0.5050\n",
      "271/388, train_loss: 0.0316, step time: 0.4858\n",
      "272/388, train_loss: 0.2296, step time: 0.4977\n",
      "273/388, train_loss: 0.0901, step time: 0.4868\n",
      "274/388, train_loss: 0.1848, step time: 0.5235\n",
      "275/388, train_loss: 0.1484, step time: 0.5983\n",
      "276/388, train_loss: 0.1282, step time: 0.5550\n",
      "277/388, train_loss: 0.1915, step time: 0.5212\n",
      "278/388, train_loss: 0.3848, step time: 0.5124\n",
      "279/388, train_loss: 0.0369, step time: 0.4865\n",
      "280/388, train_loss: 0.1431, step time: 1.1248\n",
      "281/388, train_loss: 0.1447, step time: 0.5409\n",
      "282/388, train_loss: 0.2195, step time: 0.5289\n",
      "283/388, train_loss: 0.0343, step time: 0.5062\n",
      "284/388, train_loss: 0.2450, step time: 0.5299\n",
      "285/388, train_loss: 0.3362, step time: 0.5243\n",
      "286/388, train_loss: 0.2440, step time: 0.5018\n",
      "287/388, train_loss: 0.2321, step time: 0.5421\n",
      "288/388, train_loss: 0.2587, step time: 0.5234\n",
      "289/388, train_loss: 0.0777, step time: 0.5130\n",
      "290/388, train_loss: 0.1391, step time: 0.5012\n",
      "291/388, train_loss: 0.1379, step time: 0.7168\n",
      "292/388, train_loss: 0.2116, step time: 0.5462\n",
      "293/388, train_loss: 0.1707, step time: 0.5074\n",
      "294/388, train_loss: 0.0943, step time: 0.4999\n",
      "295/388, train_loss: 0.1044, step time: 0.4904\n",
      "296/388, train_loss: 0.2163, step time: 0.4969\n",
      "297/388, train_loss: 0.0840, step time: 0.4797\n",
      "298/388, train_loss: 0.3773, step time: 0.4890\n",
      "299/388, train_loss: 0.2384, step time: 0.4918\n",
      "300/388, train_loss: 0.0433, step time: 0.4903\n",
      "301/388, train_loss: 0.2064, step time: 0.5283\n",
      "302/388, train_loss: 0.0719, step time: 0.5302\n",
      "303/388, train_loss: 0.0809, step time: 0.5202\n",
      "304/388, train_loss: 0.3705, step time: 0.5037\n",
      "305/388, train_loss: 0.1096, step time: 0.5024\n",
      "306/388, train_loss: 0.3167, step time: 0.4938\n",
      "307/388, train_loss: 0.1943, step time: 1.0303\n",
      "308/388, train_loss: 0.1215, step time: 0.5466\n",
      "309/388, train_loss: 0.2904, step time: 0.5211\n",
      "310/388, train_loss: 0.0932, step time: 0.5066\n",
      "311/388, train_loss: 0.0596, step time: 0.4870\n",
      "312/388, train_loss: 0.4382, step time: 0.5033\n",
      "313/388, train_loss: 0.1223, step time: 0.5023\n",
      "314/388, train_loss: 0.1804, step time: 0.5037\n",
      "315/388, train_loss: 0.3519, step time: 0.4952\n",
      "316/388, train_loss: 0.2593, step time: 0.5368\n",
      "317/388, train_loss: 0.1020, step time: 0.5222\n",
      "318/388, train_loss: 0.2824, step time: 0.5038\n",
      "319/388, train_loss: 0.0633, step time: 0.4850\n",
      "320/388, train_loss: 0.1625, step time: 0.5134\n",
      "321/388, train_loss: 0.1161, step time: 0.4921\n",
      "322/388, train_loss: 0.2299, step time: 0.5516\n",
      "323/388, train_loss: 0.1342, step time: 0.5378\n",
      "324/388, train_loss: 0.2409, step time: 0.6293\n",
      "325/388, train_loss: 0.4252, step time: 0.6103\n",
      "326/388, train_loss: 0.2722, step time: 0.5274\n",
      "327/388, train_loss: 0.1190, step time: 0.5200\n",
      "328/388, train_loss: 0.1466, step time: 0.5124\n",
      "329/388, train_loss: 0.0894, step time: 0.5732\n",
      "330/388, train_loss: 0.1479, step time: 0.5500\n",
      "331/388, train_loss: 0.2826, step time: 0.5126\n",
      "332/388, train_loss: 0.1002, step time: 0.4996\n",
      "333/388, train_loss: 0.1190, step time: 0.5388\n",
      "334/388, train_loss: 0.0631, step time: 0.5029\n",
      "335/388, train_loss: 0.0663, step time: 0.4985\n",
      "336/388, train_loss: 0.5165, step time: 0.4855\n",
      "337/388, train_loss: 0.1884, step time: 0.4909\n",
      "338/388, train_loss: 0.1027, step time: 1.1312\n",
      "339/388, train_loss: 0.1028, step time: 0.5319\n",
      "340/388, train_loss: 0.1981, step time: 0.5093\n",
      "341/388, train_loss: 0.5154, step time: 0.4897\n",
      "342/388, train_loss: 0.1998, step time: 0.4968\n",
      "343/388, train_loss: 0.1348, step time: 0.4985\n",
      "344/388, train_loss: 0.1651, step time: 0.4929\n",
      "345/388, train_loss: 0.1730, step time: 0.4965\n",
      "346/388, train_loss: 0.4088, step time: 0.4856\n",
      "347/388, train_loss: 0.1051, step time: 0.4928\n",
      "348/388, train_loss: 0.4910, step time: 1.0298\n",
      "349/388, train_loss: 0.1203, step time: 0.5352\n",
      "350/388, train_loss: 0.1436, step time: 0.5167\n",
      "351/388, train_loss: 0.1649, step time: 0.5063\n",
      "352/388, train_loss: 0.1567, step time: 0.4946\n",
      "353/388, train_loss: 0.0658, step time: 0.5131\n",
      "354/388, train_loss: 0.1471, step time: 0.5391\n",
      "355/388, train_loss: 0.2070, step time: 0.6297\n",
      "356/388, train_loss: 0.0620, step time: 0.5502\n",
      "357/388, train_loss: 0.2564, step time: 0.5292\n",
      "358/388, train_loss: 0.1478, step time: 0.4995\n",
      "359/388, train_loss: 0.1807, step time: 0.5113\n",
      "360/388, train_loss: 0.0567, step time: 0.4954\n",
      "361/388, train_loss: 0.0593, step time: 0.4947\n",
      "362/388, train_loss: 0.0746, step time: 0.4837\n",
      "363/388, train_loss: 0.2286, step time: 0.4897\n",
      "364/388, train_loss: 0.1547, step time: 0.5255\n",
      "365/388, train_loss: 0.2244, step time: 0.5263\n",
      "366/388, train_loss: 0.3758, step time: 0.5190\n",
      "367/388, train_loss: 0.2898, step time: 0.5205\n",
      "368/388, train_loss: 0.2173, step time: 0.4973\n",
      "369/388, train_loss: 0.1916, step time: 1.2346\n",
      "370/388, train_loss: 0.1335, step time: 0.5383\n",
      "371/388, train_loss: 0.0570, step time: 0.5019\n",
      "372/388, train_loss: 0.1467, step time: 0.5227\n",
      "373/388, train_loss: 0.1739, step time: 0.5232\n",
      "374/388, train_loss: 0.3411, step time: 0.5221\n",
      "375/388, train_loss: 0.1293, step time: 0.5452\n",
      "376/388, train_loss: 0.0929, step time: 0.5664\n",
      "377/388, train_loss: 0.1075, step time: 0.5319\n",
      "378/388, train_loss: 0.0895, step time: 0.5230\n",
      "379/388, train_loss: 0.0444, step time: 0.4904\n",
      "380/388, train_loss: 0.1467, step time: 0.5026\n",
      "381/388, train_loss: 0.1134, step time: 0.4918\n",
      "382/388, train_loss: 0.1162, step time: 0.5291\n",
      "383/388, train_loss: 0.1307, step time: 0.5017\n",
      "384/388, train_loss: 0.1930, step time: 0.4944\n",
      "385/388, train_loss: 0.0683, step time: 0.5446\n",
      "386/388, train_loss: 0.1487, step time: 0.5233\n",
      "387/388, train_loss: 0.1050, step time: 0.5014\n",
      "388/388, train_loss: 0.1327, step time: 0.5373\n",
      "epoch 90 average loss: 0.1713\n",
      "current epoch: 90 current mean dice: 0.7735 tc: 0.8277 wt: 0.9021 et: 0.5906\n",
      "best mean dice: 0.7788 at epoch: 80\n",
      "time consuming of epoch 90 is: 303.2451\n",
      "----------\n",
      "epoch 91/300\n",
      "1/388, train_loss: 0.0767, step time: 0.4755\n",
      "2/388, train_loss: 0.1017, step time: 0.4963\n",
      "3/388, train_loss: 0.0953, step time: 0.9592\n",
      "4/388, train_loss: 0.0791, step time: 0.5496\n",
      "5/388, train_loss: 0.2478, step time: 0.5103\n",
      "6/388, train_loss: 0.2724, step time: 0.4968\n",
      "7/388, train_loss: 0.2050, step time: 0.4957\n",
      "8/388, train_loss: 0.1441, step time: 0.5954\n",
      "9/388, train_loss: 0.0973, step time: 0.5215\n",
      "10/388, train_loss: 0.2415, step time: 0.7256\n",
      "11/388, train_loss: 0.2258, step time: 0.6326\n",
      "12/388, train_loss: 0.1747, step time: 0.5410\n",
      "13/388, train_loss: 0.4547, step time: 0.5012\n",
      "14/388, train_loss: 0.2408, step time: 0.6267\n",
      "15/388, train_loss: 0.3720, step time: 0.6243\n",
      "16/388, train_loss: 0.2257, step time: 0.5603\n",
      "17/388, train_loss: 0.0875, step time: 0.5207\n",
      "18/388, train_loss: 0.5127, step time: 0.5018\n",
      "19/388, train_loss: 0.1411, step time: 0.9373\n",
      "20/388, train_loss: 0.1938, step time: 0.5980\n",
      "21/388, train_loss: 0.1703, step time: 0.5533\n",
      "22/388, train_loss: 0.0575, step time: 0.5179\n",
      "23/388, train_loss: 0.1489, step time: 0.5334\n",
      "24/388, train_loss: 0.3340, step time: 0.6305\n",
      "25/388, train_loss: 0.1467, step time: 0.5631\n",
      "26/388, train_loss: 0.0655, step time: 0.5412\n",
      "27/388, train_loss: 0.0703, step time: 0.5131\n",
      "28/388, train_loss: 0.1310, step time: 0.5131\n",
      "29/388, train_loss: 0.5873, step time: 0.5287\n",
      "30/388, train_loss: 0.2424, step time: 0.5733\n",
      "31/388, train_loss: 0.0808, step time: 0.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/388, train_loss: 0.1438, step time: 0.5095\n",
      "33/388, train_loss: 0.1011, step time: 0.5908\n",
      "34/388, train_loss: 0.1019, step time: 0.5365\n",
      "35/388, train_loss: 0.0498, step time: 0.5036\n",
      "36/388, train_loss: 0.0823, step time: 0.4896\n",
      "37/388, train_loss: 0.2887, step time: 0.5192\n",
      "38/388, train_loss: 0.1814, step time: 0.5238\n",
      "39/388, train_loss: 0.4011, step time: 0.5184\n",
      "40/388, train_loss: 0.1466, step time: 0.4862\n",
      "41/388, train_loss: 0.2508, step time: 1.1270\n",
      "42/388, train_loss: 0.0635, step time: 0.5398\n",
      "43/388, train_loss: 0.1307, step time: 0.5265\n",
      "44/388, train_loss: 0.2246, step time: 0.5002\n",
      "45/388, train_loss: 0.1511, step time: 0.4857\n",
      "46/388, train_loss: 0.1291, step time: 0.9683\n",
      "47/388, train_loss: 0.1972, step time: 0.5329\n",
      "48/388, train_loss: 0.4047, step time: 0.5138\n",
      "49/388, train_loss: 0.2983, step time: 0.5025\n",
      "50/388, train_loss: 0.0868, step time: 0.4878\n",
      "51/388, train_loss: 0.1597, step time: 0.5296\n",
      "52/388, train_loss: 0.1963, step time: 0.5045\n",
      "53/388, train_loss: 0.0902, step time: 0.5028\n",
      "54/388, train_loss: 0.2083, step time: 0.5263\n",
      "55/388, train_loss: 0.1913, step time: 0.5343\n",
      "56/388, train_loss: 0.2924, step time: 0.5482\n",
      "57/388, train_loss: 0.2861, step time: 0.6565\n",
      "58/388, train_loss: 0.1278, step time: 0.5372\n",
      "59/388, train_loss: 0.3014, step time: 0.5105\n",
      "60/388, train_loss: 0.5387, step time: 0.5116\n",
      "61/388, train_loss: 0.2061, step time: 0.5570\n",
      "62/388, train_loss: 0.0948, step time: 0.5354\n",
      "63/388, train_loss: 0.1038, step time: 0.5064\n",
      "64/388, train_loss: 0.1812, step time: 0.5018\n",
      "65/388, train_loss: 0.0966, step time: 0.5009\n",
      "66/388, train_loss: 0.1155, step time: 0.4937\n",
      "67/388, train_loss: 0.0730, step time: 0.5549\n",
      "68/388, train_loss: 0.1892, step time: 0.5355\n",
      "69/388, train_loss: 0.2217, step time: 0.5043\n",
      "70/388, train_loss: 0.1244, step time: 0.5242\n",
      "71/388, train_loss: 0.3398, step time: 0.5252\n",
      "72/388, train_loss: 0.2206, step time: 0.5168\n",
      "73/388, train_loss: 0.1120, step time: 0.5122\n",
      "74/388, train_loss: 0.1336, step time: 0.4948\n",
      "75/388, train_loss: 0.1694, step time: 0.5055\n",
      "76/388, train_loss: 0.1643, step time: 0.5710\n",
      "77/388, train_loss: 0.2070, step time: 0.5966\n",
      "78/388, train_loss: 0.1950, step time: 0.5428\n",
      "79/388, train_loss: 0.1038, step time: 0.5208\n",
      "80/388, train_loss: 0.0960, step time: 0.5249\n",
      "81/388, train_loss: 0.0736, step time: 0.5116\n",
      "82/388, train_loss: 0.1634, step time: 0.4950\n",
      "83/388, train_loss: 0.1960, step time: 0.4923\n",
      "84/388, train_loss: 0.0954, step time: 0.4839\n",
      "85/388, train_loss: 0.1208, step time: 0.5113\n",
      "86/388, train_loss: 0.1443, step time: 0.9043\n",
      "87/388, train_loss: 0.2914, step time: 0.5408\n",
      "88/388, train_loss: 0.1541, step time: 0.5212\n",
      "89/388, train_loss: 0.1029, step time: 0.4944\n",
      "90/388, train_loss: 0.2103, step time: 0.5064\n",
      "91/388, train_loss: 0.2777, step time: 0.5053\n",
      "92/388, train_loss: 0.1402, step time: 0.5684\n",
      "93/388, train_loss: 0.1115, step time: 0.5402\n",
      "94/388, train_loss: 0.2220, step time: 0.5121\n",
      "95/388, train_loss: 0.2618, step time: 0.4971\n",
      "96/388, train_loss: 0.1972, step time: 1.0687\n",
      "97/388, train_loss: 0.3145, step time: 0.5337\n",
      "98/388, train_loss: 0.3467, step time: 0.5075\n",
      "99/388, train_loss: 0.2712, step time: 0.4930\n",
      "100/388, train_loss: 0.1381, step time: 0.4945\n",
      "101/388, train_loss: 0.0624, step time: 0.4916\n",
      "102/388, train_loss: 0.1144, step time: 0.4770\n",
      "103/388, train_loss: 0.2483, step time: 0.4796\n",
      "104/388, train_loss: 0.1166, step time: 0.4757\n",
      "105/388, train_loss: 0.1634, step time: 0.7687\n",
      "106/388, train_loss: 0.4351, step time: 0.5397\n",
      "107/388, train_loss: 0.1233, step time: 0.4938\n",
      "108/388, train_loss: 0.0920, step time: 0.5078\n",
      "109/388, train_loss: 0.1383, step time: 0.4967\n",
      "110/388, train_loss: 0.1482, step time: 0.5485\n",
      "111/388, train_loss: 0.1286, step time: 0.5243\n",
      "112/388, train_loss: 0.0595, step time: 0.4976\n",
      "113/388, train_loss: 0.1797, step time: 0.5350\n",
      "114/388, train_loss: 0.0855, step time: 0.5109\n",
      "115/388, train_loss: 0.1354, step time: 0.5036\n",
      "116/388, train_loss: 0.2796, step time: 0.4890\n",
      "117/388, train_loss: 0.1982, step time: 1.1774\n",
      "118/388, train_loss: 0.2286, step time: 0.5362\n",
      "119/388, train_loss: 0.1068, step time: 0.5061\n",
      "120/388, train_loss: 0.3588, step time: 0.5001\n",
      "121/388, train_loss: 0.1517, step time: 0.4887\n",
      "122/388, train_loss: 0.1275, step time: 0.4908\n",
      "123/388, train_loss: 0.1837, step time: 0.4900\n",
      "124/388, train_loss: 0.0764, step time: 0.4752\n",
      "125/388, train_loss: 0.2151, step time: 0.9770\n",
      "126/388, train_loss: 0.0822, step time: 0.5465\n",
      "127/388, train_loss: 0.1756, step time: 0.5216\n",
      "128/388, train_loss: 0.1560, step time: 0.5027\n",
      "129/388, train_loss: 0.2262, step time: 0.4944\n",
      "130/388, train_loss: 0.0797, step time: 0.4933\n",
      "131/388, train_loss: 0.1807, step time: 0.4800\n",
      "132/388, train_loss: 0.1753, step time: 0.4901\n",
      "133/388, train_loss: 0.2329, step time: 0.5078\n",
      "134/388, train_loss: 0.2069, step time: 0.5166\n",
      "135/388, train_loss: 0.2135, step time: 0.5368\n",
      "136/388, train_loss: 0.1356, step time: 0.6195\n",
      "137/388, train_loss: 0.1042, step time: 0.5257\n",
      "138/388, train_loss: 0.1033, step time: 0.4969\n",
      "139/388, train_loss: 0.0991, step time: 0.4955\n",
      "140/388, train_loss: 0.0965, step time: 0.4951\n",
      "141/388, train_loss: 0.0684, step time: 1.1962\n",
      "142/388, train_loss: 0.1537, step time: 0.5328\n",
      "143/388, train_loss: 0.1653, step time: 0.5042\n",
      "144/388, train_loss: 0.1248, step time: 0.4955\n",
      "145/388, train_loss: 0.2273, step time: 0.4831\n",
      "146/388, train_loss: 0.0926, step time: 0.4797\n",
      "147/388, train_loss: 0.1164, step time: 0.4802\n",
      "148/388, train_loss: 0.2319, step time: 0.4756\n",
      "149/388, train_loss: 0.2012, step time: 0.4728\n",
      "150/388, train_loss: 0.1593, step time: 0.4729\n",
      "151/388, train_loss: 0.1364, step time: 0.5141\n",
      "152/388, train_loss: 0.1160, step time: 0.5238\n",
      "153/388, train_loss: 0.1973, step time: 0.5261\n",
      "154/388, train_loss: 0.1366, step time: 0.4953\n",
      "155/388, train_loss: 0.3945, step time: 0.5039\n",
      "156/388, train_loss: 0.0952, step time: 0.5385\n",
      "157/388, train_loss: 0.2111, step time: 0.5278\n",
      "158/388, train_loss: 0.1739, step time: 0.5079\n",
      "159/388, train_loss: 0.0992, step time: 0.4961\n",
      "160/388, train_loss: 0.1127, step time: 0.4810\n",
      "161/388, train_loss: 0.4231, step time: 0.4860\n",
      "162/388, train_loss: 0.0657, step time: 1.1354\n",
      "163/388, train_loss: 0.1370, step time: 0.5350\n",
      "164/388, train_loss: 0.2608, step time: 0.5035\n",
      "165/388, train_loss: 0.2200, step time: 0.4824\n",
      "166/388, train_loss: 0.2804, step time: 0.4805\n",
      "167/388, train_loss: 0.0979, step time: 0.4922\n",
      "168/388, train_loss: 0.1838, step time: 0.4771\n",
      "169/388, train_loss: 0.3447, step time: 0.4784\n",
      "170/388, train_loss: 0.2873, step time: 0.4723\n",
      "171/388, train_loss: 0.1574, step time: 0.4705\n",
      "172/388, train_loss: 0.1542, step time: 0.7275\n",
      "173/388, train_loss: 0.1635, step time: 0.5549\n",
      "174/388, train_loss: 0.0945, step time: 0.5237\n",
      "175/388, train_loss: 0.1009, step time: 0.5062\n",
      "176/388, train_loss: 0.0954, step time: 0.5067\n",
      "177/388, train_loss: 0.4360, step time: 0.5072\n",
      "178/388, train_loss: 0.1080, step time: 0.4841\n",
      "179/388, train_loss: 0.1318, step time: 0.4992\n",
      "180/388, train_loss: 0.2653, step time: 0.4881\n",
      "181/388, train_loss: 0.3097, step time: 0.7426\n",
      "182/388, train_loss: 0.0895, step time: 0.5968\n",
      "183/388, train_loss: 0.1932, step time: 0.5315\n",
      "184/388, train_loss: 0.2121, step time: 0.5102\n",
      "185/388, train_loss: 0.0632, step time: 0.5106\n",
      "186/388, train_loss: 0.2324, step time: 0.4993\n",
      "187/388, train_loss: 0.0795, step time: 0.4884\n",
      "188/388, train_loss: 0.1086, step time: 1.2197\n",
      "189/388, train_loss: 0.0966, step time: 0.5444\n",
      "190/388, train_loss: 0.2534, step time: 0.5079\n",
      "191/388, train_loss: 0.1109, step time: 0.5016\n",
      "192/388, train_loss: 0.2949, step time: 0.4866\n",
      "193/388, train_loss: 0.1238, step time: 0.4901\n",
      "194/388, train_loss: 0.1394, step time: 0.4892\n",
      "195/388, train_loss: 0.1258, step time: 0.4798\n",
      "196/388, train_loss: 0.1982, step time: 0.9265\n",
      "197/388, train_loss: 0.0735, step time: 0.5416\n",
      "198/388, train_loss: 0.0461, step time: 0.5094\n",
      "199/388, train_loss: 0.0920, step time: 0.4951\n",
      "200/388, train_loss: 0.1457, step time: 0.4799\n",
      "201/388, train_loss: 0.1437, step time: 0.4735\n",
      "202/388, train_loss: 0.2327, step time: 0.4727\n",
      "203/388, train_loss: 0.1177, step time: 0.8303\n",
      "204/388, train_loss: 0.1712, step time: 0.5476\n",
      "205/388, train_loss: 0.1803, step time: 0.5230\n",
      "206/388, train_loss: 0.0776, step time: 0.5061\n",
      "207/388, train_loss: 0.1284, step time: 0.4942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/388, train_loss: 0.2681, step time: 0.5026\n",
      "209/388, train_loss: 0.1423, step time: 0.4997\n",
      "210/388, train_loss: 0.0565, step time: 0.4898\n",
      "211/388, train_loss: 0.0838, step time: 0.5080\n",
      "212/388, train_loss: 0.1476, step time: 0.4964\n",
      "213/388, train_loss: 0.1905, step time: 0.4956\n",
      "214/388, train_loss: 0.1454, step time: 0.4813\n",
      "215/388, train_loss: 0.1195, step time: 0.4795\n",
      "216/388, train_loss: 0.3688, step time: 1.0539\n",
      "217/388, train_loss: 0.2848, step time: 0.5450\n",
      "218/388, train_loss: 0.0939, step time: 0.5147\n",
      "219/388, train_loss: 0.2049, step time: 0.5071\n",
      "220/388, train_loss: 0.0375, step time: 0.4949\n",
      "221/388, train_loss: 0.0887, step time: 0.4982\n",
      "222/388, train_loss: 0.0833, step time: 0.4847\n",
      "223/388, train_loss: 0.0705, step time: 0.4815\n",
      "224/388, train_loss: 0.2164, step time: 0.4965\n",
      "225/388, train_loss: 0.0885, step time: 0.4899\n",
      "226/388, train_loss: 0.2369, step time: 0.4892\n",
      "227/388, train_loss: 0.0711, step time: 0.4772\n",
      "228/388, train_loss: 0.0934, step time: 0.4878\n",
      "229/388, train_loss: 0.1669, step time: 0.5149\n",
      "230/388, train_loss: 0.1690, step time: 0.4942\n",
      "231/388, train_loss: 0.0961, step time: 0.4824\n",
      "232/388, train_loss: 0.2053, step time: 0.4927\n",
      "233/388, train_loss: 0.0955, step time: 0.6150\n",
      "234/388, train_loss: 0.3745, step time: 0.5631\n",
      "235/388, train_loss: 0.1548, step time: 0.5255\n",
      "236/388, train_loss: 0.0852, step time: 0.5040\n",
      "237/388, train_loss: 0.2150, step time: 0.4841\n",
      "238/388, train_loss: 0.1177, step time: 0.5036\n",
      "239/388, train_loss: 0.0734, step time: 0.5125\n",
      "240/388, train_loss: 0.4555, step time: 0.5051\n",
      "241/388, train_loss: 0.1555, step time: 0.4838\n",
      "242/388, train_loss: 0.2778, step time: 0.5331\n",
      "243/388, train_loss: 0.4372, step time: 0.5141\n",
      "244/388, train_loss: 0.1269, step time: 0.9588\n",
      "245/388, train_loss: 0.4679, step time: 0.5632\n",
      "246/388, train_loss: 0.1602, step time: 0.5180\n",
      "247/388, train_loss: 0.0670, step time: 0.5049\n",
      "248/388, train_loss: 0.2473, step time: 0.5003\n",
      "249/388, train_loss: 0.1921, step time: 0.4882\n",
      "250/388, train_loss: 0.2153, step time: 1.1739\n",
      "251/388, train_loss: 0.1115, step time: 0.5324\n",
      "252/388, train_loss: 0.1745, step time: 0.5012\n",
      "253/388, train_loss: 0.1610, step time: 0.4928\n",
      "254/388, train_loss: 0.1129, step time: 0.4813\n",
      "255/388, train_loss: 0.0605, step time: 0.4807\n",
      "256/388, train_loss: 0.0968, step time: 0.4826\n",
      "257/388, train_loss: 0.1171, step time: 1.1063\n",
      "258/388, train_loss: 0.3911, step time: 0.5268\n",
      "259/388, train_loss: 0.1994, step time: 0.5046\n",
      "260/388, train_loss: 0.0990, step time: 0.4878\n",
      "261/388, train_loss: 0.1680, step time: 0.4854\n",
      "262/388, train_loss: 0.2595, step time: 0.4941\n",
      "263/388, train_loss: 0.0961, step time: 0.4821\n",
      "264/388, train_loss: 0.0829, step time: 0.4760\n",
      "265/388, train_loss: 0.3423, step time: 0.4892\n",
      "266/388, train_loss: 0.0464, step time: 0.4817\n",
      "267/388, train_loss: 0.5327, step time: 0.4984\n",
      "268/388, train_loss: 0.1202, step time: 0.5553\n",
      "269/388, train_loss: 0.1046, step time: 0.5386\n",
      "270/388, train_loss: 0.2268, step time: 0.5232\n",
      "271/388, train_loss: 0.1325, step time: 0.4958\n",
      "272/388, train_loss: 0.0690, step time: 0.4876\n",
      "273/388, train_loss: 0.1368, step time: 0.4902\n",
      "274/388, train_loss: 0.2041, step time: 0.5320\n",
      "275/388, train_loss: 0.5412, step time: 0.5198\n",
      "276/388, train_loss: 0.2163, step time: 0.5097\n",
      "277/388, train_loss: 0.1159, step time: 0.4937\n",
      "278/388, train_loss: 0.3683, step time: 0.4941\n",
      "279/388, train_loss: 0.2610, step time: 0.4892\n",
      "280/388, train_loss: 0.3432, step time: 1.1179\n",
      "281/388, train_loss: 0.1477, step time: 0.5458\n",
      "282/388, train_loss: 0.0649, step time: 0.5107\n",
      "283/388, train_loss: 0.0786, step time: 0.5057\n",
      "284/388, train_loss: 0.0317, step time: 0.4857\n",
      "285/388, train_loss: 0.2055, step time: 0.4819\n",
      "286/388, train_loss: 0.2690, step time: 0.4923\n",
      "287/388, train_loss: 0.1413, step time: 0.4777\n",
      "288/388, train_loss: 0.0987, step time: 0.4980\n",
      "289/388, train_loss: 0.1974, step time: 0.5062\n",
      "290/388, train_loss: 0.1433, step time: 0.5005\n",
      "291/388, train_loss: 0.2324, step time: 0.7225\n",
      "292/388, train_loss: 0.2620, step time: 0.5638\n",
      "293/388, train_loss: 0.1239, step time: 0.5275\n",
      "294/388, train_loss: 0.1505, step time: 0.5146\n",
      "295/388, train_loss: 0.1908, step time: 0.5214\n",
      "296/388, train_loss: 0.1845, step time: 0.6387\n",
      "297/388, train_loss: 0.0860, step time: 0.5375\n",
      "298/388, train_loss: 0.0636, step time: 0.5195\n",
      "299/388, train_loss: 0.1471, step time: 0.4991\n",
      "300/388, train_loss: 0.2533, step time: 0.5004\n",
      "301/388, train_loss: 0.1033, step time: 0.5096\n",
      "302/388, train_loss: 0.1900, step time: 0.5174\n",
      "303/388, train_loss: 0.0267, step time: 0.5361\n",
      "304/388, train_loss: 0.1221, step time: 0.5176\n",
      "305/388, train_loss: 0.0974, step time: 0.5146\n",
      "306/388, train_loss: 0.3293, step time: 0.4975\n",
      "307/388, train_loss: 0.1307, step time: 0.4837\n",
      "308/388, train_loss: 0.0827, step time: 0.5122\n",
      "309/388, train_loss: 0.1524, step time: 0.4953\n",
      "310/388, train_loss: 0.0440, step time: 0.4933\n",
      "311/388, train_loss: 0.5071, step time: 0.4798\n",
      "312/388, train_loss: 0.2020, step time: 0.5910\n",
      "313/388, train_loss: 0.1205, step time: 0.5196\n",
      "314/388, train_loss: 0.1570, step time: 0.4919\n",
      "315/388, train_loss: 0.1779, step time: 0.5155\n",
      "316/388, train_loss: 0.2655, step time: 0.5231\n",
      "317/388, train_loss: 0.0582, step time: 0.5192\n",
      "318/388, train_loss: 0.2568, step time: 0.4990\n",
      "319/388, train_loss: 0.1169, step time: 0.5053\n",
      "320/388, train_loss: 0.1250, step time: 0.4820\n",
      "321/388, train_loss: 0.0576, step time: 1.0945\n",
      "322/388, train_loss: 0.0898, step time: 0.5244\n",
      "323/388, train_loss: 0.1943, step time: 0.5077\n",
      "324/388, train_loss: 0.1562, step time: 0.4940\n",
      "325/388, train_loss: 0.0421, step time: 0.5040\n",
      "326/388, train_loss: 0.0841, step time: 0.4940\n",
      "327/388, train_loss: 0.1713, step time: 0.5269\n",
      "328/388, train_loss: 0.1342, step time: 0.5142\n",
      "329/388, train_loss: 0.0863, step time: 0.5019\n",
      "330/388, train_loss: 0.1462, step time: 0.4910\n",
      "331/388, train_loss: 0.2491, step time: 0.5016\n",
      "332/388, train_loss: 0.1235, step time: 0.4970\n",
      "333/388, train_loss: 0.3612, step time: 0.4954\n",
      "334/388, train_loss: 0.0807, step time: 0.4803\n",
      "335/388, train_loss: 0.0597, step time: 0.4810\n",
      "336/388, train_loss: 0.2815, step time: 0.5006\n",
      "337/388, train_loss: 0.2466, step time: 1.1932\n",
      "338/388, train_loss: 0.2799, step time: 0.5532\n",
      "339/388, train_loss: 0.1726, step time: 0.5177\n",
      "340/388, train_loss: 0.2356, step time: 0.4991\n",
      "341/388, train_loss: 0.0976, step time: 0.4849\n",
      "342/388, train_loss: 0.0610, step time: 0.4856\n",
      "343/388, train_loss: 0.2225, step time: 0.4928\n",
      "344/388, train_loss: 0.0420, step time: 0.5076\n",
      "345/388, train_loss: 0.1433, step time: 0.4816\n",
      "346/388, train_loss: 0.1698, step time: 0.4891\n",
      "347/388, train_loss: 0.1503, step time: 1.1905\n",
      "348/388, train_loss: 0.0896, step time: 0.5230\n",
      "349/388, train_loss: 0.0618, step time: 0.5003\n",
      "350/388, train_loss: 0.1469, step time: 0.4841\n",
      "351/388, train_loss: 0.3175, step time: 0.4973\n",
      "352/388, train_loss: 0.3584, step time: 0.4976\n",
      "353/388, train_loss: 0.1281, step time: 0.4879\n",
      "354/388, train_loss: 0.0667, step time: 0.4979\n",
      "355/388, train_loss: 0.2420, step time: 0.5404\n",
      "356/388, train_loss: 0.2061, step time: 0.5691\n",
      "357/388, train_loss: 0.1513, step time: 0.5272\n",
      "358/388, train_loss: 0.0778, step time: 0.5111\n",
      "359/388, train_loss: 0.1527, step time: 0.4872\n",
      "360/388, train_loss: 0.3117, step time: 0.5026\n",
      "361/388, train_loss: 0.0437, step time: 0.5333\n",
      "362/388, train_loss: 0.1039, step time: 0.5042\n",
      "363/388, train_loss: 0.0914, step time: 0.4914\n",
      "364/388, train_loss: 0.1076, step time: 0.5055\n",
      "365/388, train_loss: 0.2206, step time: 0.5570\n",
      "366/388, train_loss: 0.2864, step time: 0.5387\n",
      "367/388, train_loss: 0.1989, step time: 0.5943\n",
      "368/388, train_loss: 0.1337, step time: 0.5379\n",
      "369/388, train_loss: 0.3044, step time: 0.5074\n",
      "370/388, train_loss: 0.0858, step time: 0.4906\n",
      "371/388, train_loss: 0.0799, step time: 0.4981\n",
      "372/388, train_loss: 0.0488, step time: 1.1365\n",
      "373/388, train_loss: 0.2090, step time: 0.5353\n",
      "374/388, train_loss: 0.1008, step time: 0.5072\n",
      "375/388, train_loss: 0.0445, step time: 0.4960\n",
      "376/388, train_loss: 0.0784, step time: 0.4970\n",
      "377/388, train_loss: 0.2034, step time: 0.4958\n",
      "378/388, train_loss: 0.1246, step time: 0.4921\n",
      "379/388, train_loss: 0.1536, step time: 0.4809\n",
      "380/388, train_loss: 0.1059, step time: 0.4836\n",
      "381/388, train_loss: 0.2438, step time: 0.4707\n",
      "382/388, train_loss: 0.1554, step time: 0.4715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/388, train_loss: 0.3549, step time: 0.5736\n",
      "384/388, train_loss: 0.2012, step time: 0.5315\n",
      "385/388, train_loss: 0.2313, step time: 0.4917\n",
      "386/388, train_loss: 0.0574, step time: 0.4744\n",
      "387/388, train_loss: 0.0934, step time: 0.5017\n",
      "388/388, train_loss: 0.0618, step time: 0.5373\n",
      "epoch 91 average loss: 0.1731\n",
      "saved new best metric model\n",
      "current epoch: 91 current mean dice: 0.7815 tc: 0.8285 wt: 0.9075 et: 0.6083\n",
      "best mean dice: 0.7815 at epoch: 91\n",
      "time consuming of epoch 91 is: 301.2458\n",
      "----------\n",
      "epoch 92/300\n",
      "1/388, train_loss: 0.0920, step time: 0.4736\n",
      "2/388, train_loss: 0.1461, step time: 0.4790\n",
      "3/388, train_loss: 0.0937, step time: 0.4743\n",
      "4/388, train_loss: 0.2842, step time: 0.5620\n",
      "5/388, train_loss: 0.1996, step time: 0.5209\n",
      "6/388, train_loss: 0.0946, step time: 0.5881\n",
      "7/388, train_loss: 0.3485, step time: 0.6362\n",
      "8/388, train_loss: 0.1172, step time: 0.5663\n",
      "9/388, train_loss: 0.0980, step time: 0.5155\n",
      "10/388, train_loss: 0.1545, step time: 0.6798\n",
      "11/388, train_loss: 0.0951, step time: 0.5322\n",
      "12/388, train_loss: 0.3090, step time: 0.7913\n",
      "13/388, train_loss: 0.4023, step time: 0.5973\n",
      "14/388, train_loss: 0.1022, step time: 0.5285\n",
      "15/388, train_loss: 0.2960, step time: 0.5047\n",
      "16/388, train_loss: 0.0760, step time: 0.7430\n",
      "17/388, train_loss: 0.1358, step time: 0.5542\n",
      "18/388, train_loss: 0.0775, step time: 0.5261\n",
      "19/388, train_loss: 0.1266, step time: 0.5105\n",
      "20/388, train_loss: 0.3705, step time: 0.5153\n",
      "21/388, train_loss: 0.0928, step time: 0.5006\n",
      "22/388, train_loss: 0.0899, step time: 1.1615\n",
      "23/388, train_loss: 0.1966, step time: 0.5402\n",
      "24/388, train_loss: 0.0610, step time: 0.5088\n",
      "25/388, train_loss: 0.3473, step time: 0.5537\n",
      "26/388, train_loss: 0.0293, step time: 0.5320\n",
      "27/388, train_loss: 0.1868, step time: 0.5194\n",
      "28/388, train_loss: 0.2303, step time: 0.5494\n",
      "29/388, train_loss: 0.2187, step time: 0.7225\n",
      "30/388, train_loss: 0.3281, step time: 0.5617\n",
      "31/388, train_loss: 0.1095, step time: 0.5256\n",
      "32/388, train_loss: 0.1646, step time: 0.5025\n",
      "33/388, train_loss: 0.1540, step time: 1.1231\n",
      "34/388, train_loss: 0.2081, step time: 0.5510\n",
      "35/388, train_loss: 0.5736, step time: 0.5281\n",
      "36/388, train_loss: 0.3375, step time: 0.5184\n",
      "37/388, train_loss: 0.3037, step time: 0.5958\n",
      "38/388, train_loss: 0.0969, step time: 0.5542\n",
      "39/388, train_loss: 0.2592, step time: 0.5236\n",
      "40/388, train_loss: 0.1004, step time: 0.4925\n",
      "41/388, train_loss: 0.1303, step time: 0.5222\n",
      "42/388, train_loss: 0.2761, step time: 0.5777\n",
      "43/388, train_loss: 0.6730, step time: 0.5357\n",
      "44/388, train_loss: 0.0811, step time: 0.5068\n",
      "45/388, train_loss: 0.1442, step time: 0.4942\n",
      "46/388, train_loss: 0.1772, step time: 0.4922\n",
      "47/388, train_loss: 0.0879, step time: 0.4954\n",
      "48/388, train_loss: 0.3084, step time: 0.9940\n",
      "49/388, train_loss: 0.2812, step time: 0.5343\n",
      "50/388, train_loss: 0.2481, step time: 0.5029\n",
      "51/388, train_loss: 0.2801, step time: 0.5124\n",
      "52/388, train_loss: 0.1516, step time: 0.5397\n",
      "53/388, train_loss: 0.3518, step time: 0.5232\n",
      "54/388, train_loss: 0.0707, step time: 0.5149\n",
      "55/388, train_loss: 0.1255, step time: 0.5549\n",
      "56/388, train_loss: 0.0918, step time: 0.5325\n",
      "57/388, train_loss: 0.1700, step time: 0.5486\n",
      "58/388, train_loss: 0.2035, step time: 0.5782\n",
      "59/388, train_loss: 0.1813, step time: 0.5310\n",
      "60/388, train_loss: 0.3747, step time: 0.5008\n",
      "61/388, train_loss: 0.0608, step time: 0.4959\n",
      "62/388, train_loss: 0.0844, step time: 1.1339\n",
      "63/388, train_loss: 0.0674, step time: 0.5395\n",
      "64/388, train_loss: 0.1933, step time: 0.5129\n",
      "65/388, train_loss: 0.2069, step time: 0.4930\n",
      "66/388, train_loss: 0.1466, step time: 0.5717\n",
      "67/388, train_loss: 0.1947, step time: 0.5144\n",
      "68/388, train_loss: 0.1808, step time: 0.4954\n",
      "69/388, train_loss: 0.1224, step time: 0.4933\n",
      "70/388, train_loss: 0.2153, step time: 1.2089\n",
      "71/388, train_loss: 0.1485, step time: 0.5364\n",
      "72/388, train_loss: 0.0330, step time: 0.5151\n",
      "73/388, train_loss: 0.4521, step time: 0.5029\n",
      "74/388, train_loss: 0.2561, step time: 0.4969\n",
      "75/388, train_loss: 0.0903, step time: 0.5043\n",
      "76/388, train_loss: 0.2016, step time: 0.4880\n",
      "77/388, train_loss: 0.0653, step time: 0.5378\n",
      "78/388, train_loss: 0.1311, step time: 0.5284\n",
      "79/388, train_loss: 0.2637, step time: 0.5081\n",
      "80/388, train_loss: 0.1951, step time: 0.4994\n",
      "81/388, train_loss: 0.0765, step time: 1.0664\n",
      "82/388, train_loss: 0.1472, step time: 0.5421\n",
      "83/388, train_loss: 0.1935, step time: 0.5214\n",
      "84/388, train_loss: 0.3375, step time: 0.5102\n",
      "85/388, train_loss: 0.1032, step time: 0.5024\n",
      "86/388, train_loss: 0.1269, step time: 0.4907\n",
      "87/388, train_loss: 0.3176, step time: 0.5178\n",
      "88/388, train_loss: 0.3119, step time: 0.5070\n",
      "89/388, train_loss: 0.1585, step time: 0.4955\n",
      "90/388, train_loss: 0.2107, step time: 1.0549\n",
      "91/388, train_loss: 0.0617, step time: 0.5507\n",
      "92/388, train_loss: 0.1610, step time: 0.5158\n",
      "93/388, train_loss: 0.4789, step time: 0.4945\n",
      "94/388, train_loss: 0.5139, step time: 0.5285\n",
      "95/388, train_loss: 0.1001, step time: 0.5631\n",
      "96/388, train_loss: 0.0898, step time: 0.5286\n",
      "97/388, train_loss: 0.0939, step time: 0.5037\n",
      "98/388, train_loss: 0.0911, step time: 0.4840\n",
      "99/388, train_loss: 0.0668, step time: 0.4851\n",
      "100/388, train_loss: 0.1224, step time: 1.2640\n",
      "101/388, train_loss: 0.1801, step time: 0.5127\n",
      "102/388, train_loss: 0.1804, step time: 0.4840\n",
      "103/388, train_loss: 0.2452, step time: 0.5060\n",
      "104/388, train_loss: 0.2235, step time: 0.5451\n",
      "105/388, train_loss: 0.1537, step time: 0.5163\n",
      "106/388, train_loss: 0.2244, step time: 0.5000\n",
      "107/388, train_loss: 0.0714, step time: 0.4947\n",
      "108/388, train_loss: 0.2194, step time: 0.4824\n",
      "109/388, train_loss: 0.2149, step time: 0.4998\n",
      "110/388, train_loss: 0.0650, step time: 1.1847\n",
      "111/388, train_loss: 0.0840, step time: 0.5379\n",
      "112/388, train_loss: 0.1940, step time: 0.5093\n",
      "113/388, train_loss: 0.0905, step time: 0.4921\n",
      "114/388, train_loss: 0.1530, step time: 0.4938\n",
      "115/388, train_loss: 0.0537, step time: 0.4794\n",
      "116/388, train_loss: 0.1616, step time: 0.4920\n",
      "117/388, train_loss: 0.0877, step time: 0.4963\n",
      "118/388, train_loss: 0.0475, step time: 0.4859\n",
      "119/388, train_loss: 0.0957, step time: 0.4836\n",
      "120/388, train_loss: 0.2881, step time: 0.5065\n",
      "121/388, train_loss: 0.0416, step time: 0.5167\n",
      "122/388, train_loss: 0.1100, step time: 0.6212\n",
      "123/388, train_loss: 0.1387, step time: 0.5519\n",
      "124/388, train_loss: 0.0957, step time: 0.5431\n",
      "125/388, train_loss: 0.1339, step time: 0.5178\n",
      "126/388, train_loss: 0.2541, step time: 0.4904\n",
      "127/388, train_loss: 0.1537, step time: 0.9842\n",
      "128/388, train_loss: 0.1932, step time: 0.5525\n",
      "129/388, train_loss: 0.0897, step time: 0.5143\n",
      "130/388, train_loss: 0.0835, step time: 0.4950\n",
      "131/388, train_loss: 0.1099, step time: 0.4966\n",
      "132/388, train_loss: 0.2959, step time: 0.5167\n",
      "133/388, train_loss: 0.0967, step time: 0.4952\n",
      "134/388, train_loss: 0.1148, step time: 0.5202\n",
      "135/388, train_loss: 0.0688, step time: 0.5152\n",
      "136/388, train_loss: 0.2038, step time: 0.5016\n",
      "137/388, train_loss: 0.0848, step time: 0.4839\n",
      "138/388, train_loss: 0.1136, step time: 0.4902\n",
      "139/388, train_loss: 0.2572, step time: 0.5493\n",
      "140/388, train_loss: 0.1319, step time: 0.5188\n",
      "141/388, train_loss: 0.1247, step time: 0.5023\n",
      "142/388, train_loss: 0.0830, step time: 1.0346\n",
      "143/388, train_loss: 0.0867, step time: 0.5335\n",
      "144/388, train_loss: 0.1182, step time: 0.5138\n",
      "145/388, train_loss: 0.0903, step time: 0.4997\n",
      "146/388, train_loss: 0.0827, step time: 0.4902\n",
      "147/388, train_loss: 0.0840, step time: 0.4855\n",
      "148/388, train_loss: 0.1580, step time: 0.5110\n",
      "149/388, train_loss: 0.0910, step time: 0.9695\n",
      "150/388, train_loss: 0.1495, step time: 0.5429\n",
      "151/388, train_loss: 0.1273, step time: 0.5142\n",
      "152/388, train_loss: 0.2432, step time: 0.4879\n",
      "153/388, train_loss: 0.1256, step time: 0.4879\n",
      "154/388, train_loss: 0.0853, step time: 0.4861\n",
      "155/388, train_loss: 0.2022, step time: 0.4743\n",
      "156/388, train_loss: 0.1092, step time: 0.4957\n",
      "157/388, train_loss: 0.2746, step time: 0.7068\n",
      "158/388, train_loss: 0.0461, step time: 0.5411\n",
      "159/388, train_loss: 0.1902, step time: 0.5200\n",
      "160/388, train_loss: 0.0732, step time: 0.4994\n",
      "161/388, train_loss: 0.1877, step time: 0.4928\n",
      "162/388, train_loss: 0.0526, step time: 0.4912\n",
      "163/388, train_loss: 0.1362, step time: 0.4816\n",
      "164/388, train_loss: 0.1065, step time: 0.4893\n",
      "165/388, train_loss: 0.4574, step time: 1.0676\n",
      "166/388, train_loss: 0.1002, step time: 0.5258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/388, train_loss: 0.2828, step time: 0.5122\n",
      "168/388, train_loss: 0.0943, step time: 0.4889\n",
      "169/388, train_loss: 0.3326, step time: 0.4867\n",
      "170/388, train_loss: 0.2242, step time: 0.4922\n",
      "171/388, train_loss: 0.1844, step time: 0.4868\n",
      "172/388, train_loss: 0.2328, step time: 0.4935\n",
      "173/388, train_loss: 0.1389, step time: 0.4827\n",
      "174/388, train_loss: 0.0812, step time: 1.0120\n",
      "175/388, train_loss: 0.1727, step time: 0.5545\n",
      "176/388, train_loss: 0.1800, step time: 0.5308\n",
      "177/388, train_loss: 0.2614, step time: 0.5148\n",
      "178/388, train_loss: 0.1260, step time: 0.4909\n",
      "179/388, train_loss: 0.1221, step time: 0.5611\n",
      "180/388, train_loss: 0.1386, step time: 0.5255\n",
      "181/388, train_loss: 0.1114, step time: 0.5052\n",
      "182/388, train_loss: 0.1737, step time: 0.4880\n",
      "183/388, train_loss: 0.1411, step time: 0.4986\n",
      "184/388, train_loss: 0.0654, step time: 0.4978\n",
      "185/388, train_loss: 0.1242, step time: 0.4768\n",
      "186/388, train_loss: 0.1900, step time: 0.5884\n",
      "187/388, train_loss: 0.0739, step time: 0.5354\n",
      "188/388, train_loss: 0.0847, step time: 0.5189\n",
      "189/388, train_loss: 0.5002, step time: 0.5000\n",
      "190/388, train_loss: 0.2464, step time: 0.4955\n",
      "191/388, train_loss: 0.0872, step time: 0.4779\n",
      "192/388, train_loss: 0.1032, step time: 0.4907\n",
      "193/388, train_loss: 0.0768, step time: 0.4751\n",
      "194/388, train_loss: 0.2902, step time: 1.1111\n",
      "195/388, train_loss: 0.0943, step time: 0.5478\n",
      "196/388, train_loss: 0.0994, step time: 0.5193\n",
      "197/388, train_loss: 0.2114, step time: 0.5034\n",
      "198/388, train_loss: 0.1889, step time: 0.4867\n",
      "199/388, train_loss: 0.1195, step time: 0.4901\n",
      "200/388, train_loss: 0.2630, step time: 0.4940\n",
      "201/388, train_loss: 0.2014, step time: 0.4781\n",
      "202/388, train_loss: 0.0822, step time: 0.4775\n",
      "203/388, train_loss: 0.0674, step time: 0.5533\n",
      "204/388, train_loss: 0.1465, step time: 0.5509\n",
      "205/388, train_loss: 0.1493, step time: 0.5259\n",
      "206/388, train_loss: 0.1172, step time: 0.5067\n",
      "207/388, train_loss: 0.1417, step time: 0.5035\n",
      "208/388, train_loss: 0.2449, step time: 0.4818\n",
      "209/388, train_loss: 0.3262, step time: 0.4804\n",
      "210/388, train_loss: 0.1890, step time: 0.4986\n",
      "211/388, train_loss: 0.1477, step time: 0.5157\n",
      "212/388, train_loss: 0.3522, step time: 0.6244\n",
      "213/388, train_loss: 0.0742, step time: 0.5374\n",
      "214/388, train_loss: 0.1907, step time: 0.5242\n",
      "215/388, train_loss: 0.2529, step time: 0.5648\n",
      "216/388, train_loss: 0.1338, step time: 0.5324\n",
      "217/388, train_loss: 0.0654, step time: 0.5147\n",
      "218/388, train_loss: 0.1342, step time: 0.5011\n",
      "219/388, train_loss: 0.0416, step time: 0.4989\n",
      "220/388, train_loss: 0.0681, step time: 0.4827\n",
      "221/388, train_loss: 0.1652, step time: 1.1745\n",
      "222/388, train_loss: 0.3043, step time: 0.5451\n",
      "223/388, train_loss: 0.1771, step time: 0.5121\n",
      "224/388, train_loss: 0.2431, step time: 0.4921\n",
      "225/388, train_loss: 0.1074, step time: 0.4962\n",
      "226/388, train_loss: 0.1172, step time: 0.4796\n",
      "227/388, train_loss: 0.2918, step time: 0.4793\n",
      "228/388, train_loss: 0.1473, step time: 0.8259\n",
      "229/388, train_loss: 0.1374, step time: 0.5473\n",
      "230/388, train_loss: 0.0644, step time: 0.5177\n",
      "231/388, train_loss: 0.2216, step time: 0.5011\n",
      "232/388, train_loss: 0.3228, step time: 0.4960\n",
      "233/388, train_loss: 0.2495, step time: 0.5334\n",
      "234/388, train_loss: 0.1331, step time: 0.5036\n",
      "235/388, train_loss: 0.1536, step time: 0.4950\n",
      "236/388, train_loss: 0.0549, step time: 0.4982\n",
      "237/388, train_loss: 0.1501, step time: 0.4865\n",
      "238/388, train_loss: 0.1730, step time: 1.0145\n",
      "239/388, train_loss: 0.1423, step time: 0.5240\n",
      "240/388, train_loss: 0.1689, step time: 0.5051\n",
      "241/388, train_loss: 0.1813, step time: 0.4843\n",
      "242/388, train_loss: 0.2332, step time: 0.4857\n",
      "243/388, train_loss: 0.1126, step time: 0.4875\n",
      "244/388, train_loss: 0.2356, step time: 0.4879\n",
      "245/388, train_loss: 0.0954, step time: 0.5007\n",
      "246/388, train_loss: 0.2075, step time: 0.4964\n",
      "247/388, train_loss: 0.2224, step time: 0.4976\n",
      "248/388, train_loss: 0.0929, step time: 0.5044\n",
      "249/388, train_loss: 0.2660, step time: 0.4990\n",
      "250/388, train_loss: 0.1685, step time: 0.4815\n",
      "251/388, train_loss: 0.0807, step time: 0.4975\n",
      "252/388, train_loss: 0.1543, step time: 0.4906\n",
      "253/388, train_loss: 0.3645, step time: 0.4843\n",
      "254/388, train_loss: 0.1233, step time: 0.4925\n",
      "255/388, train_loss: 0.0675, step time: 0.4809\n",
      "256/388, train_loss: 0.1998, step time: 0.4829\n",
      "257/388, train_loss: 0.3377, step time: 0.9417\n",
      "258/388, train_loss: 0.2232, step time: 0.5452\n",
      "259/388, train_loss: 0.1061, step time: 0.5100\n",
      "260/388, train_loss: 0.0863, step time: 0.4923\n",
      "261/388, train_loss: 0.1845, step time: 0.4935\n",
      "262/388, train_loss: 0.2441, step time: 0.4908\n",
      "263/388, train_loss: 0.2710, step time: 0.4792\n",
      "264/388, train_loss: 0.0959, step time: 0.5458\n",
      "265/388, train_loss: 0.2622, step time: 0.5307\n",
      "266/388, train_loss: 0.2110, step time: 0.5043\n",
      "267/388, train_loss: 0.1529, step time: 0.5034\n",
      "268/388, train_loss: 0.1054, step time: 0.4960\n",
      "269/388, train_loss: 0.2740, step time: 0.4917\n",
      "270/388, train_loss: 0.0795, step time: 0.5595\n",
      "271/388, train_loss: 0.1623, step time: 0.5588\n",
      "272/388, train_loss: 0.2230, step time: 0.5186\n",
      "273/388, train_loss: 0.4383, step time: 0.4990\n",
      "274/388, train_loss: 0.2087, step time: 0.5076\n",
      "275/388, train_loss: 0.4216, step time: 0.4868\n",
      "276/388, train_loss: 0.0965, step time: 0.4972\n",
      "277/388, train_loss: 0.2195, step time: 0.4958\n",
      "278/388, train_loss: 0.0791, step time: 0.5063\n",
      "279/388, train_loss: 0.1119, step time: 0.5002\n",
      "280/388, train_loss: 0.0510, step time: 0.4845\n",
      "281/388, train_loss: 0.1904, step time: 0.5255\n",
      "282/388, train_loss: 0.2322, step time: 0.6101\n",
      "283/388, train_loss: 0.0870, step time: 0.5372\n",
      "284/388, train_loss: 0.0902, step time: 0.5071\n",
      "285/388, train_loss: 0.0587, step time: 0.4961\n",
      "286/388, train_loss: 0.2630, step time: 0.5056\n",
      "287/388, train_loss: 0.5939, step time: 0.4935\n",
      "288/388, train_loss: 0.2215, step time: 0.4916\n",
      "289/388, train_loss: 0.1076, step time: 1.1015\n",
      "290/388, train_loss: 0.1100, step time: 0.5356\n",
      "291/388, train_loss: 0.1425, step time: 0.5124\n",
      "292/388, train_loss: 0.1883, step time: 0.4902\n",
      "293/388, train_loss: 0.1331, step time: 0.4948\n",
      "294/388, train_loss: 0.1588, step time: 0.4798\n",
      "295/388, train_loss: 0.1857, step time: 0.4780\n",
      "296/388, train_loss: 0.0401, step time: 0.4862\n",
      "297/388, train_loss: 0.1598, step time: 0.4786\n",
      "298/388, train_loss: 0.1146, step time: 0.4801\n",
      "299/388, train_loss: 0.1337, step time: 0.4824\n",
      "300/388, train_loss: 0.1280, step time: 0.4763\n",
      "301/388, train_loss: 0.0806, step time: 0.9559\n",
      "302/388, train_loss: 0.0600, step time: 0.5482\n",
      "303/388, train_loss: 0.4414, step time: 0.5182\n",
      "304/388, train_loss: 0.0899, step time: 0.4999\n",
      "305/388, train_loss: 0.1510, step time: 0.4968\n",
      "306/388, train_loss: 0.0949, step time: 0.4817\n",
      "307/388, train_loss: 0.1924, step time: 1.0268\n",
      "308/388, train_loss: 0.3884, step time: 0.5204\n",
      "309/388, train_loss: 0.0819, step time: 0.4980\n",
      "310/388, train_loss: 0.1690, step time: 0.4998\n",
      "311/388, train_loss: 0.2377, step time: 0.4883\n",
      "312/388, train_loss: 0.1327, step time: 0.4864\n",
      "313/388, train_loss: 0.1455, step time: 0.4785\n",
      "314/388, train_loss: 0.1611, step time: 0.7798\n",
      "315/388, train_loss: 0.0973, step time: 0.5435\n",
      "316/388, train_loss: 0.0972, step time: 0.4950\n",
      "317/388, train_loss: 0.2940, step time: 0.4971\n",
      "318/388, train_loss: 0.1209, step time: 0.5015\n",
      "319/388, train_loss: 0.2845, step time: 0.4872\n",
      "320/388, train_loss: 0.3445, step time: 0.4897\n",
      "321/388, train_loss: 0.2427, step time: 0.4831\n",
      "322/388, train_loss: 0.1190, step time: 0.6142\n",
      "323/388, train_loss: 0.3723, step time: 0.5485\n",
      "324/388, train_loss: 0.2439, step time: 0.5163\n",
      "325/388, train_loss: 0.1270, step time: 0.4993\n",
      "326/388, train_loss: 0.1409, step time: 0.4908\n",
      "327/388, train_loss: 0.1600, step time: 0.4950\n",
      "328/388, train_loss: 0.2375, step time: 0.4817\n",
      "329/388, train_loss: 0.0566, step time: 0.4857\n",
      "330/388, train_loss: 0.1752, step time: 0.4842\n",
      "331/388, train_loss: 0.0704, step time: 0.4858\n",
      "332/388, train_loss: 0.1361, step time: 0.4911\n",
      "333/388, train_loss: 0.2361, step time: 1.1156\n",
      "334/388, train_loss: 0.1273, step time: 0.5383\n",
      "335/388, train_loss: 0.1722, step time: 0.4979\n",
      "336/388, train_loss: 0.0928, step time: 0.4908\n",
      "337/388, train_loss: 0.1608, step time: 0.4923\n",
      "338/388, train_loss: 0.1913, step time: 0.4791\n",
      "339/388, train_loss: 0.0976, step time: 0.4870\n",
      "340/388, train_loss: 0.2587, step time: 0.4843\n",
      "341/388, train_loss: 0.0900, step time: 0.4773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/388, train_loss: 0.2621, step time: 0.4879\n",
      "343/388, train_loss: 0.1558, step time: 0.5165\n",
      "344/388, train_loss: 0.1231, step time: 0.4873\n",
      "345/388, train_loss: 0.1762, step time: 0.4833\n",
      "346/388, train_loss: 0.1201, step time: 0.4904\n",
      "347/388, train_loss: 0.1765, step time: 0.9963\n",
      "348/388, train_loss: 0.1289, step time: 0.5349\n",
      "349/388, train_loss: 0.2077, step time: 0.5086\n",
      "350/388, train_loss: 0.1404, step time: 0.4981\n",
      "351/388, train_loss: 0.1680, step time: 0.4891\n",
      "352/388, train_loss: 0.1123, step time: 0.4827\n",
      "353/388, train_loss: 0.0988, step time: 0.4964\n",
      "354/388, train_loss: 0.2487, step time: 0.4907\n",
      "355/388, train_loss: 0.1917, step time: 0.8150\n",
      "356/388, train_loss: 0.1068, step time: 0.5365\n",
      "357/388, train_loss: 0.1032, step time: 0.5131\n",
      "358/388, train_loss: 0.1960, step time: 0.5093\n",
      "359/388, train_loss: 0.0524, step time: 0.4843\n",
      "360/388, train_loss: 0.1404, step time: 1.1348\n",
      "361/388, train_loss: 0.0904, step time: 0.5226\n",
      "362/388, train_loss: 0.1647, step time: 0.4952\n",
      "363/388, train_loss: 0.0814, step time: 0.4948\n",
      "364/388, train_loss: 0.4114, step time: 0.4808\n",
      "365/388, train_loss: 0.0796, step time: 0.4859\n",
      "366/388, train_loss: 0.0962, step time: 0.5032\n",
      "367/388, train_loss: 0.0706, step time: 0.4856\n",
      "368/388, train_loss: 0.2311, step time: 0.4892\n",
      "369/388, train_loss: 0.1741, step time: 0.4765\n",
      "370/388, train_loss: 0.2431, step time: 0.4748\n",
      "371/388, train_loss: 0.2427, step time: 1.1461\n",
      "372/388, train_loss: 0.0333, step time: 0.5319\n",
      "373/388, train_loss: 0.2541, step time: 0.5043\n",
      "374/388, train_loss: 0.0428, step time: 0.4829\n",
      "375/388, train_loss: 0.1014, step time: 0.4829\n",
      "376/388, train_loss: 0.5421, step time: 0.4944\n",
      "377/388, train_loss: 0.2153, step time: 0.8896\n",
      "378/388, train_loss: 0.2350, step time: 0.5252\n",
      "379/388, train_loss: 0.2934, step time: 0.4997\n",
      "380/388, train_loss: 0.3881, step time: 0.4831\n",
      "381/388, train_loss: 0.1875, step time: 0.5150\n",
      "382/388, train_loss: 0.1439, step time: 0.5036\n",
      "383/388, train_loss: 0.0425, step time: 0.4773\n",
      "384/388, train_loss: 0.1318, step time: 0.4789\n",
      "385/388, train_loss: 0.0831, step time: 0.4696\n",
      "386/388, train_loss: 0.2051, step time: 0.4774\n",
      "387/388, train_loss: 0.2120, step time: 1.2800\n",
      "388/388, train_loss: 0.0698, step time: 0.5051\n",
      "epoch 92 average loss: 0.1727\n",
      "current epoch: 92 current mean dice: 0.7767 tc: 0.8262 wt: 0.9042 et: 0.5997\n",
      "best mean dice: 0.7815 at epoch: 91\n",
      "time consuming of epoch 92 is: 303.2085\n",
      "----------\n",
      "epoch 93/300\n",
      "1/388, train_loss: 0.1752, step time: 0.4757\n",
      "2/388, train_loss: 0.2375, step time: 0.4821\n",
      "3/388, train_loss: 0.0828, step time: 1.0238\n",
      "4/388, train_loss: 0.2290, step time: 0.5471\n",
      "5/388, train_loss: 0.0645, step time: 0.5260\n",
      "6/388, train_loss: 0.2272, step time: 0.5132\n",
      "7/388, train_loss: 0.2265, step time: 0.4895\n",
      "8/388, train_loss: 0.2384, step time: 1.1106\n",
      "9/388, train_loss: 0.1795, step time: 0.5792\n",
      "10/388, train_loss: 0.0961, step time: 0.5209\n",
      "11/388, train_loss: 0.2104, step time: 0.5024\n",
      "12/388, train_loss: 0.0838, step time: 0.5138\n",
      "13/388, train_loss: 0.0837, step time: 0.5231\n",
      "14/388, train_loss: 0.1181, step time: 0.5355\n",
      "15/388, train_loss: 0.2057, step time: 0.5387\n",
      "16/388, train_loss: 0.1364, step time: 0.5263\n",
      "17/388, train_loss: 0.2077, step time: 0.5322\n",
      "18/388, train_loss: 0.1858, step time: 0.5054\n",
      "19/388, train_loss: 0.1419, step time: 0.5101\n",
      "20/388, train_loss: 0.0530, step time: 0.4928\n",
      "21/388, train_loss: 0.1001, step time: 1.1828\n",
      "22/388, train_loss: 0.1297, step time: 0.5343\n",
      "23/388, train_loss: 0.3858, step time: 0.5085\n",
      "24/388, train_loss: 0.0839, step time: 0.4864\n",
      "25/388, train_loss: 0.1854, step time: 0.4978\n",
      "26/388, train_loss: 0.1425, step time: 0.5371\n",
      "27/388, train_loss: 0.1148, step time: 0.4992\n",
      "28/388, train_loss: 0.1991, step time: 0.4986\n",
      "29/388, train_loss: 0.0835, step time: 0.5231\n",
      "30/388, train_loss: 0.1706, step time: 0.5199\n",
      "31/388, train_loss: 0.0710, step time: 0.5055\n",
      "32/388, train_loss: 0.1586, step time: 0.4896\n",
      "33/388, train_loss: 0.1570, step time: 1.1922\n",
      "34/388, train_loss: 0.2373, step time: 0.5267\n",
      "35/388, train_loss: 0.0966, step time: 0.5060\n",
      "36/388, train_loss: 0.2979, step time: 0.4970\n",
      "37/388, train_loss: 0.1447, step time: 0.5300\n",
      "38/388, train_loss: 0.0580, step time: 0.5027\n",
      "39/388, train_loss: 0.1944, step time: 0.5017\n",
      "40/388, train_loss: 0.2366, step time: 0.4855\n",
      "41/388, train_loss: 0.0711, step time: 0.4875\n",
      "42/388, train_loss: 0.3540, step time: 0.4946\n",
      "43/388, train_loss: 0.0889, step time: 1.1564\n",
      "44/388, train_loss: 0.2089, step time: 0.5512\n",
      "45/388, train_loss: 0.1895, step time: 0.5305\n",
      "46/388, train_loss: 0.1981, step time: 0.5064\n",
      "47/388, train_loss: 0.1789, step time: 0.4900\n",
      "48/388, train_loss: 0.2368, step time: 0.4932\n",
      "49/388, train_loss: 0.2034, step time: 0.4935\n",
      "50/388, train_loss: 0.1627, step time: 0.4925\n",
      "51/388, train_loss: 0.1295, step time: 0.4962\n",
      "52/388, train_loss: 0.0798, step time: 0.4785\n",
      "53/388, train_loss: 0.2016, step time: 0.6203\n",
      "54/388, train_loss: 0.1476, step time: 0.5390\n",
      "55/388, train_loss: 0.1500, step time: 0.4913\n",
      "56/388, train_loss: 0.0610, step time: 0.4952\n",
      "57/388, train_loss: 0.2568, step time: 0.5081\n",
      "58/388, train_loss: 0.1409, step time: 0.5250\n",
      "59/388, train_loss: 0.3772, step time: 0.5056\n",
      "60/388, train_loss: 0.1406, step time: 0.5314\n",
      "61/388, train_loss: 0.3580, step time: 0.5266\n",
      "62/388, train_loss: 0.1880, step time: 0.5397\n",
      "63/388, train_loss: 0.1117, step time: 0.5244\n",
      "64/388, train_loss: 0.0284, step time: 0.5582\n",
      "65/388, train_loss: 0.1130, step time: 0.5256\n",
      "66/388, train_loss: 0.1169, step time: 0.5099\n",
      "67/388, train_loss: 0.3132, step time: 0.4974\n",
      "68/388, train_loss: 0.0868, step time: 0.4983\n",
      "69/388, train_loss: 0.0790, step time: 0.5121\n",
      "70/388, train_loss: 0.0645, step time: 0.5065\n",
      "71/388, train_loss: 0.2006, step time: 0.5253\n",
      "72/388, train_loss: 0.1983, step time: 0.6111\n",
      "73/388, train_loss: 0.1509, step time: 0.5606\n",
      "74/388, train_loss: 0.5277, step time: 0.5324\n",
      "75/388, train_loss: 0.2700, step time: 0.5133\n",
      "76/388, train_loss: 0.2331, step time: 0.5094\n",
      "77/388, train_loss: 0.2029, step time: 0.5018\n",
      "78/388, train_loss: 0.0807, step time: 0.4965\n",
      "79/388, train_loss: 0.2494, step time: 1.1314\n",
      "80/388, train_loss: 0.6253, step time: 0.5426\n",
      "81/388, train_loss: 0.2192, step time: 0.5122\n",
      "82/388, train_loss: 0.1300, step time: 0.4940\n",
      "83/388, train_loss: 0.1596, step time: 0.4872\n",
      "84/388, train_loss: 0.1267, step time: 1.0012\n",
      "85/388, train_loss: 0.1511, step time: 0.5233\n",
      "86/388, train_loss: 0.2207, step time: 0.5104\n",
      "87/388, train_loss: 0.0921, step time: 0.4834\n",
      "88/388, train_loss: 0.0784, step time: 0.4863\n",
      "89/388, train_loss: 0.2754, step time: 0.5150\n",
      "90/388, train_loss: 0.2954, step time: 0.5041\n",
      "91/388, train_loss: 0.2112, step time: 0.4946\n",
      "92/388, train_loss: 0.1521, step time: 1.1455\n",
      "93/388, train_loss: 0.1540, step time: 0.5440\n",
      "94/388, train_loss: 0.1119, step time: 0.5197\n",
      "95/388, train_loss: 0.0467, step time: 0.4969\n",
      "96/388, train_loss: 0.0448, step time: 0.4895\n",
      "97/388, train_loss: 0.1386, step time: 0.4895\n",
      "98/388, train_loss: 0.0879, step time: 1.1743\n",
      "99/388, train_loss: 0.1644, step time: 0.5441\n",
      "100/388, train_loss: 0.0916, step time: 0.5048\n",
      "101/388, train_loss: 0.1282, step time: 0.4971\n",
      "102/388, train_loss: 0.0759, step time: 0.4825\n",
      "103/388, train_loss: 0.0693, step time: 0.4823\n",
      "104/388, train_loss: 0.0875, step time: 0.4994\n",
      "105/388, train_loss: 0.2229, step time: 0.9114\n",
      "106/388, train_loss: 0.1269, step time: 0.5507\n",
      "107/388, train_loss: 0.0342, step time: 0.5128\n",
      "108/388, train_loss: 0.1857, step time: 0.5011\n",
      "109/388, train_loss: 0.4195, step time: 0.5118\n",
      "110/388, train_loss: 0.1266, step time: 0.5095\n",
      "111/388, train_loss: 0.1412, step time: 0.5016\n",
      "112/388, train_loss: 0.0899, step time: 0.4847\n",
      "113/388, train_loss: 0.0517, step time: 0.4841\n",
      "114/388, train_loss: 0.0886, step time: 0.4994\n",
      "115/388, train_loss: 0.0824, step time: 0.5053\n",
      "116/388, train_loss: 0.3903, step time: 0.4872\n",
      "117/388, train_loss: 0.0772, step time: 1.0334\n",
      "118/388, train_loss: 0.1696, step time: 0.5259\n",
      "119/388, train_loss: 0.0906, step time: 0.5123\n",
      "120/388, train_loss: 0.3436, step time: 0.4939\n",
      "121/388, train_loss: 0.1190, step time: 0.4852\n",
      "122/388, train_loss: 0.0685, step time: 0.5042\n",
      "123/388, train_loss: 0.3241, step time: 0.4923\n",
      "124/388, train_loss: 0.1982, step time: 0.5076\n",
      "125/388, train_loss: 0.0867, step time: 0.5347\n",
      "126/388, train_loss: 0.2229, step time: 0.5185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/388, train_loss: 0.1493, step time: 0.5178\n",
      "128/388, train_loss: 0.1630, step time: 0.5032\n",
      "129/388, train_loss: 0.1384, step time: 0.4957\n",
      "130/388, train_loss: 0.1618, step time: 1.1443\n",
      "131/388, train_loss: 0.0859, step time: 0.5351\n",
      "132/388, train_loss: 0.1232, step time: 0.5058\n",
      "133/388, train_loss: 0.2684, step time: 0.4902\n",
      "134/388, train_loss: 0.3016, step time: 0.4966\n",
      "135/388, train_loss: 0.2534, step time: 0.4787\n",
      "136/388, train_loss: 0.2076, step time: 0.4949\n",
      "137/388, train_loss: 0.0792, step time: 0.5277\n",
      "138/388, train_loss: 0.0988, step time: 0.5035\n",
      "139/388, train_loss: 0.0635, step time: 0.4818\n",
      "140/388, train_loss: 0.3232, step time: 0.5276\n",
      "141/388, train_loss: 0.3171, step time: 0.5014\n",
      "142/388, train_loss: 0.1947, step time: 0.4980\n",
      "143/388, train_loss: 0.1148, step time: 1.2183\n",
      "144/388, train_loss: 0.1426, step time: 0.5334\n",
      "145/388, train_loss: 0.1263, step time: 0.5023\n",
      "146/388, train_loss: 0.1206, step time: 0.4991\n",
      "147/388, train_loss: 0.2886, step time: 0.4839\n",
      "148/388, train_loss: 0.1402, step time: 0.4959\n",
      "149/388, train_loss: 0.0456, step time: 0.4916\n",
      "150/388, train_loss: 0.3291, step time: 0.5116\n",
      "151/388, train_loss: 0.2496, step time: 0.5078\n",
      "152/388, train_loss: 0.0931, step time: 0.5508\n",
      "153/388, train_loss: 0.1337, step time: 0.5138\n",
      "154/388, train_loss: 0.1160, step time: 0.4940\n",
      "155/388, train_loss: 0.1306, step time: 0.5130\n",
      "156/388, train_loss: 0.1283, step time: 0.5044\n",
      "157/388, train_loss: 0.2022, step time: 0.4922\n",
      "158/388, train_loss: 0.0884, step time: 0.4937\n",
      "159/388, train_loss: 0.2011, step time: 1.1204\n",
      "160/388, train_loss: 0.4064, step time: 0.5431\n",
      "161/388, train_loss: 0.1269, step time: 0.5125\n",
      "162/388, train_loss: 0.1222, step time: 0.4941\n",
      "163/388, train_loss: 0.0928, step time: 0.4983\n",
      "164/388, train_loss: 0.2003, step time: 0.4835\n",
      "165/388, train_loss: 0.4264, step time: 0.9549\n",
      "166/388, train_loss: 0.1880, step time: 0.5353\n",
      "167/388, train_loss: 0.1190, step time: 0.5141\n",
      "168/388, train_loss: 0.0697, step time: 0.4988\n",
      "169/388, train_loss: 0.0932, step time: 0.4864\n",
      "170/388, train_loss: 0.2708, step time: 0.4934\n",
      "171/388, train_loss: 0.2620, step time: 0.4817\n",
      "172/388, train_loss: 0.0948, step time: 0.5049\n",
      "173/388, train_loss: 0.1364, step time: 0.4935\n",
      "174/388, train_loss: 0.0968, step time: 0.4927\n",
      "175/388, train_loss: 0.1927, step time: 0.4872\n",
      "176/388, train_loss: 0.0919, step time: 0.6608\n",
      "177/388, train_loss: 0.2209, step time: 0.5473\n",
      "178/388, train_loss: 0.5651, step time: 0.5410\n",
      "179/388, train_loss: 0.1006, step time: 0.5386\n",
      "180/388, train_loss: 0.4111, step time: 0.5531\n",
      "181/388, train_loss: 0.1566, step time: 0.5347\n",
      "182/388, train_loss: 0.1613, step time: 0.5093\n",
      "183/388, train_loss: 0.2042, step time: 1.1172\n",
      "184/388, train_loss: 0.4141, step time: 0.5404\n",
      "185/388, train_loss: 0.1414, step time: 0.5145\n",
      "186/388, train_loss: 0.2805, step time: 0.4915\n",
      "187/388, train_loss: 0.0443, step time: 0.4956\n",
      "188/388, train_loss: 0.1564, step time: 0.4802\n",
      "189/388, train_loss: 0.1266, step time: 0.4745\n",
      "190/388, train_loss: 0.2440, step time: 0.4838\n",
      "191/388, train_loss: 0.2227, step time: 0.4930\n",
      "192/388, train_loss: 0.0961, step time: 0.5575\n",
      "193/388, train_loss: 0.1533, step time: 0.5408\n",
      "194/388, train_loss: 0.0673, step time: 0.5186\n",
      "195/388, train_loss: 0.1370, step time: 0.6183\n",
      "196/388, train_loss: 0.0805, step time: 0.5457\n",
      "197/388, train_loss: 0.1892, step time: 0.5194\n",
      "198/388, train_loss: 0.3043, step time: 0.5042\n",
      "199/388, train_loss: 0.1683, step time: 0.4973\n",
      "200/388, train_loss: 0.1315, step time: 0.5168\n",
      "201/388, train_loss: 0.1973, step time: 0.5131\n",
      "202/388, train_loss: 0.1914, step time: 0.4855\n",
      "203/388, train_loss: 0.0858, step time: 0.5067\n",
      "204/388, train_loss: 0.2302, step time: 0.5067\n",
      "205/388, train_loss: 0.3248, step time: 0.5202\n",
      "206/388, train_loss: 0.1128, step time: 0.5570\n",
      "207/388, train_loss: 0.2341, step time: 0.5362\n",
      "208/388, train_loss: 0.1453, step time: 0.5210\n",
      "209/388, train_loss: 0.1303, step time: 0.6012\n",
      "210/388, train_loss: 0.0800, step time: 0.5410\n",
      "211/388, train_loss: 0.1793, step time: 0.4986\n",
      "212/388, train_loss: 0.0677, step time: 0.4931\n",
      "213/388, train_loss: 0.1238, step time: 0.5043\n",
      "214/388, train_loss: 0.1444, step time: 0.5073\n",
      "215/388, train_loss: 0.0445, step time: 0.4898\n",
      "216/388, train_loss: 0.2495, step time: 0.4914\n",
      "217/388, train_loss: 0.1662, step time: 0.5972\n",
      "218/388, train_loss: 0.0666, step time: 0.5513\n",
      "219/388, train_loss: 0.1193, step time: 0.5298\n",
      "220/388, train_loss: 0.1612, step time: 0.5034\n",
      "221/388, train_loss: 0.1997, step time: 0.5001\n",
      "222/388, train_loss: 0.2159, step time: 0.5296\n",
      "223/388, train_loss: 0.1326, step time: 0.5868\n",
      "224/388, train_loss: 0.1624, step time: 0.5301\n",
      "225/388, train_loss: 0.1561, step time: 0.5009\n",
      "226/388, train_loss: 0.1069, step time: 0.4786\n",
      "227/388, train_loss: 0.3128, step time: 0.6520\n",
      "228/388, train_loss: 0.0723, step time: 0.5506\n",
      "229/388, train_loss: 0.1965, step time: 0.5143\n",
      "230/388, train_loss: 0.2844, step time: 0.5004\n",
      "231/388, train_loss: 0.0875, step time: 0.4847\n",
      "232/388, train_loss: 0.2929, step time: 0.5130\n",
      "233/388, train_loss: 0.2159, step time: 0.5258\n",
      "234/388, train_loss: 0.1086, step time: 0.5143\n",
      "235/388, train_loss: 0.0421, step time: 0.5282\n",
      "236/388, train_loss: 0.1709, step time: 0.5065\n",
      "237/388, train_loss: 0.1690, step time: 0.4972\n",
      "238/388, train_loss: 0.1318, step time: 0.5105\n",
      "239/388, train_loss: 0.1197, step time: 0.5566\n",
      "240/388, train_loss: 0.1872, step time: 0.5522\n",
      "241/388, train_loss: 0.1113, step time: 0.5139\n",
      "242/388, train_loss: 0.0294, step time: 0.5089\n",
      "243/388, train_loss: 0.0744, step time: 0.4938\n",
      "244/388, train_loss: 0.1008, step time: 0.5262\n",
      "245/388, train_loss: 0.1582, step time: 0.5675\n",
      "246/388, train_loss: 0.1946, step time: 0.5308\n",
      "247/388, train_loss: 0.0881, step time: 0.5087\n",
      "248/388, train_loss: 0.1240, step time: 0.5172\n",
      "249/388, train_loss: 0.1616, step time: 0.5277\n",
      "250/388, train_loss: 0.4363, step time: 0.5133\n",
      "251/388, train_loss: 0.1193, step time: 0.4942\n",
      "252/388, train_loss: 0.1134, step time: 0.4893\n",
      "253/388, train_loss: 0.1409, step time: 0.5079\n",
      "254/388, train_loss: 0.3030, step time: 0.4975\n",
      "255/388, train_loss: 0.4325, step time: 0.5692\n",
      "256/388, train_loss: 0.0974, step time: 0.5509\n",
      "257/388, train_loss: 0.0528, step time: 0.5020\n",
      "258/388, train_loss: 0.1186, step time: 0.4963\n",
      "259/388, train_loss: 0.1706, step time: 0.9710\n",
      "260/388, train_loss: 0.1234, step time: 0.5377\n",
      "261/388, train_loss: 0.1474, step time: 0.5132\n",
      "262/388, train_loss: 0.0854, step time: 0.5107\n",
      "263/388, train_loss: 0.1511, step time: 0.5039\n",
      "264/388, train_loss: 0.1853, step time: 0.5168\n",
      "265/388, train_loss: 0.1797, step time: 0.5020\n",
      "266/388, train_loss: 0.2733, step time: 0.4804\n",
      "267/388, train_loss: 0.0601, step time: 0.5117\n",
      "268/388, train_loss: 0.1063, step time: 0.5469\n",
      "269/388, train_loss: 0.1836, step time: 0.5241\n",
      "270/388, train_loss: 0.0498, step time: 0.4976\n",
      "271/388, train_loss: 0.1071, step time: 0.4956\n",
      "272/388, train_loss: 0.1593, step time: 0.4909\n",
      "273/388, train_loss: 0.1095, step time: 0.5507\n",
      "274/388, train_loss: 0.5121, step time: 0.5410\n",
      "275/388, train_loss: 0.2048, step time: 0.5194\n",
      "276/388, train_loss: 0.0978, step time: 0.5650\n",
      "277/388, train_loss: 0.0791, step time: 0.5360\n",
      "278/388, train_loss: 0.2461, step time: 0.5148\n",
      "279/388, train_loss: 0.1326, step time: 0.5043\n",
      "280/388, train_loss: 0.0636, step time: 0.5476\n",
      "281/388, train_loss: 0.1041, step time: 0.5217\n",
      "282/388, train_loss: 0.2110, step time: 0.5177\n",
      "283/388, train_loss: 0.1868, step time: 0.5024\n",
      "284/388, train_loss: 0.0798, step time: 0.5001\n",
      "285/388, train_loss: 0.1242, step time: 0.4856\n",
      "286/388, train_loss: 0.2508, step time: 0.4985\n",
      "287/388, train_loss: 0.2119, step time: 0.5000\n",
      "288/388, train_loss: 0.1529, step time: 0.5384\n",
      "289/388, train_loss: 0.1475, step time: 0.5098\n",
      "290/388, train_loss: 0.0942, step time: 0.4856\n",
      "291/388, train_loss: 0.0547, step time: 0.5159\n",
      "292/388, train_loss: 0.1120, step time: 0.4977\n",
      "293/388, train_loss: 0.0450, step time: 0.4968\n",
      "294/388, train_loss: 0.0970, step time: 0.4933\n",
      "295/388, train_loss: 0.1270, step time: 0.4990\n",
      "296/388, train_loss: 0.2684, step time: 0.5706\n",
      "297/388, train_loss: 0.0850, step time: 0.5564\n",
      "298/388, train_loss: 0.3146, step time: 0.5317\n",
      "299/388, train_loss: 0.0943, step time: 0.5154\n",
      "300/388, train_loss: 0.0806, step time: 0.4962\n",
      "301/388, train_loss: 0.1746, step time: 0.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/388, train_loss: 0.1980, step time: 0.5589\n",
      "303/388, train_loss: 0.1575, step time: 0.5354\n",
      "304/388, train_loss: 0.2243, step time: 0.5079\n",
      "305/388, train_loss: 0.1179, step time: 0.4992\n",
      "306/388, train_loss: 0.3816, step time: 1.2112\n",
      "307/388, train_loss: 0.1720, step time: 0.5478\n",
      "308/388, train_loss: 0.5032, step time: 0.5142\n",
      "309/388, train_loss: 0.1516, step time: 0.5063\n",
      "310/388, train_loss: 0.2282, step time: 0.4894\n",
      "311/388, train_loss: 0.1750, step time: 0.4892\n",
      "312/388, train_loss: 0.0477, step time: 0.5179\n",
      "313/388, train_loss: 0.1037, step time: 0.5306\n",
      "314/388, train_loss: 0.1512, step time: 0.5072\n",
      "315/388, train_loss: 0.2380, step time: 0.4946\n",
      "316/388, train_loss: 0.1570, step time: 0.5016\n",
      "317/388, train_loss: 0.1063, step time: 0.4963\n",
      "318/388, train_loss: 0.2598, step time: 0.5176\n",
      "319/388, train_loss: 0.0596, step time: 0.5055\n",
      "320/388, train_loss: 0.0752, step time: 0.4957\n",
      "321/388, train_loss: 0.2190, step time: 0.4934\n",
      "322/388, train_loss: 0.3228, step time: 1.1154\n",
      "323/388, train_loss: 0.1369, step time: 0.5401\n",
      "324/388, train_loss: 0.1649, step time: 0.5053\n",
      "325/388, train_loss: 0.1762, step time: 0.4950\n",
      "326/388, train_loss: 0.0648, step time: 0.4932\n",
      "327/388, train_loss: 0.2152, step time: 0.4847\n",
      "328/388, train_loss: 0.3495, step time: 0.4879\n",
      "329/388, train_loss: 0.1076, step time: 0.5000\n",
      "330/388, train_loss: 0.1024, step time: 0.4960\n",
      "331/388, train_loss: 0.3375, step time: 0.4925\n",
      "332/388, train_loss: 0.5070, step time: 0.5091\n",
      "333/388, train_loss: 0.2256, step time: 0.5016\n",
      "334/388, train_loss: 0.1031, step time: 0.4808\n",
      "335/388, train_loss: 0.3754, step time: 0.5023\n",
      "336/388, train_loss: 0.0815, step time: 0.4893\n",
      "337/388, train_loss: 0.1071, step time: 0.5046\n",
      "338/388, train_loss: 0.2125, step time: 0.4849\n",
      "339/388, train_loss: 0.1127, step time: 0.4809\n",
      "340/388, train_loss: 0.3659, step time: 0.5125\n",
      "341/388, train_loss: 0.2088, step time: 0.5068\n",
      "342/388, train_loss: 0.2834, step time: 0.5026\n",
      "343/388, train_loss: 0.1608, step time: 0.5336\n",
      "344/388, train_loss: 0.0973, step time: 0.5102\n",
      "345/388, train_loss: 0.0780, step time: 0.4988\n",
      "346/388, train_loss: 0.3500, step time: 0.4863\n",
      "347/388, train_loss: 0.2766, step time: 0.4954\n",
      "348/388, train_loss: 0.1946, step time: 0.4782\n",
      "349/388, train_loss: 0.2149, step time: 0.4761\n",
      "350/388, train_loss: 0.0824, step time: 0.7069\n",
      "351/388, train_loss: 0.0867, step time: 0.5465\n",
      "352/388, train_loss: 0.0784, step time: 0.5159\n",
      "353/388, train_loss: 0.1531, step time: 0.5265\n",
      "354/388, train_loss: 0.2832, step time: 0.5056\n",
      "355/388, train_loss: 0.0653, step time: 1.1530\n",
      "356/388, train_loss: 0.0891, step time: 0.5303\n",
      "357/388, train_loss: 0.1071, step time: 0.5106\n",
      "358/388, train_loss: 0.3075, step time: 0.4916\n",
      "359/388, train_loss: 0.0745, step time: 0.4838\n",
      "360/388, train_loss: 0.0974, step time: 0.4874\n",
      "361/388, train_loss: 0.1149, step time: 0.4768\n",
      "362/388, train_loss: 0.2342, step time: 0.4837\n",
      "363/388, train_loss: 0.2407, step time: 0.4815\n",
      "364/388, train_loss: 0.0490, step time: 0.4823\n",
      "365/388, train_loss: 0.2785, step time: 0.5269\n",
      "366/388, train_loss: 0.0921, step time: 0.4994\n",
      "367/388, train_loss: 0.0905, step time: 0.4954\n",
      "368/388, train_loss: 0.1030, step time: 0.4891\n",
      "369/388, train_loss: 0.0778, step time: 0.6860\n",
      "370/388, train_loss: 0.0639, step time: 0.5701\n",
      "371/388, train_loss: 0.1221, step time: 0.5269\n",
      "372/388, train_loss: 0.2167, step time: 0.5086\n",
      "373/388, train_loss: 0.2409, step time: 0.4999\n",
      "374/388, train_loss: 0.2628, step time: 0.4909\n",
      "375/388, train_loss: 0.2548, step time: 0.4894\n",
      "376/388, train_loss: 0.1020, step time: 0.4865\n",
      "377/388, train_loss: 0.1351, step time: 0.4750\n",
      "378/388, train_loss: 0.0655, step time: 0.4793\n",
      "379/388, train_loss: 0.1052, step time: 0.4856\n",
      "380/388, train_loss: 0.1661, step time: 1.1015\n",
      "381/388, train_loss: 0.1190, step time: 0.5477\n",
      "382/388, train_loss: 0.3539, step time: 0.5227\n",
      "383/388, train_loss: 0.3078, step time: 0.5054\n",
      "384/388, train_loss: 0.1035, step time: 0.4857\n",
      "385/388, train_loss: 0.1278, step time: 0.4791\n",
      "386/388, train_loss: 0.3370, step time: 0.4692\n",
      "387/388, train_loss: 0.2120, step time: 0.6078\n",
      "388/388, train_loss: 0.1495, step time: 0.6040\n",
      "epoch 93 average loss: 0.1703\n",
      "current epoch: 93 current mean dice: 0.7683 tc: 0.8233 wt: 0.9038 et: 0.5777\n",
      "best mean dice: 0.7815 at epoch: 91\n",
      "time consuming of epoch 93 is: 299.4109\n",
      "----------\n",
      "epoch 94/300\n",
      "1/388, train_loss: 0.1846, step time: 0.4804\n",
      "2/388, train_loss: 0.1379, step time: 0.4940\n",
      "3/388, train_loss: 0.2238, step time: 0.4754\n",
      "4/388, train_loss: 0.2358, step time: 0.5521\n",
      "5/388, train_loss: 0.1938, step time: 0.5006\n",
      "6/388, train_loss: 0.2441, step time: 0.5246\n",
      "7/388, train_loss: 0.1376, step time: 0.6215\n",
      "8/388, train_loss: 0.1279, step time: 0.5595\n",
      "9/388, train_loss: 0.1452, step time: 0.5140\n",
      "10/388, train_loss: 0.1090, step time: 0.5126\n",
      "11/388, train_loss: 0.1533, step time: 0.4948\n",
      "12/388, train_loss: 0.0432, step time: 0.8570\n",
      "13/388, train_loss: 0.0850, step time: 0.5485\n",
      "14/388, train_loss: 0.0424, step time: 0.5319\n",
      "15/388, train_loss: 0.0977, step time: 0.5303\n",
      "16/388, train_loss: 0.2597, step time: 0.5020\n",
      "17/388, train_loss: 0.1871, step time: 0.4944\n",
      "18/388, train_loss: 0.1196, step time: 0.5037\n",
      "19/388, train_loss: 0.1449, step time: 0.5124\n",
      "20/388, train_loss: 0.1863, step time: 0.5066\n",
      "21/388, train_loss: 0.0732, step time: 0.4825\n",
      "22/388, train_loss: 0.0935, step time: 0.5017\n",
      "23/388, train_loss: 0.3170, step time: 0.9805\n",
      "24/388, train_loss: 0.0698, step time: 0.5466\n",
      "25/388, train_loss: 0.1825, step time: 0.5035\n",
      "26/388, train_loss: 0.2524, step time: 0.5558\n",
      "27/388, train_loss: 0.0615, step time: 0.5387\n",
      "28/388, train_loss: 0.1312, step time: 0.5070\n",
      "29/388, train_loss: 0.0339, step time: 0.4839\n",
      "30/388, train_loss: 0.0882, step time: 0.5672\n",
      "31/388, train_loss: 0.2283, step time: 0.6284\n",
      "32/388, train_loss: 0.0513, step time: 0.5417\n",
      "33/388, train_loss: 0.0975, step time: 0.5141\n",
      "34/388, train_loss: 0.0492, step time: 0.5039\n",
      "35/388, train_loss: 0.1399, step time: 0.5428\n",
      "36/388, train_loss: 0.2007, step time: 0.5120\n",
      "37/388, train_loss: 0.0945, step time: 0.5066\n",
      "38/388, train_loss: 0.0823, step time: 0.4870\n",
      "39/388, train_loss: 0.0502, step time: 1.0312\n",
      "40/388, train_loss: 0.1462, step time: 0.5373\n",
      "41/388, train_loss: 0.1849, step time: 0.5059\n",
      "42/388, train_loss: 0.3576, step time: 0.4780\n",
      "43/388, train_loss: 0.1238, step time: 0.5017\n",
      "44/388, train_loss: 0.3592, step time: 0.4968\n",
      "45/388, train_loss: 0.2848, step time: 0.4868\n",
      "46/388, train_loss: 0.1244, step time: 0.8375\n",
      "47/388, train_loss: 0.2722, step time: 0.5575\n",
      "48/388, train_loss: 0.1630, step time: 0.5211\n",
      "49/388, train_loss: 0.1379, step time: 0.4924\n",
      "50/388, train_loss: 0.0938, step time: 0.4917\n",
      "51/388, train_loss: 0.2529, step time: 0.4837\n",
      "52/388, train_loss: 0.0952, step time: 1.0096\n",
      "53/388, train_loss: 0.0473, step time: 0.5369\n",
      "54/388, train_loss: 0.0909, step time: 0.5162\n",
      "55/388, train_loss: 0.1516, step time: 0.4998\n",
      "56/388, train_loss: 0.0849, step time: 0.4887\n",
      "57/388, train_loss: 0.1106, step time: 0.4986\n",
      "58/388, train_loss: 0.0917, step time: 0.5088\n",
      "59/388, train_loss: 0.0991, step time: 0.5041\n",
      "60/388, train_loss: 0.1160, step time: 0.4885\n",
      "61/388, train_loss: 0.2825, step time: 0.4974\n",
      "62/388, train_loss: 0.1458, step time: 0.4800\n",
      "63/388, train_loss: 0.0849, step time: 0.4785\n",
      "64/388, train_loss: 0.1119, step time: 0.5489\n",
      "65/388, train_loss: 0.3725, step time: 0.5343\n",
      "66/388, train_loss: 0.1519, step time: 0.4955\n",
      "67/388, train_loss: 0.1073, step time: 0.4982\n",
      "68/388, train_loss: 0.1119, step time: 0.4906\n",
      "69/388, train_loss: 0.0729, step time: 0.5077\n",
      "70/388, train_loss: 0.0751, step time: 0.4906\n",
      "71/388, train_loss: 0.0861, step time: 0.5176\n",
      "72/388, train_loss: 0.1316, step time: 0.4964\n",
      "73/388, train_loss: 0.1952, step time: 0.4941\n",
      "74/388, train_loss: 0.3403, step time: 0.5027\n",
      "75/388, train_loss: 0.1605, step time: 0.4942\n",
      "76/388, train_loss: 0.2211, step time: 0.4906\n",
      "77/388, train_loss: 0.2454, step time: 1.1366\n",
      "78/388, train_loss: 0.2162, step time: 0.5369\n",
      "79/388, train_loss: 0.0982, step time: 0.5180\n",
      "80/388, train_loss: 0.0773, step time: 0.4935\n",
      "81/388, train_loss: 0.1197, step time: 0.4965\n",
      "82/388, train_loss: 0.2190, step time: 0.4799\n",
      "83/388, train_loss: 0.1056, step time: 0.5578\n",
      "84/388, train_loss: 0.0301, step time: 0.5453\n",
      "85/388, train_loss: 0.0750, step time: 0.5066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/388, train_loss: 0.1007, step time: 0.5027\n",
      "87/388, train_loss: 0.0816, step time: 0.5058\n",
      "88/388, train_loss: 0.1671, step time: 0.5019\n",
      "89/388, train_loss: 0.1280, step time: 0.4898\n",
      "90/388, train_loss: 0.2358, step time: 0.5002\n",
      "91/388, train_loss: 0.1156, step time: 0.5857\n",
      "92/388, train_loss: 0.0870, step time: 0.5359\n",
      "93/388, train_loss: 0.1213, step time: 0.5114\n",
      "94/388, train_loss: 0.1687, step time: 0.4966\n",
      "95/388, train_loss: 0.4979, step time: 0.5117\n",
      "96/388, train_loss: 0.1771, step time: 0.5063\n",
      "97/388, train_loss: 0.0592, step time: 0.5084\n",
      "98/388, train_loss: 0.2017, step time: 0.4909\n",
      "99/388, train_loss: 0.1912, step time: 0.4935\n",
      "100/388, train_loss: 0.2000, step time: 0.4925\n",
      "101/388, train_loss: 0.3298, step time: 0.5263\n",
      "102/388, train_loss: 0.1076, step time: 0.4926\n",
      "103/388, train_loss: 0.0949, step time: 0.5076\n",
      "104/388, train_loss: 0.2785, step time: 0.5021\n",
      "105/388, train_loss: 0.0777, step time: 0.4823\n",
      "106/388, train_loss: 0.2851, step time: 0.9733\n",
      "107/388, train_loss: 0.2701, step time: 0.5474\n",
      "108/388, train_loss: 0.0721, step time: 0.5239\n",
      "109/388, train_loss: 0.1167, step time: 0.4812\n",
      "110/388, train_loss: 0.1232, step time: 0.4947\n",
      "111/388, train_loss: 0.1748, step time: 0.4898\n",
      "112/388, train_loss: 0.1717, step time: 0.5225\n",
      "113/388, train_loss: 0.0733, step time: 0.5198\n",
      "114/388, train_loss: 0.0876, step time: 0.5031\n",
      "115/388, train_loss: 0.2755, step time: 0.4901\n",
      "116/388, train_loss: 0.1211, step time: 0.4943\n",
      "117/388, train_loss: 0.2805, step time: 0.9520\n",
      "118/388, train_loss: 0.0787, step time: 0.5411\n",
      "119/388, train_loss: 0.1630, step time: 0.5102\n",
      "120/388, train_loss: 0.2660, step time: 0.5201\n",
      "121/388, train_loss: 0.3443, step time: 0.5116\n",
      "122/388, train_loss: 0.0808, step time: 0.5025\n",
      "123/388, train_loss: 0.2957, step time: 0.4872\n",
      "124/388, train_loss: 0.1707, step time: 0.5084\n",
      "125/388, train_loss: 0.2653, step time: 0.5082\n",
      "126/388, train_loss: 0.0775, step time: 0.5552\n",
      "127/388, train_loss: 0.2424, step time: 0.5385\n",
      "128/388, train_loss: 0.1706, step time: 0.5146\n",
      "129/388, train_loss: 0.1942, step time: 0.4990\n",
      "130/388, train_loss: 0.1370, step time: 0.5435\n",
      "131/388, train_loss: 0.0978, step time: 0.5369\n",
      "132/388, train_loss: 0.1045, step time: 0.5253\n",
      "133/388, train_loss: 0.1253, step time: 0.4975\n",
      "134/388, train_loss: 0.1217, step time: 0.5036\n",
      "135/388, train_loss: 0.2867, step time: 0.4930\n",
      "136/388, train_loss: 0.1375, step time: 0.4921\n",
      "137/388, train_loss: 0.2965, step time: 0.4831\n",
      "138/388, train_loss: 0.2452, step time: 0.5926\n",
      "139/388, train_loss: 0.1286, step time: 0.5678\n",
      "140/388, train_loss: 0.0884, step time: 0.5339\n",
      "141/388, train_loss: 0.0635, step time: 0.5040\n",
      "142/388, train_loss: 0.1905, step time: 0.5007\n",
      "143/388, train_loss: 0.2081, step time: 0.4849\n",
      "144/388, train_loss: 0.1495, step time: 0.4924\n",
      "145/388, train_loss: 0.0657, step time: 0.7523\n",
      "146/388, train_loss: 0.2018, step time: 0.5617\n",
      "147/388, train_loss: 0.2006, step time: 0.5346\n",
      "148/388, train_loss: 0.1121, step time: 0.5161\n",
      "149/388, train_loss: 0.2604, step time: 0.4940\n",
      "150/388, train_loss: 0.1980, step time: 0.5232\n",
      "151/388, train_loss: 0.0392, step time: 0.5074\n",
      "152/388, train_loss: 0.2126, step time: 0.5304\n",
      "153/388, train_loss: 0.2159, step time: 0.6271\n",
      "154/388, train_loss: 0.0507, step time: 0.5387\n",
      "155/388, train_loss: 0.5517, step time: 0.5698\n",
      "156/388, train_loss: 0.1491, step time: 0.5618\n",
      "157/388, train_loss: 0.2166, step time: 0.5420\n",
      "158/388, train_loss: 0.1145, step time: 0.5141\n",
      "159/388, train_loss: 0.1446, step time: 0.4942\n",
      "160/388, train_loss: 0.0643, step time: 0.4972\n",
      "161/388, train_loss: 0.1767, step time: 0.4854\n",
      "162/388, train_loss: 0.1831, step time: 0.5124\n",
      "163/388, train_loss: 0.3480, step time: 0.5251\n",
      "164/388, train_loss: 0.1192, step time: 0.5125\n",
      "165/388, train_loss: 0.1135, step time: 0.5549\n",
      "166/388, train_loss: 0.0835, step time: 0.5274\n",
      "167/388, train_loss: 0.1633, step time: 0.5028\n",
      "168/388, train_loss: 0.1389, step time: 0.4922\n",
      "169/388, train_loss: 0.1364, step time: 0.4844\n",
      "170/388, train_loss: 0.3520, step time: 0.6379\n",
      "171/388, train_loss: 0.1868, step time: 0.5670\n",
      "172/388, train_loss: 0.2539, step time: 0.5268\n",
      "173/388, train_loss: 0.1888, step time: 0.5081\n",
      "174/388, train_loss: 0.2543, step time: 0.4826\n",
      "175/388, train_loss: 0.1553, step time: 0.4866\n",
      "176/388, train_loss: 0.2047, step time: 1.1584\n",
      "177/388, train_loss: 0.2922, step time: 0.5314\n",
      "178/388, train_loss: 0.5232, step time: 0.5073\n",
      "179/388, train_loss: 0.2155, step time: 0.4834\n",
      "180/388, train_loss: 0.0928, step time: 1.0655\n",
      "181/388, train_loss: 0.1070, step time: 0.5374\n",
      "182/388, train_loss: 0.2222, step time: 0.5090\n",
      "183/388, train_loss: 0.3149, step time: 0.4922\n",
      "184/388, train_loss: 0.2617, step time: 0.4986\n",
      "185/388, train_loss: 0.3727, step time: 0.4867\n",
      "186/388, train_loss: 0.1998, step time: 0.5113\n",
      "187/388, train_loss: 0.1247, step time: 0.4807\n",
      "188/388, train_loss: 0.0671, step time: 0.4920\n",
      "189/388, train_loss: 0.2685, step time: 0.5289\n",
      "190/388, train_loss: 0.0426, step time: 0.5067\n",
      "191/388, train_loss: 0.1864, step time: 0.4894\n",
      "192/388, train_loss: 0.0892, step time: 1.1279\n",
      "193/388, train_loss: 0.2547, step time: 0.5328\n",
      "194/388, train_loss: 0.0505, step time: 0.4999\n",
      "195/388, train_loss: 0.1059, step time: 0.4848\n",
      "196/388, train_loss: 0.4948, step time: 0.4996\n",
      "197/388, train_loss: 0.0736, step time: 0.4839\n",
      "198/388, train_loss: 0.0738, step time: 0.4748\n",
      "199/388, train_loss: 0.1126, step time: 0.4906\n",
      "200/388, train_loss: 0.1552, step time: 0.4845\n",
      "201/388, train_loss: 0.2021, step time: 1.0681\n",
      "202/388, train_loss: 0.3132, step time: 0.5480\n",
      "203/388, train_loss: 0.0972, step time: 0.5130\n",
      "204/388, train_loss: 0.1970, step time: 0.5069\n",
      "205/388, train_loss: 0.1435, step time: 0.4919\n",
      "206/388, train_loss: 0.0685, step time: 0.4927\n",
      "207/388, train_loss: 0.1056, step time: 0.4965\n",
      "208/388, train_loss: 0.0636, step time: 0.5033\n",
      "209/388, train_loss: 0.1544, step time: 0.4866\n",
      "210/388, train_loss: 0.1382, step time: 0.4856\n",
      "211/388, train_loss: 0.0622, step time: 0.4925\n",
      "212/388, train_loss: 0.3542, step time: 0.4764\n",
      "213/388, train_loss: 0.1601, step time: 0.7294\n",
      "214/388, train_loss: 0.2218, step time: 0.5497\n",
      "215/388, train_loss: 0.1641, step time: 0.5193\n",
      "216/388, train_loss: 0.0578, step time: 0.5029\n",
      "217/388, train_loss: 0.1533, step time: 0.4996\n",
      "218/388, train_loss: 0.1238, step time: 0.4860\n",
      "219/388, train_loss: 0.1469, step time: 0.4983\n",
      "220/388, train_loss: 0.1833, step time: 0.4840\n",
      "221/388, train_loss: 0.1933, step time: 0.4911\n",
      "222/388, train_loss: 0.1537, step time: 0.5548\n",
      "223/388, train_loss: 0.2184, step time: 0.5104\n",
      "224/388, train_loss: 0.1984, step time: 0.5197\n",
      "225/388, train_loss: 0.4353, step time: 0.5104\n",
      "226/388, train_loss: 0.2081, step time: 0.4916\n",
      "227/388, train_loss: 0.0771, step time: 0.5122\n",
      "228/388, train_loss: 0.1911, step time: 0.5451\n",
      "229/388, train_loss: 0.1115, step time: 0.6025\n",
      "230/388, train_loss: 0.0636, step time: 0.5650\n",
      "231/388, train_loss: 0.2275, step time: 0.5256\n",
      "232/388, train_loss: 0.1848, step time: 0.5062\n",
      "233/388, train_loss: 0.2652, step time: 0.5373\n",
      "234/388, train_loss: 0.0697, step time: 0.5129\n",
      "235/388, train_loss: 0.1649, step time: 0.5458\n",
      "236/388, train_loss: 0.2389, step time: 0.5220\n",
      "237/388, train_loss: 0.0798, step time: 0.5084\n",
      "238/388, train_loss: 0.2743, step time: 0.5468\n",
      "239/388, train_loss: 0.1836, step time: 0.5183\n",
      "240/388, train_loss: 0.1637, step time: 0.5548\n",
      "241/388, train_loss: 0.1347, step time: 0.5275\n",
      "242/388, train_loss: 0.0706, step time: 0.5032\n",
      "243/388, train_loss: 0.0640, step time: 0.5043\n",
      "244/388, train_loss: 0.2167, step time: 0.4828\n",
      "245/388, train_loss: 0.1319, step time: 0.9842\n",
      "246/388, train_loss: 0.1182, step time: 0.5637\n",
      "247/388, train_loss: 0.1541, step time: 0.5322\n",
      "248/388, train_loss: 0.3817, step time: 0.5121\n",
      "249/388, train_loss: 0.1601, step time: 0.5578\n",
      "250/388, train_loss: 0.2663, step time: 0.5265\n",
      "251/388, train_loss: 0.2677, step time: 0.5049\n",
      "252/388, train_loss: 0.1244, step time: 0.4960\n",
      "253/388, train_loss: 0.0482, step time: 0.5240\n",
      "254/388, train_loss: 0.1120, step time: 0.4956\n",
      "255/388, train_loss: 0.1283, step time: 0.5142\n",
      "256/388, train_loss: 0.3796, step time: 0.5088\n",
      "257/388, train_loss: 0.1605, step time: 0.5061\n",
      "258/388, train_loss: 0.0757, step time: 0.5381\n",
      "259/388, train_loss: 0.0462, step time: 0.5055\n",
      "260/388, train_loss: 0.1237, step time: 0.4877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/388, train_loss: 0.2143, step time: 0.5004\n",
      "262/388, train_loss: 0.1520, step time: 0.4825\n",
      "263/388, train_loss: 0.0897, step time: 0.4870\n",
      "264/388, train_loss: 0.0981, step time: 0.5010\n",
      "265/388, train_loss: 0.2342, step time: 0.9842\n",
      "266/388, train_loss: 0.0714, step time: 0.5580\n",
      "267/388, train_loss: 0.1830, step time: 0.5312\n",
      "268/388, train_loss: 0.1636, step time: 0.5106\n",
      "269/388, train_loss: 0.4103, step time: 1.1545\n",
      "270/388, train_loss: 0.1195, step time: 0.5350\n",
      "271/388, train_loss: 0.4298, step time: 0.4990\n",
      "272/388, train_loss: 0.0802, step time: 0.4925\n",
      "273/388, train_loss: 0.1029, step time: 0.4850\n",
      "274/388, train_loss: 0.1711, step time: 0.5079\n",
      "275/388, train_loss: 0.1040, step time: 0.4995\n",
      "276/388, train_loss: 0.1333, step time: 0.4817\n",
      "277/388, train_loss: 0.1054, step time: 0.5717\n",
      "278/388, train_loss: 0.1024, step time: 0.5504\n",
      "279/388, train_loss: 0.0762, step time: 0.5203\n",
      "280/388, train_loss: 0.2518, step time: 0.5100\n",
      "281/388, train_loss: 0.0978, step time: 1.2488\n",
      "282/388, train_loss: 0.1124, step time: 0.5409\n",
      "283/388, train_loss: 0.1566, step time: 0.5196\n",
      "284/388, train_loss: 0.3382, step time: 0.5030\n",
      "285/388, train_loss: 0.0302, step time: 0.4981\n",
      "286/388, train_loss: 0.0895, step time: 0.4837\n",
      "287/388, train_loss: 0.0808, step time: 0.4919\n",
      "288/388, train_loss: 0.1875, step time: 0.5278\n",
      "289/388, train_loss: 0.0704, step time: 0.5039\n",
      "290/388, train_loss: 0.1017, step time: 0.4948\n",
      "291/388, train_loss: 0.2188, step time: 0.5167\n",
      "292/388, train_loss: 0.1537, step time: 0.5051\n",
      "293/388, train_loss: 0.4543, step time: 0.4885\n",
      "294/388, train_loss: 0.0894, step time: 0.4940\n",
      "295/388, train_loss: 0.3966, step time: 0.5246\n",
      "296/388, train_loss: 0.1867, step time: 0.6144\n",
      "297/388, train_loss: 0.1001, step time: 0.5299\n",
      "298/388, train_loss: 0.3802, step time: 0.4987\n",
      "299/388, train_loss: 0.3841, step time: 0.4951\n",
      "300/388, train_loss: 0.2009, step time: 0.6027\n",
      "301/388, train_loss: 0.1156, step time: 0.5440\n",
      "302/388, train_loss: 0.1146, step time: 0.5186\n",
      "303/388, train_loss: 0.2536, step time: 0.5013\n",
      "304/388, train_loss: 0.0712, step time: 0.4914\n",
      "305/388, train_loss: 0.1847, step time: 0.5287\n",
      "306/388, train_loss: 0.2734, step time: 0.5119\n",
      "307/388, train_loss: 0.0947, step time: 0.4910\n",
      "308/388, train_loss: 0.1522, step time: 0.4886\n",
      "309/388, train_loss: 0.2343, step time: 0.5103\n",
      "310/388, train_loss: 0.1546, step time: 0.5086\n",
      "311/388, train_loss: 0.2348, step time: 0.5473\n",
      "312/388, train_loss: 0.1169, step time: 0.5249\n",
      "313/388, train_loss: 0.1686, step time: 0.5701\n",
      "314/388, train_loss: 0.2626, step time: 0.5703\n",
      "315/388, train_loss: 0.6338, step time: 0.6768\n",
      "316/388, train_loss: 0.0751, step time: 0.5608\n",
      "317/388, train_loss: 0.2279, step time: 0.5256\n",
      "318/388, train_loss: 0.0833, step time: 0.5147\n",
      "319/388, train_loss: 0.2006, step time: 0.5015\n",
      "320/388, train_loss: 0.1261, step time: 0.5017\n",
      "321/388, train_loss: 0.3098, step time: 0.4837\n",
      "322/388, train_loss: 0.1306, step time: 0.5487\n",
      "323/388, train_loss: 0.1711, step time: 0.5151\n",
      "324/388, train_loss: 0.0921, step time: 0.5017\n",
      "325/388, train_loss: 0.0656, step time: 0.4860\n",
      "326/388, train_loss: 0.1980, step time: 0.5117\n",
      "327/388, train_loss: 0.2555, step time: 0.5301\n",
      "328/388, train_loss: 0.1104, step time: 0.5071\n",
      "329/388, train_loss: 0.3307, step time: 0.5017\n",
      "330/388, train_loss: 0.0843, step time: 1.0201\n",
      "331/388, train_loss: 0.0770, step time: 0.5656\n",
      "332/388, train_loss: 0.0786, step time: 0.5230\n",
      "333/388, train_loss: 0.1614, step time: 0.5078\n",
      "334/388, train_loss: 0.1571, step time: 0.5039\n",
      "335/388, train_loss: 0.2614, step time: 0.4831\n",
      "336/388, train_loss: 0.2842, step time: 0.9827\n",
      "337/388, train_loss: 0.0955, step time: 0.5312\n",
      "338/388, train_loss: 0.1210, step time: 0.5070\n",
      "339/388, train_loss: 0.0901, step time: 0.4813\n",
      "340/388, train_loss: 0.4085, step time: 0.5044\n",
      "341/388, train_loss: 0.2796, step time: 0.5773\n",
      "342/388, train_loss: 0.2381, step time: 0.5250\n",
      "343/388, train_loss: 0.2448, step time: 0.5165\n",
      "344/388, train_loss: 0.1437, step time: 0.5009\n",
      "345/388, train_loss: 0.0926, step time: 0.4969\n",
      "346/388, train_loss: 0.1639, step time: 1.1680\n",
      "347/388, train_loss: 0.0858, step time: 0.5258\n",
      "348/388, train_loss: 0.1822, step time: 0.5066\n",
      "349/388, train_loss: 0.0731, step time: 0.4999\n",
      "350/388, train_loss: 0.1977, step time: 0.4851\n",
      "351/388, train_loss: 0.1143, step time: 0.5054\n",
      "352/388, train_loss: 0.1704, step time: 0.4953\n",
      "353/388, train_loss: 0.0800, step time: 1.1642\n",
      "354/388, train_loss: 0.0810, step time: 0.5284\n",
      "355/388, train_loss: 0.1402, step time: 0.5016\n",
      "356/388, train_loss: 0.2961, step time: 0.4951\n",
      "357/388, train_loss: 0.2766, step time: 0.4852\n",
      "358/388, train_loss: 0.1688, step time: 0.4932\n",
      "359/388, train_loss: 0.1632, step time: 0.8571\n",
      "360/388, train_loss: 0.1037, step time: 0.5380\n",
      "361/388, train_loss: 0.2493, step time: 0.4994\n",
      "362/388, train_loss: 0.1681, step time: 0.4993\n",
      "363/388, train_loss: 0.1864, step time: 0.4828\n",
      "364/388, train_loss: 0.2750, step time: 0.4868\n",
      "365/388, train_loss: 0.2445, step time: 0.6014\n",
      "366/388, train_loss: 0.1287, step time: 0.5436\n",
      "367/388, train_loss: 0.3165, step time: 0.5126\n",
      "368/388, train_loss: 0.1730, step time: 0.4951\n",
      "369/388, train_loss: 0.1039, step time: 0.4993\n",
      "370/388, train_loss: 0.0879, step time: 0.4861\n",
      "371/388, train_loss: 0.1979, step time: 0.4911\n",
      "372/388, train_loss: 0.1399, step time: 1.1611\n",
      "373/388, train_loss: 0.1476, step time: 0.5326\n",
      "374/388, train_loss: 0.0762, step time: 0.5065\n",
      "375/388, train_loss: 0.1399, step time: 0.4912\n",
      "376/388, train_loss: 0.0570, step time: 0.4938\n",
      "377/388, train_loss: 0.1308, step time: 0.4844\n",
      "378/388, train_loss: 0.1441, step time: 0.4810\n",
      "379/388, train_loss: 0.1990, step time: 0.4791\n",
      "380/388, train_loss: 0.0868, step time: 0.8929\n",
      "381/388, train_loss: 0.1035, step time: 0.5372\n",
      "382/388, train_loss: 0.1294, step time: 0.5035\n",
      "383/388, train_loss: 0.4239, step time: 0.4892\n",
      "384/388, train_loss: 0.0943, step time: 0.4873\n",
      "385/388, train_loss: 0.2132, step time: 0.4743\n",
      "386/388, train_loss: 0.0772, step time: 0.9307\n",
      "387/388, train_loss: 0.4286, step time: 0.5365\n",
      "388/388, train_loss: 0.1543, step time: 0.5065\n",
      "epoch 94 average loss: 0.1698\n",
      "current epoch: 94 current mean dice: 0.7663 tc: 0.8150 wt: 0.9017 et: 0.5822\n",
      "best mean dice: 0.7815 at epoch: 91\n",
      "time consuming of epoch 94 is: 301.0455\n",
      "----------\n",
      "epoch 95/300\n",
      "1/388, train_loss: 0.1895, step time: 0.4796\n",
      "2/388, train_loss: 0.2199, step time: 0.4824\n",
      "3/388, train_loss: 0.3164, step time: 0.4735\n",
      "4/388, train_loss: 0.0874, step time: 1.0072\n",
      "5/388, train_loss: 0.1901, step time: 0.5329\n",
      "6/388, train_loss: 0.1234, step time: 0.5424\n",
      "7/388, train_loss: 0.2597, step time: 0.5109\n",
      "8/388, train_loss: 0.1465, step time: 0.5026\n",
      "9/388, train_loss: 0.1121, step time: 0.4894\n",
      "10/388, train_loss: 0.0970, step time: 0.4870\n",
      "11/388, train_loss: 0.2552, step time: 0.5174\n",
      "12/388, train_loss: 0.3385, step time: 0.5791\n",
      "13/388, train_loss: 0.2052, step time: 0.6219\n",
      "14/388, train_loss: 0.1493, step time: 0.5446\n",
      "15/388, train_loss: 0.0917, step time: 0.4958\n",
      "16/388, train_loss: 0.1282, step time: 0.9172\n",
      "17/388, train_loss: 0.0985, step time: 0.5326\n",
      "18/388, train_loss: 0.1326, step time: 0.5046\n",
      "19/388, train_loss: 0.1002, step time: 0.4877\n",
      "20/388, train_loss: 0.0973, step time: 0.5063\n",
      "21/388, train_loss: 0.3538, step time: 0.5209\n",
      "22/388, train_loss: 0.3239, step time: 0.5124\n",
      "23/388, train_loss: 0.1980, step time: 0.4961\n",
      "24/388, train_loss: 0.0706, step time: 1.1528\n",
      "25/388, train_loss: 0.0869, step time: 0.5235\n",
      "26/388, train_loss: 0.2582, step time: 0.5080\n",
      "27/388, train_loss: 0.1918, step time: 0.5003\n",
      "28/388, train_loss: 0.1150, step time: 0.4976\n",
      "29/388, train_loss: 0.0755, step time: 0.4846\n",
      "30/388, train_loss: 0.1029, step time: 0.4866\n",
      "31/388, train_loss: 0.1219, step time: 1.0336\n",
      "32/388, train_loss: 0.1408, step time: 0.5410\n",
      "33/388, train_loss: 0.0911, step time: 0.4982\n",
      "34/388, train_loss: 0.1249, step time: 0.4946\n",
      "35/388, train_loss: 0.1027, step time: 0.4813\n",
      "36/388, train_loss: 0.1316, step time: 0.4923\n",
      "37/388, train_loss: 0.1000, step time: 0.5108\n",
      "38/388, train_loss: 0.2997, step time: 0.4992\n",
      "39/388, train_loss: 0.1034, step time: 0.4979\n",
      "40/388, train_loss: 0.1217, step time: 0.4867\n",
      "41/388, train_loss: 0.1369, step time: 0.5480\n",
      "42/388, train_loss: 0.1231, step time: 0.5181\n",
      "43/388, train_loss: 0.1969, step time: 0.4872\n",
      "44/388, train_loss: 0.1614, step time: 0.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/388, train_loss: 0.1908, step time: 0.4989\n",
      "46/388, train_loss: 0.2571, step time: 0.4972\n",
      "47/388, train_loss: 0.0747, step time: 0.5027\n",
      "48/388, train_loss: 0.1021, step time: 0.5004\n",
      "49/388, train_loss: 0.1745, step time: 0.4833\n",
      "50/388, train_loss: 0.1379, step time: 1.1558\n",
      "51/388, train_loss: 0.1927, step time: 0.5324\n",
      "52/388, train_loss: 0.1098, step time: 0.5178\n",
      "53/388, train_loss: 0.1542, step time: 0.4878\n",
      "54/388, train_loss: 0.1504, step time: 0.5454\n",
      "55/388, train_loss: 0.1101, step time: 0.5219\n",
      "56/388, train_loss: 0.0648, step time: 0.5023\n",
      "57/388, train_loss: 0.1167, step time: 0.4904\n",
      "58/388, train_loss: 0.0307, step time: 0.4946\n",
      "59/388, train_loss: 0.1094, step time: 0.4896\n",
      "60/388, train_loss: 0.0926, step time: 0.4920\n",
      "61/388, train_loss: 0.1835, step time: 0.5418\n",
      "62/388, train_loss: 0.1199, step time: 0.5083\n",
      "63/388, train_loss: 0.0938, step time: 0.5111\n",
      "64/388, train_loss: 0.0955, step time: 0.5111\n",
      "65/388, train_loss: 0.1267, step time: 0.4948\n",
      "66/388, train_loss: 0.1145, step time: 0.5006\n",
      "67/388, train_loss: 0.0959, step time: 0.5027\n",
      "68/388, train_loss: 0.1734, step time: 0.4956\n",
      "69/388, train_loss: 0.0767, step time: 0.4968\n",
      "70/388, train_loss: 0.1381, step time: 1.1380\n",
      "71/388, train_loss: 0.1165, step time: 0.5352\n",
      "72/388, train_loss: 0.2412, step time: 0.5072\n",
      "73/388, train_loss: 0.1989, step time: 0.4957\n",
      "74/388, train_loss: 0.1336, step time: 0.4959\n",
      "75/388, train_loss: 0.1250, step time: 0.4857\n",
      "76/388, train_loss: 0.0621, step time: 0.4928\n",
      "77/388, train_loss: 0.3318, step time: 0.5207\n",
      "78/388, train_loss: 0.3456, step time: 0.5238\n",
      "79/388, train_loss: 0.1611, step time: 0.4974\n",
      "80/388, train_loss: 0.1786, step time: 0.4939\n",
      "81/388, train_loss: 0.1441, step time: 0.4809\n",
      "82/388, train_loss: 0.0542, step time: 0.4772\n",
      "83/388, train_loss: 0.2171, step time: 0.4959\n",
      "84/388, train_loss: 0.2360, step time: 0.4950\n",
      "85/388, train_loss: 0.1546, step time: 0.5022\n",
      "86/388, train_loss: 0.2592, step time: 0.4884\n",
      "87/388, train_loss: 0.1840, step time: 0.4874\n",
      "88/388, train_loss: 0.1569, step time: 0.5052\n",
      "89/388, train_loss: 0.2452, step time: 0.4956\n",
      "90/388, train_loss: 0.0443, step time: 0.5146\n",
      "91/388, train_loss: 0.2297, step time: 0.5095\n",
      "92/388, train_loss: 0.0972, step time: 0.4953\n",
      "93/388, train_loss: 0.0654, step time: 0.4882\n",
      "94/388, train_loss: 0.1982, step time: 0.4997\n",
      "95/388, train_loss: 0.1042, step time: 0.5415\n",
      "96/388, train_loss: 0.2356, step time: 0.5161\n",
      "97/388, train_loss: 0.0532, step time: 0.5053\n",
      "98/388, train_loss: 0.2378, step time: 0.4897\n",
      "99/388, train_loss: 0.1010, step time: 0.4913\n",
      "100/388, train_loss: 0.5682, step time: 0.5280\n",
      "101/388, train_loss: 0.1001, step time: 0.5307\n",
      "102/388, train_loss: 0.2629, step time: 0.5176\n",
      "103/388, train_loss: 0.1761, step time: 0.5052\n",
      "104/388, train_loss: 0.1930, step time: 0.4891\n",
      "105/388, train_loss: 0.1472, step time: 1.2277\n",
      "106/388, train_loss: 0.2075, step time: 0.5509\n",
      "107/388, train_loss: 0.1750, step time: 0.5127\n",
      "108/388, train_loss: 0.1643, step time: 0.5008\n",
      "109/388, train_loss: 0.1586, step time: 0.5272\n",
      "110/388, train_loss: 0.0537, step time: 0.5049\n",
      "111/388, train_loss: 0.0719, step time: 0.4979\n",
      "112/388, train_loss: 0.1667, step time: 0.5081\n",
      "113/388, train_loss: 0.3298, step time: 0.5140\n",
      "114/388, train_loss: 0.5166, step time: 0.5542\n",
      "115/388, train_loss: 0.1995, step time: 0.5458\n",
      "116/388, train_loss: 0.0967, step time: 0.5162\n",
      "117/388, train_loss: 0.0949, step time: 0.5585\n",
      "118/388, train_loss: 0.0756, step time: 0.5482\n",
      "119/388, train_loss: 0.2729, step time: 0.5267\n",
      "120/388, train_loss: 0.2318, step time: 0.4903\n",
      "121/388, train_loss: 0.1154, step time: 0.4934\n",
      "122/388, train_loss: 0.0888, step time: 0.4836\n",
      "123/388, train_loss: 0.0853, step time: 0.7490\n",
      "124/388, train_loss: 0.0946, step time: 0.5634\n",
      "125/388, train_loss: 0.1586, step time: 0.5175\n",
      "126/388, train_loss: 0.1038, step time: 0.5001\n",
      "127/388, train_loss: 0.0903, step time: 0.4944\n",
      "128/388, train_loss: 0.0737, step time: 0.4918\n",
      "129/388, train_loss: 0.1306, step time: 0.4931\n",
      "130/388, train_loss: 0.0945, step time: 0.4967\n",
      "131/388, train_loss: 0.2628, step time: 0.5017\n",
      "132/388, train_loss: 0.1042, step time: 0.5051\n",
      "133/388, train_loss: 0.3083, step time: 0.5019\n",
      "134/388, train_loss: 0.1197, step time: 0.4976\n",
      "135/388, train_loss: 0.4858, step time: 0.7137\n",
      "136/388, train_loss: 0.3179, step time: 0.5527\n",
      "137/388, train_loss: 0.1659, step time: 0.5263\n",
      "138/388, train_loss: 0.1846, step time: 0.4995\n",
      "139/388, train_loss: 0.0989, step time: 0.4986\n",
      "140/388, train_loss: 0.1659, step time: 0.4805\n",
      "141/388, train_loss: 0.0949, step time: 1.0279\n",
      "142/388, train_loss: 0.1868, step time: 0.5383\n",
      "143/388, train_loss: 0.4464, step time: 0.5092\n",
      "144/388, train_loss: 0.1078, step time: 0.4862\n",
      "145/388, train_loss: 0.2388, step time: 0.5040\n",
      "146/388, train_loss: 0.1196, step time: 0.4984\n",
      "147/388, train_loss: 0.1186, step time: 0.4822\n",
      "148/388, train_loss: 0.2630, step time: 0.5096\n",
      "149/388, train_loss: 0.2596, step time: 0.5527\n",
      "150/388, train_loss: 0.1654, step time: 0.5154\n",
      "151/388, train_loss: 0.0610, step time: 0.5352\n",
      "152/388, train_loss: 0.2593, step time: 0.5226\n",
      "153/388, train_loss: 0.4220, step time: 0.5234\n",
      "154/388, train_loss: 0.1417, step time: 0.4996\n",
      "155/388, train_loss: 0.0917, step time: 0.4933\n",
      "156/388, train_loss: 0.3262, step time: 0.4917\n",
      "157/388, train_loss: 0.2141, step time: 1.1809\n",
      "158/388, train_loss: 0.0833, step time: 0.5521\n",
      "159/388, train_loss: 0.1015, step time: 0.5207\n",
      "160/388, train_loss: 0.2223, step time: 0.4990\n",
      "161/388, train_loss: 0.1027, step time: 0.5027\n",
      "162/388, train_loss: 0.1450, step time: 0.4818\n",
      "163/388, train_loss: 0.2419, step time: 0.4916\n",
      "164/388, train_loss: 0.2266, step time: 0.4976\n",
      "165/388, train_loss: 0.5177, step time: 0.5233\n",
      "166/388, train_loss: 0.1358, step time: 0.5128\n",
      "167/388, train_loss: 0.0887, step time: 0.4991\n",
      "168/388, train_loss: 0.1409, step time: 0.5068\n",
      "169/388, train_loss: 0.1704, step time: 0.5149\n",
      "170/388, train_loss: 0.0649, step time: 0.5097\n",
      "171/388, train_loss: 0.1576, step time: 1.0964\n",
      "172/388, train_loss: 0.1160, step time: 0.5417\n",
      "173/388, train_loss: 0.0783, step time: 0.5156\n",
      "174/388, train_loss: 0.0921, step time: 0.4977\n",
      "175/388, train_loss: 0.0941, step time: 0.4872\n",
      "176/388, train_loss: 0.1330, step time: 0.4888\n",
      "177/388, train_loss: 0.1799, step time: 0.4893\n",
      "178/388, train_loss: 0.1575, step time: 0.4858\n",
      "179/388, train_loss: 0.2018, step time: 0.5671\n",
      "180/388, train_loss: 0.2214, step time: 0.5214\n",
      "181/388, train_loss: 0.0912, step time: 0.4867\n",
      "182/388, train_loss: 0.2152, step time: 0.4871\n",
      "183/388, train_loss: 0.0615, step time: 0.5201\n",
      "184/388, train_loss: 0.2842, step time: 0.4960\n",
      "185/388, train_loss: 0.1261, step time: 1.0111\n",
      "186/388, train_loss: 0.1454, step time: 0.5461\n",
      "187/388, train_loss: 0.3098, step time: 0.5146\n",
      "188/388, train_loss: 0.0602, step time: 0.4968\n",
      "189/388, train_loss: 0.0293, step time: 0.4998\n",
      "190/388, train_loss: 0.0783, step time: 0.5326\n",
      "191/388, train_loss: 0.2040, step time: 0.5197\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    inputs=val_inputs, roi_size=(240, 240, 160), sw_batch_size=1, predictor=model, overlap=0.5\n",
    "                )\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed, best_metric: 0.7562 at epoch: 168\n"
     ]
    }
   ],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBEAAAGDCAYAAAB9bYiEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB9e0lEQVR4nO3dd3zV9fXH8dfJIOwdQPaWoQiCiIqjTrQqto7ittZiW7WtbW31Z6tWa2uXWiu1Yp2tirMWldaBiOJgyQbZyIYwQkjIurnn98f3JoQQwg0kdyTv5+ORR3K/93O/35MLfLj33PM5H3N3REREREREREQOJiXeAYiIiIiIiIhIclASQURERERERESioiSCiIiIiIiIiERFSQQRERERERERiYqSCCIiIiIiIiISFSURRERERERERCQqSiLIITEzN7Pe8Y6jJpnZIjM7Ld5xiIhURfOviEji0Jws9ZGSCPWUmf3PzO6t5PhoM9tsZmmHce4PIxPqMRWO/zty/LRDPfchxtM9ct3cyNcWM3vLzM4qP87dB7r7hzV43UXlrlliZgXlbv+fmTUws3vMbLmZ5ZnZGjN7ysy611QMIpJ4NP/GZP693MyWVDj23gGO3V4uvlwzC5tZfrnbV9ZUXCKSeDQn1/6cHLn2hxVeC+ea2ZtmdmW52/mRObhsTE3GIDVHSYT661ngKjOzCsevBp5399Bhnn8ZcE3pDTNrA5wAZB3meQ9HS3dvChwDvAf828yuq62LRSbgppFrfgzcXHrb3X8LvApcCFwBtIjENRs4o7ZiEpGEoPm3ludf4COgn5llAkTeBBwDNKpw7ATgo3Jzc1NgLXBBuWPP12KcIhJ/mpNrf04uVf61cFN3v8Ddny83/54LbKwwJ0sCUhKh/noDaAOcXHrAzFoB5wPPmdlwM/vMzLLNbJOZPWpmDapx/ueBb5lZauT25cC/gaJy10uJfAK00sy2m9nLZta63P2vRDLAu8zsIzMbWO6+Z8xsnJm9bWa7zWy6mfWKJjB33+zufwHuAX5vZimRc64xszMjP6dGqgVWRs4/28y6RO7rF/n0aoeZLTWzy6rxvJTGfyZwFjDa3We6e8jdd7n7OHd/srrnE5Gk8gaaf++hFudfd98ArAJOiRw6FlgETK1wLAWYGU3sIlJnvYHm5HuI02tiSU5KItRT7p4PvEy5zChwGfClu88DSoBbgbYE2dIzgB9U4xIbgcXA2ZHb1wDPVRhzC3ARcCrQEdgJjCt3/3+BPkA74AuCSbi8McCvgVbACuD+asQH8Hrk3EdWct9PCCb584DmwPXAHjNrQpCxfSHy2DHA38xsQDWvfSYww93XVfNxIpLkNP8CsZl/P2JvwuAUgoqwaRWOfe7uxdWMXUTqEM3JQHxfE0sSUhKhfnsWuMTMGkZuXxM5hrvPdvfPI5+QrwEeJ5jYquM54Boz60dQNvVZhfu/B9zp7uvdvZAgC3qJRdaeuftT7r673H3HmFmLco//t7vPiJSZPQ8MrmZ8GyPfW1dy3w3AL919qQfmuft2gqz0Gnd/OvLczAFeAy6t5rXbAJuq+RgRqTs0/wZqc/4tX3VwMkES4eMKx6ZWM24RqZs0Jwdq+zXxI5GKjtKv+6oZpySIQ24UIsnP3aeZ2TbgIjObCQwHvglgZn2BB4FhQGOCvyuzq3mJ14E/A9uBf1ZyfzeCNVjhcsdKgPZmtpkgi3opkAmUjmkL7Ir8vLnc4/YA1V031SnyfUcl93UBVh4g5uPNLLvcsTQq//2qsh3oW83HiEgdofk3JvPvR8CTkbLkEcCV7p5rZkdEjo0EHq5m3CJSB2lOjtlr4h+6+z+qGZskIFUiyHME2dargHfcfUvk+GPAl0Afd28O/B9QseFMldx9D0H51fepfEJZB5zr7i3LfTWMrGW9AhhNUPbfAugeeUy1YjiIbwBbgaUHiK2y9WTrgKkVYm7q7t+v5rXfB4abWedqPk5E6g7Nv7U4/7r7KoJP18YCa929tMv3Z5FjTYHPD/P3EJG6Q3NyfF4TSxJSEkGeI5iUvkukbCuiGZAD5EZKrw51Qvg/4NRI+VdFfwfuN7NuAGaWaWajy12/kCBj2xj47SFefz9m1t7MbgbuBu5w93Alw/4B3GdmfSwwyIJuum8Bfc3sajNLj3wdZ2b9qxODu7/P3m64Q80szcyamdn3zOz6w/0dRSQpaP6t/fn3Y4L1vB+XOzYtcmxWZC20iAhoTo7La2JJTkoi1HORiexToAkwsdxdPyPIfO4GngBeOsTzb3T3aQe4+y+Ra75rZrsJPhE6PnLfc8BXwAaCZjQ18WlRtpnlAQsImsNc6u5PHWDsgwRNdt4l+I/jSaCRu+8maIwzhuATrs3A74GMQ4jnEmASwXO7C1hIUCr3/iGcS0SSjObfmMy/UwkafpV/Hj6OHPvokH8bEalzNCfHZE5+1Mxyy31Vd1mIJAhz93jHICIiIiIiIiJJQJUIIiIiIiIiIhIVJRFEREREREREJCpKIoiIiIiIiIhIVJREEBEREREREZGoKIkgIiIiIiIiIlFJi9eF27Zt6927d4/X5UVEKjV79uxt7p4Z7zhiRXOxiCSi+jQXax4WkURU1TwctyRC9+7dmTVrVrwuLyJSKTP7Kt4xxJLmYhFJRPVpLtY8LCKJqKp5WMsZRERERERERCQqSiKIiIiIiIiISFSURBARERERERGRqCiJICIiIiIiIiJRURJBRERERERERKKiJIKIiIiIiIiIREVJBBGRBGJmo8xsqZmtMLPbK7m/m5lNNrP5ZvahmXUud9+1ZrY88nVtbCMXERERkfpASQQRkQRhZqnAOOBcYABwuZkNqDDsT8Bz7j4IuBf4XeSxrYG7geOB4cDdZtYqVrGLiIiISP2gJIKISOIYDqxw91XuXgRMAEZXGDMA+CDy85Ry958DvOfuO9x9J/AeMCoGMYuIiIhIPaIkgohI4ugErCt3e33kWHnzgG9Gfv4G0MzM2kT5WADMbKyZzTKzWVlZWTUSuIiIiIjUD0oiiIgkl58Bp5rZHOBUYANQUp0TuPt4dx/m7sMyMzNrI0YRERERqaOiSiJE0eirq5lNMbM5kWZf59V8qCKSyNasgezseEexV14eLFkS7yiqbQPQpdztzpFjZdx9o7t/092HAHdGjmVH81gRkWh4OEz2smW4e7xDERGpt3LW57Bn+554h1GpgyYRomz09Uvg5ciL2jHA32o6UBFJXKtWwVFHwTHHwPz5NX/+RYvg7rvhlVcOPnbHDrj3XujWDS6+GMLhmo+nFs0E+phZDzNrQDCfTiw/wMzamlnp3H0H8FTk53eAs82sVaSh4tmRYyJSB4RDIfI2bqxyTGF2NtvmzcMPc+Kb+9BDTPrGN5h1//37nKukqIgNU6eyfcECinNzD3oed2f9Bx8QLi4+rHhEROqbNR+uYVz/cTx10lMU70m8OTQtijFljb4AzKy00dficmMcaB75uQVQ9f9yInLY3MHs8M8zfXqQAGjS5NAeX1IC110HqakQCsFJJ8FLL8F5h1CPFArBHXfAzJnQsiW0aAFz5sCCBcH9TZrAqadCu3aVP378ePjJT4IqhPPPD86VkkSLttw9ZGY3E7z5TwWecvdFZnYvMMvdJwKnAb8zMwc+Am6KPHaHmd1HkIgAuNfdd8T8lxCRGufhMNNuvZWNH33E+W+/TdPOnffe587Gjz5i1RtvsGHKFMLFxTTv2ZMB119Pt69/ndQGDQ58XncKtm2jUbllTWvffZclTz1Fi169WP7iixTn5jLivvvImjOHmffeS87q1WVjm3TqxEl//CNtjzlmv3NvnT2bOX/6E9vnz+eEBx6gxwUX1NCzISJSty2ftJyXL36ZJu2bsH3pdt6//X3OfeTceIe1j2heXkfTrOse4CozWw9MAm6pkehEYswdVq6MzbX27IFHHw0+Oa+O3bvhl78M3mS/8MKhXz83F779bRgxAsaOPfTzPPwwfPwx/PWvMGMG9OkDF1wAv/kNFBVFf57iYrjiCvjTnyA/H1avhg8/hObNg3N//DEUFMDvflf549etgx/9CIYNC6oh3nwTTjzx0H+veHH3Se7e1917ufv9kWN3RRIIuPur7t4nMuYGdy8s99in3L135OvpeP0OInWJh8Ms/PvfmTJ2LAv+9je2TJ9OflYW+VlZ7Nm6lZIoJ7pdq1bx0Y9+xKsnnshb55/P+9ddx7xHHolqycDCv/89+EQ/FGJZhYl/yZNPMvUHP2DrzJn0+da3OP7ee0lJT+fzX/6S/5x5Jh/96EcsfPxxtsycuc+13J05f/oT/z7tND7+8Y/J3bCBXatW8fmdd9Jm0CBGvfoqx/zoR6x5800mfeMbTP72tykpLmbkQw9x8iOPMPjWW8GMqTffTO769WXn3bN1Kx/96Ee8f8017Nm8mePvu49uh5JVFhGphxa/upgJoyeQOTCTsbPGcvyPjmfGX2ew8r29b1CK9xQTKgjFMUqwg/3nZWaXAKPc/YbI7auB49395nJjfhI515/N7ATgSeAodw9XONdYYCxA165dh3711Vc1+suIHK7f/jZ4gz5nTlCaXx2LFsG//w0/+AG0bl31WHe45hr417/gjDPgf/+DtIPUBbnDP/4RxLd1K7RtG3zKvmxZ8Il9dcyaFbxhX7kShg4NPvn/4gsYMmTvmKVL4e234aOPgmqFwYPhzjth5Mi9YxYvhmOPhVGjgt/dLKgCuOEGmDABBg4MqgMqezMfCgVLDRo0gMJC+Na34D//CZIIP/1p5XF/5zvw/POwfDl06bLvfddeG1RALF0aLGU4VGY2292HHfoZksuwYcN81qxZ8Q5D6jh3x2qidKqa1r3/PgU7dtDrm98k5WCTbCVKior4/P/+j6/++1+adukSvFmu8LqpcYcOnPH00zTr2rXSc+zZvJmF48ez8tVXSW3YkK5nn01xbi55GzeyY9Eijrv7bvpcdtkBY1j/wQd8dMst9Bg9mnBRERunTeOiyZNJb9KEopwc/nPOOWQOHszJf/lLWdWBu7Np2jRW/+c/bF+0iNy1awHo+c1vMuzOO0lr2JBF48cz7y9/of2IEcESiJISMlq1IlxczLmvvELjDh0AWPbii8z985/pe+WVHPW975HWqFFZbDmrV/PuFVfQsG1bzv7Xv8iaO5fP77yTUEEBA2+4gX7XXENa48bVft6hfs3FmodF6q7ty7fTsEVDmrQ7eMnvrnW7GNdvHB0Gd+CKSVfQsEVDivOLGT90PIW7CrnouYtY8MICFk1YxBHHHsF1H11Xq/+3VjkPu3uVX8AJwDvlbt8B3FFhzCKgS7nbq4B2VZ136NChLnIg69e7/+9/+x9/6in3o45y37GjeucLhdzvvde9Y0f3rl3d+/d3P/dc902b9o5ZscK9YUN3cL/llujP/eWX7pdf7m4WPHbgQPd166p+zLhxwdgzzwy+/+xne+97+233ESPcP/po38e88EIw9uST3WfMcJ81K7jmT38aXZwFBe4vveR+xhnBeTp3dp861X3nTvfWrd3PPnvv2I8+cm/QIBjXq5f7mDHubdsGt087zf3GG4Pnr3374Pjmzftfb+JE9y5dgseMHBk85uGH3e+80/3UU/c+140aubdsGfz86KNV/w5r1rinp7t/97v7Hv/ii+C5+MUvonsuqkKwbOCgc2Nd+dJcLLVl8/TpPuu3v/V3rrjCJwwZ4u9ccYVv/OQTD4fDB31sqKDgsK9fsHOnvzR0qD8/YIC//Y1v+NY5c/Ybs2frVp/1wAM+8ze/8bwKE1nBzp3+3jXX+PMDBviiJ57wcDjshbt2+foPP/SlL7zgyyZM8C//9S9/5YQT/I0zz/S8cv+hlIRCvn7qVP/wppv8haOO8hcGDfIZ993n+du2lY0Jh8M++Tvf8ZeGDvXdlfynkb9tm6+aONFfGjbM/3vZZR4qKPCsuXP9+QEDfOnzz7u7+7xHHvHnBwzwHUuWVPlcFObk+Ny//MWfHzDAJ118sc//29/8+QED/JNf/MLDJSWet2mTf/Lzn/uEY4/1zZ9/vt/jwyUlBzz35hkz/MVBg/yNs84KnuuLLvLslSurjCca9Wku1jwsUjd99fFX/puGv/H7m9zvH977oRfmFnq4JOyrJq/yiWMn+rx/zttn/CvfesV/0/A3vmPVvm90Ns7e6Pem3ev3cI/f3+R+f/aMZ/0e7vH5L8yv1firmoejqURIA5YBZxB0+p4JXOHui8qN+S/wkrs/Y2b9gclAJ6/i5Mq6yoG4B+vep00LPmXv3Ts4Hg5D377Bp+fXXQdPV1KsXVwcNNWbOBEuvDD45DolBa66KiiHHzUqWE+/Zw9MmgT9+wcl802aBPd99hkMHx58Kr9xIzRsWHWsL7wAV18NjRrBLbfA8ccHFQatWsG778KRR+7/mM8/h1NOgbPOCkruf/hDGDcOnnwyqBB47LHgE/2OHWHePGjTBrZtC2Lt1Qs++SToPwDw3e/CM88EPQP69dt7jYKC4BP78eNh06ZgecDu3cEn/t26BZUCN90UxAnw4IPBp//vvw9duwZLHDIz4Z139n6qn5cHTzwRjM3PD4537Qq33gonn1z585ObCw88EDzHixfDzp1B7EOGBL0TMjODHR2ys4Pn/+KLq36+IXi+/vY3+PLL4O+Ge/Bczp0b/N2oblVGRfXp0y/QXCyVK87LY+m//sXyCRNo0bs3A66/nvYjRmBm5GdlsX3BAtoNHUqDA/yD27l0Kf+75BJSMjJo3b8/Lfv2ZcOHH7Jn82Yyhw6lVb9+7NmyhT2bNxMutxygOC+Pwh07COXn0+3rX+fE3//+kD9lmffIIyx6/HGG3HYbXz73HPlbtnDEySfT5qijaNWvH9vmzmXZiy8SDoUwMywtjf7XXUebQYP46u23WTd5Mh4KMeL+++n+9a8f8Do7Fi1i8vXX07BtW0743e/YOHUqq954gz2bN9OwTRt6fuMb9L700n36GJTK27iRty+6iDYDB3L6k08SLi5mxauvsvK118heuhQI+g6c9dxzZZUB74wZQ9Hu3Zz13HNMHDWKjiefzMgHH4zqOVk/ZQqf3XEHxbt30+m00zj54YdJSU8vuz9cUkJK6X8w1bD6zTf5/M476fOtbzHkZz8jNSOj2ueoqD7NxZqHpa4LFYZIy6h+NVhVshZn0bZfWywltlVuH93/Ea17tWbgtwZW+f/T1oVbefrkp2nSrgntjmrHkteX0KxjMyzFyFmfA5GHXvbaZfT/Rn/WTF3Ds6c9yyl3ncLXfv21/c735Rtfkrc1j6PGHEWDpg0YP2w8e7bt4eYvbya9cfp+42tCVfPwQZMIkROcBzzM3kZf95dv9BXZreEJoClBk8Wfu/u7VZ1TE2bd98UX8NVX8I1vVO9xr7wCpZWdN90U9A2AoOT/3HPhuOOC8vv//jd441lq5cqgRH/GjOBN6ty5wfFGjYJEwt/+FrzhL/XWWzB6dHCOK68Mvv7yl+DN+DnnBKXx5StMvUIjw3feCZr3jRwZjC1t9jdnTnDOcDgYc+yxex+zbh2ccAJkZAQJg1atgsTHGWcESQ4IGgNefDGcdlpw/tdeCxITL70UPKdHHbX3fFu3BomVESOChMasWTB1avBmPysLBg0Krt+oUZAoOfPM4Kvia8SCguA8bdoEb/yzs4NkR69e1fuzq4p7EFOTJofexBFg82bo2TNYznD22UFviN/8Bh55JEjkHK769MIVNBfLvtydZc8/z8LHHqMwO5sOJ5xA9vLlFGzbRsu+fQkXF5c11jvq+99n0M03V3qOydddx64VKzj/7bfJaNkSCJYGrHz1VRY/+STFeXk0bt+eRu3aBeXu7uBOWpMmNGzdmqJdu1j1xhsM/b//48grryw797Z58yjKyaH98OFlb1T3bN7MV//7H827d6fTaacBBGX+Z51FhxNO4OSHH6Y4L49F48ez/oMP2L1mTbDjgBndL7iAo7//fcyMuQ89xNp3gg1NGjRvTtdRo+hz2WW06t//oM/b1tmzmXLjjZTk54MZR5x0Er2++U06fe1rVTY2BFjx6qvMuPtuul9wAVtnzmTP5s20GTSIzl/7Gh1OOIFWAwbs88Z+zVtv8ekvfkGbQYPYsXAh573xBi2qMVnvXruWde++S9+rriLtYJnyaggVFNTo+erTXKx5WOqyrz7+in+e+U9umH4DHQZ3qJFzbpixgX8c/w9GPz2awdcNrpFzRmPj7I08MewJAPpf3J+vP/Z1mmTu/6J219pdPHnik3jY+c5n36Flt5as/WQtU389ldQGqQy6ahA9z+zJC+e/wJZ5W7jyf1fy31v+S+GuQm5aclNUSYGvPvqKZ059htN+fRqn3nXqfvcvn7ScpW8uJa1hGumN0mnRrQXDbqzelHpYyxlq60ulW3Xb0qXuLVq4p6S4L1oU/eP27AmWGxxzjPvVV7s3buxeWv15/vlB+XxOTrAcoUsX9127gqUN99/v3rRpUBb/8svB+LVr3X/9a/crr3Rfvrzy6z32WPDKNSXFfejQYNlDKBSce9SoveOefTYov7/ySvfZs4PlBE2auA8e7J6dvf95ly937949eA4+/TQ4tnhxsISgeXP3ilW1mzcHv+/kyXuP/elPQWxXXRV8v+uuyn+Hhx6KvPqOfJkFz9UHH7hHUTVc5plngsdnZLh/8kn0j4uHl14KlnU0bRrE3KePe2FhzZybelRC65qLE1rBzp0HLf0v3rPHC3bu9PwdO7xw167Dvua6Dz7w5wcM8Pevv96z5gVllqHCQl/x6qv+vzFjfMr3v++Ln3rKXz/1VJ92222VnmP1W2/58wMG+PKXXqr0/miWM4TDYZ/y/e/7i8cc49sXLvRwSYnPe/RRf37AAH9+wAB/aehQ/+iHP/T3rr3Wnx840J8fMMBfOOoo/yqyDm7+uHFBmf/ixfudu3jPHs+aN89z1qzZ775t8+f7+ilTPHQIE8rW2bN90ZNPeu7GjdV6XDgc9g/GjvXnBwzw/11+uW/69NMqn6NQYaG/fuqpwXKE22+vdpzJoj7NxZqHpS6b8I0Jfg/3+JR7ptTYOV+/+nW/h3v8lW+9UmPnjMabN77pv2n0G59yzxS/r8F9/sd2f/QV76zYZ0zOhhx/tN+j/rsWv/PN8ypZ71tOXlaeP9LnEf916q/9Hu7xRa9W402Tu798yct+f+P7fde6/f///9tRf/P7Gtznv232W783/V4ff9z4ap3bvep5OKpKhNqgrGvdtWtX8Ml4VlbwKfsppwRl+9H4zW/gV78Kyt/btIGjjw6OXXFF8Kn4nXfCffcFn5KfeGLQFPDLL4NPz887D/7+9/0b7h3ML38Jf/5zUAkwLJJru+uu4LpffQUbNgTLK3r2hPXrg2tlZATLDT79FDocIKm6bh2cfnqwnOCBB+Cee4Lmie+8E13TxnA4qLx4991gKcOcOcF1KyouDuJt0SKo0hg27NBK+ktKgqUCo0YFuyskg5ISWLEiqOg40LaP1VWfPv0CzcWJavXEiXx2xx1kHnssR33ve3Q48UTMjOK8PHZ++SWbP/2UjdOmsWPRon0a/fW96iqOve22siaCHg6zbe5cWg0YsM+nxAU7d7L4iSfoMXo0rSLrrkqKinh79GhS0tI47/XX9ylzr2jyt79NSVERZz///D7Hi/PyeOv882mUmcnZL754SKXxpQqzs/nvxReTkp5Oyz59WP/BB/S86CK6jhrF+ilT2PDhh6Q1akT3r3+dzmecwaz77mPbggWMuP9+Zt9/P5lDh3JqaSlbgivOy2PXihW0GTQoquUbi598kvmPPsrX33iDZofTSTaB1ae5WPOw1FU563N4uPvDeInT+YTOfOfT7xz2OfOy8nio80OEQ2EatW7Ez7b8LCZLGgp3F/JgxwcZcMkARj89mi0LtvD6la+TtTiLUX8ZxfCbhpO9JpvnzniOvK15XPnfK+k6svKGu+XtXL2Tp058ivbHtOfK/15ZrSV8O1fvZFz/cQy6ahAX/uPCsuMF2QX8vvXvOe2evVUKHvZqP0+HvZyhNmjCrJtKSoIlAu+8A++9F3T1v/12mDIlKM+vytq1wZvl884LljRA8IZ27lwYMyZY1rBmDZQuK/35z4P1+WPGwG23VX83hfL27IHyDaRXrQqSFj/4QbDrQMOGwRKKtLRgh4R33w3i6dOn6vNu2hQsH1i8GHr0CJ6T6iwR2Lw5iOHOO4OEidS++vTCFTQXJ6LtCxfy3tVX06JnTwqzs9mzeTMt+vShpKCA3HXBjsuWkkKbQYPoMGIEDVq2xFJSyF66lJWvvUb7449n5IMPsmvFCr744x/ZsXAhzbp3Z8RvfkPmkCFsmz+fabfeWrZm/+wXXqBp584sfvJJ5j74IKc9/jgdy2/DUonPf/UrNn38Md/48MN9js998EEWP/kkZ7/wAm0PZ1KO2Dp7NpO//W0Ajv35z+l75YFfYBXt3s0HN9zAjoULATjnpZdoU379Vx3i4TCFO3fSsE2beIdSa+rTXKx5WOqqKXdP4aP7PuLoy49m4YSF/Hz7z2nY8vCWPU17YBqT75jMSbefxCcPfMKNc26ssWUS5W1ftp1mnZrRoEmwJG32E7N5a+xbXP/p9XQ5IfjEsii3iNeueI1lby5j8PWDWfXuKopyi7jqnavoNLxT1NcqyisiNT2V1AbVT7y/ce0bLHt7Gbdtva0sSbDinRU8P+p5rn7/anqe0bPa5yylJEI997//BWviT91/uUyViorgoYeCT7rvuGP/dfSltm8PGhnOnx9UCHz+edAo8Ac/CBrwHXlk8CnxjBlBb4LyNmwIehO88QZ88EHQc2DJkuANNwSN/s46K/j5G9+A11/f+1j3oFlg8+bV+72i9bWv7W26+NlnQVXEocjKCvoxjB0LRxxRoyFKLahPL1xBc3EseDjMV5MmsW7yZIbdeSeN2rY94Nj8bdt451vfAjNGvfwy6U2bsvqNN1j95ps0ysykRZ8+tDryyAM2NVz1738z49e/JrVRI4pzcmjcoQN9xoxhxcsvk7dpE13OOosNU6bQKDOTY269lVn33UdG69ac/PDDvHvFFbQfPpxTx4076O+04LHHWPDoo1w2e3ZZhUM4FOKV44+n8xlncNIf/nDoT1gFGz/+mPRmzcgcPPigYwuzs/nw+9+nSadOjPzTn2osBom9+jQXax6WuqikuISHuz1Mh8EdGHnHSJ455ZmgieA3D95n5kDCJWEe6fUIrXq24pv/+iYPdnqQs/54Fif+rJK9xKOU/VU2zTs3JyV175uUXWt38UjvR+gwuAPXTL6GjGYZPHHcE4QKQnxv/vf2SWaHS8K8/4v3+ezPn9E4szFXv3c1HY6p+aTGgcz75zzeuOYNxn4xliOGBG80ptw1hY/v/5hfZP+CjGaH3ui2qnm4ZttkSsLZtQsuvTQojZ89e98O/lX5/POgg/+iyB4c06bBiy/u7eZfavfuoLHgl18Gn+QffTT84Q/w/e8H9zdqFCwLuPbaYKeAIUOCMv8vvgh2R5g3LxjXq1fQFO/qq/cmECBoODhoUJCguOmmfa9tVnsJBICbbw6WODz33KEnECDYgeDuu2suLhFJXLkbNrBo/Hiadu5My759AZj/6KPsXLwYgNSGDTnxd78rG7/588/5+Mc/pmmnTrQeOJDsZcso3LmTs/71Lxq2bg1A78suo3f5Lq9V6PmNb9C8Vy9m/+53dDrtNPpdcw1pjRrR94ormPvnP7P8pZc44qSTOPEPfyCjZUsat2/PBzfcwP8uuwzCYYb8/OdRXadpp+ATlj2bNtE8MmnnbdhASUEBHUaMiO7JilLHA23/UomMli05+4UXavT6IiJSuR0rd/DRfR9x7iPnktF83zerSycuJXdTLsMeH0bnEZ1p0KwBK99deVhJhOVvL2fXV7s4+09n06xjMzIHZLLq/VWHlERwdz7946e8f/v7nPCTEzj7T2eX3Tfj0Rl4ibPpi028eMGLnPHbM9g4ayOjHhm1XzVcSmoKZ//pbHqe1ZM2fdvQqkeripeqVaWVBqsnry5LIqz7dB3tB7U/rATCwSiJUMc9+WSwhr9Zs6Ds//PPg9L8PXuCLf1CoWC9fmlF5O7dQen8o49Cp05BL4NNm4I38MOHw3/+AwMGBGPd4frrYfnyoJpg1KjKqxWuuiqoaChNLEAwbuRI+P3vg+ULAwfuu/NBKbPgsa+9FvQXiKWLLw62JGzWLLbXFZHkFCoo4OMf/pDs5cvxkpKy442POIITHniAXStXsviJJ+h98cW0GzaMwuxsPrvjDhq0aEFG69asnzyZwl27OOGBB2hdOtEegraDBnHOiy/ucyy9SROOu+suBt54I40yM7FIWVi7oUM56Q9/4ONbb2XAd75D8yjX1zeJJBFyN2woSyLkrFkDQPPu3Q859ppwqFtCiohI9DzsTLx+Il999BXdTunGkOuH7HP/rL/NokXXFvQ5rw8pqSn0OL0HK99ZGTTlMyNUEGLmYzPZtXYXhbsKCYfCnPn7M2l2xN4X3jnrc3jh/Bdo3bs1vc/tzYJ/LaBZx2YcOTro5dPjzB588cQX1d5CMlQY4q2xbzHvuXk0zmzMjEdnMOLHI2jeuTlFuUV88cQXDLhkAEeOPpLXr3qdf579T9IapjHoqkEHPGfvc3pX8xmsGc06NqNtv7asnryaE392IuFQmA3TN3DMtYe/pLAqSiLUYaFQsO3dyScHPQMuvDDoT3DrrcHSgLlzgzfzEycG5fYZGcEShPXrg++/+93eN9D9+wdvqo89NjjHL34Bjz0Gr74aVB5UsYU2KSlBj4PJk4PEROfOQeVBtG/OTz899gmEUkogiNQ/Rbt3s2X6dDqfccY+b0iLdu1i/Ycf0uXMM0mvZJ/S2b/9LTu//JJT//Y3MocMIXv5cgp37KDjKaeQmpFBKD+fNW+9xczf/IZzX3mF6XffTeGOHZw9YQKt+/fH3Qnl5ZHetGmt/W6N27ff71iXs87iog8+oFFmZtTnKa1EyNuwoexY6daPzcqXk4mISJ005+k5fPXRV6Skp7D41cX7JBG2Ld3G6g9Wc/r9p5ctE+h1di+W/mcpO1bsoE2fNrx/+/tM/8t0GjRtQEaLDHI355LeJJ3zHzu/7DzTfj+NrEVZ7Mnaw5LXlgBw2q9PIzU9+NSy5xk9mfHIDNZ/tp7up3WPKu6SohL+edY/WfvxWk779Wkcc80x/LXvX5l631QuePwC5j47l4LsAo7/8fF0OaELRblFvHXjWxxz7TE0atWoRp67mtbjjB7MfXouJUUlZC3Ooii3iC4nVrPTfDUpiVCHTZwY7C7w4INBt/1bboG//AWefTZY3vDmm8Eb+m9/Gy65JHjMwIHwySdwwgn7nmvkyCDp8JOfwK9/HZxj3Tr45jfhZz87eCy9ewdfIiKJ7osHHmDVG29w1Pe/z6CbbwaCKoMp3/8+2+fN44vf/55+117LkVdeWfaGf+Xrr7PytdcYOHYsnSINaNpV6Iaa1qgRQ++4g49/+EOm3HgjW6ZPZ/BPf0rr/kFpp5nVagKhKo2rub1Jo3btSElL2yeJsHvNGhq0aEHDiuveRESkTsnbmsd7t71H15O70ml4J6Y/Mp38nfllb7Jn/m0mKekpDPnO3sRCr7ODzuIr311Jzvocpv9lOsfddBznPXoeAG99/y3mPDmHk+84mRZdW5C7OZcvnviCQdcEOw9sXbiVDdM3cNTlexvmdju1G5ZqrJq8Kuokwqy/z2Ltx2sZ/cxoBl87GIChNw5l9t9nc+LPTmT6X6bT6fhOZc0Th44dSruj29FuYA1tA1YLepzRg5njZrJ++nq2LtgKUOtJhJSDD5Fk9fDD0L17sFsCBBUDQ4cGzf1mzAiqB445JthB4c9/DpYWfPHF/gmEUkccEfRFeP/9YElEv37w9NOVL0MQEUkku9euZePHH7PsxRdZ9sILFOflVTou56uvWP3mmzRs25aFjz3GytdeI1xSwqe/+AXb589nyM9+RtvBg5n/yCO8dvLJvHHmmfz30kuZed99tB8xgqMjSYcD6Xz66Rxx8slsmT6d9scfT//rrquF37b2WUoKjTt2JLd8JcKaNWVLG0REJDGECkP7HVvy+hLGDx3PztU7D+mc79z6DsV5xZz/+PkMvGwg4eIwSycuBYIdC+Y9M4+Blw6kafu9ifFWvVrRskdLlry2hP9c9x9a92nNmb8/s+z+k+8I+t98/NuPAfjswc8IF4cZeftIzIz2R7fn2BuOLdstAaBhi4Z0Gt6J1e+vjiruguwCpt47lR5n9OCYa/aW+59y5ymkpKfw4gUvsmP5Dkb8eN/ePl1O6LJfz4dE0v207liKsXryatZ9so5mHZvRotsh7PdeDapESBK//nWw9OAnP9l3K8IDmT07aAr44IN7+xQ0bBjsMpCauu8uCenpwXmjdcYZwZaFJSXBlociIols0RNPMO/hh/c5tmDcOAZ897v0GTOmbHcBgIV//zsp6emMeuklpt91FzN+/WvWTZ7MxqlTOfb22+l39dX0//a32bFoEWsmTaJwxw4Ks7Np2qkTx/3qV6QcaBubCDPjuF/9ioV/+xuDfvjDst4Eyahp5877LWeoThNEERGpXe/f/j6zHpvFt6d9m/ZHB8vZ9mzbw5tj3yR/ez4TLpzA9Z9eH3UDvuw12Uz7/TQWvLCAU+8+lcz+mbg7Lbq2YMmrSxh87WDm/2s+hTmFHHfzcfs81szodXYvZj8+G0sxrv/k+n0SAi26tuDYG47li398wdAbhzLrsVkM/NZA2vSpeivbnmf25OP7P6ZgVwENW1S9feS0B6aRvyOfs/541j7LFZt2aMrwW4bz6R8+pXnn5vS/+NCbP8ZDo1aNOOLYI1j1/ip2b9hNlxO71Hp/oOR99VJHzZ4N27bte2zJErjnHvjVr4LtEp9/PliOcCDhcLAjQrNm8J3v7Htfevr+2yweCjMlEEQk8eVv28aixx/niJNP5sznnuOiKVM4+8UXaTVgAHP++Efe+vrX2TZ/PgC7Vq3iq7feou/ll9O4QwdGPvQQLfv0YePUqRx5zTX0u/rqsvO2HjiQY2+7jRN+9ztOe+wxTn74YRq2qfqFTqmmnTox4v77K+1PkEyalKtEKM7NpWDbtrg3VRQRibXczbmH/Il+bVr0yiI++f0nFOUW8colr1CYUwjAuz99l8JdhYz6yyiylmTx+hWvEy458BuLcCjMmg/X8MZ1b/DXPn9lzpNzGPq9oYy8YyQQJAf6X9Kfle+upGBXATPHzaTDkA50HtF5v3P1HhWsbT7p9pMqvb/0nP88658U5RZx8v8dPDHd88yeeNh55ZJXmD1+NrvW7ap03K61u/j84c8ZdNWgsl0Myjvp5yfRtENTTvrFSWU9F5JJjzN6sP6z9WSvyabzifs/tzVNSYQE4R4sJxg2DC67LLhd6tFHg6aHb7wB7dsHux18/evBTgoVFRbC5ZcHY3/xi9rdAlFEJNEteuIJSoqKGHr77bQbOpTG7drRdtAgTn/iCc545hksLY33r7mGlf/+Nwsfe4zUjAz6X389EOxo8LXx4znhd7/j2Ntui/NvkniadupE4Y4dhPbsUVNFEam3XrnsFZ4f9XzU49dMXcOnf/4UD/vBB5eTvSabSbdM4onhT/DaFa8x9b6prPtsXaVjs5ZkMfH6iXQ+oTNXvXMVO1bu4M3vvsmq91cx77l5nPjzEzn+h8cz6i+jWPbWMt792bsU5RaVPb6kqIRlby3jjWvf4E/t/8SzX3uWRS8tYtgPhvGjVT/i/MfO32c3hIGXDqSkqIT3bnuPrQu3ctxNx1X6SfiRFx7JmP+M4bR7Tqs07hZdgmqE/O359LuoH+2OOngfgi4ndWHkHSPZvmw7b934Fg93fZg5T8/Zb9wHv/wAgNN/U3m39sZtGvOTjT9h+M3DD3rNRFSaTAHoelLXWr+ePktOAMXFwfaHTz4ZVBpMmRL0HTjrLNi1K2hiOGZM0NvggguCnRR+/GM45RR4+23o2DE4T3Z2sOvChx8G/Q+iaXgoIlJX5W3cyIqXXqLnRRdV+gl5++OOY9RLLzHtpz9l+i9/CcCAG26gYevWZWMatmlDjwsvjFXISaV0m8e8jRsTZntHEZFY2r5sO2s/XgtA9lfZtOzWssrxRXlFvHb5a+RuymXjjI1c9NxFB92aMGtxFtN+N40FLy7AUowuJ3Rh3afrWPjiQqbeM5Wbvrxpn5L/otwiXr74ZdIapXHpy5fSvHNzTr//dCbfPpnlk5bTundrTvnlKQAc94PjyFqUxfSHpzPz0Zl0Or4TLbu3ZPmk5RTsLKBhq4b0Pb8vR44+kt7n9KZB0waVxthpeCead27OF098QcNWDTn68qMrHWcpxpEXHlnl73vynSeTtSiLr933tSrHlUpJTeGM357B6fefzrYl25h00yT+e8t/6X5ad1r1CBr9Lp24lPn/nM9JvziJFl0P3CsgmbcI7nJSF1IzUjEzOgzuUOvXUxIhztyDN/5vvx0sV/i//wsaFv7f/8GZZwYJhLw8KO3VlZIS/Ny7N1x6KYwYAVdeGSyDmDkzGPuvfwXHRETqswXjxoEZR//gBwcck9GyJV97/HHmPvQQG6dOpV+SNjqMh9IkQu6GDeSsXo2lptK0S+12gxYRSSRzn5lb9vPqyav32eawMp/9+TNyN+Uy5IYhzPnHHPZs38Pop0ezdtpalk1cRt7WPHqe3ZO+5/fFS5yPfvMRi19dTHrjdI7/0fGccOsJNO8clBnnbMjhr73/yrTfTmP006PLrvHez99j+9LtXPXuVWVjT7rtJNZNW8eyt5Zx/uPnk94oHQjeNJ/36Hn0v7g/qyevZvXk1az43wr6fr0vR11+FD3P7Elqg4OX9ltKsKRh+sPTGXL9ENIbp1f3qSzTvFNzrpt6XbUfZ2ZkDsjkomcv4m9H/Y2J35nINe9fEyzZuPJ1Og7ryKl3n3rIcSW69Ebp9D6nN+FQOKo/s8OlJEKcvf9+kED4wx+gtFr2nnuCbRdfey1YyjBiRLDMobxRo4LGieefD3/6Exx9dJBUuPZaOOmkmP8aIiKHLOuLL1jx6qvs2bSJvE2baNW/Pyc/9NAhn8/DYbbOnMnqiRM58uqradyh6ox8Sloax952m5YsVFPT0kqEDRvYvWYNTTp1IrVB5Z9SiYjUNeGSMPOem0fvc3uzec7mgyYRcjfn8skfPqH/xf258IkL6XZyN/5z/X94uOvDADRp14SmHZoy+fbJTL59MgANmjVg5B0jOeHWE2jcdt/O6s07NefYsccyc9xMTrnrFFr1aMWmLzYx6++zGH7zcHqe0bNsrKUYl7x8CduXbt/vU2pLMXqe0XOf8Yfi2O8cy5oP1jD8lvguB2jRtQXnPHQOb97wJh/d/xHznp1HepN0vvXvb5UlT+qqS166JGbXUhIhzn73u2A5wg9/uPfYVVcF/RFuuCFYznDPPZU/dvBgWLMGQqFg5wURkWTj7sy87z5yN2ygVd++pGZksH7yZEqKiqr1htTd2fTJJ6x58002ffophTt2kNGyJQO++91ajL5+a9i2LakZGWWVCNreUUTqk1XvBZ3wRz08ii///SWrP1iNu5eVxG+as4nNczcz8LKBNGjSgCl3T6GksIQzHwi2NTzmmmNo0a0FX039ip5n9aTz8Z2xFCNnfQ7L3l5G0e4ihlw/hEatGx0whpN+fhKz/z6baQ9M4/zHzmfSTZNoktmEr927/1KA9EbptVrm3u6odnxv3vdq7fzVMeT6ISx5dQkf3vUhKekpXPfhdWVVGXVZWsPYvbVXEiFGNm4MlhyMGAGZmcGxzz8P+h88+GDQOLFUWlqwu8IllwSNFC+pIqmUlqZdEkQkee1cvJjsZcs47le/os+YMax+6y0++8Uv2L12LS179z7o48OhEGv/9z8WP/UU2UuXktGqFUeMHMkRJ53EESNH0rBVqxj8FvWTmdGkUydy169n91df0eGEE+IdkohIzMx9ei6N2jSi7wV9KcwpZMELC8hanEW7ge3wsPP6la+zbck23v3puxx9xdHM+cccjrv5OFr33tt3p/up3el+avd9ztu8c3OG3VihBPkAmndqzuDrBzPnyTk079yc9Z+vZ/Qzo2nYsn5/umhmXPDEBbx4wYuMuHUEXU7UUruapreftSgchjvugFdfhVWrgmM9esAHH0D37kEVQuvWUNkHZd/8ZrDLwmmngapDRaSuWvnaa6RmZNDtvPMAaNEzKKfMWbkyqiTC3Acf5Mtnn6V5z54c/5vf0P3rX1dJfQw16diRrC++oKSwUE0VRaTeyN+Rz5dvfMnQ7w0lLSONHqcHlVirJ6+m3cB2rPjfCrYt2cbIO0ayY/kOZj02iwbNGnDqr2p+Tf7I20cy5x9z+PCuD+lyYheOufqYGr9GMmreuTk3zrkx3mHUWUoi1KLHHw96HZx7btAMsUuXIGFw6qnw17/CxInBUoWmTfd/rBm88ELMQxYRiZlQQQFrJk2iy1ln0SCyH23pG9FdpZnXiA0ffsimzz5j6O23l5WK5m/bxvIJE+h+wQWc8NvfYinatTjWmnTqxKZp0wBt7ygidVvBrgI2fbGJotwiVr23ipKiEoZ8O+iB0LJ7S1r1bMXqyas5/ofH89mDn9GsUzNOu+c0UhuksmvtLkqKSvbra1ATWnZryTHXHsPcp+dy3rjzsJTk3WFAkoeSCIcoFILvfCfYWeGii/a//6uv4Oc/D7ZpfPvtICkA0KtXsOvC6NHQpAnccktMwxYRSRjr3nuP4t276fXNb5YdS2vcmCadOpFTIYmw/OWX2Th1Kq0HDKDn6KAL9ZfPPku4uJijvv99JRDipLS5Imh7RxGpu3I25PD0yKfJXpNddqzjcR336THQ44weLHppERtnb2T15NWc8cAZZV3yq9pWsCZ8/W9f58SfnUjbfm1r9ToipZREOESffALPPQcvvhgkCc46a+997nuXKDzxxN4EAsCQIcFyhnPPhRtvDJYziIjUR6tef52mXbrQ7rjj9jnevGfPfZII7s6OhQsBmPPHP9Lp1KAcdPmECXQdNYrm3brFLmjZR+k2j+nNmtGwrV68ikjds2f7Hv519r/Ys30Pl756KS27tSS9STotu7XcZ1yPM3rwxRNfMPH6iaQ3SWfo2KExizG1QaoSCBJTSiJElJTA3/8Ol122t/FhVf7zn6BXwZFHBtUIU6ZA6evgp56C996Dv/0NKntte8wxsG4d6IMzEamvdq9dy5YZMxj0wx/uV0XQomdPts6YQbikhJTUVPZs2kTB9u30vvRSVr7+OnMfeojG7dsT2rOHgWPHxuk3ENhbidC8e/eyZSYiInVF4e5CXjjvBXas3MFV/7uK7qd1P+DY0r4IW+ZvYfgPh9Oo1YF3VRBJdkoiRPzpT3D77bB1K/z611WPdQ/6GZxxBjz5JJx0UlBZcOqpMGcOrF4dNES8sYpeHqmpNRq+iEhSWfnaa1hKCj0rWQ/WvGdPSgoL2bNxI027dGF7pAqh5ze/SVqTJnz5zDOkNWpE59NPp2WfPjGOXMorrURQPwQRSTQ5G3KY/8/5LHxxIQMuHcApvzyl2uf47y3/ZePsjVz22mVVJhAAmmQ2of2g9mxduJURPxpxiFGLJAd9Fg7Mnw933RX8/N//Hnz8kiWwciVceCEccQS8+25QvTB/PgwbFuy68MorqjQQEalM3saNLP3Xv+hy9tk0bt9+v/tLd2goba64Y+FCUtLSaNWvH0f/4Ac07tCBUH6+qhASQEarVrQbNoyOJ58c71BERADwsDPxhok83PVhJt8xme3LtrPszWXVP487K/63gqMvP5p+o/tF9ZhTfnUKZ/7+TFr11PbCUrfV+0qEoiK45hpo1Qq+9a1g14SsrKqXNPznP8H3Cy4IvvfuHSQWRETk4L744x8BGPLTn1Z6f/PSbR5XraLTqaeyfeFCWh55JKkNGpDaoAEnP/ww2xcupM3RR8csZqmcmXHms8/GOwwRkTKf/OET5jw5h+NuOo4RPx7Bp3/+lMUvL672eXLW55C3JY9Ox3c6+OCIAZcMqPZ1RJJRvf+s/Ne/hnnzggaIV18dLFV4552qHzNxYlBx0Cn6OUVERIDNn3/OunffZcANN9CkY8dKx2S0bEnDNm3YtWoVHg6zY9EiWg8cWHZ/m6OPpu/ll8cqZBERSRJrPlzDB3d+wFFjjuLcv55L696tad27Nfk78snfkV+tc22cuRGATsP1gl+konqdRFi3Dh54AL797aCq4NhjoV27qpc0bN4M06cHSxlERCR64eJiZt1/P027dGHA9ddXObZ0h4bda9dSnJtLm6OOilGUIiKSjHZv2s2rY16ldZ/WnD/+/LJmr617B1uh7Vixo1rn2zBjAynpKbQ/Zv9ldyL1Xb1OIrz7LoTDUFpRm5IC55wD//tfsFtDZd56K6hWUBJBRKR6lk2YQM6qVRz785+TmpFR5dgWPXuya9WqsqaKrZVEEBGRKvzn2/+haHcRl712GRnN9v4f06ZPG6D6SYSNMzfSflB70jLq/epvkf3U6yTC5MnQoQMMKLd86bzzYMcOmDmz8sdMnBhs2zhoUGxiFBGpK1a8/DKZxx5Lp6997aBjm/fqRXFODhs//JDUhg1p0atXDCIUEZFktH35dla+s5KT7zyZdgPb7XNfq56twKqXRPCws3HWRi1lEDmAepNE2Lw5qCAo5R4kEc44A8pvbX322UFFQmVLGrKz4b33gioEbYctIhK9XStXkrNqFd3OPbesxLQqpTs0rP/gA1r3709Kmj4JEhGRys17bh6WYgy+bvB+96U1TKN55+bsWL5vEmHSzZOYMHoCuVty93vM9uXbKcwppONxlffuEanvokoimNkoM1tqZivM7PZK7n/IzOZGvpaZWXaNR3oYJk+Gjh3hxRf3Hlu4ELZuDZII5bVuDccfX3kS4S9/gYICOMhSXhERqWDde+8B0PnMM6MaX7pDQ0lhoZYyiIjIAXnYmf/cfHqe1ZNmHZtVOqZNnzb7VCKES8LMe24eSycu5fHBj7N6yup9xm+YsQGATsepEkGkMgdNIphZKjAOOBcYAFxuZvvsX+Lut7r7YHcfDPwVeL0WYj0k+flw441B5cGjj+49Pnly8L1iEgHg3HOD5Qxbt+49tmsXPPwwjB4NgwfXZsQiInXPuvffp+3gwTRu1+7gg4FG7dqR1qQJoH4IIiJyYGs+XMOutbs45tpjDjimVe9W+yQRti/dTtHuIk78+Yk0bNmQf575T2Y8OqPs/o0zN5LeJJ22/dvWauwiySqaSoThwAp3X+XuRcAEYHQV4y8HXqzi/pi67z5YuRK+8Q347DNYsCA4Pnky9OkDXbvu/5jzzgu+P/bY3mOPPBIsZ7jrrloPWUSkTsldt46dS5bQ5ayzon6MmZUtadDODCIiciDznp1HRvMM+l3U74BjWvduzZ5teyjILgBg/fT1AAz59hC+O/O79B7Vm3d/+i47V+0EgiRCx6EdSUmtNyu/Raolmn8ZnYB15W6vjxzbj5l1A3oAHxx+aIdv/nz44x/huuvgiSegQQMYPx5CIZg6tfIqBAi2erzsMrjnHhg3DnJy4KGH4Pzzg/tERGpLFMvHuprZFDObY2bzzey8yPHuZpZfbmnZ32MffeXWRUq/ukS5lKFUyyOPJKNVK5pVlu0VEZF6ryi3iMWvLWbAZQNIb5R+wHEVd2jYMGMDGS0yaNO3DQ2aNuCCJy4gJT2Fd3/2LiVFJWyas0n9EESqUNOdqsYAr7p7pRskmtlYYCxA11p+UVhSAt/9LrRqBX/6E7RpA5dcAv/8J1x0EezefeAkghn8619QWAg33wyvvgo7d8Ldd9dqyCJSz5VbPnYWQcJ2pplNdPfF5Yb9EnjZ3R+LLC2bBHSP3Lcysqwsoax77z1a9e9P086dq/W4Y378Y/pdey2Wok+CRERkf4tfW0xxXjGDrx1c5bjWvVsDQRKh47CObJyxkU7HdcJSgka/zTo24+Q7T+aD//uAz//yOSWFJUoiiFQhmldmG4Au5W53jhyrzBiqWMrg7uPdfZi7D8vMzIw+ykMwfTrMmAEPPBAkECDojbBrF9xyS5AoqGqXsfR0eOklGDUKPvwwWOIwbFithiwiEs3yMQeaR35uAWyMYXzVtmfLFrbNnVvtKgSAhq1alS1pEBERqWj+c/Np1asVXU7qUuW4Vr1aAcGuC8X5xWyZv4WOw/dNEpxw6wm07NGSybcH1XPa3lHkwKJJIswE+phZDzNrQJAomFhxkJn1A1oBn9VsiIdmRqQ3Sml/A4CTT4Z+/WDJEhgyZG9y4UAyMuD114MKhEceqb1YRUQiolk+dg9wlZmtJ6hCuKXcfT0iyxymmtnJB7qImY01s1lmNisrK6uGQq/c+kNcyiAiIlKVkuIS1n6yliNHH3nQrYPTG6XTvHNzdq7YyeY5mwmHwvslCdIapnH2n8/Gw06jNo1o2b1lLUYvktwOmkRw9xBwM/AOsISgjHaRmd1rZheWGzoGmODuXjuhVs+MGdClC3TosPeYGYwdG/x8oKUMFTVqFPRG6NWrxkMUETkUlwPPuHtn4Dzgn2aWAmwCurr7EOAnwAtm1ryyE8SyKmzjtGk069aNFr171+p1RERiIYq+NdeZWVa5/jQ3xCPOZLF6ymr+2uevzH5idrUfu33pdkoKSzji2COiGt+6d2t2rNixd/vGSioN+l3UjyNHHxlVYkKkPouqJ4K7TyL4xKv8sbsq3L6n5sI6fDNnwnHH7X/8uutg0iS4+uqYhyQicjDRLB/7DjAKwN0/M7OGQFt33woURo7PNrOVQF9gVq1HXYXspUtpp7VgIlIHRNm3BuAld7855gEmkXBJmI/u+4ip904FhyWvLWHod4dW6xyb524GoMPgDgcZGWjdpzVfvvElG2ZsoHnn5jQ7otl+Y8yMMW+MqVYcIvVRTTdWTAg7dsCKFXBDJbnfVq3gvfdiH5OISBTKlo8RJA/GAFdUGLMWOAN4xsz6Aw2BLDPLBHa4e4mZ9QT6AKtiF/r+CrOz2bN5My379o1nGCIiNaWsbw2AmZX2ramYRJAquDsTRk9g+dvLOeaaYygpLmHF/1bg7tX69H/z3M2kZqTS9si2UY1v3bs1e7L2sPqD1XQdqV1/RA5HnWx5PSvyuVtllQgiIokqyuVjPwW+a2bzCBrZXhdZRnYKMN/M5gKvAt9z9x0x/yXKyV6+HEBJBBGpK6Ld9vziyBa8r5pZpR3/YtmbJtHkrMth+dvLGfl/I7no2YvocXoPCnYWsGN59f7L2jx3M+2Pbk9KWnRvZ0p3aMjbkqemiSKHqU5WIpQ2VRxavaooEZG4O9jysUjZ7EmVPO414LVaD7AaspctA6DlkUfGORIRkZh5E3jR3QvN7EbgWeD0ioPcfTwwHmDYsGEJ0U8sVtZ9FuRh+n+zPwCdjg/e0K+fvp42fQ/S9TzC3dk8dzP9vtEv6uu27tO67GclEUQOT52sRJg5M9iFoUWLeEciIlJ/ZS9bRkbLljSq5eaNIiIxctC+Ne6+3d0LIzf/AegjrQrWf7aetEZptB/UHoDMAZk0aNqADdMPtIP8/nZv2E3+9nyOGBJdU0WAVj2DbR4xOGJo9I8Tkf3VuSSCe1CJoKUMIiLxlb1sGS379lWHaxGpKw667bmZlX93eiHB0jQpZ/1n6+l0XCdS01MBSElNoeOwjvskETzsPH3y08wYN6PSc1S3qSJAgyYNaNaxGZkDMslolnEYv4GI1LkkwoYNsHkzDB8e70hEROovD4fJXr5c/RBEpM6Ism/ND81sUaRvzQ+B6+ITbWIKFYTYNGcTnU/ovM/xTsd3YvO8zYQKQgCsmbqGtdPWsvyt5ZWeZ/PczWDQ7uh21br+8B8OZ8SPRxxa8CJSps71RCjth6BKBBGR+Mldt46S/Hz1QxCROiWKvjV3AHfEOq5EteKdFXQc2pHGbRsDsHH2RsLF4UqTCOHiMJvmbKLLCV2Y+/RcALYs2FLpeTfP3Uzr3q2rXVEw8hcjq/9LiMh+6lwlwsyZkJ4OxxwT70hEROqvsqaKffrEORIREYmH3M25PH/u87z703fLjq3/bD0AXU7Yd9OKzscHSYUN0zdQmFPI4lcXk944Peh9sCN/v3Nvnru5WksZRKRm1bkkwowZMGgQNGwY70hEROqvncuWgRkteveOdygiIhIHaz5cAw4LX1rInm17gCCJ0KpnK5q0a7LP2GYdm9G8c3M2TN/A4lcXE8oPceJtJwL7VyMU7Cpg58qdSiKIxFGdSiKEwzBrlpYyiIjEW/ayZTTr1o20Ro3iHYqIiMTB6g9Wk5qRSklhCXOemoO7s+6zdfstZSjV6fhOrJ++nrnPzKXNkW049rvHArB1wdZ9xm2ZHyQVlEQQiZ86lURYuRJycpREEBGJt+ylS9VUUUSkHlv9wWp6n9Obbqd2Y9Zjs8hek03uptwqkwjZq7NZ+/FaBl83mGYdm9GodaP9KhEOZWcGEalZdSqJsDWSqOzSpepxIiJSe4rz8shdt05JBBGRemrX2l3sXLmT7l/rznE3HUf2mmw+vOtDYP9+CKVK+yJYijHo6kGYGe2ObsfW+ftWImyeu5km7ZrQ9Iimtfo7iMiB1akkwu7dwfemmlNEROJm14oVALTSzgwiIvXS6imrAehxeg/6XdSPZh2bMf9f80lvnE77Qe0rfcwRQ4/AUo1eZ/eieafmALQf1J6tC7fiYS8bt2XuFjoM7oCZ1f4vIiKVqlNJhNzc4LuSCCIi8VO2M4MqEURE6qU1H6yhcdvGtDuqHanpqRw7Nuhv0PG4jqSkVf72o0GTBlz8wsWc89A5ZcfaHd2Ootwisr/KBmD3pt1smrOJzidWviRCRGJDSQQREalR2cuWkdakCU06dox3KCIiEmPuzuopq+l+WncsJagWGDp2KKkZqXQ7pVuVjx142UDa9mtbdrv90UHVQmkzxUUvLwKHo751VC1FLyLRUBJBRERqTKiggHWTJ9N20CAspU79FyMiIlHYuXInOety6H5697JjzY5oxvcXfJ+Rt4+s1rkyB2YCe3doWDRhER0Gd9gn0SAisVenXuEpiSAiEl/LJ0wgf8sWBo4dG+9QREQkDlZ/EOmH8LUe+xxv06cN6Y3Tq3WujGYZtOrZiq0LtrJz9U7Wf76egWMG1lisInJo6lwSISUFGjaMdyQiIvVPcW4ui594gg4nnkj74cPjHY6IiMTBmilraHpEU9oc2aZGztfu6HZsmb+FRS8tArSUQSQR1LkkQtOmoGatIiKxt+SZZyjMzmbwj38c71BERCQOSvsh9Di9R43tntDu6HZsX7adec/No/MJnWnZvWWNnFdEDl2dTCKIiEhsFWzfzpfPPEPXc86h9UCVmoqI1Ee5m3PJ25JHp+M71dg52w9qj4edbUu2cdQYVSGIJAIlEURE5LAtefppSoqKGHTLLfEORURE4mTnqp0AtO7dusbOWbpDAwYDLh1QY+cVkUOnJIKIiBy27QsW0HbwYJr36HHwwSIikvQ2zdlEcX7xPsd2rowkEXrVXBKhde/WpDVMo/tp3Wl2RLMaO6+IHDolEURE5LAV5eSQ0bJlvMMQEZEY2L58O+OHjmf2+Nn7HN+xcgcYtOjWosaulZKWwkXPXcQ5D51TY+cUkcOTFu8AalJuLrRvH+8oRETqn6KcHBo0bx7vMEREJAYWvLAAHLbM37LP8exV2bTo0oK0jJp9izHwUvXaEUkkqkQQEZHDVrRrl5IIIiL1gLuz4PkFAGxbsm2f+3as3EGrXq3iEZaIxJCSCCIicljCxcWE8vOVRBARqQc2ztrIjuU7aNS6EduWbMPdy+7buXInrXoqiSBS1ymJICIih6UoJwdASQQRkXpgwQsLSG2QyvE/Op6C7ALytuYBUJRbRN7WPFUiiNQDdSaJ4K4kgohIPCiJICJSP4RLwiyasIg+X+9D5xGdgb1LGkq3d1QlgkjdV2eSCIWFUFKiJIKISKyVJRFa1Fw3bhERSTxrpqwhd3MuR19xNG37twUga0kWsDeJUJPbO4pIYooqiWBmo8xsqZmtMLPbDzDmMjNbbGaLzOyFmg3z4HJzg+9KIoiIxJYqEURE6ocFzy8go3kGfc/vS/POzUlvks62L4NKhB0rdwBoOYNIPXDQ/VfMLBUYB5wFrAdmmtlEd19cbkwf4A7gJHffaWbtaivgA1ESQUQkPop27QIgvVmzOEciIiK1JVQYYvFrixlwyQDSGgZvIdr2a7t3OcPKnTRs2ZBGrRrFM0wRiYFoKhGGAyvcfZW7FwETgNEVxnwXGOfuOwHcfWvNhnlwSiKIiMSHKhFEROq+bV9uo2h3Eb3O6VV2LLN/5j49EVSFIFI/RJNE6ASsK3d7feRYeX2Bvmb2iZl9bmajKjuRmY01s1lmNisrK+vQIj4AJRFEROJDSQQRkbqvdNlC5oDMsmNt+7clZ30OhbsLtb2jSD1SU40V04A+wGnA5cATZtay4iB3H+/uw9x9WGZmZsW7D4uSCCIi8VGUk0Nqo0akNmgQ71BERKSWbFuyDUsx2vRpU3asrLni4iyy12SrEkGknogmibAB6FLudufIsfLWAxPdvdjdVwPLCJIKMaMkgohIfBTl5KgKQUSkjtv25TZa9mhZ1g8Bgp4IAKveW0U4FNbODCL1RDRJhJlAHzPrYWYNgDHAxApj3iCoQsDM2hIsb1hVc2EenJIIIiLxoSSCiEjdt23JtrKkQanWvVuTkpbCsreWAWg5g0g9cdAkgruHgJuBd4AlwMvuvsjM7jWzCyPD3gG2m9liYApwm7tvr62gK6MkgohIfCiJICJSt4VLwmxftn2/JEJqeiqte7dmw4ygSFnLGUTqh4Nu8Qjg7pOASRWO3VXuZwd+EvmKCyURRETioygnhyYdO8Y7DBERqSW7vtpFqCBU1gOhvLb92rLty22kpKfQvLMSyiL1QU01Voy70iRC48bxjUNEpL5RJYKISN1WujNDxUoE2NtcsWX3lqSk1pm3FiJShTrzLz03N0ggpKbGOxIRkfpFSQQRkbotmiSCmiqK1B91KonQrFm8oxAROTxmNsrMlprZCjO7vZL7u5rZFDObY2bzzey8cvfdEXncUjM7JxbxhkMhQnl5SiKIiNRhWUuyaJzZmMZt9i/5LU0stOzZMsZRiUi8RNUTIRnk5qofgogkNzNLBcYBZxFsnTvTzCa6++Jyw35J0OD2MTMbQNCvpnvk5zHAQKAj8L6Z9XX3ktqMuWj3bgAatGhRm5cREZE42v7ldjL7Z1Z6X2b/TDJaZNBxmHrjiNQXdaoSQUkEEUlyw4EV7r7K3YuACcDoCmMcKP3YvwWwMfLzaGCCuxe6+2pgReR8taooJwdAlQgiInVY1pIs2vRrU+l9DZo24Na1tzL42sGxDUpE4kZJBBGRxNEJWFfu9vrIsfLuAa4ys/UEVQi3VOOxAJjZWDObZWazsrKyDivgol27AGig9WQiInXSnm17yN+ef8BKBICM5hlYisUwKhGJJyURRESSy+XAM+7eGTgP+KeZVWsud/fx7j7M3YdlZh74RWE0VIkgIlK3ZS0Jks2VNVUUkfpJSQQRkcSxAehS7nbnyLHyvgO8DODunwENgbZRPrbGKYkgIlK3le3M0F9JBBEJKIkgIpI4ZgJ9zKyHmTUgaJQ4scKYtcAZAGbWnyCJkBUZN8bMMsysB9AHmFHbAReXJhHUWFFEpE7atmQb6Y3TadFF87yIBLQ7g4hIgnD3kJndDLwDpAJPufsiM7sXmOXuE4GfAk+Y2a0ETRavc3cHFpnZy8BiIATcVNs7M4AqEURE6rptX26jzZFt1PNARMooiSAikkDcfRJBw8Tyx+4q9/Ni4KQDPPZ+4P5aDbCCopwcUjMySM3IiOVlRUQkRrYt2UbnEzrHOwwRSSB1YjlDcTEUFiqJICISa0U5OapCEBGpo4rzi8n+Klv9EERkH3UiiZCXF3xXEkFEJLaKdu1SEkFEpI7KWpwFTpXbO4pI/VMnkgi5ucF3JRFERGJLlQgiInXXlvlbAGh/TPs4RyIiiURJBBEROWRFOTmkK4kgIlInbZm/hfTG6bTq2SreoYhIAlESQUREDlnR7t3a3lFEpI7aOn8r7Y5qR0pqnXjLICI1pE7MCEoiiIjEh5YziIjUTe7O5nmbaTeoXbxDEZEEoySCiIgcknBJCcW7dyuJICJSB+VuziV/ez7tB6kfgojsS0kEERE5JMW7dwPQoFmzOEciIiI1raypopIIIlKBkggiInJIinJyAFSJICJSB5UlEY5WEkFE9lUnkgiRD8OURBARiSElEURE6q6t87fSvHNzGrVuFO9QRCTB1IkkQmklQpMm8Y1DRKQ+KUsiaHcGEZE6Z8v8LVrKICKVqjNJhIwMSE+PdyQiIvWHKhFEROqmkqISspZkaWcGEalUnUkiaCmDiEhsKYkgIlI3bVu6jXBxWJUIIlIpJRFEROSQFO3aBSiJICJS12hnBhGpipIIIiJySIpyckhJTye1YcN4hyIiIjVoy/wtpDZIpU3fNvEORUQSkJIIIiJySIpycmjQvDlmFu9QRESkBm2dv5XMAZmkpqfGOxQRSUBKIoiIyCEpysnRzgwiUq+Y2SgzW2pmK8zs9irGXWxmbmbDYhlfTdHODCJSlaiSCAebMM3sOjPLMrO5ka8baj7UA1MSQUQk9opzc0lv1izeYYiIxISZpQLjgHOBAcDlZjagknHNgB8B02MbYc3YsWIHuzfupt3R2plBRCp30CRCtBMm8JK7D458/aOG46ySkggiIrEXLi4mtUGDeIchIhIrw4EV7r7K3YuACcDoSsbdB/weKIhlcDWhpLiEf1/9bzKaZzDg0spe7ouIRFeJEO2EGTdKIoiIxF64uBhL1XpZEak3OgHryt1eHzlWxsyOBbq4+9tVncjMxprZLDOblZWVVfORViF/Zz7hknCl931494es/3w9548/n5bdWsY0LhFJHmlRjKlswjy+knEXm9kpwDLgVndfV8mYWqEkgohI7HlJCSlKIoiIAGBmKcCDwHUHG+vu44HxAMOGDfPajWyvzfM2M37oeNIy0ugwuANHDD2CLid1ofup3dm6aCvTHpjGkBuGcNS3jopVSCKShKJJIkTjTeBFdy80sxuBZ4HTKw4ys7HAWICuXbvWyIVLSmDPHiURRERiLVxSgqXV1H8jIiIJbwPQpdztzpFjpZoBRwEfRnat6QBMNLML3X1WzKKswmd//oz0RukMvn4wm7/YzJyn5jDjrzMASElPoW2/tpz7l3PjHKWIJLpoXv0dbMLE3beXu/kP4A+Vnag2sq579gTflUQQEYktD4VIURJBROqPmUAfM+tB8Fp4DHBF6Z3uvgtoW3rbzD4EfpYoCYTdG3ez8MWFDPvBsLJEQTgUZtMXm1gzdQ2bv9jMKb86hfTG6XGOVEQSXTSv/qqcMAHM7Ah33xS5eSGwpEajrEJubvBdSQQRkdjykhL1RBCResPdQ2Z2M/AOkAo85e6LzOxeYJa7T4xvhFWbMW4G4ZIwI340ouxYSloKnYZ3otPwTlU8UkRkXwdNIkQ5Yf7QzC4EQsAOolgLVlOURBARiY9wKKSeCCJSr7j7JGBShWN3HWDsabGIKRrFe4qZ/ffZ9LuoH616top3OCKS5KKqQz3YhOnudwB31Gxo0VESQUQkPtQTQUQkOcx7bh75O/IZceuIgw8WETmIaLZ4TGj5+cH3Ro3iG4eISH2jnggiIonPw87nD39Ox2Ed6TqyZhqbi0j9lvRJhMLC4HtGRnzjEBGpb9QTQUQk8a2fvp7tS7cz/IfDiewaISJyWJI+iVBUFHxv0CC+cYiI1DfhkhL1RBARSXBrP14LQO9RveMciYjUFUmfRFAlgohIfIRDIfVEEBFJcGunraXNkW1oktkk3qGISB2R9EkEVSKIiMSHh0JaziAiksA87Kz7ZJ16IYhIjUr6JIIqEURE4sNLStRYUUQkgW37chv5O/KVRBCRGpX0SQRVIoiIxEdYSQQRkYS2dlrQD0FJBBGpSUmfRFAlgohI7Lm7ljOIiCS4tdPW0qR9E1r1ahXvUESkDkn6JIIqEUREYs9LSgCURBARSWBrp62l68iu2tpRRGpU0icRVIkgIhJ7pUkELWcQEUlMORtyyF6draUMIlLjkj6JoEoEEZHYC4dCgCoRREQS1bpP1gHqhyAiNS/pkwillQjp6fGNQ0SkJpjZKDNbamYrzOz2Su5/yMzmRr6WmVl2uftKyt03sTbjVCWCiEhiWzttLelN0ukwuEO8QxGROibpX/0VFQVVCFrqJSLJzsxSgXHAWcB6YKaZTXT3xaVj3P3WcuNvAYaUO0W+uw+ORaxh9UQQEUloa6etpfPxnUlJS/rPDEUkwST9rFJYqH4IIlJnDAdWuPsqdy8CJgCjqxh/OfBiTCKrwCPLGVSJICKSeApzCtkybwtdRnaJdygiUgclfRKhtBJBRKQO6ASsK3d7feTYfsysG9AD+KDc4YZmNsvMPjeziw50ETMbGxk3Kysr65ACVU8EEZHEte3LbXjY6Ti0Y7xDEZE6KOmTCKpEEJF6agzwqruXlDvWzd2HAVcAD5tZr8oe6O7j3X2Yuw/LzMw8pIuX9URQEkFEJOEU5QWdxzNa6EWyiNS8pE8iqBJBROqQDUD52tPOkWOVGUOFpQzuviHyfRXwIfv2S6hRZT0R1NVWRCThFO8pBiC9keZoEal5dSKJoEoEEakjZgJ9zKyHmTUgSBTst8uCmfUDWgGflTvWyswyIj+3BU4CFld8bE0p64mgSgQRkYQTyg/m6PTGSiKISM1L+o5YhYWqRBCRusHdQ2Z2M/AOkAo85e6LzOxeYJa7lyYUxgAT3N3LPbw/8LiZhQkSxA+U39WhxmPV7gwiIgmrtBIhrVHSv9QXkQSU9DOLKhFEpC5x90nApArH7qpw+55KHvcpcHStBldOWLsziIgkrLLlDKpEEJFakPTLGVSJICISe9qdQUQkcRXnqyeCiNSepE8iqBJBRCT2ynZnUCWCiEjCUSWCiNSmpE8iqBJBRCT21BNBRCRxhfJDWKqRkp70L/VFJAEl/cyiSgQRkdhTTwQRkcRVvKeY9EbpmFm8QxGROijpkwiqRBARiT31RBARSVzF+cVayiAitSbpkwiqRBARiT0tZxARSVyhPSFt7ygitSbpkwiqRBARib2yxorp+qRLRCTRqBJBRGpT0icRVIkgIhJ7ZT0RVIkgIpJwSnsiiIjUhqRPIqgSQUQk9rScQUQkcRXvUSWCiNSeqJIIZjbKzJaa2Qozu72KcRebmZvZsJoLsWpFRUoiiIjEWlljRe3OICKScEL5ISURRKTWHDSJYGapwDjgXGAAcLmZDahkXDPgR8D0mg6yKoWFWs4gIhJrruUMIiIJq3hPsRorikitiaYSYTiwwt1XuXsRMAEYXcm4+4DfAwU1GF+VSkogHFYlgohIrIVLGyuqEkFEJOGosaKI1KZokgidgHXlbq+PHCtjZscCXdz97apOZGZjzWyWmc3KysqqdrAVFRYG31WJICISW+qJICKSuFSJICK16bAbK5pZCvAg8NODjXX38e4+zN2HZWZmHu6lKSoKvqsSQUQkttQTQUQkcakngojUpmiSCBuALuVud44cK9UMOAr40MzWACOAibForqhKBBGR+FBPBBGRxKUtHkWkNkWTRJgJ9DGzHmbWABgDTCy90913uXtbd+/u7t2Bz4EL3X1WrURcjioRRETiI6zlDCIiCcnDTqhAlQgiUnsOmkRw9xBwM/AOsAR42d0Xmdm9ZnZhbQdYFVUiiIjEh6uxoohIQgoVBJViSiKISG2J6tWfu08CJlU4dtcBxp52+GFFR5UIIiLxoZ4IIiKJqXhPMYAaK4pIrTnsxorxpEoEEZH4KNudISWp/xsREalzivODJIIqEUSktiT1qz9VIoiIxIeXlGBpaZhZvEMREZFySisR1FhRRGpLUicRVIkgIhIf4eJi7cwgIpKAQvnqiSAitSupkwiqRBARiY9wSYl2ZhARSUDqiSAitS2pkwiqRBARiQ8vKdHODCIiCUg9EUSktiV1EkGVCCIi8eGhkHZmEBFJQOqJICK1LamTCKpEEBGJj3BJiXoiiIgkoLIkgioRRKSWJHUSQZUIIiLx4aGQeiKIiCQgNVYUkdqW1EkEVSKIiMRHOLLFo4iIJBY1VhSR2pbUSQRVIoiIxIeHQmqsKCKSgNRYUURqW1InEVSJICISH+qJICKSmNRYUURqW1InEVSJICISH15Sop4IIiIJKJQfIiU9hZS0pH6ZLyIJLKlnl9JKBCURRERiK6wtHkVEElLxnmJVIYhIrUrqJEJREaSmBl8iIhI7HgppOYOISAIq3lOsfggiUquSOolQWKh+CCIi8RAuKVFjRRGRBBTKDymJICK1KqmTCEVFWsogIhIPri0eRUQSUvGeYm3vKCK1KqmTCKpEEBGJDw+F1FhRRCQBFedrOYOI1K6kTiKoEkFEJD7C6okgIpKQ1FhRRGpbUicRVIkgInWNmY0ys6VmtsLMbq/k/ofMbG7ka5mZZZe771ozWx75urY24wxrOYOISEJSTwQRqW1JnURQJYKI1CVmlgqMA84FBgCXm9mA8mPc/VZ3H+zug4G/Aq9HHtsauBs4HhgO3G1mrWorVg+F1FhRROqdKBK93zOzBZFE77SKc3gsqCeCiNS2pE8iqBJBROqQ4cAKd1/l7kXABGB0FeMvB16M/HwO8J6773D3ncB7wKjaCtRLStQTQUTqlWgSvcAL7n50JNH7B+DB2EapnggiUvuSOolQWKhKBBGpUzoB68rdXh85th8z6wb0AD6o7mNrQrikRD0RRKS+OWii191zyt1sAngM4wMiPRGURBCRWpTUtU6qRBCRemwM8Kq7l1T3gWY2FhgL0LVr10O6uLZ4FJF6qLJk7fEVB5nZTcBPgAbA6bEJbS8tZxCR2qZKBBGRxLEB6FLudufIscqMYe9Shmo91t3Hu/swdx+WmZl5SIGGi4tViSAiUgl3H+fuvYBfAL+sbIyZjTWzWWY2Kysrq0avr8aKIlLbkjqJoEoEEaljZgJ9zKyHmTUgSBRMrDjIzPoBrYDPyh1+BzjbzFpFGiqeHTlWK9QTQUTqoeokeiFY7nBRZXfURDK3MuGSMCVFJdriUURqVVInEVSJICJ1ibuHgJsJ3vwvAV5290Vmdq+ZXVhu6Bhggrt7ucfuAO4jSETMBO6NHKsV4ZIS7c4gIvXNQRO9Ztan3M2vA8tjGB+h/BCAKhFEpFYl9StAVSKISF3j7pOASRWO3VXh9j0HeOxTwFO1Flz5a4VC6okgIvWKu4fMrDTRmwo8VZroBWa5+0TgZjM7EygGdgLXxjLG4j3FAOqJICK1KqlnGFUiiIjER1jLGUSkHjpYotfdfxTzoMopzg+SCKpEEJHalNTLGVSJICISHx4KqbGiiEiCKa1EUE8EEalNUSURzGyUmS01sxVmdnsl93/PzBaY2Vwzm2ZmA2o+1P2pEkFEJD7C2uJRRCThlCURVIkgIrXooEkEM0sFxgHnAgOAyytJErzg7ke7+2DgD8CDNR1oZVSJICISe+4eVCIoiSAiklDUWFFEYiGaSoThwAp3X+XuRQTb1YwuP8Ddc8rdbAI4MaBKBBGR2PNwGEA9EUREEowaK4pILEQzw3QC1pW7vR44vuIgM7sJ+AnQADi9shOZ2VhgLEDXrl2rG+s+3KG4WEkEEZFY85ISAFUiiIgkGDVWFJFYqLHGiu4+zt17Ab8AfnmAMePdfZi7D8vMzDys6xUVBd+1nEFEJLbCxcGLVFUiiIgkFjVWFJFYiCaJsAHoUu5258ixA5kAXHQYMUWlNImgSgQRkdgqq0RQEkFEJKGoJ4KIxEI0SYSZQB8z62FmDYAxwMTyA8ysT7mbXweW11yIlSssDL6rEkFEJLbCkSSCdmcQEUks6okgIrFw0BnG3UNmdjPwDpAKPOXui8zsXmCWu08EbjazM4FiYCdwbW0GDapEEBGJFw8Fn3SpJ4KISGJRTwQRiYWoXgG6+yRgUoVjd5X7+Uc1HNdBqRJBRCQ+SpczqCeCiEhiUU8EEYmFGmusGGuqRBARiY9waSWCkggiIgmleE8xqRmpWIrFOxQRqcOSNomgSgQRkfhQTwQRkcQUyg9pKYOI1LqkTSKoEkFEJD7UE0FEJDEV7ynWUgYRqXVJm0RQJYKISHyoJ4KISGJSJYKIxELSJhFUiSAiEh+lyxlUiSAikliK9xRre0cRqXVJm0RQJYKISHyEi4Pu36pEEBFJLMX5xapEEJFal7RJBFUiiIjEh5YziIgkpuI9SiKISO1L2iSCKhFEROLDtZxBRCQhhfJDaqwoIrUuaZMIqkQQEYmPsHZnEBFJSKpEEJFYSNokgioRRETiQ8sZREQSkxorikgsJG0SQZUIIiLxEVYSQUQkIamxoojEQtImEVSJICISH67lDCIiCUmVCCISC0mbRFAlgohIfJT2RFAlgohIYgnlh1SJICK1LmmTCKpEEBGJD+3OICKSeEqKSwiHwtqdQURqXdImEUorEfQaVkQktsp6ImgCFhFJGKH8oEpMlQgiUtuSNolQWBhUIZjFOxIRkfqlrCeCljOIiCSM4j3FgJIIIlL7kjaJUFSkfggiIvGgnggiIomnNImgxooiUtuSNolQWokgIiKxpZ4IIiKJpzhflQgiEhtJm0RQJYKISHyUVSIoiSAikjDKljOosaKI1LKkTSKoEkFEJD7KKhG0nEFEJGEUZBcAkNFCL5BFpHYlbRJBlQgiIvFRmkRQTwQRkcSRtyUPgKYdmsY5EhGp65I2iaBKBBGR+ChdzqCeCCIiiSN3cy4ATdsriSAitStpkwiqRBARiQ/tziAiknhyt+SS1iiNBs30AllEaldSJxFUiSAiEntaziAiknjyNufRtH1TzCzeoYhIHZe0SYTCQlUiiIjEg5eUYGlpeqEqIpJAcrfkqh+CiMRE0iYRVIkgIhIf4VBIOzOIiCSY3M25NGnfJN5hiEg9kLRJBFUiiEhdZGajzGypma0ws9sPMOYyM1tsZovM7IVyx0vMbG7ka2JtxRgOhbSUQUQkweRtyVMlgojERNK21lYlgojUNWaWCowDzgLWAzPNbKK7Ly43pg9wB3CSu+80s3blTpHv7oNrO87S5QwiIpIYwqEweVl5qkQQkZiIqhLhYJ+MmdlPIp+KzTezyWbWreZD3ZcqEUSkDhoOrHD3Ve5eBEwARlcY811gnLvvBHD3rTGOEQ+FtL2jiEgC2bNtDziqRBCRmDhoEqHcJ2PnAgOAy81sQIVhc4Bh7j4IeBX4Q00HWpEqEUSkDuoErCt3e33kWHl9gb5m9omZfW5mo8rd19DMZkWOX1RbQYZLStQTQUQkgeRuzgWgaXslEUSk9kXzUVLZJ2MAZlb6yVhZea27Tyk3/nPgqpoMsjKqRBCReioN6AOcBnQGPjKzo909G+jm7hvMrCfwgZktcPeVFU9gZmOBsQBdu3atdgBeUqKeCCIiCSR3S5BE0HIGEYmFaJYzRPPJWHnfAf57OEFFQ5UIIlIHbQC6lLvdOXKsvPXARHcvdvfVwDKCpALuviHyfRXwITCksou4+3h3H+buwzIzM6sdZFg9EUREEkpZJYKWM4hIDNTo7gxmdhUwDPjjAe4fGym1nZWVlXVY11IlgojUQTOBPmbWw8waAGOAirssvEFQhYCZtSVY3rDKzFqZWUa54ydRrmKsJnlxsZYziIgkkLwteYCWM4hIbESTRIjmkzHM7EzgTuBCdy+s7ESH++lXeapEEJG6xt1DwM3AO8AS4GV3X2Rm95rZhZFh7wDbzWwxMAW4zd23A/2BWWY2L3L8gfK7OtSkcEmJGiuKiCSQ3M25pDdJp0FTfcImIrUvmleBZZ+MESQPxgBXlB9gZkOAx4FRsegUHgpBOKxKBBGpe9x9EjCpwrG7yv3swE8iX+XHfAocHZMY1RNBRCSh5G3JUxWCiMTMQSsRovxk7I9AU+AVM5trZhXLb2tUUVHwXUkEEZHYC4dC6okgIpJAcjfnqh+CiMRMVK8Co/hk7MwajqtKhZHFElrOICISe64tHkVEEkrullza9G0T7zBEpJ6o0caKsaJKBBGR+AmHQlrOICKSQFSJICKxlJRJBFUiiIjEj6uxoohIwigpLiF/ez5N2jeJdygiUk8kZRJBlQgiIvHj6okgIpIw8rZGtndUJYKIxEhSJhFUiSAiEj9h9UQQEUkYeVsiSQTtziAiMZKUSQRVIoiIxI+XlKgSQUQkQeRuzgVUiSAisZOUSQRVIoiIxE+4uFiVCCIiCSJ3S5BEUE8EEYmVpEwi5OcH3xs2jG8cIiL1kZeUaHcGEZEEUVaJoOUMIhIjSZlEyMkJvjdvHt84RETqo7B2ZxARSRh5W/Jo0KwB6Y3T4x2KiNQTSZlE2L07+K4kgohI7KkngohI4sjdnKt+CCISU0mZRFAlgohI/IRDIfVEEBFJEHlb8rSUQURiKimTCKWVCM2axTcOEZH6yEMh9UQQEUkQqkQQkVhLyiRCTg6kpkKjRvGORESk/glrOYOI1FNmNsrMlprZCjO7vZL7f2Jmi81svplNNrNutR1T7uZc7cwgIjGVlEmE3buDKgSzeEciIlL/eCikxooiUu+YWSowDjgXGABcbmYDKgybAwxz90HAq8AfajOmUGGIguwCVSKISEwlZRIhJ0f9EERE4iWsLR5FpH4aDqxw91XuXgRMAEaXH+DuU9x9T+Tm50Dn2gwob0segCoRRCSmlEQQEZFq8ZISNVYUkfqoE7Cu3O31kWMH8h3gv7UZUO6WXAA1VhSRmErKetTS5QwiIhJ74VBIPRFERKpgZlcBw4BTD3D/WGAsQNeuXQ/5Ovnb8wFonNn4kM8hIlJdqkQQEZGouXvQE0GVCCJS/2wAupS73TlybB9mdiZwJ3ChuxdWdiJ3H+/uw9x9WGZm5iEHVJBdAEDDlg0P+RwiItWVlEkEVSKIiMSHh8MAqkQQkfpoJtDHzHqYWQNgDDCx/AAzGwI8TpBA2FrbASmJICLxkJRJBFUiiIjEh5eUAGh3BhGpd9w9BNwMvAMsAV5290Vmdq+ZXRgZ9kegKfCKmc01s4kHOF2NUBJBROIhKV8FKokgIhIfHgoBaHcGEamX3H0SMKnCsbvK/XxmLOMpyC4gtUEqaQ2T8iW9iCSppKtECIchN1fLGURE4iFcWomgJIKISNwVZBfQsGVDzCzeoYhIPZJ0SYS8PHBXJYKISDyESysRtJxBRCTuSpMIIiKxlHRJhN27g++qRBARib3S5QzqiSAiEn9KIohIPCRdEiEnJ/iuSgQRkdgrbayonggiIvGnJIKIxEPSJRFUiSAiEj/qiSAikjgKsgto2EpJBBGJraRLIqgSQUQkflw9EUREEkbBTlUiiEjsKYkgIiJRC2uLRxGRhODuWs4gInGRdEkELWcQEYkf13IGEZGEECoIUVJUoiSCiMRc0iURVIkgIhI/ZT0R0tPjHImISP1WkF0AoCSCiMRcVEkEMxtlZkvNbIWZ3V7J/aeY2RdmFjKzS2o+zL1UiSAiEj+u5QwiIglBSQQRiZeDJhHMLBUYB5wLDAAuN7MBFYatBa4DXqjpACvKyYH0dMjIqO0riYhIRWFt8SgikhCURBCReImmvfZwYIW7rwIwswnAaGBx6QB3XxO5L1wLMe4jJydYymBW21cSEZGKSisRUrQ7g4hIXCmJICLxEs1yhk7AunK310eOVZuZjTWzWWY2Kysr61BOwe7dWsogIhIv2p1BRCQxKIkgIvES08aK7j7e3Ye5+7DMzMxDOkdpJYKIiMRe2e4MqkQQEYkrJRFEJF6iSSJsALqUu905ciwuVIkgIhI/6okgIpIYlEQQkXiJJokwE+hjZj3MrAEwBphYu2EdmCoRRKQuO9huOJExl5nZYjNbZGYvlDt+rZktj3xdWxvxqSeCiEhiKMguIK1hGmkNNR+LSGwdNIng7iHgZuAdYAnwsrsvMrN7zexCADM7zszWA5cCj5vZotoKePduJRFEpG6KZjccM+sD3AGc5O4DgR9HjrcG7gaOJ2iIe7eZtarpGNUTQUQkMRRkF6gKQUTiIqrUpbtPAiZVOHZXuZ9nEixzqHU5OVrOICJ11kF3wwG+C4xz950A7r41cvwc4D133xF57HvAKODFmgywrCeCkggiInFVsFNJBBGJj5g2VqwJWs4gInVYNLvh9AX6mtknZva5mY2qxmMPW1lPhPT0mj61iIhUgyoRRCRekmoRVUkJ5OWpEkFE6rU0oA9wGkEF2EdmdnR1TmBmY4GxAF27dq3Wxct6IqgSQUQkrgqyC2jcpnG8wxCReiipKhFyc4PvqkQQkToqmt1w1gMT3b3Y3VcDywiSClHvpHM42+26dmcQEUkIqkQQkXhJqiTC7t3BdyURRKSOimY3nDcIqhAws7YEyxtWETS/PdvMWkUaKp4dOVajSpczaHcGEZH4KsguIKNlRrzDEJF6KKleBebkBN+1nEFE6iJ3D5lZ6W44qcBTpbvhALPcfSJ7kwWLgRLgNnffDmBm9xEkIgDuLW2yWJPCxcWAKhFEROLJ3VWJICJxk5RJBFUiiEhdFcVuOA78JPJV8bFPAU/VanyqRBARibtQfohwcVhJBBGJi6RczqBKBBGR+FBPBBGR+CvILgBQEkFE4iKpkgiqRBARia9w6e4MqkQQEYkbJRFEJJ6SKomgxooiIvGlSgQRkfgrTSI0atUozpGISH2UVEkENVYUEYmv0koEJRFEROJHlQgiEk9JlURQTwQRkfjykhIsLQ0zi3coIiL1lpIIIhJPSZVEyMmBjAxo0CDekYiI1E/hUIgUVSGIiMRV/s58QEkEEYmPpEsiqB+CiEj8eEmJljKIiMRZaSVCRouMOEciIvVRUiURdu9WEkFEJJ7CkeUMIiISPwXZBaQ1SiMtQ/OxiMReUiURcnLUD0FEJJ68uFjLGURE4qwgu0BLGUQkbpIqiaBKBBGR+AprOYOISNwVZhcqiSAicZNUSQRVIoiIxJeXlJCi5QwiInGlSgQRiaekSyKoEkFEJH7UE0FEJP6URBCReEqqJIKWM4iIxJdri0cRkbhTEkFE4impkghaziAiEl/hUEg9EURE4qwgu4CGrZREEJH4SJokQigE+fmqRBARiSf1RBARiS93VyWCiMRV0iQRdu8OvqsSQUQkfsKhkHoiiIjEUfGeYsKhsJIIIhI3SZdEUCWCiEj8eEmJeiKIiMRRQXYBgJIIIhI3SZNEyMkJviuJICISP15Sop4IIiJxVLBTSQQRia+kSyJoOYOISPyEQyH1RBARiSNVIohIvCXNK8HGjeHss6Fjx3hHIiJSf7UeOBAzi3cYIiL1VnrjdHqd3YvmnVSeKyLxkTRJhMGD4Z134h2FiEj9duxtt8U7BBGReu2IY4/gqneuincYIlKPJc1yBhERERERERGJLyURRERERERERCQqUSURzGyUmS01sxVmdnsl92eY2UuR+6ebWfcaj1RERERERERE4uqgSQQzSwXGAecCA4DLzWxAhWHfAXa6e2/gIeD3NR2oiIiIiIiIiMRXNJUIw4EV7r7K3YuACcDoCmNGA89Gfn4VOMPUvltERERERESkTokmidAJWFfu9vrIsUrHuHsI2AW0qXgiMxtrZrPMbFZWVtahRSwiIiIiIiIicRHTxoruPt7dh7n7sMzMzFheWkREREREREQOUzRJhA1Al3K3O0eOVTrGzNKAFsD2mghQRERERERERBJDNEmEmUAfM+thZg2AMcDECmMmAtdGfr4E+MDdvebCFBEREREREZF4SzvYAHcPmdnNwDtAKvCUuy8ys3uBWe4+EXgS+KeZrQB2ECQaRERERERERKQOOWgSAcDdJwGTKhy7q9zPBcClNRuaiIiIiIiIiCSSmDZWFBEREREREZHkpSSCiIiIiIiIiETF4tX/0MyygK+iGNoW2FbL4RwKxVU9iRhXIsYEiqu6ajqubu5eb/ag1VxcKxIxJlBc1ZWIcSViTFA7cdWbuVjzcK1RXNFLxJhAcVVXzF4Txy2JEC0zm+Xuw+IdR0WKq3oSMa5EjAkUV3Ulalx1TaI+z4kYVyLGBIqruhIxrkSMCRI3rromUZ9nxVU9iRhXIsYEiqu6YhmXljOIiIiIiIiISFSURBARERERERGRqCRDEmF8vAM4AMVVPYkYVyLGBIqruhI1rromUZ/nRIwrEWMCxVVdiRhXIsYEiRtXXZOoz7Piqp5EjCsRYwLFVV0xiyvheyKIiIiIiIiISGJIhkoEEREREREREUkACZ1EMLNRZrbUzFaY2e1xjKOLmU0xs8VmtsjMfhQ5fo+ZbTCzuZGv82Ic1xozWxC59qzIsdZm9p6ZLY98bxXjmI4s93zMNbMcM/txPJ4rM3vKzLaa2cJyxyp9fizwSOTv2nwzOzbGcf3RzL6MXPvfZtYycry7meWXe97+HsOYDvhnZmZ3RJ6rpWZ2Tm3EVEVcL5WLaY2ZzY0cj8lzVd9oHo4qNs3FVceScHNxIs7DVcSluVg0Fx88Ls3DVceScPNwFXHpNXH0ccVvHnb3hPwCUoGVQE+gATAPGBCnWI4Ajo383AxYBgwA7gF+FsfnaA3QtsKxPwC3R36+Hfh9nP8MNwPd4vFcAacAxwILD/b8AOcB/wUMGAFMj3FcZwNpkZ9/Xy6u7uXHxTimSv/MIn/35wEZQI/Iv9PUWMVV4f4/A3fF8rmqT1+ah6OOTXNx1ddPuLk4EefhKuLSXFzPvzQXRxWX5uGqr59w83AVcek1cZRxVbg/pvNwIlciDAdWuPsqdy8CJgCj4xGIu29y9y8iP+8GlgCd4hFLFEYDz0Z+fha4KH6hcAaw0t2/isfF3f0jYEeFwwd6fkYDz3ngc6ClmR0Rq7jc/V13D0Vufg50ro1rVyemKowGJrh7obuvBlYQ/HuNaVxmZsBlwIu1cW0BNA8fDs3FEYk4FyfiPHyguKqgubj+0Fx8aDQPRyTiPHyguOI9F2sejk4iJxE6AevK3V5PAkxSZtYdGAJMjxy6OVJu81Ssy6QAB941s9lmNjZyrL27b4r8vBloH+OYyhvDvn+Z4/lclTrQ85NIf9+uJ8gAl+phZnPMbKqZnRzjWCr7M0uU5+pkYIu7Ly93LJ7PVV2UKH/W+0iweRg0Fx+KRJ+LE2keBs3F9V2i/FnvI8HmYs3D1Zfo8zAk1lysebicRE4iJBwzawq8BvzY3XOAx4BewGBgE0EZSSyNdPdjgXOBm8zslPJ3elDPEpftN8ysAXAh8ErkULyfq/3E8/k5EDO7EwgBz0cObQK6uvsQ4CfAC2bWPEbhJNyfWQWXs+9/yPF8riRGEnAeBs3FhyXR5uIEm4chAf/MKtBcXA8l4FysefgwJNo8DAk3Fyfcn1kFMZ+HEzmJsAHoUu5258ixuDCzdILJ8nl3fx3A3be4e4m7h4EnqKXylQNx9w2R71uBf0euv6W05CjyfWssYyrnXOALd98SiTGuz1U5B3p+4v73zcyuA84HroxM5kTKo7ZHfp5NsNaqbyziqeLPLBGeqzTgm8BLpcfi+VzVYXH/sy4vEefhSAyai6svIefiRJuHI9fUXCxx/7MuLxHnYs3DhyQh5+FIPNeRQHOx5uH9JXISYSbQx8x6RDJ4Y4CJ8Qgkss7kSWCJuz9Y7nj59UHfABZWfGwtxtTEzJqV/kzQhGQhwXN0bWTYtcB/YhVTBftkxOL5XFVwoOdnInCNBUYAu8qVeNU6MxsF/By40N33lDueaWapkZ97An2AVTGK6UB/ZhOBMWaWYWY9IjHNiEVM5ZwJfOnu60sPxPO5qsM0Dx88Ls3Fhybh5uJEnIcj19RcLJqLq45J8/ChSbh5GBJzLtY8XAmv5Q6Xh/NF0B10GUH25M44xjGSoMRnPjA38nUe8E9gQeT4ROCIGMbUk6Ab6DxgUenzA7QBJgPLgfeB1nF4vpoA24EW5Y7F/LkimLA3AcUEa5S+c6Dnh6AD7bjI37UFwLAYx7WCYE1V6d+vv0fGXhz5850LfAFcEMOYDvhnBtwZea6WAufG8rmKHH8G+F6FsTF5rurbl+bhg8alufjgcSTcXJyI83AVcWku1pfm4qpj0jx88DgSbh6uIi69Jo4yrsjxuMzDFrmQiIiIiIiIiEiVEnk5g4iIiIiIiIgkECURRERERERERCQqSiKIiIiIiIiISFSURBARERERERGRqCiJICIiIiIiIiJRURJB6gUzO83M3op3HCIi9ZnmYhGR+NI8LDVBSQQRERERERERiYqSCJJQzOwqM5thZnPN7HEzSzWzXDN7yMwWmdlkM8uMjB1sZp+b2Xwz+7eZtYoc721m75vZPDP7wsx6RU7f1MxeNbMvzex5M7O4/aIiIglMc7GISHxpHpZEpiSCJAwz6w98CzjJ3QcDJcCVQBNglrsPBKYCd0ce8hzwC3cfBCwod/x5YJy7HwOcCGyKHB8C/BgYAPQETqrlX0lEJOloLhYRiS/Nw5Lo0uIdgEg5ZwBDgZmRhGgjYCsQBl6KjPkX8LqZtQBauvvUyPFngVfMrBnQyd3/DeDuBQCR881w9/WR23OB7sC0Wv+tRESSi+ZiEZH40jwsCU1JBEkkBjzr7nfsc9DsVxXG+SGev7DczyXo77+ISGU0F4uIxJfmYUloWs4giWQycImZtQMws9Zm1o3g7+klkTFXANPcfRew08xOjhy/Gpjq7ruB9WZ2UeQcGWbWOJa/hIhIktNcLCISX5qHJaEp6yQJw90Xm9kvgXfNLAUoBm4C8oDhkfu2EqwRA7gW+HtkQlwFfDty/GrgcTO7N3KOS2P4a4iIJDXNxSIi8aV5WBKduR9qFYxIbJhZrrs3jXccIiL1meZiEZH40jwsiULLGUREREREREQkKqpEEBEREREREZGoqBJBRERERERERKKiJIKIiIiIiIiIREVJBBERERERERGJipIIIiIiIiIiIhIVJRFEREREREREJCpKIoiIiIiIiIhIVP4fthNyiQLq6RUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWMAAAFSCAYAAACXPc1rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9eZBl6XneBz7n7nvem2tlVtbS1dXd1d1Ao4EmdlAgBFIgDVAwRI0kypRojWQq5JFGDlthSR7ZVniksDyhGEkTM2OZQ9mWRh5KpAxREkkRMgmDYDOwNtDofatC7ZmV6827b3nP/JH1fPmcL8+tXqoqa3t/ETcy8y7nfOfcPO95v+ddviAMQxiGYRiGYRiGYRiGYRiGYRi3l8SdHoBhGIZhGIZhGIZhGIZhGMaDgImxhmEYhmEYhmEYhmEYhmEYh4CJsYZhGIZhGIZhGIZhGIZhGIeAibGGYRiGYRiGYRiGYRiGYRiHgImxhmEYhmEYhmEYhmEYhmEYh4CJsYZhGIZhGIZhGIZhGIZhGIeAibEGACAIgpeDIPixOz2O90oQBOeDIPjxOz0OJQiCk0EQhEEQpO70WAzDuLswm3vrMZtrGMYkzObeeszmGoZxI8zu3nrM7t5fmBhrAADCMHwyDMOv3elxPEgEQTAdBMG/DIKgHQTBhSAI/uSdHpNhGIeD2dzDJwiCvxgEwXeDIOgHQfA/3+nxGIZxeJjNPVyCIMgGQfCPrvu3zSAIng+C4Kfu9LgMwzg8zO4ePkEQ/NMgCFaCIGgEQfBGEAR/7k6PyZiMKeqGcef4fwEYAFgA8DSA3wiC4AdhGL58R0dlGIZxf3IVwN8C8DkA+Ts8FsMwjPuZFIBLAD4N4CKAfw/ArwRB8P4wDM/fyYEZhmHcx/y3AP5sGIb9IAjOAPhaEATfD8PwuTs9MOMglhlrAIim4QdB8DeDIPjV65GVZhAELwZB8GgQBH89CIK1IAguBUHwh+SzfyYIglevv/dcEAR/3tv2f349QnM1CII/dz21/vT117JBEPzdIAguBkFwLQiCfxgEwcRJchAE/5Hs65UgCD4kLz8dBMELQRDsBEHwz4MgyF3/TC0Igl8PgmA9CILt678vyza/FgTB/zUIgt+/vt1/FwTB7PXXWArw89fHuBEEwf9FPpsIguCvBUFwNgiCzSAIfiUIgul3cL6LAH4GwH8ZhmErDMNnAfxrAH/q7T5rGMa9j9ncw7W5ABCG4ZfDMPw1AJvv5P2GYdw/mM09XJsbhmE7DMO/GYbh+TAMx2EY/jqAHwJ45u0+axjG/YHZ3Tvi674chmGff15/PPxOPmscPibGGpP4aQD/XwA1AN8H8BXs/b8cBfDfAPgf5L1rAL4AoALgzwD4ezRiQRD8JID/FMCPAzgN4Me8/fwdAI9iLzP09PXt/1dxAwqC4P8A4G8C+NPX9/WHEZ1U/zEAPwngIQBPAfgPrz+fAPA/ATgB4DiALoD/p7f5P3l97PMAMgD+ivf6pwA8BuCzAP6rIAgev/78XwLw72Mv8r8EYBt7Ga9vx6MARmEYviHP/QDAk+/gs4Zh3H+YzY1yq22uYRiGYjY3ym21uUEQLGDvPFj1l2E8uJjdjXJb7G4QBP/vIAg6AF4DsALgN9/pZ41DJgxDe9gDAM4D+PHrv/9NAP+bvPbTAFoAktf/LmMvylKdsK1fA/CXr//+PwL4b+W109c/expAAKAN4GF5/eMAfjhhu1/hdieM/+fk7/8bgH844b1PA9iWv78G4G/I3/8xgN+6/vvJ6+Ndlte/DeBPXP/9VQCfldcWAQyxV57Fz6ZixvCjAFa95/4jAF+70/8L9rCHPW7/w2zu4dpcbzx/C8D/fKf/B+xhD3sc3sNs7h21uWkAvw3gf7jT/wf2sIc9Du9hdveO2t0k9sTevwEgfaf/F+wR/7CescYkrsnvXQAbYRjuyt8AUAJQD/Ya8v/X2ItAJQAUALx4/T1LAL4r27okv89df+9zQRDwuQB7xiOOYwDO3mDMq/J75/q+EQRBAcDfw15Uq3b99XIQBEk5Jv+zpbfZNl8/AeBfBkEwltd3sdcH9ka0sBd9UyoAmm/zOcMw7k/M5t542zdrcw3DMBSzuTfe9i2xuUEQJLCXCTcA8BffyWcMw7hvMbt7423fMl/3+hieDYLg5wD8BQD/j3f6WePwsDYFxk0RBEEWwP8K4O8CWAjDsIq9VHhavxUAy/KRY/L7BvYM75NhGFavP6bCMPQNFbmE99bz5D/DXgnAR8MwrAD4Axz+e9hW3Jh+SsZfDcMwF4bhlbf53BsAUkEQPCLPfQBWvmUYxg0wm/ueba5hGMa7xmzue7e5wZ4S8o+wJyD8TBiGw1swHsMw7nPM7t5SXzcF6xl712JirHGzZABkAawDGF2PYv0hef1XAPyZIAgevx5B+i/5QhiGYwD/H+z1gJkHgCAIjgZB8LkJ+/olAH8lCIJngj1OB0Fw4h2MsYw9o1y/3vz6v36Xx3gj/iGAv81xBEEwFwTBF9/uQ2EYtgF8GcB/EwRBMQiCTwL4IvayBwzDMCZhNvc92Nzr701dX3ghCSAZBEEuCAKrEDIM40aYzX2PNhfAfw/gcQA/HYZh9+3ebBiGcR2zu+/B7gZBMB8EwZ8IgqAUBEHy+jH/LIDfuYVjM24hJsYaN0UYhk0A/2fsGcVt7DWq/tfy+r/FXlr8/w7gLQDfvP4SV/n7q3w+CIIG9npKPTZhX78K4G8D+P9hr5z/1wC8k5UF/z6APPYiZd8E8Fvv8PDeCf8Ae8f774IgaF7f/kff4Wf/4+vjWgPwywD+QhiGlhlrGMZEzObelM39G9hznP8agJ+7/vvfuIVjMwzjPsNs7nuzuddFhD+PvT6Kq0EQtK4//oNbODbDMO5DzO6+Z183xF5LgsvYO29/F8B/Eobhv77hp4w7RhDuNfg1jEPh+iqBLwHIhmE4utPjMQzDuJ8xm2sYhnF4mM01DMM4XMzuGvcqlhlr3HaCIPhSEATZIAhqAP47AP/GDKVhGMbtwWyuYRjG4WE21zAM43Axu2vcD5gYaxwGfx57pfhnsbcS4F+4s8MxDMO4rzGbaxiGcXiYzTUMwzhczO4a9zy3rU1BEAQ/ib1+F0kAvxSG4d+5LTsyDMMwzOYahmEcImZzDcMwDhezu4Zh3E/cFjE2CIIkgDcA/AT2Ggh/B8DPhmH4yi3fmWEYxgOO2VzDMIzDw2yuYRjG4WJ21zCM+43b1abgIwDeCsPwXBiGAwD/DMAXb9O+DMMwHnTM5hqGYRweZnMNwzAOF7O7hmHcV6Ru03aPArgkf18G8NFJby4UCmG1Wr1NQzEMw3hn1Ot1dDqd4E6P4z3wrmwuAOTz+XBqauq2DsowDOPt2NnZQbfbvdfs7ru2ublcLiyXy7d1UIZhGG9Hs9lEr9e712wu8C7tbj6fDyuVym0flGEYxo1oNBoT/dzbJca+LUEQ/AKAXwCAqakp/Lk/9+fu1FAMwzAAAL/0S790p4dwW1G7W6lU8Kf/9J++wyMyDONB55/8k39yp4dw21CbWyqV8KUvfekOj8gwjAedf/kv/+WdHsJtQ21uuVzGz/7sz97hERmG8aDzy7/8yxNfu11tCq4AOCZ/L19/zhGG4S+GYfgjYRj+SKFQuE3DMAzDeCB4W5sLRO1uPp8/tMEZhmHcZ7xrm5vL5Q5tcIZhGPch70pfMD/XMIy7ndslxn4HwCNBEDwUBEEGwJ8A8K9v074MwzAedMzmGoZhHB5mcw3DMA4Xs7uGYdxX3JY2BWEYjoIg+IsAvgIgCeB/DMPw5duxL8MwjAcds7mGYRiHh9lcwzCMw8XsrmEY9xu3rWdsGIa/CeA3b9f2DcMwjH3M5hqGYRweZnMNwzAOF7O7hmHcT9yuNgWGYRiGYRiGYRiGYRiGYRiGYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYhYGKsYRiGYRiGYRiGYRiGYRjGIWBirGEYhmEYhmEYhmEYhmEYxiFgYqxhGIZhGIZhGIZhGIZhGMYh8J7F2CAIjgVB8L8HQfBKEAQvB0Hwl68/Px0Ewf8WBMGb13/Wbt1wDcMwHlzM7hqGYRweZnMNwzAOD7O5hmE8SNxMZuwIwH8WhuETAD4G4P8UBMETAP4agN8Jw/ARAL9z/W/DMAzj5jG7axiGcXiYzTUMwzg8zOYahvHA8J7F2DAMV8Iw/N7135sAXgVwFMAXAfzj62/7xwD+/Zsco2EYhgGzu4ZhGIeJ2VzDMIzDw2yuYRgPErekZ2wQBCcBfBDAtwAshGG4cv2lVQALEz7zC0EQfDcIgu92Op1bMQzDMIwHhpu1u91u93AGahiGcR9wsza31+sdzkANwzDuA8zPNQzjfuemxdggCEoA/lcA/0kYhg19LQzDEEAY97kwDH8xDMMfCcPwRwqFws0OwzAM44HhVtjdfD5/CCM1DMO497kVNjeXyx3CSA3DMO59zM81DONB4KbE2CAI0tgzlP9LGIZfvv70tSAIFq+/vghg7eaGaBiGYRCzu4ZhGIeH2VzDMIzDw2yuYRgPCu9ZjA2CIADwjwC8Gobh/11e+tcAfv767z8P4F+99+EZhmEYxOyuYRjG4WE21zAM4/Awm2sYxoNE6iY++0kAfwrAi0EQPH/9uf8CwN8B8CtBEPxZABcA/LGbGqFhGIZBzO4ahmEcHmZzDcMwDg+zuYZhPDC8ZzE2DMNnAQQTXv7se92uYRiGEY/ZXcMwjMPDbK5hGMbhYTbXMIwHiZtewMswDMMwDMMwDMMwDMMwDMN4e0yMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BEyMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BEyMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BEyMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BEyMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BEyMNQzDMAzDMAzDMAzDMAzDOARMjDUMwzAMwzAMwzAMwzAMwzgETIw1DMMwDMMwDMMwDMMwDMM4BFJ3egDGvUmr1UKr1UIYhgCAIAgQBIH7m8/pT/99o9EI2WwWtVot8h7DMAzjIN1uF+12+8DzaoeVRGI/3kq7OxwOkc1mUa1Wb+tYDcMw7nW63S56vd6B532/Vn9X3zcMQ/T7faTTaVQqFfN1DcMwbkC73Y71c4GoT6v4dnV3dxeZTAZTU1Nmc427HhNjjffEiy++iG9+85sIwxBhGCKdTiMIAuzu7gLYM5iJRALJZBLAnqFMJpNIJBLIZDLY3d3FxsYGHnroIXzpS18yY2kYhvE2nD17Fi+99BLG4zHCMMTu7q77CUTtLn/nIwgCDIdDrK2t4cSJE/jCF74w0bE1DMMwgPPnz+O11147kHCQSqUQBEGsneVrqVQKw+EQFy5cwNLSEj7zmc+Yr2sYhnED3njjDTz//PPub9pM2lf6rWqP1f7u7u5iZ2cHx44dw0/91E+ZzTXuekyMNd4VOzs7uHLlCra2tpyoGoZhxAgCiAgFfvbscDjEeDzGYDBAo9HA2bNnEQQBxuOxe9yI2dlZzM/P3/ZjNQzDuBtoNBpYWVnB5uamc0TDMDxQjQDsO64UbPleViOkUimMRiNcvXoVwF4GwXA4dLZ7EjMzM5idnb3Vh2YYhnHX0Wq1sLa2hkajgUwm45737S2AiEBA3zeRSGA4HGI4HCKTyWA8HmNrawsAMBqNMBgMMBqNbjiGarWK6enpW3hUhmEYdyeNRgOrq6vY2NiI1QFGo9EBMZZ+LjUI+sRhGKLb7eLSpUsA9vzh3d3dt9UXpqenzc81Dh0TY413xaVLl/Brv/ZrmJ+fx9LSEnq9HsbjcaQcS0VVPigEhGGIwWDgjOfW1ha++tWvYnd3F6PRCN1uF8Ph8IZj+OQnP2lirGEYDwxXrlzBb/zGb2B+fh5zc3PO2aRTOhwOEYZhRIDVbFm+J5lMYnZ2FkEQ4Ac/+AH6/T46nQ4ajcbEsjDysY99DD/6oz96G4/SMAzj7mB1dRVf+9rXsLCwgCNHjjj/lXZWhdRUKoVUKnXA7+V75ubmkE6nceHCBXQ6HXQ6HWxtbaHVat1wDE8//bSJsYZhPBCsrKzgK1/5CvL5PPL5fCTwxRZb/B3AxASuRCKBXC6HbreLb33rWxgOhxgMBuj1ehgMBjccw0c+8hETY41Dx8TYB5hWq4WXXnoJlUoFjz/++DtK5U+lUigWi8hkMkgkEi7LCtiLUqXTaQCIOK0qDqhxHY/HSKVSyOfzGA6H6Pf7CILAOb3cDrebTqeRy+UwGAzw4osvYnl5GbVa7VafFsMwjNtGp9PByy+/jEqlgkcfffQd291SqYRcLodUau+2rVUHqVTK2VtCW0wSiYT7DLO3giBAJpNBsVhEOp12r7HEtlwuI5PJoFAoIJPJ4PXXX8fi4iIqlcotPCOGYRi3j263izfffBOlUgkPPfTQO7K56XQa5XIZuVzOtdsC9rOzCO2xZmslEgmMx2P3OW0bk0qlkMlkUKlUkM1mI61mgiBAPp9HJpNBqVRCNpvFpUuXUKvVUCqVbuUpMQzDuG10Oh28+uqrKJfLeOSRR96RzU0mk8jn8ygUCigWiwcqu+jfMvmL+oL/OxBtW8DWXZlMJmLLmUiWTCbd661WCy+88AKOHTtm+oJxaJgY+wDTbrfx7LPP4vjx4zhz5szbGsswDN0EPZvNRnrCqgFkqQAjVjSKfgQrmUwiCAJks1n0+31nDLW9AbO/CoUC8vk8arUams0mnn/+efe3jg/YL9OlqGAYhnG30Gq18M1vfhPLy8vv2EmlMJDP593kH4ALXGlvbn97tKPJZNK1JAD2M2ez2awTYrm9IAiQy+Vw7NgxlEolzM7O4sKFC3jllVeQz+dNjDUM456h3W7j+eefx+LiIk6ePPmObG4mk0G1WkWhUHBBK6KTeNpOiql+woHfx5vbYQBtNBphd3fX+cDVahVTU1NYXFzE+vo6zp8/78TZSZivaxjG3QBtUavVwne+8x0sLy/j9OnT7yrpoFAoOHunayIAiLREHI1GLgg2Go0ilbi0x/wMBVeFgbVsNotsNotisYhWq4WVlRUUi8V3tMit2V3jVmBi7ANGv99HGIYolUo4cuQIfu7nfg6j0QgXL17E1NTUxEhQvV7Hc889h0aj4Z4Lw9CJshQI1Bgyu4oT/Ha7HSkraDQaSCQSSKfTzrCqmJBIJNz2KdJubW2h0+mg3W7jxRdfxMWLF1EulxEEAVZWVpwz2+v10Gw28fjjj+PRRx+9/SfWMAxjAlevXkWv18Pc3Bzy+Ty+8IUvYHd3F2+99dYNe1T1+31cu3YNnU4Hx48fRz6fRy6XA7BnHzVwpRlZg8HA9SvkYjIADrSSYUWCirCsVsjlcs42b29vYzgcIpvNYnV1Fa1WC5VKBePxGK+//joSiQTm5ubQbrexvb2NRx99FKdPnz6082sYhqGwYqtSqWB5eRl/5I/8EYxGI2xsbEQm+3Gf6/V6SCQSOH78uKvK8tdFoLiq7Qloc4F9+0x7zDJb2mZmc6XTaWSzWVSrVZcZlkwmsbm56Xzm1dVVNBoNFItFjMdjXLhwwdncTqeDzc1NnDp1CidOnDick2sYhiGEYYi1tTX0ej1ks1kMBgO8733vQzKZxEsvvYS5uTksLi7GfnZ3dxfdbhfVahWf/exnUS6XUSwWXcIWbW8ymXR2Vj8LIGKDR6MRGo0Ger0e6vU62u02dnZ2Jq5JQx+62WxiNBohmUzihz/8ITY2NpxPTb2iUCi4lgePPPIITp06dXtOqPFAYWLsfQqbVauwCez3FkwkEsjn8/jQhz6Ea9eu4Tvf+Q6SySTK5bLbhq603e12ce7cOSfAplIp50Sy7IqLc+lCMZlMxjmou7u76PV6LoLF3i0UAljWpQ+W5SaTSYxGI7TbbfR6PQyHQ6ysrGBtbc311Dp37hwSiQSOHDmCTqeD9fV11Go1PPTQQwDgjLlFsgzDuB3Q9qndpSNHAbNYLOKJJ57A+vo6XnjhBWQyGczMzERsIBkOh9je3gYA1Go1l8VK55SOJYXUZDKJ4XCIXq+HXq8HYC/zlXaP1Q0MfnEfFA7Y+7BYLDrbHoYhOp2OEw6azSba7TZ2d3exu7uLN998E0EQYDAYYGdnB5cvX0atVsPJkyfdcZjdNQzjdqCZU7SLfB7Yy3DNZrMu0/SFF15AIpGIlMH69on9tWdmZpwfTPFVW8PQT+UY+v0+BoOBSypg1it9436/HxkbEw2YEZZMJpHNZjEcDtFut52NZk/v0WiE0WiEH/7wh+4+0Wg0cOnSJVSrVRw9etQdg9lcwzBuB+yH7dvbnZ0dtNttV01w5MgRtNttrKysIJvN4siRI+69/CyF0OFwiGKxiIcffhilUgnFYhGpVMoFvGhrx+Mxut2uE2kZ8OJ2+v0+hsMhtra20Gw2sbq6inq97nxUXxfRIFq32wWwZ9u3trawvb3tAnAbGxsIggDVahX9fh/NZhPVahXLy8tuW2ZzjffKTYuxQRAkAXwXwJUwDL8QBMFDAP4ZgBkAzwH4U2EY3rhjsnHLuXr1Kn7nd34HtVoNS0tLqFarKBaLzsCtr69jOByi1Wqh1Wphe3sb7XYbq6urzpjwvRQIPv/5zyOfz6NcLrtJOxAtje10Oq4UliVcAJz4ytcGgwHW19dRr9dx4cKFA5mxy8vLqFQqOHLkiCsDY5Rra2sLm5ubLqOLEa98Po8gCJwIMT09jXPnzuHKlSuuDOGTn/zkOyo9MIy7FbO5dy+rq6v43d/9XWe78vk8stksZmZmsLCw4ATLnZ0dDAYD1Go1jEYjXL58GfV6Hb1eD0eOHEEul3MT/GKxCADOMVVBgBNyzcBiCW06nXYCArDfQzafz7vxclVvQmeyUqm4/XG7mUzGfZZ2OpFI4PTp0y4bIZvN4tixY7h8+TJWVlZcn++PfvSj1trAuKcxu3t3sr6+jm9/+9uo1Wo4evSo661dqVSQTqfdAoWtVstlvPZ6Paytrblg1pkzZzA1NQVgPwCmaxfoBNvPrKK9TSQSKJVKTligrzsej11v2HK57ARV2lCuv1AoFJzwEIYhKpWK85Xp69LOLiwsANgTNQqFAk6cOIF6vY5vfOMbyGQyyGQyeOyxx1AoFA7jKzCM24LZ3LuTtbU1PPvss8jn85iennYBL2DPh+z1ek4YBeCy/uv1ukvYOnbsGAqFgqvuqlarzt9st9vodrtOsKUvqusgaPKCisL0OVlttru7i06n4+x/r9dDt9t1ATytWOBr7XYbzWYT3W4XOzs7rpqYdj6XyyGbzeLy5ctYXV11esjHPvYxdx8xjHfDrciM/csAXgXAmdZ/B+DvhWH4z4Ig+IcA/iyA//4W7Me4AWEYupVZWT7KaFCpVHITeBq2TqeDwWCARqOB4XCIdDrtMq3oJNJY7e7uIpPJ4NixY06M9TOzGOFixtRgMIgItsB+TxddrCuZTGJjY8MZQooNlUoF09PTmJmZiYixzEJg9gEdVWZ16TGk02knNFNI2NnZcZlfFsEy7lHM5t4l0OYBe8Gr0WiEer2OMAyRz+fdBJrCACsDONHPZrMYj8fo9/vo9XrodDro9/tuQg4g0pZAsxDoiGp/bRUP1PYC+8It7SQzt7SNAZ1e2njN9E0mky44B8AdR7lcdoE9ignNZhPb29vIZrPI5XJoNBrOSTa7a9yjmN29CwjD0GUwMbu11Wq50lj6nrRbFDKbzSbG47Gr4vIXK1Q7mMvlXHUZsO/fAogsSqsZYpqcAER7GwL7bRPo4/LBygVmyNKmp9Npl7GllWEUBnTspVLJJVaweqLdbrvtm8017lHM5t4FaJvBQqGAfr+Pra0tF+zJ5XLuoYIpF8zK5XLI5/MuMYBrGKgvSiFWM121PQGzYrkNvtdPOuDvhULBfa5UKqFcLrusWSZDsG0Bfdl+v492u+0qcrPZrBOVuX/a3SAI0O120e/3nZ3d2dlBOp12iWGG8U65KTE2CIJlAJ8H8LcB/KfB3n/fHwTwJ6+/5R8D+JswY3nbGQwG+PVf/3WMx2P8/M//PI4fP47Pfe5zaLVaaDQaqNfr2N7eRqPRcD1RWLp15MgRnDp1yq1gqD1dVTQ4evSoy5LyhQGN8lOI0IUMALiFC5hlBcCtHLu2toarV6+iXC6jVCrh5MmTbiEDTf0/duyYO5YrV66gXq87MZcRNgBO8KVhH41G6HQ6Llv4J3/yJyMZYoZxL2A29+5iOBziK1/5CkajEb70pS9hdnYWn/nMZ1x0nZH2wWCAXC6HcrnsbC+hw8ggGh1YPtTW0pbpyrIAXMsAX4DlZzR7SxeOARD5nb1imQXA13U7zPpidgGFCx5np9NBLpdDrVZz4vN3vvMdVCoVfOpTn4ocu2HcC5jdvXsYDof4+te/jjAM8TM/8zM4ceIEfvzHfzzyerPZxPr6OkajEdLpNFKpFCqVisuYpfhK35KBMNpIZqsyIKULZGmLASYY8L18nYEvTVYA9irH2EaGIkKhUEAmk0Eul4vYewa+dJEv2tNWq+XEWSYy5HK5yCJjL730EnK5HJ555pnI84ZxL2A29+5hOBzid37nd9Dv9/HJT34SYRji8ccfd5mmfDAQtLi4iKmpKTzyyCNOiGX2aafTQa/XczaJ68Ew0OQnFXCBLb8iQavB1FcG4PQLQt81m82iUCi4ZDQmj5FEIoFqtera0zAINxwOcf78eTSbTVy5csW1o9HK4SAI8K1vfQuVSgWf/vSnXRKFYbwTbjYz9u8D+M8BsNHoDIB6GIaj639fBnA05nMIguAXAPwCAEvrfo/U63U0Gg03GWb6f71eBwDX1zWTybgoUi6Xw2g0QrFYRKFQwPz8PKrVKqrVqots0cgx4qPRdf7tR300c4CvTYoMqePKjAAKENls1gkCdC79lcJp3EulkotoUXxVsZiRNjXiw+EQnU4Ha2trLuoF7DcBTyQSmJ6eNkNq3K38fbxHmwtE7a6Vjb83Go2Gs7sURROJhHMKmQXLSDqdQPbYVnsG7IuxhHZOe8PSXupiBWpfk8mks9vchvbR0kwsZl7RidS+s7TFzJDVgJrfm5ECMIBIb0QKysy61Z7hzWbT2Wl+hmOcnp42oda4W/n7uAW+7qRFo4wbwz7V9NkojnY6HedDatAJ2K8OyGQyLlspm81GMlBpd7W9C4B35MuqXfaf93+qgMv2XRSE9aFZYr6fzPuM9voG4I6J9pgCMXvMtlqtiLCrgbxKpWJCrXG38vdxC2yuroNivHMajQaazSbS6TTG47Fbq6Xdbrts01wu50RJ6gTpdBozMzMolUooFArOF1bfUdsXAvvJBCrEAvs2lrZ5d3c3YnPjntdqLmbVAvs+sj7Ub+X+NBNXExhmZmacPsIki16vF8mwpSi9ubnpjpHHw59c+8EwlPcsxgZB8AUAa2EYPhcEwY+928+HYfiLAH4RAJaWlsK3ebsRww9+8AN84xvfwIkTJ1CtVjE3N4dEIoHnn38eAJzTx34tbFmQy+Vw/PhxFAoFVKtVN4FmewE6a5zcM92fPVxonH3HVx09CgCc+ANwxpaf6fV66Pf77rl0Oo1CoYByuYxCoeCEWRU1hsOhc7KnpqaQy+WwuLjoImG9Xg9bW1vOoHJf3W7XCbGj0Qhf//rXI+UJzDZLJBL4/Oc/b6vSGncdN2tzgajdPXLkiNnd98CLL76Ib3/72y57/6GHHoqU4heLRUxNTbkJt28DtfyVjiIdTnVG2TpGKw+AfaGVn1OhVkVdrtrNbTDbi9ULtKvpdNqNSwNhmhWm4i3HTUGWtpYlY9qnkE5vr9fDeDzG+fPnnR3mCrfsq/i5z30Ox44du51fnWG8a26lrzs3N2c29z3wxhtv4IUXXsDy8rJbqCqVSuH8+fMHyv6ZSVWtVpFOp1EqlVxSAhDN9FeBgHaUtphwQu5XeqlwSpvsL+BCG0m/mhVjtLtMQKA99UttdUxMRBiPx+7nzMyM8435aDabbn2FwWCAs2fPut7gvCdQhPj4xz/uFtUxjLuFW2lzFxYWzOa+B1555RU899xzmJubQ6FQcBVTbE8wNzeHYrGISqWCUqnkEqRYeUW/jxVi/qLiGqRnEpWfPKCLIwJwlV/a0kDFU+6byWTpdBrlctklQly7dg31ej3Sg1tb0dDOslqC20+lUjhx4gQSiQQee+wxdLtdtFotJ8hub2+7JK9+v49vfOMbbnwcDzWGz372szh+/Pid+VKNu5abyYz9JIA/HATBvwcgh72eLv8AQDUIgtT16NUygCs3P0wDAFqtFs6dO4dKpYLFxUXk83ksLCy4bIFyuYxsNovp6WnnfLJnn2aVUjCIi4hrlpROvPk7U/Y1c4AOIyf9FE0ZLePnNHrECTh7u+TzeeTzeVQqFVSr1UhGrPbo0tUP+RmKCoVCwfUS83u7UHBgxhaNf7PZdMdEQ7y6ugoAWFxctKwB427CbO4doNPp4Ny5cyiXy5ifn0epVMKJEyfc78yEpSNK0dNvMQBEM1b9zKq48iz/PX6GljqPfj8t/0GnVSf4tMPaK1xFX7Xf6hCrGEyHVUUM2n729mYAj2QyGdeHi8e9s7Pj2hvEtVswjDuE2d1Dptvt4vLlyyiXy1hYWEC5XMbS0hIqlYorNWXgR7NK1R7Sh6ToOalSC8CBDFdNGgAOLtpFe0jbpzZYs2t926qf1/uD/7q23FK7rp8H4I5T+yaWSiX0+31ks1nXi3wwGKDVajmBgD4911TgoryGcZdgNveQ6XQ6uHDhAnK5HKrVqltwkLb1yJEjKBaLmJ2dRTabRbFYdPpCNpt1/h/n1wxC0U7yOdo8zfCnnfQDXupHMxHMD3bxPXwft0U7p5oEg1BsmcBxqQ2mqEtdgWNW/1fvN9rWhtVybHuztrbmsmWBPXu9vr4OADhy5IjpC4bjPd99wzD86wD+OgBcj1z9lTAM/4MgCH4VwB/F3oqHPw/gX938MA0A2Nrawm/+5m/i4Ycfxmc+8xlMT0/jAx/4gIv01Go116eFGaaMCNHIMTrDn8yciivpp3FTUZWp/fwMHV4tfWJEjCULhEaQKypy4bBiseiypebm5rCwsOCMG+H4aSB7vR7m5uacAML3ttttpNNp9Pt9dLvdA4ICo11cYZHnodfrubYOb7zxBq5cuYLPfvazZiyNuwazuXeGra0t/PZv/zaOHz+OT37yk1hYWMDCwoJzyigIcIECdeLUbtEBjQsu0SnkxNsvtQVwwGH1A2KpVCqyT2B/kS1tX6DOLcfkB95U0AAQcaK5b2aLJRIJ9Pt9dDodN07eGzgGFWmZQQvsO+LA3gq97XYb73//+00YMO4azO4ePjs7O3j22Wdx8uRJzM7Ouh6ELAllMgErEJgpqmgCAYADYqhfDqvv5ba0jYu/Ddpav2xWF6Chr8n3MHMrrg2CL/5q1RbHRrup+8xms6jVaq6qgT766uoqut0u6vU6Op1OpF847wHM6Hr00UfN5hp3DWZzD5/t7W189atfxdzcHM6cOYNkMoljx465NgRnzpzBzMwMlpaWIn6r+nCdTicShKK91hYEDHTRRjIj1k860JYu/J3tEtkCi4IrX0+lUm4tAy6sSJ2DD+oHg8Egsqgif/rVXczqZfIAqxu0LRgXBF9eXkY+n8d4PMb6+jq+853vYHNz07V7SKfTOHfuHFZWVlzSmWEAN98zNo6/CuCfBUHwtwB8H8A/ug37eCBJp9OYnp5GGIY4d+6cMyCMsCwsLKBYLKJUKjmRVFsQ6GReM2ABRAwLX6ch0zR+/uTEXBfIosHSBWa0PwuNHyfqavAKhQKmp6ddpitLfAmPgyvLMkPCz3qguAvAtUDQPlq8OfCGQDE3lUq5Ei6O9dvf/jZqtRqefvppc1SNuxmzubeRXC6Hhx9+2GULULDkiqss6WdQSEul/Ek/0ZW3VQBgoAtARBT1P6PVCQyu+fZdBVCW7tIhBBBxYOmE8qGCrQrH2kuWtpKZsplMxpXC0tbq/YHjBOAyiSnk8nh6vR7eeOMNt4ij2V3jLsbs7m2CC8aWSiW3BkIymXQLIObzeWd7NcOU9jkuCzauzUBcYAtAJPDkb8u37SrWUnRgBhZ950mTbgbb/HY1mqWl+FVoXMRL70t8D1f6phhMn5nrRjBTbDwe480330SxWMTx48fN5hp3M2ZzbxOpVAq1Wg3FYhGpVApHjhxxAZ50Oo3Z2VlX0cR5Ne0fbaSKrpqhChwMgBG/AkwTEvyqMtpRvkd95H6/H9Ef+Dx9S46N/rBv+wl1Bvqu7M1Nu8hqr1KphHK5HBkjEzMSiQTm5ubw9NNPo9lsYmtry/U9bzab2N3dxauvvopKpYJHHnnEbK5xa8TYMAy/BuBr138/B+Ajt2K7xj6MZtdqNYzHY1y4cAEzMzOoVqtuES4aS2Zo0aDRaaORAuKzBNSI+mVa2o9QDSCNLZ1PTsSB6OrbFDppRNmeIAxDdLtdVxrBlgoUSDk+HgfFWPao8csWmKFG0ZdjoxCsNxAATrhOpVKo1+suqtZut7G1tYW5uTk8+eSTb1vmZhiHidncwyOXy+HEiROu1QltExe70p5/FDeBqF1V2wsgMsnWqgPtq63lqZr9SjFUM6b8XrTqFHOyz3HqwgKcwNOOUrD1neG4fooqQrCqQO8zmsXrV1xks1knprRaLZep0O/3XUsI66tl3G2Y3T0cuCJ3KpVCs9l0faxLpRKy2awri1X/j/ZOg0g64Z4kBKg/6AfAFN8/5kNX4/aTEGjDeU/QnomaLau+OselIoWOifcHTXJgRloY7vcG51gY4NO+js1mE51Ox61svrGxgVKphOXl5Vvy/RnGrcJs7u2HfuLU1JRLUJqbm8Pi4qJLkuK6Kwz+xCUO0CcFcECI1Z+KLmhL++1XtPJv+q4URDVoxoVhuQ2tzOJ46fNyu3Hj8dsYqE3WsbC9Ihf7ZpYvAKfD1Go19Ho9tFotXLx4EaurqxiNRmg2m3jrrbcwNTWFU6dOmb5g3JbMWOMW0+/38eKLL6LdbiOXy2FhYQGPPvqoMwbsn8WsUt+500gRo0S6cqtGsTQ7Vsuw1BDpgjSchNMQt9ttlyXF/TH6zhIDOqj5fN4Z90wm43rQMDvLF2MZgePx6WqHQNToptNpzM/Po91uo91uOyGYNwfecFjWNRgMsLa25kq6tLn4b//2b+Po0aP44Ac/aAbTMB4QxuP9RQHVJuVyObegAdvAqN1lEMgvzaL9ou0C9kVWPjcajdDv9wHAbVPLtDTwpZmwtG90UoH9kiuOk1llmq2qbQs0G4tih/aHJXov4HlSETmbzbpxav8wAK6/oQrCdGZ5zprNJpLJJK5cuYJisYj5+fnb/2UbhnHHGY/Hrn1UrVZDoVBwPbkZOPIXtwIOZrT6Norb9t/P3/2WWNpaQD+nfjF/Z+aUtvryEwpoR2lz2UNbxQD/d+3L7R+TjkEDdswQ8222bp++N6vTRqMRSqUSgiDA1atXXV9I83UN4/5nMBhgZWUFmUwGX/ziF1GtVrG4uBjJNN3Z2XHiJBOu1MZokhW3Sf/Pt69EfWN9jtvjOi7AvtirawwMh0O0Wq1IEgQfrLLi83ELGNLGavUXdQa2JqB4y33odtPpNMIwRL/fd/coni+uXUOffjweY3l5GUtLS1haWkKj0cDly5exu7uLs2fPolKp4OjRo2ZzH2BMjL0D8OKMi8BrNlMikXCT87W1NQyHQ5RKJczOzuLhhx92mVn8jN8LkNvze1hpdIgOXlwWQZyx9CP03B/Hq5EpPyOMjjSPXVfxZkNwjp3Hog61Oty6qBjR46TzzpuHnmeWGGezWZTLZTdOXYhmOBw6w3r58mUkk0m3HZ4/Ky0wjHsHDUrFoTaUE2dmA1AU5U/aXv5Ne6C2UQNbakfVOfVLtNSGqj1XZ9DP3qJd1wVlNFBFwdjPmNLJuYrEal+5HZ38awYXEF1sRrPAaFe1X60faAPgziMdfGbYtlqtA9kWlkFgGPcOeu0qcXaOaMC8WCyiWq1GPsPfJ2WwkjhBlvjP+5ViwMFFu/wWMH7mKd8TV0Krtprjn5SZFXcc/jZ0/2q/aat1QVq/ugKAmzdQFKav3Gw23TaIL3wbhnH3wuzMG6EVVgBcdeqZM2dcte21a9ewvr6OZrPp5sJ+/1eidpl/awKCP4fXz/uf1dc1uOUnj9HOUVOg/82/aZe1UoHtWbQto1+B5msfastV09A2jvxb5w2azDAejzE1NYVCoeDaLPb7fde2gAkcHI+ug2M8GJiadMiMx2N897vfxdbWFiqVSqR3ahiGqNfr2N7exgc/+EEsLi7i5ZdfRrPZxPLyMsrlMk6dOoVqtYqZmZmI86VGgQ2xtYWAH52igdPJMoCIUWD2Eo0gM2A1A4BGjUaHi7hks1m3Pf5UA6OGdTAYRBYC02NRQ62/q+HljaLZbDphV0sa0um0Oyc850tLS66UgMc0MzODVquFjY0N12+L2b5hGOKrX/2qWxnxwx/+ME6fPn17/kkMw7iljMdjPP/889je3katVotUBqRSKezs7KDRaOD9738/FhcXcfnyZXS7XSe0FotFFAoFTE1NuUCOltprqwFdQVYXDVDhQW0psF8mFoYhstmsEyU1i1YdSH6G2QeMxKtwS7SETF/Xkll/wu8LHbzP6AJkWolA0YUtEdhygMdVLBZdz3OKvLq4jQYQ2WaGmRmvvfaa297DDz9smbKGcQ8wHo/x+uuvo9FouMwh2rREIuF83cceewwLCwvOhpTLZVdmymAXsB/8B/Yn9doygMSJAPoZCrH+3/77gH3RVOHf2v/a96/jhGINdqlPrm0V+D5fgOB+aX+5OrcG3fxkBZ5rrWjj9ihIcJ+sTqCYcPHiRfT7ffR6PRw7dgyzs7Nv820bhnGnGY/H+P73v+/ajmjSEu1Gp9PBBz/4QRw9etQtzvW5z33Oldzv7OzgypUr6Pf7kXVU/MoC2hQt+1ex1LfJflKCrzuoffJtLtsJdrtdDAYDNBoNhGHoWhUy+3U4HKLb7Tp9wl+AURcciwuEcVEwfpb3oWQyGfkcfV5m4nLfvV7PVUrQlvL4OLZyuYxCoYBEIoFer4dGo4HhcIhLly6h2Wyi0Wjg9OnTOHLkyE39Lxj3FibG3gH0ItXn2Pup1Wqh3W67rCwAqFarqNVqmJubQ6FQcP1QaFA4sdYsKZ0sa/aBRn80e4vb0UwmOnh+KSs/QwPM1H8elx+xjzPkRDPGJo1LnW5fxNCVEgE448lsNc3y4g2H4gCzxThGitFsobC7u4tyuYxut4vNzU2MRiPXa6vVarlFfAzDuLvRvlF0qPwWJ7QzdELZgkAFWGbF+hmxnECrDdPegQAiIrBmHGmWEgDnDAIHV/om3Kef5erb+jh77aMTf7+yws9S0IeeM/9+oVkBtLt0QlXcJTxeDczxnsIeXix/swxZw7h38CsAGPCnbS0UCq68njZMF6HV7RC1TZrJ6dtWDTBNElYnjVl/+n4sX4vrM+u3OIgTdfW86PgmZfLqPUX3M0l09lvh6N9+VRkDgcDe6uF+mxmzuYZx9xOGoUsY0rkv/TC+BuwlTDFYXqvVkMlkXBB8Z2fHfcbXKvz1DvwkL3/uPslO+9vQ9oe6PY6Roif9QQ3iq73VCjhfGParGvxxxAXQeAzaeozP+cFAf7ua8MBEDQYZudgXACfiagYuExXM5j4YmIp0yCQSCTzzzDMIwxDVahVBEGAwGKDZbOLq1asui2htbQ3r6+t49NFHXdkAG0LzAtXJtUan2POUF3iv13PiAVefBaLZqTRy/X7fZXwxss5xa4YCDSKN2/b2tpssazSKWak06Fr6qtE1PSY12DrR95tpj8d7fWW2t7edIeMq3TRgvV7PiS6ZTMaJ2RQF6GAPh0Ok02lMTU0hn8+7qBvHSxF2ZWUFV69exfb2Nn7v934PH/7why1rwDDuchKJBD7wgQ9gPB6jUChgPB676DqzY7U1zNTUFMrlMqamplz2KzO7KMwyEKO2SFfR3t3dddlFtHnM2ld75zu03B5/950xfc0viy0Wi84OMpikDmXcZH88HkfEZc1S0Ig/98Xt0rbS4WSFh26X+5qennZCtooFvFf4ZWHAfvuCYrEY6ZG7sbGB6enpiSuUG4Zx56HNZQ9q+nC9Xg/1eh1nzpzB3NwcZmdnUSgUsLq66gLdahf9rFEgmp1Km0T7QLvhC5D86WdoxYmg6p/q+3h/YPDMr0LQRbv8c+EnKKiQqmOIE2210mJ3d9fZUT/zC4ATNfxSV22NoD43sG+rM5kMcrkc8vm8O+bRaIRGo4FyuezmAIZh3H0kEgmcPHkSc3NzzkZwXt9sNjE7O4tisYhHH30US0tLrqKKbflarRa63a6rcFXBUf0xfobiJ1/jmjG+4Kk/1adNJBKRBceHw6HbN7DnAzIxIZPJRBYI137d3LafjUsfnPuidqHibFxlAe07q7x0O/T5WaGrfjODWur/+8lew+HQ3QuTySTm5uaQTqfdvGR3dxdXr15FvV5HsVg0P/cBwcTYQ6DVaqHZbKJarbo+WHSktNwTgJus00hMT0+7z1Ek9SPwk5wx9oXSHiqcOKtj6DunQDQDQf/W6LwaXTrFKmr4Tbd1zL6TzG37v6uB1Yk6M4lV5FUnnMdLw8sFCyikqPFVh5gOKbCfoaXjrVarkayObrfr2hr4EcFqter6nRmGcbh0Oh00m03Xq4kTTJbIUyBgoImRd2bGA3DRa7Yi0N6xFBPjBE4/m5Tv8Sfqvg3i+CZlUum2VSTQ8ehCYnpvUHE0TuTV8ajoofbeX0CM4wUQCdbRieZxU8jW7CoVIfzz5Gcg8D0UgGn7KdzofYhCumEYh0u320W73UalUokscqiBdmbDMiOWAq1OjOnr+dmpRMVLzYxSW6Il+tzejTJhdbv6N7froz29gah9nbQ9PuePe9JYuG+19f7aEL6d530qLqtKbe6kLFx+ntDf1SCc2uvd3V2USiWUSqWJx2IYxu2h3W5jZ2fHVcxyDqv2jskDxWIRpVLJtSWhLeD8lW0AeM37gqqfmQ9EM2XVrt2oEiBOt4jLVuVnuLC3tspSwVcTFPztqc/tZ8r6NlDtrB6zjsu/77CalvZRq+H0mGgrNZCo50T/ZgtFtnvc2dlx1WH8DqampjA1NXXgfBn3JibGHgKXL1/G9773PXziE59wGUIAsL297bIE2J+lUChgdnYWlUoF+Xwey8vLESOrjh8ntiwrojNYKpUik9dWq4VOp+NKFwBEnC01Tv7CLUDUKWSqPbPAfGeOK3Zr+b/v3FEE4Tb5k+PwSwP0+Uwmg93dXXQ6HbTbbQwGA+TzeaRSqUj/18FggFar5YzXwsKCmxioA6yZcRyTRr7CMHQN0VOpFBYWFrCwsIClpSW0Wi28+uqr2NzcxLlz59y+OO5PfOIT+PjHP37T/z+GYbx7rly5gueffx4f+chH8Mgjjzgb1Gg0MB6PXZuSXC7nHCI6opVKxYkGtF8arKG4GDfZpi3TFVxpZ2j7+dm4LCm/R6y2VuC+NCuA42QGGhB1StWxBqJ9vIFo6RbHyUAeJ95attrv952oosE3foYBPwrEiUTCZRH7GVl67vwFItTp5zlNJBLo9/uo1+vo9Xr44Q9/6CpAmMH79NNP4wMf+MBN//8YhvHuWF9fx+uvv46nn34a09PTzn52Oh1nOxKJBIrFomtjcu3aNXS73cg6AESDSnEtAriiNRf9Y6BIV+KeVOpJH5rb4k8NmtEH1HJT3Te37WeA+ckHmpzg+8O+QMrPcHsaEOPnOB9Q3512U9d68NEyWBUK9J5DX1vXm2DWW7/fx6VLlyIL3HY6HTzxxBN4/PHHJ/5fGIZxe7h06RK+973v4dSpU1hYWHDXa7fbddfw3Nwczpw540RZZmhSQGw2m5HFreISo9Q/o/3jXJ/voy2jndLMVQ0eacBdbbVWFuijUqkgm826tRx0kbK4z8QJqL6fq/ZP989j0naPvi4B7FdKpNNpl03MqmQ/81a1DD+xjag/ncvlUC6X3XoQZ8+exdbWFtbX1zEYDNDr9fDMM8/gmWeeuen/H+PuwMTY20C328Vrr72GYrGIU6dOYXZ2Fk8//TRmZ2cjjmm73XYNnLnCnhpLIJqZxOd8583PItXspkwm48r21VCpMVJDw8/S4Koo6/c/9J1cnYD7YgDbCGi6PtFyKl8o1ePVDFXNdvVXMe/3+66Egv0d+T7/PKlT7JfE+Rm53HcikXBtDpaWllAsFtHr9dBsNrGxseGykTc2NvDKK6+4hWweffRRyx4wjNtEr9fDm2++6ezuzMwMnnzySbfYIZ1TtZecwDMApa0ItLxWxVM6TNqbleIuI9x0vtiXiyVOdE65Xc1Q9e2pH6WP+10FjDhh4e2gMKAPtd9x2Qpx7Qx0Xype07Yzm1gdd/9BceadZPRShKnVahgMBs5Bp2jwwx/+EO12G7u7u3jkkUcOtFAwDOPm6ff7OH/+PAqFAk6cOIG5uTlks1ksLCwgm81Geo76E9PhcOgC6r7vSrSs1bcLaj99YdX3cbVEn2jmLD+n21Z84VPfw6CRb891G3p8vr0EDq6b4I9R7XKc/x9X/uvjf1aJa7XA/fkL+larVSfasMy51+thZWUFrVYLo9EIJ0+edNUlhmHcOnq9Hs6ePYtUKuV826NHj6JQKDj/k9dvoVDAkSNHMDs7i4cffjgiwPpBdg2E+0ImAAwGA+dr6jx+NBo5/46+H/fv96b2EwqAqG30M/z9zFOtIiaaPKaVCv48Xu2fJj7Qj41rl6gVwDomJkAAcL3OWa2rlWT6Gd2OnyXr33s0OzmR2G89MT09ja2tLVy8eBFbW1t49dVX3Zzi9OnT5ufew5gYextotVr46le/isXFRRw7dgzLy8v4wAc+gF6vh8FggJ2dHRdJ6fV62N7eRrVaxdzcXGRCrv0HgWj5ka765wurOpnOZrMuczSdTkdKjAiNCx0/Os8qkIZheMBgx4mxmi3A13RxLR6XflaF0jhj6o+RULT2V/im6DIej90q6GpgGY3jDYTHzMgUxRR1vlVY4NhZatftdhEEAdbW1lzj80QigatXr+LatWvY3t7GeDzG/Py8ibGGcZtotVr43d/9XSwtLeHUqVM4evQoTp8+7ZzNZrOJXq+HcrnsHB6dxCcSCdcKhpmczPDXlitxDp0GzFhuRLvM15m5ubu7G1nISh9xWaNql4GoU+m3hHm3qABNe6itXuIcWT0nHKe+lzZS++PSxuuq3Toh8Cf/6rSyrYSKGWwhc/ToUQyHQ+zs7Li+aDs7O1hfX8fKygp6vR7m5+fNSTWM20Cn08G3v/1tHDlyBA899BBOnjyJ2dlZ9Ho99Pt9bG1tod/vRwRK2otut3ugusoXCX3/UoNfwL7oqMF6FRvG47ETB3TfcWKlv1/fDut+dWy0+5MCU35WlC/YxlVWAPvtxngMvlDA96rdjRM//PHGCbI6dmbN8XmKIX7P7zAM0Wq1EAQBut0uzp075zLXZmZmTIw1jNtAu93Gs88+i2KxiKeeegrFYhGPP/64EytZuVQul1Gr1fCxj30MMzMzWFpawvr6OtbW1px9poCoQTPaF7U3+hqDMjofLpVKLmAE7Ntlv92JZrGqGBln16lz6PoH2poQ2LdPmtDlt03Q5Au+zm1oMgLn/7Sp2uaQ2+Ln2UaAOo4uXs7P0l+lXVadhvMBfY+eC60uS6VSeOqppzAe762XcO7cOWxsbODatWu4cuWK67Frfu69jYmxt4FkMomZmRnMzs6iVqshlUqh2Wyi2Wyi2+2i1Wo5wwkAlUoFlUrFlfinUilnfHZ2dtBut1EsFiNNnvXBi5Z/a+8RdUIZzdGoj2YM+Jm2GsHSFH8tCeD7VOhUkUAj+OqQarRIo1LA/kTdH4v+zQgg36/bB/ZuAuwzw/foT4rGatj9MgLCY2BkUA0sV1s/duwYSqWSE9uvXbvm9sdsMN9RNgzj1pFKpVyP7Vwuh/F47EQ6dZaazaYLyPiLvfA9jEz7gSPaNtpS7dVKJyqTyaDX66HT6UQEVjrKAFy2PG2COrG6L45LxU514Ph+PzBGfAFZP8NSLBVLKcjy/cxA1UUQ/e3HCSa6HxVwgf1+38x28LPGNLim54RibjKZRK/Xi2SBsUUNW0IAiCzAaBjGrYe+7szMjFtUr9/vu0QDAJEANxCtdFLbwQloXJUWmSSi6nZ8QYD49kQ/69sxvt8fs5bl8nl/USsVAOL2qVlcQHRBxknj1e36pba6zTjf1ccXfzkGFVnjspA1aYHfSSqVQrVadYkmhULhQNKEYRi3DoqfTDSiXsDkguXlZZRKJSwtLaFcLrt1S65evepajTDbvlgsIpvNotPpRPQAnfP7SV8MoKkOwJ6ztBlq7/VztBt+kpnO+YGDrbO63e5Em6Jj4rbpQwJwoud4PHZ2in5nEASuQkPFWB6D2kVNyBiPx07HoZajPqtWMnN8hMfmay88P6PRKFJ5x7aMQRBgamoKJ0+eRBiGOH/+PC5cuHDDtjTGvYOJsbeBZDLpmitToOt0Omg0Gu4C1guyUCi4ByfELLvkAiUADkSmAbhIlUZXdFErvocOFp9rt9tuAnwjMVYFWEbL/awBjfioEaKhVgPFybZGirQ8QDNcOWYAEcOswolmUOkNIJlMRiLz/rFRcOCY+DtvUv6EQAUJjVhRxJibm0M+n0en08HKygrW1tbcvv2SZsMwbj2JRMIt3pTJZFxfafYU5bXMzHm1w+p88jpn/yggWvLEII+W63NSrllMGpzS7E7a0iAIIouIqYDJ49HMXc3o8gWNSRN53baKANwun/cffB+Pyf+sn5GlorAfXKO91KxXiuOcQOix0Xn2BV2KwX5mw2g0cuI775/M5JokUhuGcfMkEgmUy2WXTMCgda/XcxNotRnqG/rXpfZlVb9L7duNslo145O+pAqK/GxcAGmSjdD9q11WtN9q3Od1H/5DJ+S+3dfzMGlsfE/c+fTnCSoS+6/7SQhx+/WzkSkacHV12uG4DF3DMG4NDD7n83nn9+iiqgsLC5ibm8Pp06ddwkC328XW1pYTD2mX8/l8pEKJPhp9LL/KCzhYHcBt6dxf5+W+jhC3HWC/hZfeLwirK3S7Om/XSl7VEHz9QvuKq83TNRD4fqJVZzxebkczifmT0G/Wc6fHzfHrGDkW1TGAvTlLNpt1c5tyuYx+v4+rV6+6hAnzc+9tTIy9DSSTSVSrVaTTaayurrpFtLhIF0thFU3xpwFkdudwOMTZs2cxHu/1JSyXy5ibm0Mul3Ov8/2cQPsRdM2EpQgwHo9dppYaVRVWOZHW6JOKvhrBUieMji8zR1WUVfFY+whyjGrQ1GD5vWf9CD8X4tHXfIdcez9y+zxevfkkEnuLTDD7dXd3N7KYDd/P46VTevr0aWSzWWxtbR0oyzAH1TBuH8wYCIIA165dc9efLvCktoB2sdvtOmdGHVF/MqvCgrYkoABI0RXYt6eZTMb1NKVt2d3dRavVOlCeTzQLSye+zOb1swZ0Mq5CAe8DfklrnC2isMzMU9pQ9uXO5/MR51OdR/9epkIKHUt+jhmxdKA1M5afoRhLUVdLx3huCoUCwjB0CwLRHmsWA1ea9TPXDMO4NSSTe2sdpFIpXLt2LWIjU6lUpC0WfUniB8j5HAM0ACJtU2gT327iqYKvbld/AjcWOicJkyooq/jLv9VWAThwv9HKMB2vf0w6iffHHrd/fZ8vLmtvRv5UAQZApHLB/040mMjxawuF3d1dpNNp1Go1FItFF7A0DOPWEwSBaz3YbredjXz66adx/PhxlEolV/nJ1lha6aV+EsXa2dlZ5yt3Oh3s7Oy4lgC052x/4FeZ6hxX7TRfI2pvgH1Blf6ejsu/J7TbbQBw41dfnv4zNRT6nMwAVqFUK5J5LjlO1UCA/YChJlRoGwi//Q7fo3aZGoGfsBBXFaJisSZ/sRKMrxWLRRw5cgQf/OAHMTc3h+eee87de417FxNjbwPMdk2lUq5cSxePoQipTpZenDrBZm/YTqfjVhBkz0HuSyeg6oj5USl1suKMJbOX9HPqOOp4+Tsn1jpmABHjzOPQDCjNDqABVfGSx+YbGD+apsbQLyPwV3/0Mxx4LHyP9s4BEOmjqBG5uOgchZdUKoVGo4FKpeJEoN3dXfd986ZIodowjFtDIpFALpdzlQUaaKEjymuRUPxj+anvDKq9UREzCALXtkQDPio4ZDKZSCkUABc8i8ti8jOv/Ox8XyD2j90Xg/lTbZov3HK/FGO1rF8F4VQq5e4/vhirATg9rxqwo+BAB5mv6TZ5n+K5UhFHqxbY51ZtvGYo8yeFZRWxGaw0DOPmoc2l8OpnxWswyPcP+bqP36JKJ8R+Fqi/Pb5ffxLaEt+HjEP3Gbct34fU59WH1vHF+Z9x24t7XccSJ7rq+fR95rh5gB8EU79b9+efbx2fP/cA4BIYaHd5z/IDdoZhvHfor2nS09zcHI4fP34gAUvL8n1flT4eg9u8TtkGSkXFwWDghFk/c9VP9iK+DfeDTr5dUlSzYPspXUdHfXu+X+8dmjSlvmXcOP35vfrgHLMmbPjH61dcUNPga5zrq69NmNTh38vU/9Xzkc1mkc1mMTs7i3Q6jXPnzmFnZ8d91+rn3ijgaNxd2B3yNpBMJlGr1QAA29vbyGQyqNVqrsQSQMQw0GHp9XruQlOnKZ1O48yZMxiNRi7Kde3aNdfaYGZmxvWSBfYvYhUhNetVDZU/ifdFBd8J47bUyKpI4S/EpUIto3XKeDx2htYXhjWCREGU21dnkWPRNgo8B4PBAP1+3wm1aqD4OqOH7XbbieeaFVwul1EqlTA/Px8p41WDqtnD09PTeN/73oc333wTZ8+edZOV119/HQBw5coVzM7O4tOf/rQ5qYZxi0ilUlhYWIiIdePxXs/mTCbj7C1/0g7QxqrYSedNhQT+nc1mIyIi+2X5rUjYKoXl8uxNFQQBisWis5vpdNrZfa2QUHtIZ5pOIG0cbSQrEDTzn8dCB422mu/Rfrd8xJWX8fh9+612m2VgWmGgvbvoxLN0mYsfNBoNd96azaYTa2lfs9ksqtWq+y5Z/QAgcj/jRIILJA4GA1fSdenSJYxGI2xubqJareKZZ56xLALDuAUkk0nMz8+7xUyJVhnRrqjv6E9egX2Rj/6v2hUNzGtPa7WBug2d2Gtwf9J74sYRJ8DqJH7S+7hP3X5cgO1GiRGTtqvbU1utWW/afoaokDEcDtHv9yOL6vKewjmKLubIcdDH1ZY+QbCXpbe9vY16vY6HHnoI09PTSCT2+ki+9tpryOfzeOyxx8zmGsYtgH5OLpfD3NwclpaWcPz4cRQKBWxsbDi7wqCzZpEy+xXYD9Qnk0l0Oh133WcyGSwuLrr5cKvVcpmZfsKS2kAKlX6CEm14nBCrPnZcZj4XGqO95KJjmt1KtM0CfVdfjI0TfOnbKmo7/YAXbWuhUMBgMMBgMHCJW9pqQd9XLpfdeKamppBOp90cgn3Wt7e3nc3Vc6z+/mAwQLvdxtraGvL5PJaWlvDoo48im83i/PnzePPNN7Gzs4Pp6Wl85CMfsWrcewhTgm4jvMhzuZxbgEszRPke/hyNRpHJdBiGTvjL5/NOjGQ/WY1wa/aRTqT9CBDxI/e+wKo/9TV/GzS6/vH4mUp6LGoUdbx+6dYkZzguM4DbAqJitDqOACJGnJGkfr+Pbrfrbj5sZk6BhQJtqVRCNpt1i6DFnRN+5+xdybKt4XDoyqHZtmJtbQ2lUgmVSuXAdgzDeHdQlKRDA0Qz2RmB1kg6f8ZlYin6nF9SpD2wNapOQVGzMZllSmHTF0O1X7VC28/P8jluI06M5Rh80ULFjLj+rnGVFHoeFJ2c097y3Gik3s+UAvb6YHW7XZfNTOhwM5jHPoT8DtSe+98/xQQ9HgoPaneLxSLK5XLs/5FhGO8M2jbaM1+EVF9WbZJm/MRtU7OofJHSt9U32lcck+y7PwbiZ5LFfS4ui9cfs74/ThC+EXFj9rftZ4npuFWMZZmvijQUvMfjsfsuaX/98+GPS8VwfjesDAT2AmNbW1vI5XIuWGYYxnsjCPaqfgqFAqrVqhNmx+O9Bap8+6ut9zQTFYj24KaPRj/SD8poQheZZJN8/2ySfhCHnzCmz/vVWfqaipf6k8er1Ql6fnhu9Dnff/bhudDWjf59SwVptvmifdWqWK5tQ+Fas3s1wU6TRzqdjqvEnZqawszMjGuFycXINjY2kM/nTV+4RzAx9jawu7uLRqOBarXqeohqppZO2jUzltGpfD4fcXD4uXQ6jVKphN3dXczOzqLdbqPb7bq+r8zA0smq/vSNFyPhhBN7HSfFRUaY/AkxJ77cDktw1aHzs80YJeM4+L44oTVOuNXXOW51bikGcBX13d1dtNttt+KkGut+v+/65HCxHz+i1u12MTc3h1QqhVqthpmZGSfA8Calhp7ZcEEQYGZmxvV0oZB78uRJNBoN/PIv/zLe97734XOf+9wt/f8zjAeRRCLh+mVVKhV3fZLt7W2X9a7974Bohj9bwwDRlbk1yNPv913Evt1uu8VL1GklYbjfe5VipWb8ZzIZd4/QIJaOTftOsXcVs355z2CrBT8gVygUIiIsn9fFGfS+BET7gamDydd4DviT2Q/aDgIAms0mBoOBu6cBe9kO7KU+GAwwPz/vAl2j0QivvvoqOp2OO6fD4RBTU1NuVWC9Z2oWLdtM9Ho9JBIJJ8JS7F1cXMT29jb+xb/4F3jiiSfwB//gH7z1/4SG8QARBMGBNRBoVxkY0kxY2lEV+XSy60+KAUQCaxrQ0c/6Wa+KBsri/Et/37TLFAQ4ZiYT+D244yb4cb6ron/Hic3+e9Se+xmx/oOvM2GDx68ZY1y/gj4yvzP6qOx1rvcev7RWvyveY+hnv//970ehUECn08Hly5fxW7/1W3j44Yfx8Y9//MD3YxjGO4fVCDMzM3jqqafQ7XaxuroamUdrb+3hcIhUKuV8RABujkxbRZ+JwXkG2FhVxmQAtYdAdGFrIGrP6RPyeV8cJb7wqNvltrRKgvaZfiD3qf45UV/a7+cKRKsd/HuKf29QPYPnSDUSajis4GLiBAP/PE7OEegTs52WtoHgMTCZQRf6ZcJYMplEqVTC8ePHMT8/j93dXayvr7uKs1/7tV/DmTNn8OlPf/pW/esZtxETY28BYRji2rVr6HQ6bnXCQqGAUqmEYrEYWV3b768ahqG7qOkUjUajSP9C4hsOvxeKZkzRkPgR/knZX/7kXwVGPqfGjK/7gqsuIkY0as6/4zITNLPCzyB4J9+Bigz68DO3+Dv3xYw6GkptXM4FesbjMer1OrLZLKampiJRxjh4w8jn8yiVSmg0Gk4gzmQyKBaLOHbsGIrFIjY2NtDpdNDtdnH06FHLHjCMd8jGxgZ6vR6mp6edkEonSMVYtVV+6c4kOwtEy0tVDODfzDLSKHlc6xFux+/jpNkHKljoZ2ir1QHkT35OWw5w4k1brTbbP2YVbuOyDeKyIPwyMNpT2ls6lQzS6XkvFAqRRSe58vrOzo5zqHk83B9LwbQNjb8Ig1Zf6HmgA89jL5VKeOSRR1wmQbPZRLvdxpEjR8zuGsbbEIYh6vU6BoOBW6SW9ovw2uP7gWh/QuKLmL6gGueD+p/1xVzdb5zvGOf3+q/7QqqO7d1wo2zWGzFJxPWDZSqw+n69Btd8scMfkx4b7Xe323XfrSaP6LnxzyMTOVgRweBfLpdzrSx2dnbQ7XbR6/UwPz/v1r4wDCOeMAydyFar1ZBOpzE9PY1SqYRer+eC2ioUcs5KX0nFSm5Tr1+1Fzo35ns49/fHxdd8e6XJZkD8PD7OB+Xz/tw6LksWOBjMi/Nz9b6i9xJfT4l7HYBLLNNFev1qNm6LVbNq+znv5zEyeY7aA5MZdHuaFDJJYxgMBmi1Wm5s+lkm7lGbYmLCkSNHUCwWY7dn3FlMjL0FhGGI73znO7h48SKWl5dRrVbx5JNPYm5uDrVaLRLR4aQd2DcUjEJzwSe2K1Cj4kdvEom9RcLS6bTLkOVn1EhyYqtCr1++pE4yDbjvfAZB4ATiOMPKfaoYotm0vBkA+71s4rIKNKsqzsnW8egNIAz3V+FlCRYXTaNQoNvSzLhisYhMJuPEBGZzsV0BywguX76MVCrlhB//uPxj4nc0NzfnbqidTgelUglzc3P45Cc/ic3NTbzxxht46623cPnyZfzRP/pH8eijj97o380wDOxda6+88gquXbuGD3/4w5GSLUab2UifC3dpiwB/kq8OqDq2fiCKTi0z7emMskQsboEo2gO2m2F/RXXuCH/3I/RhGLpj820Px6cOMLBf7cDn4ybovv2KE2f1py/AUpRmtnCv13PjrFQqLmMgmUzi2LFjGA6HKBQKWFtbQ6fTwfb2NtbX190YisUistmsc2B7vZ4TfEqlUuQeyvuU9mJnRgeDaixty+VyWFhYwJe+9CWsr6/jzTffxIsvvoizZ8/iD//hP4zTp0+/239Bw3jgeOutt7C5uYlnnnkG1WrVlaTT1gRBEAl808/Siav6t3EiLNEsLPX5/Ek4Ubvm+7AaoIn7W9F9+qKB7itOTNbPx4mxcUKvb5PjxBL+VP9V38tzTD+Yn+fk3xeHdbLPMbTbbRf4KpfLmJqairzOew7HoOdW72EbGxuu0mFqagp/4A/8ATQaDVy8eBEXL17E6uoqfvzHfxwnTpw4cH4Mw9gnDEP84Ac/wNWrV/HhD38YCwsLeOqpp7C7u4srV65Ern/O0zVrlJWo9IV90ZH74Hyd+gJ7ofrXttoo2ny1QypG6nvUdvu+ZVzARyu3CG0s7aEmW3AfvhgbpyOoP+/beH6e9zS2ydLEAv2MbmtmZsYJrjyfrPLS5ALVJJLJJIrFIubn550OwUxkTeLT4w+CAK1WC4PBALOzs65CjhVirMDtdDp4/vnnsba2hnq9js9//vM4derUe/tHNG4rJsbeJOfPn3ci3bFjxzA3N4dyueyEUs3mUeeTk3lOsNn3D9i7WNkPhBexlrASGjlGbGgofOeV5aNxFzV/9x1XdXY1KqYigDpkajDjmmv7RpLb0s/GOdC+U6rPE4oMek40W0qF2lardWBFRd5o+DmWBPDcseF5GIbY2NgAABw/fhwzMzPuc76AAuyLzoVCAcViEZcvX3YiMRdcy+fzOHr0KDKZDI4ePYpr166hXq/jfe97n2UNGMYEOKErl8uYnp52FQgU7bTcCojaC9/BpC2graBwSxvC92rWlGYSqJjrT+x9++W3IYgL6ug2lbhA2KRsBf/9Omb/fXHb80UHvxehbitOeOBnaPdp1+v1OpLJJKampty2tre30Wq1XPCMAre20GHrHxVf1ZHmfY/BPE4IUqkU2u02ADi7nkwmUa1W8fDDDyORSGB2dhbr6+toNBp4/PHHXfDNMIx9rl69ivX1dVQqFczOzqJcLkdaEag4qYEh3wYB0QVS6PfF2SQ/MSDOz4r73KQqgLjXVCjg+ybZQV900G3GBbXeCXHZapoooWNWgUPHR3vIv3UxG45NhRgmVlD00Mwutp7hehe0wxxnnI+ufjQArK6uIp1OOyGY9nx+fh7ZbBaLi4vY2tpCo9HA6dOnzeYaRgwXL17E2toa0uk0Tp48ife9730uGUgD4Wpn/axKin+0x1qpxM/4fUr9rFTgYGUtf6qNjvNbdfuTAl/qS6o9VJ87Dr7mV1HEoWP3tQY/uKf29UZBKF1bxr/3jcf7C9YyQSGTybgWakxYa7VaCMO9ZK1sNut6vHa73QNain/f07ZnrMbmeeQ8plKpuCSylZUVNBoNPProo6Yv3GWYGHuTvPnmm/jWt76Fj3zkIzh27BjK5bJrrp1KpdDv9yO9RekgAXAZO0wzB+CiKsyg4oP9YDUyRIPKqAjRyb5OUDXlnVGeOEdKhQL9W0vRNINVDagKE365qZ+NxogaBWu/fJj7eTs0S0ojSnoedBVIlnUAiBwTvyftQ5NMJlGv1925W1tbcwvAVKtVdzPQno46MWE2V6fTcc21O50OisUihsMhisUiZmZmcPLkSezu7uJXf/VXcfHiRZw4ccKMpWFM4OzZs3juuefwMz/zMzh9+jTa7bbLuNKSHTo86kCxr6o6Vyqyqt3Qhbb8AA8dYbXJfv8/opNm2gbaR36WP+MysIDoJPidiKo3EmPjHOu47flZWHFihDrxHCcDWQww8vxubW2hWCzi+PHjrleuOq5s+wAg0ju93W6j0Wi47yGfz7t7C+2stqUJw9CV6nEyQic3kUigVqvhyJEjWFhYwPb2Nn7lV34FFy9exLFjx0wYMIwYLly4gJdffhlf/OIX8fDDD7uepD5+xlGceMksLZ0Uq/2Iy97ia754ChysslJ7RBurAfi4zDBfUPa3r/eLuP37AvO7RffvC9sqSOj4VYBQQUTtO+0x73OsHKDNVCgctFotpFIp9Ho9d6/igl+s6vDHrL19V1ZWEIb7C3mNRiNks1nMzMzg+PHjSCQS+PVf/3VcuXIFR48eNZtrGDFcuHAB3//+9/GRj3wEJ0+exIc+9CGUSiWcP38+kjxAW0zfCNgXQKlBqJ3QRAAmK/k2WntJc3tAfLm/JoOpzZ8k8k6y1bp/iqz+e4mOIe55ZZJt931g+pQ6Dl/b8F/X/WrC3Wg0cokAADA9PY18Pu+qSaampjAajXDp0iU0Gg2srKzg6NGjWFxcdNW9k+51/MnvPggCpylwzFwvgdW5QRDgu9/9Lra3t7G8vGz6wl2GibE3CUvOp6enMT097VLGKXYyeuU7iwBcPyZ/QkkBlc6Pf/GpcVNjomKnbwy1XBXYi7oEQeBS8LkNNaR+NF7LU33UgfSjZWpQ/UwHjov78QUHFUv94+dzFLJVPPZLK7LZrHMkWbqqAgPFAE7cmanMkq1Op4N6vQ5gz9Ctrq5iPB5HerBoT0ftoZjJZFwWiTZQr9frKJfLrkH6aDRCoVBAoVDAs88+i+npaXzyk5+MZIgZhgHMzs7i4YcfRqlUco3sec3ywUmnLiqodkPtGe2QZtWqzdSHopm42mNP7bzaVDq3vgAB7NtJLQHjWH2b74utuj2131oCq6/r2PyJvuLba92GH7SjbdU2EFoBAuyXxq6trblzX6lU3LnrdDrOoeT3xu+Azmaj0cDu7i7S6bSrMKBN117huqiZv5I4v2sVHtLpNL7xjW+gVqvhox/9qNldwxBmZmZw4sQJN9mPEwvVn1WbqcEm/RlnV3VCr/vh3/7iX35gSif3/r70ffqa//ukibsvJsRNyuO2oWPzx0e0bUJc4M/fryZx6Lb1PHLNA65jocKKnwTBfSeTSTcvYNICt+NnMPvfN8dG28nX+Hn14fP5PPL5PL7//e+jUqng6aefjm3xYxgPKhTSlpeXsbS0hM3NTdTrdXS73UjVLRD1AYfDIZrNJoB9exlX3RSX5c6f+j76WH6LRV/gnZQA8E4SFfx9x72mmoL/ef813+fWsfoiL4kLTjHh4kaZt3yfnptsNovp6Wn3HNejaTQargVXKpXCzMxMREA/d+4cUqmUW7yWz+s51XOpC5356yXw/GurimQyiW9961uYnp7GM888Y37uXYKJse8ROhXZbBbVahXlchmlUglTU1MuG0udUF54ejHzovGdV15sGpnyJ9NxjpdvKHTf2hw6DEO0Wq3Yz8VFmXQ7NPR+ab4vEPiRON/Z5vb4OV9U9g2ovt8Xov2+jiqs+Ma8UCggDPcXS2PLAkaXKO5yOzSgwF6JLQ0cf6eQwCwsjp1C+mg0ciKRijxhGKLdbkcy9/j/lM1m8cYbb5goYBgevP5LpRIWFhbcpJHXFwVYP0OWC4qoHdVIPa9NFV8pstJGqjirDqeKuDpZ9YVOvh+IOs5xmQb6HrV7cYG9uIm77sfPto3LTiBxzmZcFoSfDQEgkm3Fc8Qe6P44m82me08ul3P9BTOZjKtEoJPJ42UvNK78S3vJY6RNpz2lEEv7qf3DdXXefr/vvu+33noL1WoVP/IjP/Lu/jEN4z5Fbe6RI0ci/a71pw8noH4wiD9ViI2zgZOCWv4+g2C/HD9OGPXFWB6TvuYfa1zAy9+2Pn+jc6DbnYR/P/A/q++7kW88Ho+doEmbpvMQwsQDf5vcr/YrpC/rt0LQ/er9gLY6CAJ0u93IMeg46etevHgR5XIZ73//+02MNQzs+6f5fB61Wg0zMzOYnp5Gs9l0109c1SirTVU4VV1B7e87EWMBRALZfnasn8gVp0FQK+Hfk+ypv99J7+HxxAWp+LpuK+6+MUmM9X1j+pJ+xdykMdH35XxA13fg+aOQzgW+a7Wam6Nsbm5ifX0ds7OzyGazkUxdPX+qC2nrCR2jr5fQJ04mkzh37hy2trbw9NNPHzgW485gYux75MqVK3jllVdQKpVw5swZHD9+HNPT065lADNtOMlkhqxG9fP5fKSPq06a6QTphaYXmDpXdGD8TNk4EY+Gyneo46JWuigDUdGTAjO3q5F6Nep+eRdLIpidqtnDesPgvn1xl3/znGgjbO7TX4XQ79XI80AhlSWzU1NTuHr1KtrttpvcZzIZ5PN5t3plp9NBs9lEv9/H/Pw8EomEEwp2dnYi3ykANBoNt3AXDTBLpRkp43msVqvo9/suC9cwjH02Njbwwx/+EFNTUzh58iSKxaKzO6lUyvXqphPECDOvd+2f3el0MB6PXY9uDS6pAKsirGbO0obGOV7Ed2418HYjx05/+hNlv3TM3wePmxFxfk6rJIg/8fdFXW2rQLvKABJ/xtlxPjQbKgj2+nHT7vEex3N74sQJJBIJrK6uotVq4cqVK26RNN4buEAbV4fVoBnHy++MmVfT09Po9XruPsyWFuwhm0gkUK1WnW03DGOfer2Oq1evYnFxER/4wAdclZEfYNdJsD/ppR3RhUx43frE2U/dj/7U13VMKkBqb3DdblymU1xSgwqTvu/tH6OKFdwPj1vHGWfT/XOggjTf6593P9Cm50WFUQ0+7u7uLUpbLBZdlpYflMxkMm6tg62tLbTbbSf00O7T/vr3yyAIUCgUnDhBuwzsLSbG81mr1VwvcMMw9llZWcFrr72GD3zgA/jUpz51wH9tt9vuWg6CvaovVv349pfVQ3xvnI1Tu6YZlfTdmLCkYmVckMff5o1+kkm2juPX+b/aVbWjcWKwf3/ykwhod1WsJGqf4zJrOW79qc/zvOg553FvbW25czsYDFwrl6NHj7qkkvF4bzHFbDaLfD6PVqvljld7AGurR01C0wQvPS7+L3AdHOPuwcTY90iv18P6+joKhQIqlQoKhUKkr6uubhdnsOiIBkHgItRqYPyohjpckyJCim90/SiR9tuLM5JquHwhdBIqpMY1ueZPHhONirYe8KNQNyoXnhSNiztH3KcfCeR3wfdRmOVNj69rlgGAyIJgFIGYCcBj5Lj6/b7LzKOx1XPGDDAVnovFIorF4g3Pt2E8aPT7fezs7Dib62eiqm3gtaoZkolEIjKJ9O2S4jt5vjDL61X7ygLvrq/VJIdWhVg/qs99qAOtgTr927d5vhgbNwb/uJkVpU6p9nzU/ej9ip+jGEFhQG2+wgUOuAgBxdJerxfJfNXvVsu3eCxqr7PZrBPNKbQyW1cX+uL3Z3bXMKIwgJFMJlGpVJzA5l8jvn3yJ6lxWU7vFX9SPul1/u5nW/k+76Sg2KSJvm7r3aB22xch/LHc6PxO8vm5jziRWcUOrRrRSgG+RwXZnZ0dN+FXNFFEM7d8YZb79+/V3E8+n3eBTMMw4NoxlUolnDp1Cuvr6xEBVv0XTY7S+SR9PV3cC4i3jb7eoDqEBqHU3yRqb3Q7/Ft/KpMETt/u0k7pPlRT8AXYuCxWXzdQ/1HvH5N0Fh3jpHufaix8v9pTjoffHQNTXNyLdjCfz7vsWU0Cof8LRCv7dPx6jMRPSksmk64dotncuwcTY98j6XQa5XLZXSQU73K5nMuUAvZE27gJMLC3QIkuOqJ9RovFInK5HLrdLsIwdKvyMcqtYqdefL5woFEZNS5++nucseJxFAqFSAmuRl2azSa63S7W19fR7/ddlF1T4rkdXVhFo0eMoDOqp8aWr1MIpVjKbFUta9KyK/7kDUUX7AqCIGLMed443qNHj2I4HOLy5ctot9totVqRBcASib1FDobDIb7//e8jk8lgaWkJpVIJi4uLLpqlogMzI1hmy4w9jp8Nu8+dO4dr167hC1/4AmZnZyOLJRjGg065XMaJEyeQzWbR7XaRTCZdVJjXeiqVwnA4jCxUQrs8Go3cYnrdbtdlZvH65Of9/nh8TtsfEIqx5EbRed+5o93hvulcaTVF3MPvna2Oqv+3PynmT/++QPHUX2TMv99wHxyf9j73g4fcDyfyuVzOReYHgwH6/b6zxY1GA0EQoFQquaysVquFa9euodlsol6vu/vL0aNHkcvlMBgMkEgkUCqVnFDPHt35fN6JsTx+2tN+v4/NzU10u13s7u7i6tWrWF1dxU/91E+5EjHDMPb81OXlZSQSCWxsbKBYLMZmtE7KUoqzTbRhu7u7kSD3jUTJOB9abZSfsUR8EVFtmxKX3Us/laX6WgmgtjyuVU2cIE3fU/1of1w6lrc7l3wuTmDxRWhNaOBxTU1NoVwuY2try90T+ZlcLuda/+RyOVc5QDF3ZmbGbUtbGWgVGtt/sYc7feN0Oo2rV6/i0qVL+NSnPoVqtWrtuAzjOgsLC/jEJz6Bhx9+GNVqFc1m0/lLwH5FJ7UDzh/VBtNG93q9iB/kC6lAtEKW2/MD62rz1N76yWIqTMYF+VVjSCQSzn+mr875PduN0bZyfOPxGP1+H6PRyAXqVZT0kzM02O4nqPn+NrdLPUePl6jvT3g+/ApjiuWaEFIqlZyGwEW9i8Ui5ufnMRwOXdIIs5E5v0kk9np4axJbp9Nx1YC0w5xT8LxyTpNMJrG1tYVGo4Gf+ImfwMzMjNncuwgTY98ho9EI6+vrCIK9Epzd3V23+BIdG10Yis9PmiTzdzVG3I46av6EeVIkXgVf34HzHTfgoGNLo+FnfulknEaKRmQwGDgxdnNz0wme3Hcmk3Elwn4LgThUmFDhF9g3htqPUFskqOMZl/Xg3xx8MYPnj+ICbw66OiFRg05RttVqOQOqorpGMSkm8wajzj/HwBtuqVRyq38bxoPKaDTC5uYmkskkpqenkU6nMTU1FbFvdMpon7RVitoUXtd0WhnYoq32g1r8SftFB0f7ZQPxIsHbEZfRpY6wloXp/WCSGMvx+BNuX6CYlFGl743LZtB7CsekkX+1r5ql4QvVLKGigEvnkfdPDVxRBCgWi85R5rH7GbAUef3got4XKKQD+4tc0FHl/ovFosvMNYwHkdFohHq9jmQy6RIO8vm8u06ZjaR2g0Kcj++D8r1xmVSTbBM/54uLyo1ssC9O+oKEftbPCgP27bIG1nU7fpBNj2sSvvis54Tj8Mf2brOYOD79XtT31tfT6XSkyoCic7vdjv0M7a5+JzwGDZDquCni8h5FYaFQKJjNNR5oRqMRtra2AOyVkmcyGRw5cgT5fN4JfRT0aAuZHBTXqgrYtx1ct4S+sdouX1vw7Z/+9PGrsoifcMD3+i0HeSwUE9m+jxVSfF0TB5gUlclkXCCelU4qCvtBQe7XF2PV71ffludUk8L0fuCjAUb//PgZvNr+UecwXJg2bvtMkOA49HtiGy+eH44nTvg2P/fuxcTYd0iz2cSXv/xlpFIpPPnkkyiXyzh16pS74Hq9HoIgcP0/q9WqW9yLUSvfeaUAOjU1BQAuO0gjNOwzSnxjq4ZQMwz0/RQwffHPd9Q0OsWLfzweY2trC81mExcvXsTW1hauXLmCnZ0dNBoNJ4JQxFxeXnZNqbk9ZjoxosNejdz3eDxGKpVCuVx2ET+Ondtn+T9FChpllqJS1Ob2+Hk/gqg9y3gO/KgazwF7AK+trUXKBcbjsWuu3el03DHRoWUGAQXZbreLfr/vbjBzc3ORdgXMIo7LNjGMB5l2u41/82/+DaampvD5z38ehUIBMzMz7vqnfbh27RqAPRuWy+XcCqUsOWd2Dm0F+x7S8dPgl+/EaE9prnpK260T+7iSIeIHfvzMfADOTrTbbTcRZoBPA32+U6aZR8Vi0YmYzCzQljm0+epIM7Cl2WoMQqnDyofaUP+YVExVEZv3KW6D9lR7KTILYHNzE+Px2PXSWlxcdJUi9Xrdfec89lQqhUqlEjmfdN5ZDjYcDlEoFJDP550Np2DAdheGYezZoa9+9asolUr4sR/7Meeb0b6xsoe2gb6M+m066eVzGhyjmADsi3g6KVb85+OEzji7+3aCp76HdpUTe5aJ9nq9A2Is7SmFaj64DoBvE4Hoega+H04YJAT2BWAVMfxEDk3A8AUD7lOTFsIwRL/fR6/XiwTJwjBEuVxGNpuNZGutr6/j2rVrrqqEmbIUUgqFgqs00Ul+JpNBo9HAeDx2WWD08VkaeyOh2jAeNNrtNv7tv/23SKVSOHPmDB5//HF8+MMfRjKZdBVDuVzOVfZ0u93I3BtARJAEDi5CNRwOUSqV3DUZBIGrTOL8mHN0FfVInGCpfqGKoXoP0IQG+ufpdNr1/5+bm0OxWHSVoJVKxWkQTHaiXd7Z2cFgMHB2qtvtot1uuwUJ1V6SuECfH0ijn69Vcczq14VhdXu6TU0q4PbV/tMvpi3OZrNOX8jn8+h0Orhw4YKz+6zs0mq84XDoFnDrdDqR7z0IAre+AoVfznV4Lv0F5I27i5tSf4IgqAL4JQDvAxAC+D8CeB3APwdwEsB5AH8sDMPtm9nPnWQ8HuPcuXOo1+uun9zs7CyKxSIqlYq7ePSiDoL93qEADhgH4OAK1XwfnRwt9/HRybF+XsVZNTR+hIjvJWpAaeC0lHRzcxOtVgurq6vY2dlBvV5Hu91Gp9NxWRMsCZ2dnUWhUECtVnMCwWg0QrPZjGRzUajleGkw/OwFOrGpVCrST1bHHlcKoZE/X8DQiYGeR42c0eHUTDh9P7el46RB5H6Zqedn96rowWwBCjF+SZ1h+DwodvfChQvY2dlBrVbD9PS0i5zTqdAWA0EQbT3SbrcjdoaTZQAuE4hiH3Bwgs/rno4jJ6TEd/Y4Zt2WZhhomSsnrnSQ+TvtrS8EMKijwqVmC8RlVE0SQnyhYtKkWAVmfU6zwPxqDj78oJKeWz+rjvcHfxIBwLXmYcUBA2/ctx77YDBAp9OJLKDIdhXcJwVqFYV0UmMYk3hQbO7Vq1fRbDYxNTWFarXqgllqu/xMUAoCtCkUaoGDCwMqDIr4ATCiYmqcEOlvn8fgZ3GpUABEA0i87nkMtMes/NK1A1hez0B8qVRyk1zae+3lyH3EiY/q//N9PH6dwMfdZ9RHjrPfamvjMmu1ekKzWTn/YIaVLyzzp7Zh8885vyvtQcvgJ0WOZrPpAo6GMYkHxeZeunQJ9XrdJQ8sLS2hWCyi2+06u+gvzEV/lotK01dSm8BrW21Nv9+PCJEUY/2KTj9wpXZKn9M5ul8ZofNiFTqpFZTLZeRyOUxPTyOXy7mAEKsw/ACdHgfti9od2jN/HQF/TDx/uj1tzaV+u+8Hx+kNRPc36f7EuT+1AW0DMzU15ZLcqMWw+pp+Lytn+f8A7Nt5zh+4Px4Xk9G63a75uXcxN5uK9w8A/FYYhn80CIIMgAKA/wLA74Rh+HeCIPhrAP4agL96k/u5Y4xGI3z961/H1tYWPvShD2F+fh5nzpxxWY68MBiVoSHpdDpu1WaWnOtkVUVAEgSBExx8YwBEMwT0s37EQ7OD1ND4TqpmM6kxbjab6HQ6WF9fR6fTwcbGBnq9HprNpsv6pNM6OzuLqakpHDlyBIVCAXNzcygUCpiennbZXfV6Hc1m00XH2HOsWCxGBFiOUTPUtHejGiBCw8ltaPo/DSsjXHrcjFL5mcEqumazWQwGg0gbCUaaeJ75PfI5LnTB/x0VV+ngMiOr3++7z25vb2NlZcVW8zbeCQ+E3f393/99NJtNfPrTn8b8/DxqtZq7ztkjSbPRVcBkDzyWxGsGvWaMDgaDSDmUZiNxcRFmBfmOLqG91ZIuv+UAKx0YuaZjREe63W67DAAVlf0gEifSPAa1PzrJVrsK7DucfpmW/tT9aGapL3BQqNa2DzpxV+FF7zH8nd8bxW7eT/gZijR0SrnNTCbjMq80+Mn3bm1t4erVqxHBu1AouHtupVJBLpdzQm0mk8H6+jpWV1fR7XZv8X+wcZ9x39vc3d1dfPe730W73cZHP/pRzM/PY2ZmBsPhEO1229kO+rq0L/1+360JoBmXanOA/VJVYPIiU75ISfy//YxT3YafJeoHi9S20t5q5dR4PHYiha5STV+X9wT6+olEAq1WC1tbW+4epEEvFRI0McMXmXlcflBPbbjvx8aVD+u2eJ50zqD3Cr0XMHPVDzryXsXvkxmyPLcq4PAYtQotkUhEFulaX1/HhQsX0Gw2J47dMPAA2NzRaIRvfOMbqNfrePzxx7G8vIwPfehDGI/HuHbtmrtWWX1A+8FsyVwu56ppOc+lXVY7okJlp9Nx29JAOKuF+HucPqHQpmgFlD9v5/jpb+dyOZcROzU1hWw2i6mpKefbMRNUfWht+QcgUu2kLWM4DvrYnHv7x6D+p1a8+scWBIGrKua9S8+R7lf1GvWnNXOY8PPsv83zUi6XcfbsWZw/fx7NZhPJZNIJ8+VyGQDQaDRc4ha/Jya1sa8w90shNp1OY2dnB1tbW+j1ejf9P2vcHt6zGBsEwRSAPwDgPwSAMAwHAAZBEHwRwI9df9s/BvA13MPGEtgvSZ2fn8f8/HwkzV/LQTX6TvGVF/NgMIgYKxoA38AB0Sb/vuMWly2g0SmOdzweR1LZ1ZDr+2jEWVY/HA7dxU6BQ5tP04GlM1qpVNzK5swi4+JX6pCxL4yOodfrOcHDz7Lys095vlQc4IRAJwb8ruIiedwWjbFvJPVzdDKHwyE6nQ663a5b7MWPHGq0S1f+5s2D4gFXB2eki+UYQRBgZ2cHly9fNlHAuCEPmt1NJBKoVquuTNbvucyJXyKxV/JO+8NSWi33jJvU0qbRPtKuqJPm2wm1T2qP2E5AFwHgQ8u/VIylU6mZsyoaKIlEwjm1tF++MEondjAYuEkzj0WdbAAHHG61jxrQ43u5bbW3vrgARPt6+/cbdaj1XphMJiOVAXTo9R6i3wv3w++/1+u5hRb5vfH88h7NMj9ObDKZDDY3N/Haa6+h1Wrdkv9Z4/7jQbK5AJyvOzU15YRXXfCVNkV9SyB+he1JguEkIVZthh+I8sfo2xNfvIxLPvAFWX34fiWPixVxDASxTQHtJFunMBDoJ1rQR42rKtBzoW3GOGfQscedQ70v8Zh5fOrLqnCi61kwYKn3Bn6/fLAKji16eOycJzBhgXbWF2H4CMO9tmPr6+vm6xo35EGzuel0GsePH8fMzIxr+6GtAnSRJj5HHwlA5HrVeTTfy+tbsz+12kr9TiC+V7XaKQrE6o/7+oaKsqoh6MJSFDcpfHJ8zAD1Mzlpy7LZrAsEqn6hLQW4XX+Oz6QLX1DV5DWtouP7fTuun9X98jz59zQ+9DyNx2O0Wq2IWD07O4vNzU00Gg0cPXoUmUwGnU4HrVYLrVbL3WsIvzMGEJlowCSUZDLpkur8xDTj7uFmMmMfArAO4H8KguADAJ4D8JcBLIRhuHL9PasAFm5uiHcONU6pVAoLCwuuDB+IllwyQ6vVaiEIAtfDjiX/nBTqhUjj6jub/uSYz8U5X74hAfaFRPYz3d3djThKGj1jWWy9Xne9WBlxYvkmDT4zdvUir1QqLtJF49zr9XDp0iVnhPU9NAxhGKLb7bqyhLgIkp4DOsYUWzRrTG8oekPyb0JE9+VnamgWMUWUdruNVqvlSp/1pqbflfYaY4lJEOxlRrNlA0s1GAnj9hqNBq5evQoAzuk1jBjue7tLeD2yXJbXHCd3tEEalOGie7RzcVmm3LY6in65qR8p9x0wFRk0EMNFDWk7/NVZOfnV4NckfOeYx673Eb9fI22/vsb3+qVnei58++eXHvM4NeOAk27ex7iduKw1vq6Ch2Yr83v0e8H64jTvMRyfBh4pxvL88L6gmWzZbDaSGV2v1/HWW2+59xlGDA+EzVVflxVOzN5h0IOTPb5fbaraYfWrgBsvPuVnkSpadqrv4YP2gvZA36vHpX4bf9fgkgqSavfYL3dmZsb5cBrI7/f7qNfrGA6HrtJJfXwVlDVzlKgP6fvw/Oln+vK9ceeWdlTvU9pGjfcQPb+a1ap+MW0k7auKFRRjOV/I5XIuq5YJB2EYuvPFcfX7fbfuBABXfmsYHve9zfX1hWPHjqFUKrlAvQaJADgBUj9TLBYBIOLD6nwYiPYvpV6g9sX32fwguv83/ScNTsVV+6o2oVmaHCv3T3+SCQmJRMJV4KruobaQFU78HP1ptfFMgPMz/XVeQFuvx6/j4/j1fPlCrtr5SUKsPqf6QxiGaDabkb7cMzMz2NjYQKPRcD42q+fo47IaRe09s2W5AJzqDRRjee6Mu4+bEWNTAD4E4C+FYfitIAj+AfZKBhxhGIZBEMQ2pQuC4BcA/AIAt4DV3UQYhnjhhRdw9epVTE9Pu3Rxiol6wfKCDoLACWwsweFrjAwze8vPUgX2M5V4AarhjXNuNUOIQiqzomg0NHKjjiovYAqvfM53zHixc0y6kA1Lj5LJpIvWsLSiWq2iUCi475ZOOp15NWS+E6tGXcVunnNd+ZXj1vfwePT8aVRRiTOaNNZra2vY2trC5uZmJBKl2+F51CwBwkjXeDx2WRPM9qITz/YPx44dw4kTJwDAldQaRgy3zO7erf9jr732GtbX1/HYY485G6OOjDoZtE8U7hgdZ2Q7kdhboInCpAZPgGhfK/6tzps6YfypwS4NwNBR4vMqBPuTfY5Nnak4IYH4ggLPA8fd7/ed4MiVaYvFYmSCzLFrBq2/T9pB34FlNhjFWGY5AdFMCWC/TNXvCchsD+5bS4NVLNfsW94r6Mjze1xbW3PBQg08zs/POxGB42SG7Pb2tiuT63Q6qNfrmJubw0//9E+7cd+t14RxR7llNvduXcH4rbfewubmJh577DGUSiVX6aUVSsyqVL9T/SY/Ows42GvwRqJsnLjrZ7eqrVQRVgVLX0hQdGLsV5nxGONssQao+KAISzunC3jF2XJNvNBgIO9rvBfRPmu5v++/+2PjuSF6fJqFxvf5/jaDhr1ez42R+0wkEm71bQbPaP95zjg30EoGjm0wGKBer6PT6WB7extzc3P4zGc+AwCu965heNwym8sS77uJMAzxyiuvYHV1FQsLCygUCq5Mn/4j29oBQKfTibTMA/b9JF13QAVJDfoAB9eJ0Z8qwuqD17Yu4JfP5132qD9XJ3Fzat0m963BeYqxKhxzLq+LlXW7XVy7dg3dbhfNZvPAYmZqD/0gE/erx6+2X20mP0sflP6yL4hrpQhttIrTavN5rjkvYc9cjpe+aiKx19plenoa09PTeOutt9BsNp3GwPMSlwCh92e2yzx69Cgefvhhd2x34zXxoHMzYuxlAJfDMPzW9b//BfaM5bUgCBbDMFwJgmARwFrch8Mw/EUAvwgAS0tLd9XSmjRsV69exblz5/DII49gdnbWGR/fKBKdXLdaLWcQ+bw6RJNQYwUgYhD0b+4vCILIwi8U+oD9C1PL+/UYVSgg6mxyzDRMKhRTkKXBpxC8s7ODQqGA+fl5VCoVzMzMuPJTZoX5Bp+ZXJx0a6TJz5rg+dHSODrDWo7Lc+2LLHoj4/P+DYKvN5tN50TqjXHS/wwjbLxRaaTSz8hgKXWz2XTCE8VYw7gBt8zuHjly5K60u2tra7h69So+/vGPY2ZmxtkEP6PdD4ZRlGX5OrB3fTObUoWBuAm7BtV8EVadN07AO52OW3l6OBy63l2shqBN4Hbi9qv2SsemtsYXNNQW67YBOMFSM0A1S8u/t/gCiF8xwOf88epkgUEmZlzx/Pk9vlVEUcdbHXK+TwULisHMHuD2GYBUB5YrnKfTafe9UEBmRQYzYi9evIjl5WWzu8bbccts7tzc3F1nc8fjMdbX17G2tuZ8Xb3mgaht9EU+3w/2J+b8vG5PoT3zbUWcwKq9XuP8V46L+5rks6koq+ObhJ8RNRqNUK/XnWg6yc/2x8Xj0sm+nj+9z+m9QEt4/fH6/ju3o76vTtjpF+s2eE/TAJtO+LUNAbfB13WOw0xZPce01Y1GA2trazh27BiOHDlyw/NtPPDcMpu7sLBwV9lciqarq6u4ePEinnrqKczNzbn2IRqgJirAqS/rt1dRf+5GgS/gYKa9L6DSx2YAnK0SWMnqJ4gp/r41Y9bfP22in2Clv1Nwpt+5s7MTWWtB2wT449GWW7pPPU9qy3gOeHzUQfzzw21qawhNGOHv/hyC4rYmto3HY7TbbffdJ5N7vbmZjKLrYfjHp3qNP0YmIczPz+Ohhx468D0Zdw/vWYwNw3A1CIJLQRA8Fobh6wA+C+CV64+fB/B3rv/8V7dkpIfICy+8gO9///uuRHZ5edm1J9B+SzopVrEN2Csp0CzV8Xjs0vmB6OTczyTgtrUkTLOP6Jxtb2+jXq+7XiH+GGgc/H6E3CedWgq4NPg6Jl88YGbqeDyONJJeW1tDKpXCxz/+cVQqFUxPTyORSEQWpIkTmn1xls9xoq+NwH3xgsfD88r2ERQBdF90NHU7dDL9fZ8/fx5bW1t49dVXsbOzg2azGclI0wyQSqUSMbhBsNdHmP1g2bZhPB4f6HWZy+WwsrKCl19+GdPT0yYKGG/L/Wx3X375Zbz44osuo56TP5aHapYnsC8QqA2mnQX2hUXaBxUY1RHzI+lqW/g8S4MYjW+3225FaBX8OEn1HU/+TXuur9PW6n41GKXv5X2BC/2F4V4p6MmTJ5HP5yOtT+hk8rh4j9AsLF8MZcCMdor3Oz+Di2PTTCiKz+l0+kD/XH5OWw9wwuEH2ViBwoxm9gsejUZYW1tzAbJ+v+/Ktaanp5HNZlEqlSKZZLzHhdczuegE1+t1fPe730UqlcL8/Pyt+Pc17lPuZ5v75ptv4vXXX8fU1BQWFxdRq9VQKBQik3wV1RQVMulL+UGkuIkjX1M0C9cPyvDa58PfZpzoEGd/6ae9k88T9ZOZ7EB/0F9MUG2tZozpPtTXZaYV7SN9Xa4toP43+x+qKKDnU9F2EioqaEUC7TurOba3t11CB4NfKsqUSiXMz88jk8lgZWXFrSlBkZbHw/uLZobxmFZXV/Hmm2+iVCqZGGvckPvZ5r788st45ZVXkEjsrYfw5JNPYnZ2NtIGyg/K06ekbdF5LK9pJkQB0dZTccQF0HRum8lkUCgUXGunuMzZOH9ak5n8MWhAiMfDv+mj+4IqE74SiQTa7TbeeOMN18YB2F/sla0Y9ZzpvQOIBo103GrnVF/ge/V70Z8AIvaedlO3rdXJfD/fyyQR2uB2u43t7W3s7OygXC5jenraVd01Gg1nb33bSjGa9w2teGi1Wrhw4QKWlpbe0f+mcee4mcxYAPhLAP6XYG+lw3MA/gyABIBfCYLgzwK4AOCP3eQ+Dp1Op4PNzU0cOXLELVDF0i11sHjR6oPQiSNxTqgaQN/5ijN8fp+Vfr8f6ZXC9/qGkZ+JK73y+9L4AqyOVeEYdD9sT1Aul115vmZH+TeAOKebz/HGwhuBZmzRUPK4tVyC+/GzBVSQ8M8tx0JhpV6vY2NjwzXL1nPAbFeOif0IKcRQMNdxs3zLXxFzd3d/4Zkb9Y40DI/70u72ej1sb2+7kkjaW15zvO58Gxo34VVnRctBfTQLU+24ihG7u7tuAT/2bWq1Ws4p1NIwTkhVsNSx+tmsftWB2i3/uDhedbh5fFycioEyzRzTKgiOSQVsxT8O7Tc4Cd57aFt1/CoCazBNz71msqmNZWZAqVSKVFYwAMhsANpcngMG8iZlYzCreWdnx1WRGMbbcN/a3J2dHczMzKBarbrJnl5jAA74lSpu+nbqRrbiRsT5bBQ5/QBLXKJAHHGCsP/6242dSRWchNMnLJfLkQm7igh+dmucHfLvPToG30f1j+NGxxTn/6sPzPOYTCZdRqwen96HaIsZFNVFFf17nGZtqT/O17SKxDDeAfelze12u9jY2MDCwkJEX9jZ2XHz5Ti7qwIgsJ/pqa0FJwmwk+b1GphnsFx9LyagaTutOOKCX/rTTxRTIVZ9RNofHRuDYbQfTG7ie+gDa7sAbpfb9qu7iNo59U810UN1E/1u1Kb6eoIflFQ0aQOAs708rkQi4XxZ2mfaZu5Dk1L4t38MWu1gNvfu56bE2DAMnwfwIzEvffZmtnunmZubw+OPP46HHnoIc3NzOHbs2IHFPbTfnl5sfpo7szXVoaLxYKZk3IrdcT1lB4MBNjY20Gw2sbGxEZlgc2Kq+9GVCFVcAOBKNmlItNSKkRZfBKCTplkCbOD96U9/GjMzM1hcXHQGkU4eDQR7P6pTzQf7VWmfRWA/kjQajSKCAwBsbW2h2+26m4Xfw0xvHjTU+h4ae2ZwraysYH19HRcvXsTOzo7rPcnvkMIvhWZgr++m9qmlAMEejvyeOXY67/1+HxcvXsTW1ta7/O80HnTuZ7v7xBNP4Pjx46jVaqhUKi4rlpNBXj+8dv1sLb9voe8g+VF6fS4IAuf0dDod1we21+u5yDTbz/glVVp1oA6fX/6lE1jN5uXkmGPz90HoYLF86dSpUyiXy24hB2bMBkGAdrvtsqB4rlRIoZOqDjmb+/O9nBz451kFa77OLFgNVGlPR27PPy9Es5d5/gaDAba2tty558IGQRC4RSN5n9V+WbwfanuDUqmE8XiMN954A6urq+/039Iw7lubOz09jVOnTuH9738/jhw5cqDUXLOIdOLJybmik0USJ0L6f/tBekI/lNVlvv2O2z7HqXZNRc+4hAD1bX3ReTgcuswkblNbofhig96b/PPgJ274x8Qsf06umWSg2V8cf1z1hS+E6tg0u1ir1fi7Biw1O45ltKzI6/V67j6hPjQFa63y4KPdbmNlZQVra7EV5YYRy/1qc/P5PGZmZvDYY49haWkJuVzO9ffUubcKiDoPBvbb4tGH9BOqtJIpTmikfaKN0QWk1GdSW6JVW+oTxlU1sAKMdrXf70d0EW5TfT62f+S4MpkMtra20Gq18PLLLzsfXANYwF4fdgaJaIOpKdC2+ZVu9K2Z8MHx6v2OugKPQzOEVUyd1B+c+Mlg6+vrrhqBwU++P5vN4sSJE27f6+vr6HQ6ThvRvuQ8Jq3W5lh7vR5WVlawvb39nv5HjcPnZjNj70sKhQLm5uYwPT2Nqamp2BWc/clynHgKRHuS6u8qGlC000W7ePEyisIy2UajgXa77cqJ1EjSgdZVwf2oujqjfgRex6vHolE3YF+IHo1GqNVqmJqacj9pTDUj1nfAtSxWjZQfyfKFVTqTHCePzxey+ZxOFvyMNF9EYNYb+z0mk/uNyinE83ntR+n3Dtb96s1Ux0UxuVQq2cqGhnGdQqGAmZkZl1mv0W7N5tSJpto2jfD7zpCiTpnacmbAMgI/GAxc1jr7RlMYAA62EFBniH/HibP+GFSUVYc5TgDl9rgQFR1E2k4Gv4IgcK1n6FTrPSBuhVm+RnHCFw/0nqKojdbn9B6n51q/M71H6fc5Hu+v4J1M7i8QqQsVUDDQgKY/FtpbdXqnp6fdyrKG8SDDRUJKpRLy+fzE3viTbCjwzhbpmrRNtXO+bx23cJXaDz/45dtfxffBfFvl+78KbRcFWPrqcdv3z8ckv5s/VcjQ86FJCf7CNDquSXMOPVe+KKt+NrepC7bxfstgFwUbLc9V31fPvx4PALe+xNTUFOr1+oGxGsaDBhN8mBVLv83vN+3bJR8VQ4GD/aL9snydt9J/4qJc9COpdfCa97UL1T3Ujqp/p2NRe67icZxfyYQEvn84HMau20LbQ3vD8TJDWOf66gdy/k5bp3ZW5xX+T/V76U+ybYz6sv73ptpEMpmMzCHUT+dxa3YrzwH9Xt0/X7vRPIfJEK1W613+dxp3ChNjY5idnUWxWMT09DSKxaKLAtMpUkeRF0NcT0J1DjVSxKgPBQdmf+XzeReJ39nZQa/Xc9mfGxsbLiuKUR86hYymtdttjMfjSAamPiZFgLQPCi/ouDYLHBsAF2F/+OGHcerUKdRqtciK28ySopHW0iaufEhhw18ZUG8SfI7vVSM2Ho9d1hz37d+8NFLH5xjF19UGG42GKz3mKod0RGdmZtw5oGjLmxYX7dFWCRyXL/rS6OZyOUxNTeHYsWPY3d3FCy+8cMv+dw3jXoX2ltcWs+B5ffuCmz/5V/vmT1TVHvNvFTCbzSYajQY2NjbQbrddL+5er4cgCCJZueoE+b2sFXV61UHmWNUR5fNacsagDR1GrQ44cuQIqtWqCxJStOh2u+6zDC5p70A63ByfOuv+OQLgSlnZl5w2TAVnHoe/ECPHweCciip0aIfDYaQPFp3T0WjkejP6gTT+LwRB4Oy+H9TSDD4uPME2Ok888QQSiQR+//d//23+Iw3j/oYJBzMzM06MVUFAgyacMPr2F4hmbyn6eb5PbYZmdXHSGTepjqsk4OcZlPGFA46Tvqw/WdV7BG2F9vjzM1eLxaKrktLj0zYF2oJK/WntFa7nxBdfeD/i++ln+4shqgjCsavwq6KJjsn3R/m6ZldRcGC7LWZK09fl8ehCPvwsx8oxMWD40EMPIQgCvPTSS/H/iIbxgFAoFLCwsID5+XnMzs6i1WpFFkSNEwSBaHUq8atBgf1FpTTrk++hZsDrklmlvn1Se+1XVmkgjH6dtjtUW6s2RUVhP0DEOTmwJ1Zzfn7+/Hmsra25dlK0Tdq6ivqJtnhgIhuwf+9ipn8mk4lUwPE+w7kGxV2Oj2Ph+ef51MpjHqtm/FLbod/dbrfRbDZdm5tarYZsNot2u+3mO7T/TAprNpvodrsREZrnmvdEv+oZ2MuwPXXqFIIgwMWLF2/uH9Y4FEyMFTqdDra2tlwmjU7YfQfSzy7S6IkvxPqCrZb/8GJiOSZ7E7ZarUhPWGYFqQHQCIlO+nXSrZmccWKCZhqoQafYC+yLAlqWVa1WkcvlMDc3F8nQUqOsDqM6p2pcOF4acY6Xk2e+xpuB9iX0M+d8aBw5BgDOWWTvql6vh36/7zKQ9TgpArGdBPepTboLhcKB70RFWX8SwGgXx5HL5XDmzBn0ej1873vfc/t45JFHDrTGMIz7kV6vh3q97krvKeypIxiHPznVz/nQfvBz4/H+AoQUYvnwszD5GXUwNYsgLqqu7/Wz9v0gjZ8Zq4KsP/ZisYhyueya9esYVSzluaBIy/tVGIaRRRH0XE06byoi++/zJwL8LrTCQc/RcDg8kKWr2+Q9jYKuBvN4DvSexm3rvUazbXmOgiBAp9NBKpXC5uYmSqUSPvGJTyAIAhcMSyQSOH36tFUrGPc97BVbqVRQLpcRBIETYv0WLFp5FCfE+qhP6T+v17efoanJDmpTfJ/VD7Rr5YRe9/p+XwQlcTbPr7SiCK2LWul2aKP4u591eiO0YkK3R7vNRAm+R4/NT5aYNC62FNOEC51LcJI/KSNM7yH+Q9H7Gs8TA3TJZBLlchlPPfUUwjDEq6++CmDPfh8/ftwFCA3jfqXVamF9fR3j8RjT09MuSAwcXByR+M/74mxcAoCKsQxUM7hCwZHP6Zw+bn8qymrQjDBYpHaPY9Px+CKuBtu09QKwd29qNptoNptotVqRbHyKnL7PrQkFupCif350LQW9z/h2C0BEs1A9Q4Vt9duBfU2iUChEsmh1gXS21NJENBWRfd+d37uev7jWE/o/wGMqFAo4duwYWq2WC4Qlk0k89NBD5ufeZZgYK9Trdbz88stYXFzE4uJipBzfLzNihqtOFofD4YHGyiriqhDLMtMwDLG9vY3t7W1cunQJ9XrdZWVp+jp7NmnkXrfLbTNziKuP8/Oarq+okeF72ZdFG3fzc+zBcuLECSwvL2N2dtatcg7AiccqyGrWFiOAQRCgWCw6R5DP8/xouQSPUXuZAXC9rFQs9Y2SOoWj0cgJMGtray6jWMsVgmB/lUPerPyxhGHoVvPmudXMXf0/4XemEw6Wa1EY+bEf+zF84xvfwLPPPgsALvPNxFjjQaDZbOLNN9/E8ePHcfz4cZfhX6vVYrNh4yaiGhDRUkvgYL8+Xqfb29toNBpYXV1Fo9HAzs7OgQwnLSEldHTZX4uBKFYkcH+avaVOF6P2moHAe4pml/qT6mQyiWq1itnZWScMaAmrVh9wmxpk4j1EW+P4AofCMWkbAD9jjO/h+eFPFVR5f9JAnX4/frYdt5HNZt250mOLq/RQIUfFWJ5TBjQHgwESiQSmpqbwsz/7s/jyl7+Mr3zlKwD27O7CwoI5qcZ9T7PZxFtvvYXHHnsMy8vLLiFAV/JW/CwtINqjFMABHyxOkOW1yMopzabyhVTuT8truT0/m1+zv/heHbv/nNop/Zy2CuMYuKq4VmrFCbDcrlaExYnC/IxuT88jbZ76z8zY8o/Jt7v8Xe8JrVbrgMjOKjv69tpzm+PkOLQKkPMe/X74Xn3wXsHy69FohLm5OfyhP/SH8LWvfc35utlsFjMzMybGGvc9W1tb+N73vofTp0/j9OnTbk0EIL5yQK9XzbgEEBEJAUT8MPp21AGq1SoKhQKmpqZcZRXtKn0rvU6JL7DqPD4umBYXjKGd8INOtN/8nT4bfcQrV67g6tWrEXE4kdjv26oBIU3e4hhUPNb7FqsvfJurfiPfz4Qp1Qj0dWap6j2LYnetVnP7ZKsFJmHVajXkcrnIvU/b0XB+Qi2B35XaVv9+pxoN/x8GgwHK5TLe97734dVXX8Xzzz8PYM/mzs3NmZ97l2FirMCLmxNtZlAC0Qkjo0lqONXo6MU/Go2cWKgZsalUCpcvX0ar1cL58+fRbDaxtrbmLkJuhxNSvfBofLjaISkUCpHJNo2ubxjpPKkzqaX7KhyqM8xtzszMYHp6GtVq9UDzaL9MmAaH4vJ4PHYTe2DPaLBtActwVSygEaKhzWazkSwpPzLF4+Sx0sj5Pbj4u5bfqvjCY9ESDnWy9Tz5cLLBc+JH7nh+uNhNr9dzi2ecPXvWVvg2Hih0Mk0BkT2yaesAuAVCtOxUJ9VEM6n8jCiWJnU6HVy5cgXtdhtbW1uRRWKA/Uk+t6UL8TE7lRUBuhDjJGdabaoGb9QWafsb38nl/WNqasrZeR0r7S0znPR12nxmHnO8vsBCe6kigu5by1r1mPQ4+X1yXHxN7TDvC3HnGYALuPE13m8p6KqzTweWkwoVDzgW3qP4nkaj4VoE/ciP/AgWFhbwe7/3e9jc3HzP/8OGcS+RSqVQKBRQKBSQzWYji6PEZdkQvfY0W97Hz7DitU/BV7M0uT+1n34Gpi8U+uPxx+n7g/qTdonb9QVNFZOBqL+v+P6dZlr5fqluW0WUuIoutdsUWXx03/o5PZ8qjtI2+gKLVneo/0sbrYt9MTmB++Xx6fEQngvel7jvRCKBU6dOoVgs4qWXXrKehsYDA23u3NwcTpw44aqb6DtqEMgXWv3gi4/6ckwSYKLU7OwsstksisWi859VBFS/l9cs57B6/dMv5eLfkwJBOiY+OJfXtXfUvqk4Sx9d/UoN+vB9HHdc8Enn/Exw0NYDcfew0WiEVqvlREp/sUPi2+64igH9Plj1zPlLsVhEPp93ojbHwCQUZh9rdiz9evVr47QdHp8mgCQSCSwtLaFYLOLixYuRhDHj7sHEWEEn3ewrwp52vGgoDqjh5GeB6IXJCTYnvyxtpVFYWVnBxsYGXn75ZbTbbezs7LjPFgoF14dUDRad0kwmg1KphEqlErn4VBhm5i7HpZNendTSACYSCbdaqgoa2jOmUChgenoa09PTKJfLBxxkNap0xDQ7NgzDyKJXKtRqPyqOgcfL46CTp5lm+pM3Mp5HLVlQ0YMGVifvjCRyXIxA0WFXQdefSBB+D+w1mUgkXMacGnS+jyUMs7OzmJ2ddf1xNNsibiJgGPcTamN4Tagzwms+buKsoh+f53VMdBLc6XRcf1j+rp9VVAzIZDLI5/OoVquoVquoVCqRVWd9kYCo00jnjsenWZ+0L5qlwM+xvxf7WfPz/pg1C4HQhlN48YUNtfV+ZgO3SUFCJws8Tn/S4Efsfdvp20J+/7Szvuiq9y7tPau2mIFEXQiDn9XFHsIwRKvVwtTUFBKJBB5//HE8+eSTeOONN7C5uRk5HrO7xv0KJ4WcuNPu+HZM/Vu99vW65DXtB74Is3ToF9Huqc/G61P3z33oRH6SneXnNXHBPw7fptEe+AJlnIgwSXjWsVLk4Lh0juDbO/WNdUz+ubuR/dGAmd5j/EQFTUJQP5u2kfMT35fn/wSh366+qf89E/5P8HkNwh09ehTHjx/HlStXXNau2VzjfofVU7VaDUtLS25Bp7hrzw+sxAmfQLS9iWoMuVzOzdGPHDniWu6pH0rBldulTeLcdDAYoNPpHOgLq5mxcf4mcHBR2N3dXadnAAeDNRSjWb3KKlpft1C76SdPqJDJ6luOOW6MOnZNhOB4NDHCT4jjMarP6n+H/MmF11UP4OJaahc1QOknFej4fU2HY9OELx43xzU3N4e5uTlsbW1ha2srYnP97Rt3BhNjhXw+j6NHj6JSqbjnKALwIlNnRB86maSRSqfTbkEaltSnUim8/vrrqNfreP3117Gzs4PNzU0MBgP0ej1Xos99UwhgecHU1BSy2axrDaARNT9KTWOmTpGWsQZBEIkWsZdMNptFv99HPp93RoLbzWazKJfLSKfTrq2AOqoUXrvdrstw44OGjTcAlvhznBqRohOuYrROANT4Trpp6fM0tuwHqX1bNcOLRo2O9WAwcGINt6ORK9+o8TspFAqRG4F/c6IBpyPM1Yy5YMK/+3f/zglKR48exac+9SkzmMZ9CdtyJJNJbG5uusxIOqosrwcOlsuqIBcEgcs28Cd3QbC3cCAzYtknlnYpTgSgLchkMqhWqyiXy26hG2bwU1hVu6sZUbRhnMjS3rJZP+2J/k5BkeIAP8+MWBUa1X7rZFwDc7ynsTdkt9uNiMzM/PeFDT/LgfB5daBVENf36Od1Xxy73uv8+5hum9vTtgthGLrAFZ1WnldgX+Dn/wQ/32w2ce3aNYzHY8zOzqJcLrt71Fe/+lV3ThcXF/Gxj33MxAHjvqNUKuHkyZPIZDKRtlhqA+OybYD9DFY/QyjOB+N2memkFQEqeNI+0Napb0TbeSP/RyfEPm/3HP122h0eP31Q/k1o57QKTG0QgMgkWl9TsUUn8NyunnMdn/qO3Oek46E/69t3LqyVSCQOrJquCRKaRUs77bf90f3pMfHepKXNKmb7/zPD4RDf/OY3na87NzeHD37wg2ZzjfuOYrGIhx56CDMzM+7/Xee6WhEFRPvD+te/+pu8Njl3n5+fR6lUwsLCAvL5vFukSxPMNNMV2L/mm80m+v0+2u2284n4fn/Ozc/dCN9mqM1jYgD9s0Rib00BLl5F9F6j9xZNUEsmk26xK2bGavUXK4njWuPoTyaD8Jh9X1h9e45Nk8ao30xNTWF3d9e1J+j1eiiXy8jn82i3205/0HPIbVEzUE2EPqkG1Ohzq8Cutljvm/r7cDjE17/+dfc/eOTIEXz4wx82feEOY2KskE6nUalUIr1g9SdwcNLLn1pmq5kDvDh5MQ0GA1y7dg3r6+tYWVlxK17zAlPnj9vQiWu5XEYmk3ETcyDaU1DHp+P2UYHQj/ZoxIsRLW6Lgq2K0urE8maigoKfjRpnyNXx1fJ/ndRPGrO+R39XoVRFCn1oRp0f7dMJPPev4o+2LeD7+X+QSqVcdEuNLbAvYqiIT4GdIvCFCxcix9HpdJxDy/44hnE/wCz/fr+PbrcLAC6rXaPDwEG7qGVEKk4StQdsfdBoNNBsNp0QC+w7WX6mE8XYQqHgKhF4/WlWpjrROqHXhQg1oESbqZ+hLfHFWAoSzIhlCxR9+DZfMw6mpqYwPT2NdDrtgmHqyKr9ps1X+ztpYqyBMg2Kxd13dALP/bIXutrRuP35fcwoWviTA79qhcdGW8kJD8Whzc1NJzRwDJcuXXL7YhaICutmd437gUwmg0ql4gLLOuH24bUGRBc6ibtW/UwlZsFqtj+f9wU+PnSCrddc3GQxzs7cCBUzfUFWj1PFg0l2SZMf9NxpoJB/+z6gft4/d/696EaijGaz8fzpd8Tz6YsbbDmmQTnafn/scXZVs838/fNYfBE6Lrg3Ho+xsrLint/d3XVVZYDZXOP+IZPJuIW79Drx55uqHwAHbQYQnTtr0gDXoimXy64Sij4OfVb1nbg/TaLq9Xpot9vO99YAON8fR5xg6YuoGnTj+zQIyEW16TPr+VCtQf1V2p3BYOAyatUGUnT0g0iTtBEKm5OO2bfzfvIGbRaFXT9ZRBPcJt0P/OOjDffPG9+r36c/z1Abz/N45cqVyHlQm2v6wp3BzrjAfnr1eh1bW1sRhwzYv1gouGlWEn/SiSwWi5G+pwDw8ssv4/Lly1hdXUW73Ua9Xnd9aRm5UieUmbU0sLlcDuVyGQAiwiaNDw2NRuN1ks/x0Rmj4OGj0XDNGNAespqVS6NNI0ZRhU6qZp/yPANwooAet5YlMxuME2oaFWbWau/DuBuAfjecxPPmwnGyLQO3n0wmUalUXDmwiuwq3LTbbZcRx7FwnBRoaNS0dDYM9xb/4njZ0yefz0eyE5TLly/jn/7Tf+qM5Y/+6I/iiSeeeA//4YZx98GG9+vr62i1Wm6xEmY9ql3VPs7JZNJlXulznDzyumVQZG1tDfV6HZubm66Uyc8u0Kh9Op12mZNLS0uupFPbDGiUWgVW2hsVbrkYIO2EBm/UqdQyeyDax8vPWPN7SNGGjkYjt3DDsWPHUKlU8Nprr6HX6zlbw37jeo/w28LovYP2kePTljm8V9C2+llvwH6GL1smsHROS2jVCZ3UHkbtO8+nn7mm9z6duGSzWdRqNff3zs6O+1/wuXr1Kv75P//n7u9PfOITeOyxx97rv7lh3DWUy2WcPHkSV69exZUrV9Dr9dz1pxlFvmgJ7E/+fNFR7QZXj2YFkR+wAqKLu1BMoN+rGVPEtw9xY9LAuN9aQMUNFVn5nE5offFT96PnhwIGt+8HxHgMWgGhY1Vxwoc+Ke8FGnTTY1KRgvvwxWEG+Xi+GVj0BRnui0EyiiPD4TCSuczvia/74g6/Z1bP8V6jLRHijnltbQ2/8Ru/4c77Bz/4QZw6derA+wzjXoN+ZLVadc/FBYR89FrhNap+HxN55ufnUa1WMT09jVwu53wsnSOrP0URrtfruZZd9Xo9Yj98UZh2RMesPiFR39sPztMu0d53u103Pv/46Wdq1SrHp/70eLy/6FW1WsV4PMbOzo6797AairaLn/MF7zAM3TnhfELvF3o+1GfndqmJrK+vo9vt4tq1a2g2m+5+ls1m0ev13PHz/Pn23Q9ssX8v1zDimIIgiAQ49f7IMVFD0eNUVlZW8OUvf9n9/ZGPfASPPvrogfcZtxcTYwX+g+/u7qLdbrsJOVGHhc4PL051rFQ0YFZpv99Hq9XC9vY2Wq1WpEEzt60Tan6e/V94IeuEX506P3KjE2x9L4/TzwLQ54kaHf993I/uj/vys2B1LLp9/3i5H24nCALnBPpRKJ2o+z1h9Fxwv9qDRXs1MuuVQqh/7jU6p86237NWM9/UofeNqh/B0slHEATuJspVH4E9h3Z9fd0dU6fTmfQvbBj3HP6EVMVKRoB9WzIej53I6U9qNduTNoKTRk4stURLI8maCcRKBLaaUYdYbc8kO8z3qp3RCLUKBpohAeDABN1flEqFDN2n2vZcLucqPdTJzOfzkfuW/3mO078/+GKsnneO2c8O9u+RPLfq6PI8+VUGKtRwW/529f9Evxu1uf4kgW0vdFJDUVl7WQ4GA2xsbLjPUnQxjHsdip9+Nk6cj6IZOcT35RStHPJtJNGJrO8za8a+7i/OvvK1Sfhj9H1c/byfOfVOtqnzAfp+Khr7wTM9Ft+fnjRWtdPcrn4//nY5Bk1A0Mo7XXSX36t/DtW/nnRO/HPp3y/4nC/AE/Zgp5gL7GXGbW9vu/dQuDCMe51kMukWffWzYdVG0N/ROTr/9q9V+srsQ1ooFNyaN74N1YA2qxXoE3Y6HRdAI3pf4M84rSGOSfcGjoM/VUDc3d2NtFWhjdS2DZoZrOu5aEKYn5BBn47HpsleFIC1ekATE+LstI7F1zDo0zLRi1XPfhCKfu6k86aVGjfK4PXPP98/6fzrekhqc3XxWrO5dwYTYwX2K9ne3sbGxgaWl5fdRJwRD2ZFEWZL8WLL5/PO4WHvkJWVFZw/fx71et0ZAl5onFDyQaNaqVRQKpVcj0KWsGsUhAacRsdf+EVLo1TEoGGiQw4cdEj9mwUAN4nlg02yuX1mhGqfmfF47Iw8J86dTsdlhVL08MuRuA9GgtTA0pAxWsRxaO8Zjovj2dnZQbfbRbPZRK/Xi0TNyuWyE7wZrfPT+/3Jh/a71ZscS59VRNaJfxiGkewwZgnz/D/yyCMoFot4/vnn3bEbxv0Mr9VMJoNiseiux2KxiEQi4YRMXuuMltPeZjIZ174ln887p4fvZ/Cr2Wyi1WpF+uH5ggAftVoNxWIRy8vLzr5qkEknw5qVRFEPiDbS57XMbHsVRFVEpo2jkw3sl6SpI64CqJ8pwc/WajUcPXoUjUYDa2traDQa7twxoj7J2aP9pe1jJgJb+NB28pjUVmqgyxdg9djiHG8Vd+kw53I5tz+tnPADe9wOx8Lvmfdr7i+fz7tj0gUUAeDSpUuR+7th3I+MRiO3OItehzrZ1aCyZmeqj6XQRtLfUzGWr8clHNCG03f2EyD8dQ40OKTj0s/wGOhf6/M3ws+M9YM/vhDLcfB5Chw8Ngqg6lP625002dax6HnU1+IqGBhw5Gc6nQ56vR5arRYKhQKKxaLrD6n3k2QyGWlBxu/EF4C04o2vaVatZiTz/PuVJwBw4sQJZDIZvPrqqxboMu572BqGC0Wr/6l+I2HmPeeT/oJOnPtXKhXMzc2hUqm49Qz8RbLpi3a7XdeiqdlsYmtrC71eD41G48D2eQ+g/+3bKd4XdNErPs95L+fJmqykwTn//qCBOE1sUjvJ7XMdnUajEVkMC9izZ7Ozs+6YVZdQf5yiNefkWsml9p5j8+9f/O54zrnOTrPZRLvdRrPZRCKRQK1Wc3MT9aH9YJXvu6rWEoahqyTT9mw8Xk184Lj9ti/T09NIJBK4ePFibDWYcecwMVZgqruWUfIi1QmiTqA1mq+/swSTma28WPk+Cri8SPgcF/sqFApuQSc6pzRIvjDoZ4xxnOoUcZJKh4lGQEVdjc6p6KrGldv2o2VaPqHl/CrKquNG8YUZqDTmuj2NivmGG9hvZs3yKyAqjtAx1Ybk/KnOY9z/gTqg/KmCvJ+RoNvzM/vUseeNieNXgSmR2OsxXK1Wsbi46FZ892+C6+vreOutt7C0tOTKvwzjXoXXKktq2OiedkFLSvngdU77SLFPHVANkqjDqIsn+FBwpCispaJEs5XiSrr8yaif8Q/sCcs8Bl77RCPnvgAy6Vyow51IJJzYSxG63W5HtsnxqJ3zxc04u88JQBiGkYXLNIjHUix1oP2sZZ4j2j51djXir8/TSeXkQNHvnei90P+u/O+nWCxiNBq5Hu47OzsHxJv19XWcO3cOi4uLyOfzB8ZgGPcSnNxxAqm+IHAw+4fPTcq6icuAVfutn1c/SZMZJmULxYmtk9BEB//9cZlOvj3U5/Tzvp3UY+Vneb/RYwZwQOzWz/vj1b6O/nHpBJ1oNq6OQ+cDwL6/ydXQ/Ywt7l/vRwzaqa/s9/jW79a3uYT/XxQAxuMx8vm8Cxi2Wi2sra0d8HW3t7dx6dIlzM3NucQFw7gXoe80Ho8jmag6r/Wvbc1w96uRqDPQZ/WrS3nNagJXt9tFu912D79dV9y81h+X3/5F8X1I36br6zqX91/3bTaPxRd2FfqJFKL9xbp8e0UbqIkHGtyjPdMWhv59iz9zuZwTdimcc40K6j+aAOHbdv6tNpuBSN939Y9Bz536z3Fr2mSzWRSLRczNzbnEQ38sm5ubOH/+vFsAzjgcTIwVGEXI5/NYWlpCsViMGDpgv5yRzbKB6MSVr1UqFfePX6vV0Ov1UK/X0Wg0nBOkmaSZTAZTU1OoVquo1WqYnp5GqVRCsVh05Z/+CrS8iGhwGD1RIdA3QnQSOVlm5J6rg+vCNOpkAftOmpaXqrjA4+FnR6NRxNgz44jbYINxXSWcx6KCBI0Tt8dzrlEjAJHIIt/b6XTcSr7D4RBbW1tOROB55D64fzqtvtCgC8DwOLhdCgQaPeODx603GjrF7IvJ1ylC1Wo1bG9v47d/+7cPZGo999xz+MEPfoA//sf/OB566KGb/bc3jDsKrzs6LVzkgLaKdo2TVM1ap0NE+0VHkUEOXmODwQDZbBYAnE1qt9sHBDqKsIuLi87++6Iir2PaPDpMfomt2l/aDbYfUWdSHSXaXzqGGrVn2xS+pk4ane1er4dcLodqtYrd3V1sbGzg8uXLaLfbrv+4isBq44gvyPgOKjP6t7a2nOOprR0AuAw52nN+VitHmK3F88rvkMIx7b/2U2evXvYk5/fB+yHPA/vW+pULdJI1kxcAZmZmUCwWMTU1hZ2dHfzgBz84IPr+4Ac/wMsvv4wvfelLOHHixK2/EAzjkGE/OWaI83pT+0rUp/FRIdAP3mgwREU++tCsSNLkBBUqdfLvl7xr1qsvptJ2qb/GCb2ffKDiqR94iwvMcyz6GfXn1T/3g/J6zvzMXn+iructkUhE/EW+PhqNIusX+BUPPEdqO2nDubI75wyahczkEp4/bkdbaPlBM20bpj49M6W5lkIymUS5XEa5XMaxY8ewvb2N3/qt3zrg67766qt444038BM/8RNYXl6GYdyrMFC9vb2Ner2OTqfjMulVaPMFON9uUnDkwrdcT0bnsvR1mBDFa297exubm5vY2tpCq9VyPhrHpxWfKlaqEKzJYfqT49PfaTPUXtCe07fm83qc/r2EiRP6WU2Qo/8IwPXCXllZibQk1EoNzdLVYBPFVWoJzHrV7H4GD5m0USgUUCgUkEql3OJnFL2bzaZLsGKFgn+OiN7b+L8QV30Ql1Sg/2M8Rh6nzkcoGi8tLaHdbuOb3/zmgQzZF198Ea+88gp++qd/GsePH5/072zcYkyMFUajEVqtFqrVKqrVaqTvKy98dVaBaI8PnTgD+5mhyWQS1WoVy8vLyOVy6HQ6zimiI0WRlxd2sVh0BkGjJZMyMv2MIN/x88VTfl4FXDWIfnaXfsbfRtxkV3um0AjR4Z6bm0M+n3cLycT1ZvFT93VCr+PgPnnc/B6YoUsxgAtusVevbkMdfkbUksmkKzdjhp5meWgmbFwmHJ1RAE5EoSOtWWzj8ThSoqXOfRAEqNVqzqjre0ajEV599VVsb2/jiSeesKwB456l1+the3sb09PTbvEBXXiADqJO/IB9WxwXHFKbyGt4PB67a3E4HLqF9LRXlQbBeN3TPk5CHWQt+fVf1wBZXBaab1d5PLSNams0ou9P5Pmg3aUwyUAPnTOdANBmaYYGz6Megx4Te59x+zwOthXQ1gIqkvsijdpLdf4phvLzOtnnMXD8vkijYjbv15o5oA4ux8BMk2KxiGq1im6368RinpPRaIQ33ngD9XodZ86cceKzYdxLNJtNnD17NjLx1MoutU3vJitVq4JUyFPhUu1lXP9oMkmk1Gte/WE/q3LS3yoUvh3+PUeTAPR1FWIHg4HLhNJjmbRt/X3S+fX9RrXTeq+LE3y1WoKVd5rVpec+rnLCP3/+fvi8LxBokEyDY3o/0HNaq9XQarUO2Nzd3V388Ic/RLPZxKlTp8zmGvckOzs7eOmllwDsr2CvlZFA9Jrzrz2/UorPd7td1Ot1dLtdlzgWBIELvNNvGg6HaLVabi7sZ6ZqMFvXVFB/TQN06oNO8sM0kYn74PN+MoDaUn5eUa2FNiwMQ7cYLYXLbrfrEqT8djuTgokcmwrhFFjpQ8ZVbujxpdNpjEYj166AmgdbT+qx++dcM3/jMqXjMpT9wB2f0/8Z//+H55zfb6lUQrfbPaBB7O7u4s0330Sj0cAjjzxiNvcQMDFW6Pf72N7exuzsLJaWliIZT/6E2o90+84N/6Ep6E1PTyObzWJ5edmtXMsLj4aRLRJUcGSmqQp/mqmqafhEL1yOlX/zdV7gvBHQkABwvVx9o6XOoEaW1IBrH1dGiXZ2dtBsNjE1NYVCoYCZmRnXyByIlv9zm74RAvadO43a8byNx+OIeK7RIe3hov0ieUOkoMsbDX9vt9tue9w/z7+2FgD2I3Z6Y+L3w1R/Tk50IbHd3V2XCabfEbc7Pz+Per0eEWP5vueeew7FYhHHjx83Mda4Z2m327hw4QIKhYLre+W3cQEOio1ANFNVrzs6G5o9y4WsarWas810Umk7arWaa1GjtsiffOr1yvsCF8ShEKh2mPcNOsbMzlThVh1gzYigqKgtc/x+jHHR83q9jq2tLdfKoVgsunMLHMwy88ftj90//8wYoNPLrAeK50EQYGtrK1Jpwf7eek/iedHxq13n96w9yXhPZoBL7aYK+MB+Rpe2eFBxg8fI8rZEIoH5+Xns7OxEhAGO9/nnn3d215xU416kXq9jdXUVCwsLWFhYcH7ujYRRP0CjNpjv0cx9Eideqj3UBAa+J05U1O3QV9TXdaxxcLs6OY8L+scRlyigtpL3Ewosbydax02ifbFa0cSDMNyvbNPP67xAX6Nfz3UoWDVBv1+/v7iEB3/e4J8THb8vCtPn7XQ6keNSUSOdTmM4HGJ2dhbpdDrW5r722muuYtFsrnEvsrGxgatXr2JmZgazs7Nurqzze6J6gz6vVWEA3Hy21+tFdAkmd9H/5bXMRClmyfvCnrb+U5unQe1cLhfxsVWDUH9Zg3l+ggID/yoMakBe/V/un+cilUqhWCw6W852kDyf6+vrrj2hnks/SYrHTbhPJhFotbDf3sz/nAqyDOhzG/l8HqVSKZJMofdQnnMmNvC86HmYJLzGBfHUFqtNVpvLJIlKpYJUKnWgZ3cYhnjppZdw7tw5HD161GzuIWBirMAJGZ0dzb4hvOg0UqKRI3VegP0VmCl88mID9i8g/qNTtNMLkQZH+xuqsYzL9uGx0IHzM5C4D3WwmZ3LMlFG1rT0V0VS7X2iBoXH2u/30Ww2sbq6CmCvhcCRI0fcwjg8z3HZYRoh0n43GnljZhaf4/mJazHAni4UvrVUlQJNt9tFr9dzLQJ0Ya2dnZ1I/95cLueagnPST+GAGW9+5pp+PzyfXBxBb2Z6c2WWBY+tVCphamrKLXB09uxZdLtdfP3rX3fjXVxcxNNPPz1xQmIYdxss39JyVZ0k+tlP/oP4QRD/dT9wpava0uli5qg6b+oM8XpmdqY+x89x/8zs9CfavIeo8+q3P/DtNM+D71Cq/U8kEiiVShiNRtjY2EC/30cymXQCc7VadQK32nBuB0BETPHtF8fD92sGHCs99DsMw9BNtLkt3jt13Cyjo7hLp5ffv5bbxTmkKhb72QD6two+/v2Fx6T3O+6LfTUpZK+vr6Pf7+PZZ591dndhYQFPPfXUhP9ww7i7oF/EtlraQkoFAA108advM/zfARyw3f6EULcfl20UN161Q3EZRjfKevLf509UJwnIej/xxwnABdM7nQ6azaa7h3DdB1+E8O0Xj2lS0NF/L4N5HL/OPzQQqLadY1ChxUfnMrw38DnNcKYN1+Pn+dHx69/qn+v3rnOa4XDohGwATsCYmppCsVjEuXPn0Ol08Nxzz7n7w+zsLM6cOWO+rnFPwJ70+XzeLZ6omZvEb3WnNlkD15wr81rne2h79TrVIJZmvvrBFr8/tO5fK2b9+wT3ofvUwBevWfrT3Ifaf53bx13TicR+i6x8Pu/akFEk1gQwVvxye76PrvNzRZ9rt9vY3d1FtVp1/vPu7i62t7fR6/WwtbWFarXqEkjo0ycSCVSrVaf3dLtdNBoNd340K5lj4rnV885xxuk7/j2C71Wf1/eJfX+YWgPHwraa9HOvXbuGwWCAb37zmy4hbWFhAU8++aTZ3NuAibGCGht1Rvxor1482o8D2I8e66ScE3M6Q36EWPv48YLyRVv/otJsLsL36Jj5nI6Zx8dt6fb8rE/NTvNLo3yHy7/QuXgMexXWajXMzc05wSPO+KqIyu1qRoAacV804fGos8n3ZTKZSNkrM5YZ5WO0UDPWaLA7nY4zbCyJ4HdMMVYNnZ8Vp9+PCi/8vvUGxeNkDzf+3wBwPb4WFhZQqVSwurqKVquFl19+2e3jiSeewNNPP32D/3LDuLvQ7FUtW+Vr6rTpNeJHhv3gEOFztCW0bRT/2KtVnWAgmr2lE2AVANWJ8isW4hYI84VDfQ+PlfvR1jF0yNSe+MIjA2jNZtM5f+zNx0mt3pc4Hj1PfoZanECh9y61gQxg0ub6i6rxM1pVok6mltXpfYx2WO2s2lJfhNBzrd+TjkPFCv/+rqIPAGfzWdmxs7ODXq+H1157zb3nsccew/vf/35zUo17Ak0q8NtuxYmOii+I+v/zarPVZ9bXdFtqT4hvd/xr099nnEDMbcZlu/p2ZNJx877hCwj6GjNiu90uSqWSy9TSNQz8Sf/bHUvc+3k8KnbqufbnI5xrULzhcTPYp/cUtct6vvSc6Pb0POrvPBY9X3qu1bZqgoWudg7AraFx9OhR1Go15+uePXvW7e/UqVM4c+ZM7HkyjLsN2grNTKWfAxxMkIoTYhnIJn5/aPqNzLLkXFLx/WTVK3z/iT/94Bywb4v+/+z9Waxl2XUdCo59+r67fRcZERnJyMjMSKaSZJJKdqIsUbYaS4bKdrkpv3p4Bf9UoYD6qvfn+qiP91FAoQADr2DY5Q5CqV65UwE0DMm2LMuUSIpJMqlMMiPbiMi4cft7+r7Z9XE51hl73n2CycykeCNiT+Di3nvO2XuvtfbZY8015phzWX9ZxQJh2KvH6/kf5Dvx3FpmRfkUAAHVL2tThwXwiI06HmHjo3vLJJNJFItFjMdjtNttTCYT1Ot1d77hcOhKoXme5/bDIbb1ej0n5uI4895a8YHlDPibhKuuH3Ssrb9O0++GCuZsOQoqeineaDQaGAwGePPNN925BoMBnn322YX3KbIPbxEZa2w6nSKRSCCbzQYcGyVEqX60IMjjlTDQhTnVjnwgWBeWi0pdfCswK2ATfKg8spuRWEeX5wpTYCkAKoFIp45EIItO83W7OQEwd3q73S46nQ6Oj4+RTCbxzDPPYHV1FZVKBcVi0anR2D6CHs9hyx7ouHMc1cHXSUrTbNWhjcViWFpaQrlcdmpWXp/EMyfKdruNZrMJAKhUKm6DMQIhx4SKEm4kk8/nnaNL5W2v10O/33ftZUSMilgdV36G6gAWeGd0iu+dnJw4oL969SpWV1dx69atcxsfRBbZw2LWGdPaexr0IFmnmMOglaovgWAtJqrY2+22+3w8HncR7VKpFEhHYvCGi1TrrPGZ5VzBtqii1Dq2TIFXMlSJTAbD1KHVRbYNvNFYG4u4yDFLJpMol8soFAooFovniAFeW/GV84yOr27AYIlv/k11KDdHpONJBUMsFkOj0QgQ2p53VsN2eXnZlWFJJBKB3bY59+ixGlDk9XlPrNpBFc+e57nSE0oQ8HPczIK11+7du+cwle0oFouIxWJYWVlBoVDA4eFhKOEeWWQX3abTqautR7+QmKeCgbAAhVW3PojMpH/Fc+kzGGbWn1QLOz8/9yBSWM+tPrs9Joxwtf4/CYzpdIpOp+PwKZVKYWNjA4VCwZEFbJuOaVg9cb2+tssSCcA8Oy2dTgf2m9C5g1km3CxY02CBed1uJQLYT9a6ZeCSpdPUNFMOCIpFeB7OLSSC9Tuj8+NoNEKr1cJoNMLR0ZHDU5aMY0bb2toaisUi7t69G6rsjSyyh8H4PDIgwqC1xS77v1XLhmEds8l4Pr2WlooCgiUH+b/6m1qySQNWXDOzjYppOh9YLoJ75/Cc9Hl1TlF/lOchscwsjvX1dUwmE7z//vuIxc4yW9vttstsJbbpJmNWwGC5HPV/bdka7iOk4jsKy5LJJAaDAQ4PD9FqtZBOp7GxseGywSaTicPl8XiMSqXiAvpUzGq5GKp9yRPxO2LbTFy2/jgxltwQcV/HgKUa6e+enp46zB2NRuh0Om6OZjnJ/f39yM/9C7CIjA0xBRh13vibi3maVYTyYVElD51fEnK6GGa6vYKZ/bHX0gWlVTVYJ9ZG3/m5sKi8jXTzM3qMHRcFV62DUygUsLq6iuXlZdRqtXPKL20fr2d/LAhZFZP2R1VZPIaf40Ijk8m4iUHTVAleGl3jPWLxbYIjJxtgDnwkJEiOkCjS2juaPjKbzQLkDcdGJzFLKjMVuNfrodvtug3frOKLgBrVeYnsYTCLKXQguMusKmfpEPG5UIdTn3cguMM1lUt8PpXcpDJWHVJ10DSoBQQVA8B5R1kxS18PU4wtetbDyAq7kCUJTbzl+yQ9stmsKwmziFRWPFZsJaboPBHWVnUCNQuEjjJJHr2/PJ73gDUC6ZwzKMf7q4sJDQTymvo9sum0GoTUTdPC5lV+j2wdWs5tdJBZf1jvEVMQGaSLLLKLbIppGgDSgMyDsMieC5jXzrPXIDbZ8z/IrH9H03N8GCPehZEaYZhm+0k/kL4i5514PO5SPHO5XKgPHmZ2vLS/YWOg6lQlWHQ9oeSIJdGJZZr1p7ippuSEJWRtn5SMtW0Om+94fqvQ0jXWYDBAp9Nx84TdFI2YS8VZZJE9TMY1K2u72sCN2qJglz6DGnzmZ9Untj6N9W+JISRGdU5Qf0nX/ZYbsD4sXydJqsfymQ8ry8DjVRxBVSxLEdC/ZLkCqo15rK4r+FvnPf1bsc6uR5jlOxgMXAm1VCqFQqGA4XDoNsBSgQeP1XvBDCsAAX+UKlluGKzXtiVi2Da7/4EGD/U9FbjxfT2/mq6VdNwjP/cvxiIyVsx+ORmh6Pf78H0fuVwuUFuKxEA8Hnc1YHQhN5sFU+YZuWEESzfnokScgKTlCoD5w6ORHBtNUuDj+/pZgp4FJPadhAWdPZYT0NqAs9nM7dLNouEAUK/XXZ3YZDKJK1euoFKpYHNz0/VDI3c6ibC/CookG9RhU+JSQYaKOb5XKBQCET91EldXV5HJZHB6euqiS1zwk1Blnd9CoeAAMZvNYmlpyZFCPO/S0pIrKM5rdLtdHB0dubFjX2xJAwIy1Q2e56FaraJYLLqUiEuXLuHw8BCvvvqqS7/odruBlIbBYODG8vbt2/gX/+Jf4FOf+hQ+97nPfRyPRWSR/VSNONnv952ynv8DZ+U5crmcCzzQgaHalTjqeZ5zaOjsMTg0m82QzWZdfcTpdOrU58PhMFBkX53GTqcTWMDqJlrAnADUIJjWjiXRS0KU8wdxnZig+K64TYzWmty+76Pb7bq6VZPJxKXFsu4TcYlpUYqDxClNHdayNRoUUiedxvEjScp5LZVKOXziPMO+EAfZ736/j1qt5uYHBsGoFqARe3XBrlkTYSopnfP4ndCaiOwz+z+bzVwtslqtBgB49tlnsbe3hz/90z91/dzd3cXe3l6AlKHdvXsXv/u7v4sXXngBn/70pz/kkxBZZH8xFovFXLAmn89jMBi454rv6cKNpr4an1NVZQJwWVXqr/JYBkYYKLbkHNvGgBLPoWSglrhSAiCM4NXAOTFag07Ec7aN+KPPtiULqTglNjEDoFKpuLlBfVUeF6ZmCyMhiMWW7NRxtAF9+uKaRVEsFt3cSCUW7xsVvDaQxv9Z1itMAKFrG7aJC3m9/wAC5INmsEynUzcnLi8vw/d9rKysoNls4vbt247wJuZaVR4A7O3t4Wtf+xqeeeYZ3Lx589y9jyyyi2b00XK5nNvEulwuIx6PuzU08YV+k/qONoCkoiziBsldEoFKhvI51+CODbgoFinmUHGrNfw1qGNJPx6j2a7D4dDtKaBEH31c8iB6fCIx3wCdGQHx+NneNvV6PeDf6riE4SdxWfHcBvGt8TjOkeQyCoWCKwfGDRVPT09drWve22KxiOXlZSwtLbnNg+lzEhOTyaRbK3BcdGw1Q41rIxWTse9KshKHOTf6vo9isYh8Po9yuYzZbIbt7W2cnp7i7bffdmuM4XCIo6Mjdz6dC3d3d/Gv//W/xnPPPYdPfepTP+nXP7IHWETGis1ms4DyUBU5ajaiwi8sHR8lFe3DzcWwRqv4ZQ8ri8DrKZjQrBLLRnj0MzyPOtL6o8ogtgMIOtp0SrVGFs/LYtd0tLlzKyX7lugOi5bbcdP22mPDTJ1aRpQ0FcLzPAeAJF7p3BGwSJAS8DkZ0PHmpKgLEUbodMw42XAC04iVlmVQFQb/5hjy2uPxGEtLS27c6QSTZCgUCoGx6na7OD09xd7enut3rVaLIlmRXUhj1gCxl5vWhSlPgTmOUZmkaZf2R0k3Bq7UoZzNZoF6T2FRceKBbtoIBPHGOsjEIFvHz2IX26NKBk0r1WsoljHNiGVS+Gzzurz2gxb0YW0OUxNY51xxWklZq+onhurxxK7BYOD6mEqlkMlkXC1WLiaUZGFfdM6zJDhNnWy912EZJPqdogKb35nBYICVlRUMBgP0ej3X11Qq5XCf31+SNCcnJ27TSuJupNqK7KIZiS1mAenzoQs7NeunWQJU31fcClNSaTssJqqQgMfx/GF4FraA1n5Y7NK/iQFhfrQNhPGHeKK+IeuP2z5avNX+kkzV8Qsbc4vJfJ1zi35GsZDELANyqkbj57lmYVk2HkcfVzGdxyihSqzluNi6h/yMjqslTkg00NfN5/NuLUIyn/OZbqxL8rzRaDjywPM8lMvlCHMju5A2mUwckZfJZJwvAZzPHAKCm7YqTmr2p66V9Tw0xYqw9ii+atDaYqv1R3+c6fn4PzEvHo87zOF1xuOxExPwOI5PoVBw5KLyMhqc0aCREpAWvxbNFw9qf5iilLify+UcXpEHotKVm4FTUZtMJt2GXrZkj733urbQ8bb3kuS61qPl8Wyz+tL6GkuDUfzF+YDfUwZm9Zhut4tGo4GDgwN3nUqlEvELH9EiMlas2+3i9u3bLgrCXfQICEyTUceKqiAuLkm+MT2d9UEnk4nbLEqBlotFqrxI4mmaABAEal000qhYJSFnyQCSqDxHr9c753By0UljOyqVSmAx3ev10Ol00Ol00O12MZlMUCwWkcvlsLOzg0KhgJWVFUcgAMGaiey/1sgFEHDAlCzhJKPjAczJawIzCcxCoRBYaHS7XQeIVJ6ORiM0m028//77AM4AjaQA7x3LKxSLRWQyGZRKpQCoKZCqypZKNXXied+m0ylOT08DfSHpw75rJIylCNbW1nD37l28+eabjojZ2tpypLfv+2g2m06dfefOHbz22mtu7P/6X//r2N7e/tielcgi+7is2+3izp07TjVAxUCpVHLp9owcEy+4ezVV4dy5WutvEScLhQKAeZoWyd5Op+OwW51EJRCorB0MBg5DlMhTR5PGRaO2hVhhsY1KAS0p0mg0AuVN9BrEF5LWxIpyueywgo6a1oNUUtoGwhSXdbGuhCf7zDmPcxbVDnTseI8ABIhVtoEZA0z5WlpaQiaTwerqKl5//XUcHBzgqaeeQiqVcnOUqrXYVuIq5zy7yFfFCEvOhPWbpulkbO/6+jr+6l/9q3jnnXfw+uuvu+8ba0OyTnij0XCKlnfeecdt7JVMJvFbv/Vb2Nzc/PAPR2SR/RSs2+3i7t27GI/H6PV6qNVqgfRKDejb+n98rhio1+ATn30GqHUjWn4GOK/6tAStqqx4rJpVbCoJqCQwMU39TA3QKM6ybyQG6Lezv8Q7zcJIp9MoFosBrFZFki7cLUlCfzCMpGY72Sf7w/knn8+fU7txMa5leIB5/V5VsLJO6zPPPOPmDt6nZDIZyN7jWFAFxrHg/MlxVrKg3W67PpM8sWSs3t9cLocbN27g9PQUh4eHbq5ZWlpySm4ALiV5NBphf3/fbeyVSCTw1a9+Faurq4gssotko9EIjUYD169fx7Vr11zZLGbVtlott35jsJd8gPo9wLw8nqrd+R79ScUbkp/WB6afBMARiEr8KjFoSWNmLmiWl6pzVdWra3L2bWtrywmcrHiCxCUFR/Q73377bZfRpsEach7qm4fhOttvBVIa4OdnkskkMpmMw3n685xv0uk01tbWUK1WcffuXbfhFeeGYrGIjY0NdDodNBoNd54f/OAH2N3dxfPPP49sNut8egZFdc5R0p394L2yfeB3gmsiJb/DiF7+Xa1WcfPmTRwfH+P+/ftuzbO8vIx8Pu/mD/Izg8EA9+7dcxt7JRIJ/Pqv/zo2NjY+lufkcbWIjBXL5/O4dOmSUxEyOq8RaF3YEhxVCm5VO6rs1PeBeUqoBRG+pwtRdTbVWdZoERevjArTodS0fzpXXEDrQp+REYIbF7G6SzaLPHNzKgAuTZ/p9VrbyQJtmDKCpu8p0WojRNo+posROPX+KNnJ8aLDWS6X4XmeS1MeDAaO0Dk5OQEAHB8fIx6PY2VlxaU4K6hxUuN9oxNPJ5FjpOS7giqJGX6eSj+rSOAYF4tFrK+vu40j2F6OnU4c7DuLm7/77rvo9/u4fPlypBqI7EIZgzjVatUFRFiWgEEwxQNNfaUDEqY+VUwGEMCOWCzmCulbkkCVWTyeGEIVrj2/XUjTVF2gTjMXuOyPZh4o0QoEF+V2Ycz0euKu9sMqAHQxz/Hg6/rb4rNeV9VhWkrGEiJK6qpzzOM7nQ6azSaOjo5calqz2cT+/j6Wlpbcpol2ftR28r7xe6GLfsX8MCUE+6T9433muHAxkM/nsbq66uqVMTjAceIiJJPJuAUWx+b27dvo9Xq4fPly6AZskUX2s7BMJoOVlRWUy2XnN3BDUprNALDPkcU9/k3/hfio/rNVLdGfCsPQBymZVJkbdixxxi6wLamgWKz4aHFWF8ScjzTgFoaZYe3XcQzrl447+6nkMdvC+UnnKnt93bgQgPP5efx0OnUBJm5au7S0FPCdFRc5fkw95rn0XrPNeryud8LaqoQQjfUVia0sJ6FzN+dNjgvrRt67dw+DwQCbm5sR5kZ2YSybzWJjYwP5fP6culPXfly/aiaWXU/zuQgjHJWYVaWkPnf0m5Ts47Otzz9xhO2xeGp9SZYM5LOrWaOa8ZlMJl2Nbc2+ZUDfXo/Cr3a7jV6v5/AQCGao2UwM2iKewc4fekw+n3frEOU0dK6Ix+NOMML7R16AWavEL/IDx8fH2N3dxerqKorFIsrlsmujBrXUj+YP39MAo659GCTV/pK3UJ9X1xTE5Fwuh2q16oh6e290LWQx9+7duxgMBtja2ooUsh/SoplKbHV1FSsrK24hpQt+ggqdMI1Ca8FjOjmUqvPLrg8/MK/FEY/H3cKTr2vUmcadnAk04/HYPSystcj6JawHQnKVqt3pdOp2LgUQSAOgIrTb7boHl9Fn1qOixF53t2ZkaG1tDZVKBUtLS24MSJLStIYN+wrARW6Y5qRgwc9pNIr94g6KlUrFKelUbcBz8X4wpSyRSGBtbQ2lUgm5XM4paPnZ+/fvo9lsYnl5Gf1+Hy+88ALy+TxSqdQ5UpvlLAiM/X4/QMZy7FSZRqKANWCp3qXKmipfnSCSySRqtRqWl5fR6XRc33mtWCzmCGYAqNVqGI/HePPNN3F0dIQ/+qM/QqVSwd/7e3/PgX9kkV0EI+4SPxmRJd7qs8DvO51EW18pLK3SYisxKJ1Ou+dQyUJ1clWRoDXBmaJqFVs0DZrp4p6va6DMOuSsHUZcUYKAlslkkMlksLS0hFKphI2NDZdaSqJUnUxti6Y4sT28hh6ji2jNWCBuqfPJucT3faRSKVdSRcec89d4PHZlbQaDAa5fv47NzU3s7+/jhz/8odutdnt72+E1x8/uzs5sE6ukU4fRkrP6OvtE4zzOOl7T6Vl9xevXrzs8Z2kInaM5j+qmZKPRCF//+tdRLpfxt/7W30KxWPxQz0dkkX3cxp2dScTSl6T/p4EtiyHA4oUuTcm7WGye6qj158LUrxrU0QWk+tA8Fpj7lPQzPc9z/pwSqnyPuE3TklskkXltG3gn+cosBE3f5PkVT7V/6tNaYpvjbdcISm5zjlBClHjIMdOx9H0/QBRzgZ3JZFwZAKpKW60WyuUyhsMhLl265MaH5bQ04KYBLwYySdiyX0pcK7FgF/V8T+d2XXMtLy8HlMhsEzMqSMbmcjn4vo/d3V00Gg288sorKBaL+PVf/3WXFRNZZD9rq9VqqNVqriSTEloUVAEIKGKJV4oxVI5SKEWs5jOpynff9wOCIOUweA6Sh8QVPqfAGY4UCgW3HwBrT9u5gD/FYvFcqQHgDC/YJ/aLOMoasbFYDJlMxq1PPc9Do9FwZfdOT09xdHTk1sm8tpKxfE0DQkCwfIvOaXazQz0XfetarYZEIhEoVaWlxFKpFEqlEuLxOJrNJnzfR6fTcf2ZTqcOo7rdLt555x289tprjoe4efOmE2OwrcR8cjdcCxCTFTc1o4HGY+iPEoM5h2jpNp67Wq068lh9beI/SzxSfFer1bC7u4tms4lvfOMbKJVK+O3f/u2IjP2QFpGxxuwDaRf6dP4ISlQxEZi4wGPJAW4IAszVrfxyc0GuxII+5ARrYJ5yCswf1pOTE1fegJtAcUEOIABOBPtcLueiOHygmT7BKAswVzcR3NlOnocKTNa+Yf0bjdwxyqZja4lWXosRJr0H/Fsd4+FwiF6v51SndMji8bgjJWkEDT0nJ6NarebOwfurC4XZ7Cz9tN/vY2dnB/F4HCcnJ+7+snarEqhsnypdtaYNgV83vuG4cjGhqR78LMkgtks3UCOZoEo+nmM2O9tgLJ/Po9PpBDZsiCyyi2QaGSdOKNmpilJgng4aRhgsUnIpsalRfZs6G+b4slad1stjwX1dgPK3kp8a2OLrSujp4pRkp9bZVrxkf+mkM3rP5584pAt8u0hXvNBxsIolJZTpnHFRb8kWz/MCqb2cE21mQyaTcQuEXq+HePxs4woGA4m7sVgMly5dCmAf26yYSvJdg122X0qI6I8tI6MkFL8TXICwPq/ivSVxOd8xuEkCRYOtkUV2UczzPBfE5j4J9AEtcajPE41+qmYgWQJVFVjAnOCzxKua4hH/tsQd01+VPCD22LqoSpJan0zJPlWA6m/2gz/aZzueOg6Lxk/bxbZb4lbnEa4VGNTSY/m3jrGOhyrG2P5UKoXT01NH0sxmZ/WuuccDg1/2ntj5xc4Bei8toa6BPvaBgU29VzaAqXO+PTfnE9rW1hZWV1fR6XSceCayyC6K2eeVfh59Q30+lHDVYy3pxvfoN+uam34KMUiD0vSj9DlVv5siMQbplI9QEpfX1D1O6AMpSUli2Pd9t+khM7kotmA/2B4KJQaDgVPEEiPZbhuk43u2riqN/SDe6DzEfjA4WalUkM1mAxtns8+KWxyTbDYbKAXDoKDneSiVSs7v531ot9tOdQvM5yJVTauvaYOD+sMxB+YlEzXjTr8z7C9/kxxnm3lNzgP8blJsocHMnZ0drKysoNVqOUVzZB/OIjJ2gSk5wAdb1a/j8djV6KTjycgHU00ZHQLmhCNBWB0sOll8mO0upXy41ZmbTCY4OTnB0dGRAwCWV6DzououPtgstEyp/2QyQbfbxe7uLgqFAiqVCgAEnEAuQDXNiTvysXYeI14EVAVEdVLVKVNHVdUNOjElEgnXDhKx7XbbRaDYP46rRvdIVujmPARd7s5Yr9fdfSFBQrBlbZpLly659AIS1NxwSJWwqiAjsOmmbmwXvzNKsHMS5f1WopVjxv9JBOuu5iTVtW7jbDbD6uoqUqkU7t+/HzpxRRbZRbGwBZ2N6uoCU2sW6mJRF3zAPIocRs7yOtbRAYK1CDOZjHv2qOBXEoJGQkDb4nmec3JIZlIZSnURSUcSeLyGLrpVYc+xyeVyyOfz54JWHCdgriIjJthFcZgRs9gfOsX9fj8Q2OJ5SDjTeWNfOJacG0hSqjL45OQkEKGv1+tuTDQbROcxPW8YKaPfKd5LxT4lzDWYZj9DnGXfLanOc3OuoUPP67bb7cD/kUV2kYxKpEajgXq9HqgZyGeB/iaAwDNCzCEO24UiF6kMVChJSVO818Utr6t+sG4um8/nA5lXqoLVALV9numbEUM16MWFq5KxmuHFH5sRQf9VSV9d8PPa+pvXCxMoaF80GKYBSvWhLZYreaLXIsFSqVSccp9jzD0g2D+S0Oyfzis208AKLtgGe191/uL8rbUS6eOGkbE6J2uGh64vVlZWkEqlsL+/796PLLKLZnxedR1pA8H84edVOGBxjRigPhD9MT4HfJ5oXDPqMXz+ADih1fLyslvTaoasYn82m0WhUHDEJXE/nU5jaWkJ6XTaZaD2+32USqWAMEjFEPybOM3xIT5pQMquF2zAkLVONZhPwZbiFLPvWOe1XC6jXC47xel7773nyiSoD6v4Stxk8J5CrcFggHQ6jXK5jHq9HtjXp9VqoVgsulrtJJvZd+UV1B9XHOZ6gWQp+QUNJCrm6ga1/J+fZXu5ViEZa/f94Vh63tm+Op7n4e7du4E5I7Kf3CIyNsT4hWLqoUb6CUKxWMzJ8blI1B8qRVkvROuN2siXkogET93VTqPs6gBRscW6oIxGqULUgnCpVEI+n0cmk0Gr1cLrr7+O2exMQVmpVFCr1VCv151jps42QX42m7nIFkEom80im826UgH60KoSSUkTLeZvndd+v4/RaITj42MMBgPU63UHzgS6bDYbiGRpOzledKC1niIANJtNTCYTVKtV9Ho95xAnEglcuXIF1WoVx8fH2N/fd2kkXIirqpgApQ6jLgo0ss++c3LTzdKsipiAr0S2Rvs4ac9mM1fnjfdC07pns5mrMRvVz4rsIhs3ECSucKFMZ5RYApzfzI+mBCgQTMO3hKs6ODyfPsMaOOLzmE6nXcS70WgAgHMs6SBpGRjNgtDrMYVfI/PEev0sMUmJY2IO+0BHmRirhLCej/ioqa1h40iM4rm5aKCzRgeW9cH4mXa77fDOjicJ72636+a76XTqAoHtdhtf/epX8aUvfQm/93u/h9PTU+zu7jqyWQNdSk7wfnNMdLHCe6LzAe+LBkl5b9lOvs9x7HQ6aLfb7hx67+jQcoNHDcQpgRVZZBfRTk5OnKiAvpL6bDZ4S5+DZhWYND6fJARYZoulP2jq8yluK34DcM8+1ZyxWOxcNpaeU59PLsI7nY5rsxIQvJ4SyfycCjIscWrJVp1bbGBMSyVoO/V/xX/dzFXVYBog1HGeTqfnSjDoXEaChP4gcFYTcWtrC9Vq1X0PdG5UYtsG2SwpoGNniWXr/+q8pyo7nT+J43b9w3uhY6KKX12v2eBrZJFdBCP2cD2nfhIwV4Xq889nm99/ILghomInM0zb7bYjOKmAtEFp/q84wmtzwzy25+DgAN1uF71eD7FYzGXFlsvlwJ4qxKJcLodLly5hNjtTgXINPR6PAxu7EtfYz2w2GyB74/Gzzcc068EKIBR72SeOA8+jojmel2UVarUa8vm82yiQ1+O8qL4yeQeLw+RFOMeRnOW8WigUEIvF8Bu/8Rt44YUX8J//839GvV533werXrXrGIvnir96jAodiLOKm7yOCubIUbFMgWZ9Kf5yflLjOSM/96NZxM6EGMGIqS4arQfmX06mI5IAJfGq0SM+lFZ9BQR32KYDRoAiMPKLbyNAwByMuRin2pVRLzrJBCYSl7lcDsAZ+XF6eurAqFgsolQqodPpOKdPVQB0mrRPqtLiNdh+HS9VpykpqQtrgqZGDU9OTtDpdHBwcODqsPKcpVIJ4/EYnU4H0+nUTTa8tu/7gXo0XEiQBCDJQsUFbXl5GYlEAvfu3UOz2XQlHJjGN5vN091UUWUXI2yDdfKV5OC4qkqC3y397jFSxeOVnNJopt6zWCyGbrfryIEILCO7yMZgS6VScRt0aGSfpvhpny8lb3VhqUqjsAUznxt1SlWBpEoFOjtaJoXOHlORFJt1AUunhYQoo/eWPOazah11XYzzsxrksg4a8UHJXfZN+23Hl9hM55mq0MFg4OZFjgmdbCVjdbz0XjFbAJjjIFNmf+EXfgH5fB7//t//e7TbbZycnATqEmomiWYMsC+8Huckfhe0X0rA6N8cLwaymE3Ba2qJBn5PeC+4+RHV08xwUEI8ssguorH+PDcApN+ghCmN7yk2qc+iRgxlkAkIYpcu9miW+LWLU8Ul9YfsuVX1o883/WJNi11kqjgNI3v1NxAsTcD/FY/ZX6vWVOLbYi5xkjjNYKAlg3kM5yltJ8/LazEQCJyRNqxhSdEB/UStC87XVDyhhKz6tva7oEStGud1YL4JpxIF/Ay/i+wz+2+DbFqySAngyCK7aEZsBII4w/fUB6XPan0bAOeeAVV76rX4PoPx9I2sz6cqdA3AA2d43Ww20Wq1MBgMXO1pZmcxEMJzk/ytVqsYDodoNpvnBGv0O3UMeJxmn8ViMcctZLPZQDvV19V5SQPuNsg3m81ce4vFInK5nNtMi+r6VCrlNirXY9XfU1U+8Z2ZyUoA874x6+TmzZvY2NjAH/3RH7msY4okVExBTmgRfqrPr8IEFYBoBp8GwjQIBuBcmUQVqOgcyrlG5x7OL5Gf+9HsI5Gxnuf9nwD87wD4AP4cwH8PYAPA7wJYAvAKgP+N7/ujhSe5gPbnf/7nePXVV/H5z38eV65cQblcRjwed4oYJWOn02lgoxKSe4PBIAB8+nAxSkRAqlarTnFD01QcILghDM9PsKBiqdFowPM8l0pK9Wo+n0ehUECxWMR0OnXHkajjzrrJZBLD4dCpnpiWoH2ic8aoda1WQ6lUcspT9lHBXp0kBRaN8tE4tlpegJt09ft9V/icY5NIJFyKBBUQp6en6PV6aDQajiwvFApIp9OoVCqOpIzFYq7eYi6XQ7PZdMpb3/extraGXq+Hv/t3/y5ms7NarVeuXMHTTz+N733ve3j//fcBAIVCAX/zb/5NFAqFc8qwMDJbo5xK1IRtOqEpHHR+OW5sJ+visu5Lo9EIbDQzm52VXMhmsxFgPgL2qOLurVu38P3vfx9f/vKXce3aNZTL5UC9VpKKtmSLGp0fjaATt7jZnhKhqjT1vLMsBQatlIzVv+k06XNL4k1TZInXJCWUpBsMBm5HWZ6fagEqDDivsJ9hRHKr1XK1wGm6WKZTSsW8OlgaSAtTOpFwJIal02m3eSUxk/itY9HtdgEEN1dgu1h3S4ONDLK9+uqryGQyeOKJJ1AsFnHr1i2nYigUCigUCmg2m+4cyWQS165dC9xH7nSuJR5UeQEEFbJhjroGwnh9YizHSzMkSMDzewXAZSMcHx8HSgdF9nDao4q5t2/fxhtvvOF8XVUiAkHhgS72uYgG5s8Ty0sBc1xmEOPw8DCANarGoim5q+cm1qRSKVQqFXjePD1yb28PAAK1Q/kMW2URBRb5fP4cnnKByedca3rzWGKfqt4XKZpouhjm58P6y2OJ2SQfLMlLooDt141ndV7ktdXvtmrX2Wzm5qD19XWMRiP883/+z90YlstlLC0tYW9vD41GA75/tjnjs88+i3Q6HZhj+cM+kWBgv5Tssd8tm42gpApVZfo94/eK91VJklgshrt378L3fdy4ccMFOyN7+OxRxdzvfOc7uHXrFl588UVsbW0hl8s58ozPB59VYB5E0frW9FktnvCZAYC1tTUnVtI1tw1SqZ+ngfvDw0O3lh4MBtjb23Oqda63k8mk2/Rcn10+k9zMtV6vu+vQV6LilM84sZ1ErO5vw0268/m8K1eo4izFGmKPls9hP9W35f9agoFEK302i5v0BUncWn9cA10A3P46nuehWCwG/NLPfe5z2Nvbwz/+x//Ynb9Wq2F1dRX37t3D6emp45Q++9nPuk3A+F3Qe0WSld8DYrXyVeyL7/uBDDcAjjNSAtxyGGEZ0ty/p9FoOFFcZB/OPjQZ63neFoD/I4BnfN/ve573vwD4XwP4VQD/d9/3f9fzvP8ngP8BwP/8sbT2L8iazSbu3buHk5MT1Go1J5u3jqMuuPnl5OJSiz2TeOMD1G63A2SbOjY22mEdKqZBkkwlKLH2EnBWGN+mpNNZAYK7Y9NxpaPJh5oPqi7elSjUjb00QqQREyUSVAGgYwgEVWG8DkEil8u5vhDUCEI8lgWoOZ4aJef9Ya1HTdXl56hqsoqrYrGITqeD7373u/B939XR2d7extHREXZ3d106tarTbJ/4W4FKFV0AzpU1oLPJdimxYxUdlkyggo1RPUY6qUCI7OG1Rxl3O52O29251+s5h0jJPFUM6aKdz5I6RvxbN9sjTqpTwd905GjEEHVMSKjq801cVLOLT4sH9hpWIcFgS9h56dQq+btIJcFrqGKeeK2vs0/8YfuJlxwfHSutzU0nn0Q075ca7x/bqIEoBowymQxyuZzbtEHbkMlk0Ol00Gq13LypqgDFU95Xnbf5HsfPkrHabs45ShioagOYqyGUWNYALHE37L5H9vDYo4y5vV4Px8fH6PV6gewZJV8tqUnTv+mf6Od0ITgYDFy6qZqewy4G9W8G+vnscqGpvrf1L9UU24h3dpHKc9v26ef0f+KNPYbX42+7UA5rk+KVZjYpPgLBun06N2kZBW2v9iOMtOVxJIRIZNJXzOfzaLVaro63ZpJZgoPfA+1/2LjZ/wEEfOOwcbTYbs/BQEAsFgsEMSN7OO1RxtxGo4E7d+7g6tWrLgsTmPuCGtTSQIX+retdftetklEzABY9D3Z9yrWsprpzbuh0OpjNZq7kjG1LmC9MH5XZYvF4PLBe1tIrFtfYJ/UvlUxc1A/FBJqun3UtzpKObJ8KmIA5Ka4b0hLzldSm8X6w9BU3qSVxrWUVt7e3MZlM8K1vfctxExTndTodNBqNwHfDCh/UN9cx13IE2hd9jXO1ErvE9LDgl94XLROjnEOUffvR7KOWKUgAyHqeNwaQA7AH4BcB/O0fvf/PAfxf8JCBJe3WrVs4Pj7GjRs3UKlUsLGx4Qrg80vc7XYd8TUajfDuu++i3W7j8PDQkaYEFBoJQ6bQE5D0M1Rz6QNHR4rEgjpYAFz9k83NTRftJpCUSiUUCgUXsSHxR8URnbHZbOYWudwFutPpOBJDFWEAHPFMAlgdaH3QAZwDXK3zp8pbVWfozouz2cztltputx24MGpGh3Zra+uc4ot1cO/evYvpdIqVlRVHeJBgINGTSCRQLpexs7ODVquFu3fvolQq4bnnnsNTTz2FGzduYDAYYHl5GdevX0epVMJoNMLR0VFgcxntk94nez3dTI33lJMho5qWlCIJDsx3KKaamdcj+MfjcVcnLLJHwh5p3B0MBoHaVAzIMPDBHUmZtqn4SudPF2R0pEim0Uml86PlP5iaSWeP+Kw7v1LZTiUDo/NsCxeszCAA5gp3EslM8dLMAH6Wz74SgVYxz8Aay8OoIpTOlg2m2Qg3nWC+HubQEUPT6fQ5ZRPPw+unUqlz94eBwsFgEEijUqc/TInK8WJdw2KxiEql4hQTnIsZROSPbkqjfdXvhhLuYWMUj8ddIEvnXs5ZugBSpYIqMnK5HAqFglPuRfbQ2yONuXfv3kW/30e1WkUmk0GtVnPPIZ8fXYwqIaZ+my6kNeBBAm84HAayHYD5orlarbqU0VQq5UQQnufh+PgYp6enODw8dJvkep7nMDeVSrkNYthmXVTS3ybWaRDL933n22oWhlVu6uKU5ycZoSpaSzIA81q7xE9LjvB8PJbtUr+QASlezxf1k6bqUnmmfqEGEhmMJDkej8exubnpRCjZbBY7OztYXl7G2tqaI2WXlpYcxhMPx+OxC5xqsF83ptTApJIiHCPNJOFYcMxofE/Ltuk6A4DLciOJHdlDb4805r7//vsYjUZYXl5GNpvF8vJyoHRHOp0O+Iie57kNsVQgRn+Kvkiv1wsQnkzt130AVFXJdeZgMMDR0ZHLQCWWEYPX1tZQrVZx7do15HI5lMtlrK6uYmtrC61WK7ApLX/X63UcHx9jb2/P+UScF1gqoFKpIJPJuLqzVMPSVyaHwQD9cDhEq9VyfVDiWH1UXfMqUcwx8jwPrVYLyWQS9XodhUIBKysrrlwj8bfRaKDRaDjf1vd99Pt9ZDIZt4E5N2/l2LZaLQBw6wb60Pys53n46le/irfffht/8Ad/gGw2iytXrmBjYwNbW1tIJBJYWlrCzs6Oaw/nVWKnYiqNOK++Lv/mnMH7SXGEEv9cZ/F7xzlWj9FyNVTGcq6J7MPbhyZjfd/f9Tzv/wbgLoA+gN/HWdpAw/d9hiTuAdgKO97zvL8P4O8DQLlc/rDN+KlYtVrFzs4OfN/H6ekp9vf33eI8n8+7B5KODYFD0wyYcgoEa5hw8RqLxVxBZ1Ud0dTRpHFxTIdSIz3qABKguCimY8bUAKtSVTWZOn78jDqEbJslXBUQebwq1PiaNW2DOm26oAaCzpdu5KKKAo1AkaTQhXo2m3XqOBLqTKlVolRrj7HwdqFQQLlcdqDL//m94D1R0FrUX432K3jqop/v8d5Q+cXz8z1+LwAEJmBOavx+rqysIJ/PnysLEdnDZR8n7pZKpZ9+g38CK5VK2N7exmw2w8nJiXvW+IxY54uOgdbTAoL1A/k/ST4bcdcF46IaU4rd+r5ihT0OwDnc0wW9nsMqtBQD+boeoz90oHReICbyh9dWpa+eQ03bCsxJhLCouMVJEsLEP6paGXWnQ8i5iYtwneOA+UKEG00yIMdr5PN5FxxkqQR1SNW5tH1UokPfs0SAtk2JEb0vHGvNcimXy6hUKq7Wea1Wc4uOyB5O+zgxt1Ao/PQb/BNYoVDA+vo6ptOp290ZmGOR1nu1JKw+G4o31k/RQFAY7ugik9ekf6sBEl0k8hm09RHVf2KwXv1jxXEaX7cKI1W+hqlRbX/UZw7zd/UzGiAKw3X173iM3gdti/rrxFk9v7ZXFXY8huPEDDn+kAwiBhO/lUi38wmvqWOo12BbNTtj0XciDNPtbw24kVAvl8vn5ubIHi77ODGXmHZRrFQqYXNzE77vo16vI5E42+iJZKSuZ4FgrU+u6a2qXMlYBr5VOKXPJD9nsVzxQYVMij0AzmG+YqL6Xgx6cW1NMQTPxSA++61rfLZT+0hSllwLfUyretUfGtusggael34qBUyager/SBilcwtLcalQajqdBgJjPEb9b/abfeEanSIDlnukn5vNZp3YjYIUu4bQe8q5wQo8dL7U+diaPZ8K7mgUVvAzp6enaLfbKJVKARFFZD+5fZQyBVUAvwngCoAGgP8vgL/8QY/3ff8fAfhHALC5uXmhckqee+45PPPMM/ja176GN954A81mE6lUCuVy2TknBC3ruOTzeZRKJVy5cuXcYpjRY003UOeKpsSBAgvJB9ZMabfb6Pf7LlpEFdLBwQFisRj29vbcg0fCdHt726WKTadTV1tUN/8C5rVXGI2i4ohkJ4F0Op0GNnexC2vreHIslLT0fd8BrEbE+FldPHOM0+k0+v1+AJQJ9P1+36kwSqUS8vk8VldXkUwm8fzzz2M8HjsQef/99904xGJnOyVyLBjpu3r1KorFootgAmcBhNFohN///d9Ht9vF3/k7fwe1Ws2VnKBSmJOrKlvpzLLN6XTaEflU8QEIKIsZeeL3JhaLBQp++z9Sd3ByWl5exvr6Or773e/i4OAAf+Nv/A3s7OxEDupDbh8n7q6vr18o3H322Wfx9NNP4xvf+Ab+7M/+DJ/61KdQLpfRarUCCzc6PcD8GeGCTKO41sHk80c1PJ87qqksoalOKB0afY75bLNddF75LOvn1TEEcM5ZVMWC1iRkH7notD+5XA75fN4pnEg28hjFWA1WqYU57Oyz9pvnDXPuWUeMeMb+aXoXg5espUpnXZ1Az5vX1rpy5Qri8bMdbzl3MgviBz/4AcbjMb70pS+5DTQ5j9p7SVNigMb5Q0tf2HHgGLF2Go0qsWaziXQ6jWKxiJdeegk///M/j3/4D/8h3njjDfzWb/0WNjc3Iyf1IbaPE3NXVlYuFOY+9dRTuHr1Kr797W9jf38fly5dcqorKqX4vPAZJK4w64ZBb+KLqmpomtIZRoYCcD5sp9Nx2MagzGw2c7X/qQriuZRc0GeZ/iL9IrZXMZtYVSqV3GKfx2qfdP7Rmv9KwKopYaFm+x6GNUrEarkUxV714zgH8v4Q39Wn5D2kkIBtVp/R8zysr6873574mc1mUSgU8MYbb2A4HOLFF19091PHxga6uObRoGLY/KNBMw1ucSyUaLbEArNVUqkUXnvtNRwcHOCXf/mXsbq6Gvm6D7F9nJi7trZ2oTD3xo0beOqpp/Bf/+t/xXvvvef28+DGWKVSyQWkbZDm4OBgYQCEz7H6pQCc+lEJQeIa/VMGvjOZjFOhkvhk4Nz3z7Ig7t+/j3K5jHQ6jW6368RpKpxg1hKvXS6XUa1WsbKy4nzC5eVll2lsgzP0FenHkotgJrKWE7CiL5s5qr65rpeBOXZyk7GTkxPkcjnnV3MfFg3S9/t9NBoN3L171xHX5H84N+g9VD90Npuh3W6j2Ww6LPvkJz+J2WyGQqHgOBXWNv/Od76D4XCIr3zlKyiVSo5/4LymtXp13cPvBDGYpXnIP+lGiBwzG0DleXq9niOYeR/Y71u3buHg4AC/+qu/is3NzQhzP4J9lBXCLwF4z/f9IwDwPO/fAPg8gIrneYkfRa+2Aex+9Gb+xRqjJE888QTi8Tju37/vvoipVApLS0vOQWF6Jh0FS7QC55VUmvrF94HzNVTD3uMDns1mkc/n3eZWfDBJItKpVceTRCfBjc6n1guhMzuZTNDr9RzhyYdeo9rAPCqnhIZ9INl3jY4Dc6KWDhgBX6Pi/E1yQ/vJhTQnDqpHbWoYozlc+M9mM7eZVblcRq/XQ6vVCqTWErgmkwnq9bq7NnCWIlytVhGPx/Hcc89hMBigUCi4740u+K3awUb/FThV/cqUL93NUYkl/k/HVVN/SYI0m03k83lsbGxgd3cX/X4f165dc6R1ZA+lPbK4y+dhfX3dOTiqaiLhyOdao/NcPKojajGZzxHPS+yhE0WnxS6g2TZ1XHzfd46WKkVVrc9jLJbZv/U1dRiBoNo2TEXEcgi6wZS2labHhV17kfKIxvOGqSr0f7ZT7xsA5/TrPeEPCVzuaJtIJHB6eorvfve7gflGyaCtrS2H0VapRhWdvfeKtdo+4q6qtrSkA+8Nv18MTFL1oSTIaDRCvV7H+vo6bt68iZOTE/T7fVy9evUcCRHZQ2OPPOaura25mvisawfMSVQ+U5rxpZlH+gxalRRw3o99kNEXos+mNU61PIEGyUg4qmpKFVOK3ZpZpa8rVivpp+0H5kSrKnf1c9pH+rxqiwhcnZP4vz2Or1vsDRtDS4jqmDBjSokMrh0mkwlKpZLDZmbwbW9vuw1/WSpBr8U2Wn+XY6RZBWpKVLBfGsxUf1oDpPyf34lqtYpUKoWTkxMMBgPs7OxEmPvw2iONuclkEpubm/A8D81mM1DSkH5HWE1SfT70f32+NZCkOKVKW32u9Hg+8/SpJ5MJCoUCJpOJ86m4mSJ9JpY9oQ+k/i/9cPrKwDz4Qq7AlskiRinRqL4Yz6s4qfhgsWIRnvJaHGMrfur3+0gmkxgMBs7/Ux96MBg4bGX7KQwB5mIN9pc8BH9ms7N9Eg4ODlx74vG4K30AAFtbW24TMPWrrRJY+6drB8V8YM6j8LulPrv6z1ZIqBkyxHvP81Cr1eB5Ho6OjjAYDHD58uWIX/iQ9lHI2LsAPud5Xg5naQR/CcC3AfwhgP8VznY8/O8A/N5HbeTPwjzPw4svvojnnnsO/+pf/SscHR1hOp0ik8ngE5/4BJaXl53iRSPCTBGgc6j16fjF58OpTiYfBBKaJET5pbeLTl0Y06mkczUej9FoNHDr1q3A8bqjOAsus9YticfpdOrqNXLnao3AKNnKlCiqZ2ezWUA1QNOHWh1Rq1BSVYIlP+hkU62qxCVrw3AXxHK57BbiVO6S1OTCmVGv2WyGw8NDnJycBCI/4/HY1WC9e/cuer0etre34fs+CoUCstks1tfXcenSpcDCgdG8MEfaqrSAee1eTj6chKle4L2lGluVKSTKGU0jyE+nU1dXd2NjA8ViEd/85jcxnU6xtbUVgeXDbY807gJnaq1r166hXq87/GRtKeIPnxs6Dnw+NKVVHT1LxlHpSMeEAS7iF80ujjVwxOCLFq63i2/iFo+3zq+aOlnEN0vAWqKDUWrdAIDn0tp+QHCDCJqeX8+rTr6+rvjNwI/iNM9JJ1edbGCeEqzq1UqlgnK5jKeeegpLS0soFou4ffs2/uk//aeB8aFyI5lMYnl5GcBc1ctxoQPM74EuLmwQUdXIGmzUeVhJaH7HOL+zLjG/N5lMBq1WC2+99RaeeeYZPPPMM/gn/+Sf4PT0FBsbGxEx8PDaI4+5V69eBQCnHmXgq1AouO82g1WLAjY0G/ghXliCTl8HggrJ2Wzm/BtimOfNa6ay/ivTSofDofPDdTMyG9yiv0zVl/WdaXaBq3MHP6/CC+03r0NTf1DnkzBTQlYX2jaQpkF4e23FZGIiEFTIsf96Piqhi8Ui1tfXXeYfMxKWl5fd5+iTq5/N+VjvP9vExb9iLE0JdUswaH1C/a4oNvP+P/HEE8hms/jDP/xD9Pt9lw0X2UNpjzTmep6HZ555BteuXcN/+k//CY1GA+12O5Btm8vl3Oc1YKJZSlYZCcBhoe57oFhMDAzDGPrOSsqpD8TP0r/mWtMG8ZTQ43pWiUv1r3g9DXKTD9Cye1rCwQoDFokbaBoIsj69DUjR2D6WCNAyXL7vO0Uw+8vxGQwGaLVagfUEBXDkMDqdDvr9Pu7fv4/XX3/djVsqlXJ1awuFgiunqPMP+RsNCvI+8zM2WKWiNiXC9X6oeIFt5vwxnU5xcnLiMkzoO1+5cgXb29v4kz/5E4zHY5dZEdlPbh+lZuw3Pc/7VwC+A2AC4Ls4Swv4GoDf9Tzv//qj1/7Jx9HQn6WRxHvyySexsrKClZUV5HI59yW3jprvz1NCNTqhTigwT88CEFgskvTUSIwqdMKcNFVi0VGk80Q1rEZuUqmUSxFIJBLY3d0NgAUBPZPJoFqtngMvEhF0wrrdrnvgNc1JUy1sOpI6pRxjXeizv3Se2T4unhUYfd9Hs9kM7AKbSCSc40jAYnoFcFbO4OjoyEUmmUK7vLyMdDqNra0teJ6Hy5cvI5PJYGlpCUtLSwEyhYpUbZ/+6OTJfiopzTHmPdE+MVKp3xkldriA4MSkKRj8To5GI7TbbQDBHZIjezjtccJdYmatVnPpW/q6Pktal4rPji5e7XPH8wBwTiQdTHVcVaFD0/rZAM5toqILQJaIYZuIa3zWlRTk86yEpTrFmgbFtmraVDKZRLfbDWBJGNlqMcAquYBgnTJLBltCWRXCqmRTcpbHaV9JxG5vb2N1dRW+7+Pk5AS7u7s4Pj7GtWvXEI/HXfkXLcWgBAMdTDqOOn7qWNq+E39tv9R4jyx5pH0i9rbbbfi+j26364ghlsyJcPfhtccFc0l2xuNxl77ODVcYBH7QsWF/L/pMmFlyFpj7yBoQYiDNYjnVnvYZVd9XfUlikfVFeT4Nhmvgj9hFHNZjFU/0vGHKTiW1w3BZx0U/p3ONYp/6uRbvVNCgGWjEVN/3USwWXa3CWq3mSkLoXKBCEF6T3wu9nopL+J6qrfR+6/rI4q3NstMSDPa8JO+5lojs4bXHBXOBs+80nzmWaCKpp0EY63cpLlg/z66tlZzTZ1cxJyywpGt8GrGM+60wIDIYDNDpdDAej9FqtTCbzTegZr94/GQywenpKWKxGOr1OgAEBAgkLMlFtFotjEYjp8pNp9OuhAL9Lw3UWIy042HHSNfzHGtiyWw2L/3ATAH62CqA4DE8FzNHODacb0jw9vt95PN5fPGLX0QsFkOxWHQleXRdYAN7xHL9PoT56vw8cVVN5yOeRzcf1+8AfXtmD2v72Eb6CJF9ePtIhcx83/8HAP6BefldAC99lPNeFNMveTqdxuXLl7G8vOyk2Uqy0ZSM5bEk7Bh94KLbOipKwikxYIExjMgE5oteglC1WnV1Vkhg8uHhBFCpVAAA+/v77lx0aqk4Y41Ttr3X6wUKfNM55sNJ5UIulws4q+ynjhPf40SgDzvvARf4ugO3KhN4LxhZ5DVyuZxTuRK8eA3gbDI5Pj52gM9IXLVaRa1Ww7Vr15DP512aBhW4NpLG82upALaRr+nC3zqtVo2nRCv7o+S0biRDsGRbbJRMa9BaQI7s4bRHHXeBYLpRuVx2ZVm4mKbpsxSWtqOEID+v73HhphFsvs9nl5jJ81jnRxeI6vgQ4xV7SVRyscjnWRUOVAqwPcQNpoEp0Wo3SCCuc2GtagJd/Ksp0biINGC/lEDWBbeOEbFWiQKeg58bjUbOsd3Y2MDOzg729vbQaDRw79499Pt9PPnkk4jHz3ZBtwFITZfWTA1bUzeMCFUygefja5q+ZwOIHBdd9PAezmYzl5nAnXepYIjqxT789jhgLjBXPNHXYbqkFR0QMxaRsGEYo78t+WnPS7MLTMVl+knEIC2nxWMZuCFZa9VkinnaDvaZn51M5hv68W9mJNlglfY1TIFmP6O+IK9rg0b6eW0f1xWqFAsLPPGzthwYyQ22/fLly4E22t/2RwlZtkFxk8dx7tDyDzq2ShDp2sKOE+dalgzTYGu/33dtj4iBh98edczlc8DMg1qt5jZZVszRZ16xRL/7FkPUFLfDAkf2R9unPpEldGnEOdaQZVo9cLbGZvkbJRdZEkXT7dnWZDKJXq/n6tEyM9f3/cB+K6wbq+sB7a/6cvY9HTsbFFKMpE2nU+d3s0Z1JpM5R+hqhgXnBir3WU6RfBJwxlG89NJL5wRnyiXoXMU+KQegY6c/2k/tu+WS2FYVS+hxxGMS6zyWY0S/f1GJt8g+mEWrhAXm+z5eeeUV7O7uolqtYmdnB5ubm8jlcs5xUCWnApY6JioL5xdVU0mt6eI7DID5vzoqBGwlDEguZLNZVKtVdDod9Ho9HB8fu9R2AG4nXYIjNyNhTVU6R8lkErlczj2wbAvbykgWI0bc4IoRn2w2G1AKs52qFtMojTpsCqr8UaP6lkalrh7HKBYnunq9jk6ng3q9jsFggF6vh0wm43YXrlar8DzP7WbI8gW8Pwpi/NHUVY4NTdus6VzAvLYLz6NjrEpYrZWrYMjP8r5rKQM9xk6ikUV2Ee3u3btotVp44oknUCqVUK1WHQlJPOKzYhdrmqZOsw4KcZoqMCVG6Uxx4a6OC5X+xCgbQCNBqQSo4oOSs5lMxmEC8Yd4o+1LpVJOVUBnUDMmeH2qgjqdjtuIoFaroVwuuywBXo+4ZOcUHUeOa1gAicY+afCL/bELBZK4xK5EIoFarYYnnngC5XIZvn+W2dBoNNy8pW3jOZQA5xgpGcC+kaC2jiXPp/NO2OsMRrJ8D1UYuuENj+d8oI46yWB1ViOL7KJar9dzPoKmgPPZUl+GuEHssYEuNRu40CCO/Zyel9ehqe9n/WINrtAX9P15jX/6ugyCMYOg0+k4XCEhqQtyBsYsIcnrdjodJ1DQDW0tfiphEKYitaaqNUtQWwKZ9c9tcF7nBi7wNa2WOEXyPZPJuP6rj65KMbab422z4/Q7YP1MSwzTf1ViXTGbn6NAwo6P1obnruS6uc+isY0ssotgvu/jhz/8IY6OjlAul7G2tuY2CNfyhLQwRbmuQ+251RQz1A+mH6X+siXa9EfXpLFYLIArJF3Z5pOTE7RaLQyHQ2QyGZd2XyqVHNakUilX9oQZXvRliQXEMuKx7mcDwG04Rr89LLhmscDOH4pH9L9V1KQYxfOR4+B5Wq2Wwx4AKJVKDmNHoxFOT0/d9TjX8j5r6SvNpPU8z42xkrG8p/a7QUwk0av+LH+HBUWZDR3WfxpFEcCcZE6lUq6U5WAwCNQRj+wnt4iMDTEuxu7fv4/33nsPL730EtbX11EqldzCnU6PPjh8naavqQO6aDGsi05LnFknVE3PbWXpJFLpUB0eHjqikgt9yuh7vZ4jClhjlg83d/ejqQNGMGIEaDweOycvm82ec1Jt3yzo24hUWN91bBlxZ6RKo0v8nCqNCYisjUtFE1O08vm8U2MRJDWCp0DGYzl23HlRHXh1pLXPapbcVWKa907LIXBi0kVHLpcLKBI4YTI1IqwgfGSRXRSjI9JoNFCv1/GJT3zCBXK0yD8dJHUY+H3XHV357Gj9KhodUT4XGuXVkgG2fqwlKYDzGKW1tpSM4Pu8rmYAaESahAVr2TItSjelUmed+E3HFpgTDyzBwOtZDH4QrnJc+X+YU6uBNRsI4uuKR6qqLRQKWF1ddePJYBb7R/KdbVNSXcdNiXGeS7FW67YDwY3StI98j/MYAEe+coy1hqGSsTpG9AuUGIkssotoxFxmEDHoxfIEtEUkm31/0XtWRa7krlVjhf1tSVLgfB1Xq1y1KnceE4/HnVBA/UNih/q1mmnF33ausOfgT9iagP9rex60gLX+siUbuJBXLLM4r2WzeE6do0jGcPGvBLD6oTbTSy2sH5Zc1/ZrkFKJHp0HrUqNwcpYLOYIdvX/FbcjzI3sohq/00dHR7h//z7W19dRqVQcmamKWJr9266Tw16jKZ7puYjL9IGBeXZX2Hqcz6RyHuoTUeQVi8VciQXfP1OzEnt08y/1mzguNkjONuoGgMQgzYDSjVVVLBY2fmph3IOdX6xAjH5dNpsNBKuIsQwQsXwXNwjnGqbT6WA0GrlglnI3Ft+Zsat+MNtgg34cU83MsxxCWN9GoxFarZbjL+y1KJ4g5mo2Bjc6Yxsi+/AWkbEh9r3vfQ/f/e53kclkcOnSJTz//PNYXV11i1rubAecr4XEh4ugqhGWRYTtIqDQz2sUmoCoDhcwXzgy8kTzfd+BIAlEjWQnEglXr2RjY8M55mwzyVkWsuau0brILZVKmEwmaDQajpztdDpuQ4BcLofV1VUXHVMHlySh583Tulj2wJITYZMOFWOs41uv1x1IsMwAVWa+f7ZRwMnJCZrNJo6OjtDpdNDpdJBOp7G0tBQgBZT8YI1GOnzT6RS/93u/h7t372I2myGXy+Hq1auoVqvY3NwMKMGosuD3gt8b63wqsNMB7vV6ODg4CKgJuBjg50kakZDhGLXbbUwmE/ziL/4iNjY2AkXhI4vsItlrr72G73//+7h27Rp2dnawurqKSqUSIFiVfFNHRglXG6ixanat8TSdnm1AoKUCuAhXJaYq+4H54pPOo+/75+q50kHU2tGKqXxGqRyg48WFMftD/OX8owQfF7Tlctk9+8SM8XjsSrdomhXHhmNIB02JCzsvhTmKPF6VHHyfGy7wvHTymRVQqVSwurqKnZ0d3L17F/v7+4jH4yiVSg4vtX9sM/Hz61//Oo6OjuD7ZxtobW1toVwuY3V1NZTcYRusU2oJA3WQu90u9vb2XBsUe2mJRAKlUsnNS6xDzjnoC1/4AlZXVwOBzMgiuyj25ptv4o033sD169exurrqFreqNFcfEwgGauzCfRGJYIM79n3+rbijfp7icZipwEHVXgym81xU/XLH6k6n4zKeFHPpq1nCV9vFjUyYggogoDTV8gFa+9YGf8JIYLv4V5yyGVfEfd1gTbGdgTq2tdvtolarBfxwq5rTPvPYb37zmzg9PcVsdlYnfG1tDblcDtVq9ZxgQu/lbDbPNNESNlbxynFsNpvniGD9HqRSKVQqlYAQIRaL4fT0FIPBAJ/+9KexvLwc+bqRXUj7wQ9+gNdeew3VahXb29uuDBdNn+Efxw2oP6Z+kzUr0tFM3dls5jbgVl9HsZgY5HmeU14qOdvtdt2mgCqc0OwDrsXJB+TzecRiscB+MxqssWp8+o2e52FjY8Ot05PJJIrFolPi6j4x5DfULPGqwX6S0mFzD317Zg7n83kcHx/j5OQER0dH6Pf7jmdYWlpCLpdz+EdBFP9npq0GGJm9puT366+/juPjYwBnPvXa2hoKhQKWl5cDgT8NqPFYLZmo/ruun0ajETqdDo6PjwPkrQ2EJpNJNJtNV66R12k2mxiPx/jiF78Y+bkf0SIyNsT6/T5OTk4csUbw4JdYJf0albfASUCwSqYwgKPZxWFYNCvsOvpZ67DSGVRSgQSCtt2CE0FQ0y6pnGA7VMVEACUBQfJYFbd8sO2GEPrwW8fXkixsm01l0HqGNgrEvhCc2u022u22c8bVYWWEiADMyUeVcCQhdOOv0WiE4+PjH1sj0C42NHLFsev3+w6ku91uoHg21cZc8HMMeAxLLmSzWXcObjwXWWQX1RjooGKfkWVd2C1SC9DUkXkQVvKzukC0jpkSvbrY13MpOad1Sm2U2OK4Yp9uzKWBPs4Dir26YYCOg9aRJSHLMVUVgSp09XhdSC+ysOi8nk8JGTunAHNiNRY72ziGGRdU5gHBzc94blW48TUG24i/dOxrtdq5cWc7lGy3P+r8k7i1uJvJZJBOp9FutwO1KnVhQ4eeRPTS0hKWl5cXjmlkkf2sTUk+JWCBxaUHwn4/CDsepEzi+4uuqedVAmHRdZR0sHjF51/7y88piaiLXJ7Dpm7GYvPMI4tVYap5O1aWULEkrGLXg8ZW/Uc1DRaqqlX3EbBGEsWSFTyWkuemQwAAv8hJREFUfvJ4PHYEEjfWtD68to3BTvXN+Ro/NxgMHK7zeJIb3BiIGR4kMkhCM2Dp+z6q1Sqq1erC8Yossp+l0bdgaQL6uXzmLF8ABPFCMfBBvABNxUSKe8RDJQT1+VQstDhqMUl9KPppQHCzWwaDWLqAQguWcFF/kTiq/jh9KxUk0N8kScoxIM79pGazknk/NDippCrLV3Hdziw+FX9p/8lB6JhrVkMYz+F58/JlnU7n3P4Wdo1Es9kHJF+VdOd3kZjr+2fihlQq5coz8p6yFJryC/Tjubl5ZB/eIjI2xEqlEra2tvDMM89ge3sb6XQak8nEkYj8nwTcZDIJqI4ABBwfpooCQWJUP2sj46pIUCeR0X5VrpKE06gOH3KNsKmql6kCLGEwHA6dQpS1pKiw1MgQ20QZPsGHtrOz45wkOm+NRgOtVgvvvvsuYrGYq727vLx8jrSwKfqcpOhoaQRfI1ez2dluf71eD6enp47QYdoEz81asW+99RZarRZOT09d30ajkTu+3++7caASIJ/PB2rXjMdj/Mqv/Ap6vR5effVVHB8f44033sBoNMK1a9dc25TIIXHCBb91TPf29tBsNvH+++9jOByGRvD+0l/6S3juuefwb//tv8XR0RGAM9BuNBruOi+99BK+8IUvuO8UUwwii+yi2tNPP41Lly65XWVJaKnzoz+6oNQFNk0DRqpK5QKaGQIaeKGTo1jH86jDSQxmcCSZTKJQKDhsUnWqOqRsg27OReMzTqUuP0/niNdRvCX2a6bGaDRCPp8/F9jRa6njp2SEDTAqIWwVEzye2Mz3FHN1rIiv+XweKysrjthkqZgw4pzjyHmBQbybN2/i2rVruHPnDjqdDt555x30+30XcNJ7ZevIhn1HWOPs/v37gc1hFHe//OUv4+mnn8bXvvY1h7vAWX002qc+9Sm89NJLbhwj3I3sItuzzz6L5557zikXi8ViaDmjRYEv4gD/B86TBHoOG6BRnCGe6KJSF638n+fi4lIX6cxe0uCKbvrCQA4D6TwXfV1uoGN9NiAY9KfNZjO3sSTbqpvFciFPEQTbocTpg9RsSm7o+HF+sGV02GYlYTkPcb3CeUFVbBwbq6wiiTudTvGJT3wCOzs72N/fR7/fx+7uLobDYWARrkQPvxuz2Qy9Xi8wrwBAq9VCt9vF6elpoEaljseNGzfw5JNP4g//8A+df+t5ntuB3fd93Lx5E5/85Cfd8VxrRRbZRbSNjQ28+OKL2NnZQblcds/0cDh0WKYBFkuMAvOMVn3+rbJRMVg/G4vFHG8RFnh5UICM7aSxxIHN4uJzTmykD9jv93F4eIjpdIqlpSVks1ksLS05HoNGDCF2KcnK66gP63keyuUyksmkC+jcuXPHzQWKa8Rf9cnJM6iClMY+5fN5Rz6ORiPcv38fJycnbsPWWOyspGMul0OtVkMsFnO4p1kaJKS1/AqAwH3na5///OcxGo3w9ttvo9Fo4M6dO/A8D9evXw+Q0SqwA+CupfMcAJcJzL2DwviFZ599FtevX8fv//7vO1Wu53loNptuDH/u534On/70p90xEeZ+dIvIWLF+v496vQ7f97G2tuZSeWytPuB87SbgvELApgPoZ/i3BT8bnVdTIF6kRvA8L+BYWTAlcGppAL5vo0uMErE8AVVf6pTydSU+CMQkAAgSJBlYYJ+v8xhg7jCzTSRkdMxtBIh95pixvbppF8/b6XQCBIA6tQRKW7uRBcU5iemkx02A1tbWkEql0O/3UavV3L3U+6abcbEvg8HAtafX6+Hk5MRFqmytQxojb1tbW/B9H/fv33efzefzWFtbw+rqaiD1JbLILqoxNTGTybgNp7QkCU0X6tYxDYsMA+drQnGRqgp6VUHZ0iRhSlpbesYq/XXXalXna1vD1A+8Bvtm2xOm1uJnrEKXz75G+HU8LI6qUkKNmR1hDjqN7dFzKnnCMSPex2IxlEolJBIJt7Fkv99HNpsN1ORlW7VWGMeGgcdSqeRqQHLTL22rJQbYXkuWNxqND4S7uVwOGxsb8H0fe3t77h6zDE+tVosI2MguvDE9sVgsolgsumeY/ozWDl1kYYSt9UfVLJbb//Vzem1V2dvrqe+pClMeZ8+tATx9nQSr+rhhalPFFF5DMVg/p4tq2yc7r2h7+LmwNYYSNHqsksd8TdWoSkDQXx4Oh+dEH7yGzid8j34na5gPh8NzpQDsPKxzL4kBYman03GqskVKXZZ8WFlZge/7ODo6cj57NptFrVZz9TYji+wiG/3c2Wzm/ATdrwCYcwZh2KOm63brkynuAAjwFnxf/TKLM9Y/DSNr+Rnrm9JH03OGEbzEIAaMlLdQ7sIqXOmP8hj170hmauDe9/2AP2czoNS3Zfs0MAjAZTrRx9d1POdJ4m8ul3N7tzArmGKPfr/vuATLy+hcpdkpmUwGmUwGy8vLSKVSGA6HqFQqAaEETfvBcaAAgkG1VquFfr8fKFVpLZlMIpvNYm1tDbPZDAcHBw5zKaIjkR7Zx2cRGSu2v7+P//bf/hueeuopvPjii9jc3HRkrKqldKGtUSeSivpA8WEniIQRs2EOFY/XyJcCDcFL28DP6E7QGlkD4DZmoIPGei2TycSpPy0g0uGiCiEs5ZV9I3nKTbvG4zEKhQImkwnu37+Pdrvt5O/ccbBQKLhIPCcIKneZfgoEo1oaYScwx+NxV4uGn2U7qQZgGzqdjiNe2Q9Olqw7yMnl5OQEiUQC7XY7sEs2He5EIoFnnnkGs9kML7zwQiA1gO1mO4F5Kli/38fx8THef/99HBwcuCjUgxY0tEQigV/4hV9Au93Gv/yX/9IpBXZ2dvDX/tpfO0fkRxbZRbWjoyN861vfwvPPP4+bN286JxUIT+9nlFnJSZoSg9aJ5G/W1uazzyg7o/zEMi4gNdqsG4Qw2ENFLDBPQSMGKC6qg0enj44ZHU9Nf2e7tGaudb44JzFIRByvVCqBOYmf1yCTKrxIfqujbiPmHHs9lzq9GsSyP9zYcTqdIp1O49KlS2i329jb28P+/j4ODw+xtbXlyhZYoodziab/JhIJrK+vYzab4dKlSwDmSjsltHU+5v/Hx8c4ODjA4eGhU7Z+ENyNxWL4whe+gG63i9/93d91aq3t7W382q/92rlMj8giu4jWaDTwxhtvYGdnBzs7O1haWkKhUHBZO/RFrOqVr4UFZWhWCBBmdjGpJKUGpOjXKSGr2KP1CxlgsdkHbBP/piKU88h0OnX1U3VzGWIOr2cV9WHlylhHltkIXPDquoBtsb45X+M5rVKNAgfivAaYOA9odh2z03SXbrZ9OBw63579UL+V7bAkcywWw+rqKnzfx9bW1rn5yNYYVvJjMBigXq+j0Wig0+l8ILzlNT/96U+j1+vha1/7GlqtFgBgbW0NX/nKVyLMjeyhsMPDQ3znO9/B9evX8YlPfOKcUp6YxMxbm/ml60nFNeUJlBi0fh59NPq+qkZXP9sKz7T2qBKx6jfz8ywfosF9DYZT8AXMMavX6wX8YW2XqneBYIkGm6lFkRfHgtwB6+Hy/Jq1oNhhCWT2ifMiz0f8AeDWECy5tb6+jmKx6DB2b2/P+fKtVgsHBwdOwcu5wOKgFbx5noennnoKk8kE169fd/OSjq/67GzXZDJBu93G8fEx6vU6Wq3WB8bceDyOn//5n0en08G/+Tf/xqliNzc38Su/8isRv/BTsIiMFUsmkyiVSiiXyy7aqos43fhF/7eOHz9jI1QEpAdFjPi37rhtI9QEISDowPH6BElG2BTUNeKmDpemMNgfko90VLnIVzBhHdkwtRqvSTDjJgDNZhO5XM5tZkOzhLWCuaZu6XhNJhNHGicSiXOTTb1edwpU3TFQJzUq85iKQKBmX5Ww0PHkeZR80PQuJWHY3n6/jzfffBPtdhv1et2lM/wkRiLp+eefR7fbBXCWAmNJm8giu8hG3C0WiygUCg77NCMBwLnnSFWtluQEcM7RURxRp1LrvOrmA5bIJZ7w8yxNw/ep8CEeKFayL7o4134ojmg7VXkVpm6wgTwlL6hyYwqypkXpufQ6vK511G0UXccxjDhgVJ6EAB15qnbL5TL6/b5zlDm38L7Z+6VkDYAAaWJJY0u+6vzd6XTw3nvvodlsotFofGjcTafTePbZZx3urq+v/9ha4ZFFdlEsFou5mm8MftEvUaGBmv1fcVbft59Tn9AqQq26xxKuKljQxaldDPI4zfjSz/Fc8fhZqS5eQ4NwihUWcxQ3w9Sy2k8SK/pZzQKzY2Wx2NZLVKJC5xd+Xn19na/YH2a7UewAwAkvOC9ZssNiGYUR6lcrcRs25hyL4XCI/f19DAYD539/GMxNpVJ46qmnXDozy5xFvm5kD4ORlFOfJcwsEasYaAU+ur7m//Zz6sepWt6eX/8mDql6lljDUllct+tnWFrPYppyESpKo9/Mv4lxtAdhL31avY4GmrRcFj+vQq4HCeSUlKYqVucM7Usul3MlEcvlMjKZDIbDIXq9HrrdrttAnBuYca5h2xU77ZzKcdUMBxWk6XjoPkDdbhf37t1Dr9dzwrMPyy/cuHEDvV4PALC6uhph7k/JotWDWCaTwerqKlZWVtzOshpBUaWpKpZ0AU1FFwk/XRgCwVp2dnGsX3Iu9LWuqAKfbspiwVLrqSjAzWYzt5kWH3wSydzsifVvCbgKgLpxl6qufN93NbFGo1GA5NBUBKYgt1otDAYDHB0doVwuBzZd0YlHJxglvtXZVCeb7U2lUq5GFpUB9Xodh4eH8H3/HBnLcxYKBaysrKBYLCKbzYbe57C0CSW3qbojGUsH3DrP7XYb3/72txemCnxQS6fT+NKXvvSRzhFZZD9Ly2QyWFtbw/LyststFUCAjLUEJVVASiACQVWWJQpVaQXMU/yJtarAUmeHWK1RdRbop2PNjfOUNGD5AuKp53mOsFUnUzcMVEJY5w5b1kCDbBrFB+CO526u7XY7oI5SzAOCjqd15HXesfOUBgbDnP7hcIhms+k255rNZs4pXV5extHREU5PTzGdTpHL5c4RLEpE60YvJHX5/QAQcFDZP46D3t9Go/Gx4G4qlcLLL7/8kc4RWWQ/K0ulUiiXyyiXyy5DiaofZkDR9wHOq111wR0mOrAEJQM3NLsw1PMpttAUC/S5V5LTEpt6HT03U4NV0BBG8upcoLhp02nVDwXmRCaJBvqcKmLgeCwiZPge20Vs0xRcnR+4x4P1Oz3vbNNez/MCalSm+DOoRZ9Wz8d7wCAisxZYw9uOoRLYSjj1+33cvn17YV8/qKVSKbz44osf6RyRRfazskQi4WpyW9WrBrc1WKFiK+B8femwgJnFVuUANCDD89vzKOYQTwAEslVZBtBu6hdGxioe0gckl6L9DQvu2HNYQlL/JiYOBgNXUkr3flAM1SA/sYttVFylUplcBgDX9slk4oKZq6urKBaLrv4txV/tdtsFPcvlsiOsNeNO5wR+H5QjUj5Bf2ufdJ3jeWf7x7z55psfC+Z+9rOf/UjniOyDWUTGiulClNEcLvpULaWf5Wsa8abqwEa3aRrtCHPIwsgEXWjSWSJgEnDUCDqWSADmjhL/5nmYQmAVaTo+bJ8ep21VUFPSlGrV2eysXs5gMHAbZb3//vuuvIC14XDoFtU8t6YnaHs4mdFJVDXW6uoqMpkMDg4OMJvN3OLD989q4pIULpfLLq2W0Ty9R3ais2QAI2462fJ/jYSlUik8/fTT6Ha76HQ6AbJXyV2SN0oA/+AHP8Dp6SlefPHFaKfuyB56U4dIF9XWsdQAkqpbgfOLbqvU5Gu64KaTRcIBgHv+7O6yAM4tpC2+s1aoqpW4AF+khAojJZj6xIwEnQOUlLZOm46RqqhIOLdaLbfA53U5j9jUNIt5SnKyj0pU8/dwOES328VgMAjUpeJvuwsrlQU8py3xwrZxEzP2M5PJBLJR7KJFVW6cvz3Pw8rKCr7yla+g0Wjg+Pj4nCPLe87asbpZxRtvvIF6vY5PfvKTgQBiZJE9bKZqG6qJ+LqqKWlhwS77fxiu8ZlUf01t0bE2aKYiAMVgi/fWl7b4qj6r4r5ei2bx1mKfnlMJWfVViWkahGOQTvvGv4ndivG64FaBBMkSkqm6WaSOP0vEeJ7nMtEYqNMFvPZb1yZWIcex4lzCtluFLdufz+dx9epVpxbjmCrOk5Sh701799130Ww2cePGDVQqlXPfn8gie1hMA+tK7qlQy/eDNU4ttlg/V/GD/+uxaro+t59XPFNi1OIw16j09bR8lAblFa81QKO+mPVt1ae2/WQwzPZHj+W4sRQjS0hVKhUMBgOXxWTXFBpIChM1qOiBvIJdGxDjuZbnmPZ6PZeZRt+WZKyWjdE+aECNr1tlrMVmDV5SgHDlyhX0+32HubbvirnKL7z11luo1+u4efNm5Of+BVpExhrjolMX0/xfI+IKGAQ5ddwUhMIW3ErGKiixDfqjD5qCI99nxMqqh/iwKhnLNvJaCoxaB0aJEZ4zLGKljinHwRLIFpQp1W80GhgOhzg8PEStVnMRfO0DCdV0Ou2cPzqh/Cz7z+tSBUb1cDweR7VaRTabddekMzwej5HJZFCpVFAsFpHL5Zz6leNglR/aZ95vArSdMOh0ss0AHAF06dIlNJtNHB4eunaywDf70u123Q64tDt37uDevXu4du1aRMZG9kgYnzUbIdbfQDDyr2orJWktEWsDaBq8oXOspIHipTqFYe3T+YKYonOGOnp2HtA22f91oavv6TjonMT5QXeF1UChEp3EVJqSvmEkOIDAfKFzi/5N8pKpqBr95/FUEtigH++bOr68HsdVF/BUu4bNVfqdIunCPlUqFbzwwgvY39/H22+/HSCU1dnm8UoMvP/++7h//z6uXr0aOamRPdSmC25d7NGHY+1/q+Cy5wDOlyew2Gv9TZoSsWG+Ml8PC2bZgJ3F/DBFkM4NYSTGByWL2aYwgsAStvxfxxeYB/a0Xzqf8TW73tDasOpTep4XwFu+xs9wbcB64iTb1Y/V/+0aRMeXfWbQkOpZzZTTmoe5XA5bW1vodDqo1+sOs6mI43VJXCjm7u3t4eDgADs7OxEZG9lDbYpn6kfZQJRirvUBLWZZDCGO8P8wLNYSJGE4aYPaFveVdFWSUfHN8iHsi3Ij6usu4lUsnlqxlz1HLBZzogiuq+mrkTxWTLeqVCsGYTuVeFXjvaIP2ev1Av7sYDAIbPpNP1lxW314cg92LgsrUWDLFfCHat719XV0Oh2cnp6ew3eel7668gv37t3D3t4erly5Evm5f4EWkbFi8Xgc+XzekXu6SFOzi1VVStlIOh1bqoDColX6cDGSwTofduEMBOu+koS1wMtzczFqUwXo8FBhpCSsAinPowBpVQGq1GWkRcFGiYxEIuEefNZW6ff76Ha7OD4+dvVVwtrB/ipATqdTl36ldQeTySSWlpYCZCyJlnq9jrfeeguTyQT5fB6lUgnVatWlYWmUDEAgUmmJDk5MvE/8rCpLGOmicoCfWV1dRS6Xc9+NeDzuUj40Ond8fIzXX389lMyJLLJHwVRZaSPs3L1ZTdUESprSbA09Rn/twpLpmCT01GFSssE6xWwzjdjG9mgJGnX22DdioCoIiDt00IkJWrvcOpHdbjdwLXXwdcHv+2c1sTUAqBtScg6zWRZKVurYKiHNeYqL6X6/767BEg0MVhEzuZN3pVIJ4GMikXD4qCWC+B7Hj2QsSSN1YDkGuuutzo/T6RTlchlPP/20c4yphGUZn1wuh0ajEdisIbLIHjWbTCbo9/sOy5hGq8EWu9jn3zQl1MIW0+oP2wWtDVaF+a+Kz5bEtL631vbjOfT/MHWV9Wn1GFvORAPxwLw+LBBMzVf8nkwmLkug0+kgm80G+kIjvuvGYupvAkC73Q741xoMtOe0/RgOhyiVSiiVSk6kYclxjqdmWtjz8X8KJLjpGzBPJ9bjk8kkKpUKcrkcqtVqYJxns5nzlePxOPb39/HKK69Evm5kj6QpL6CkKTD3tdRX4rOreAcE9wjQkk2KjUqYct2p4iyudS2XQdNADf+3IgV+xooa2A4K2hQj7Txi8Rc4L8SwuEbTYB3xcjAYuDGeTCY4PDxEOp3GysoK6vU6Op3OOb+S/EUqlXL3Qfd5mE6naLfbAd85lUq5DcuYScs61p1OBycnJ9jf30cymcTa2lpgU0cV14WVXggTCSjuK0Gs6xQGuTzPc9kPev+UhwHO/OZGo+FUw5H97CwiY8USiUSAjAXOR6o0WqKApE6hGr/4TIkNUztpQXxdqCtxYJ1fdXTVWdP3tb3WCeV7BCIuavUYPZ/9W0FYSVN1bi3oqlOuzioJgV6vF9jMSz9vx4zjxsUE7x/bxDq4HHOOb6lUcnVxeW/y+Tzy+bybTMIICb2uKrg0Yqb3Qe+znfCo2GVf6RRTBaYq5nQ67WouRhbZo2iMZtN50nRLKm80om8JAVWI2kCRPsfqiGogxdbHtrgbhoV6LRu1V2fK4p6+x2NtJD4MI62zax1kGzhj2zWLgg6nOoJsr6Y4LSIs9Lz6WxW3tiaZprjp2LG2GDHQLgx4v33fd33XckFaQghAYHdZGzDj9TkOwFmwrlwuO1UAVbAMhFG9awOvkUX2KJhmQakPk0qlAgFzxT7rh1nM0f/VLDnKYxf5mIuODSOC7XmsWaxi3+1coT4ef9uF7oPMCgY4H/C6XNTTZ7W1bi32ad+VmLEYq+SwLYlj74liofqs2m4lR8LWDGwf31O1sfZZA4TEbyU69Jr6Pn35yCJ71IzrOyVSgXD1qw368PgwDOQ6Vv1E9fP4bCquW3yzFnYdFY3xHGGBOraJ7VokrFJs5jykpnjCc9j26hpAhReKqaPRCOl02tWqtZlrbJcKIoA5nvI+6Dypvj45iHj8bLPcWCyGTqeDTqfjynXZet6Wr1FBlxKytlyazVSxxLTeFwo5lFtiH/mdY63xyM/92VtExootLy/j6tWrASWpLqCtc6SyfNaB4sOrqQhUZelrvu87NU6z2XQFp33fdw6xApCtCaiLdwus/DzVrwpm/OGGW7rTKo/Vh1VBjeQC37eEMI1qYJ4PgKv7N5lM3ITRbrfdZ1WRRvAkqappDVqP8PT0FL1eD/l8HplMBtVqFel02hGtCk48jot+Rui5AyJ3CdQxoqmilffT1nbheKmjb39oxWIRAJwajAQxAKeKuHfvHgaDAeLxs01nIqCM7FG1tbU13LhxA8BcITuZTAIKTHXk1JnR4BAdL76uEW8+s7PZDJ1OB6PRyBX673Q6TjFE/FOVq5o6NmwTCTwqbxdlU4QFxXgeu2kXsZkYqEEv62Srg6iLex7LmlXlctn1H5gTmIqVmq7EsdDFAYlXOonsL3FVNzuYTqeOQNVNK6bTKd555x10Oh2kUilHzHLMcrlcgMy2ixViJYNYdsy1ZjjPwf7wfPwsCVj+cNfv2eysREyEu5E9ira8vIwrV64EnmkAgcwmPv920QeEL8R1YczPaOBLSVvP8xw22AWlfV6Jc3bjWX6G2E+/kK8z6ysMx/V6+lvxWX1exUni/CLTwCFxkSW4er2eS10lVuZyOcTj8QD2KvFhS0hkMhm3K7veH/6v2Oz7vlM6k4w5OjpyfjKPVXLVkrJ2rrUkOv1/Dehxs1/ORzpP8/xce9y+fdvNxcfHxwvHNbLIHmarVqsBZTjNkpvpdNr9r2ZxTDGQvlYymUQul3MCK36Gm8S22200m81zQXzaIlEXTTkJu75VAYFmc4X1Iew6yrHotdQn1hJS/E1/XNcKzLpi0JEZW6lUCpVK5VwmlYoWwvzxRCKBUqnk7g99a0vo0pdlLe+1tTVkMhm0Wq1z9bwHg0GgDUrA2k1xeQ3NWtE5STGW2SEcK+VD6PseHh66eUw3dozsZ2cRGSuWSqUcUabEJnC+NMEiB1UjG0Bw8yY+bEp08iEE5qmWGuWytUoe9NBYZUKYgkDrtljCUo1EoZ5HwU6daiC4o66SDvwcnVKr9NL39VocQy4ONHrEulJ0vpnGTAeTRHAY4LNt/AwnLVUGh5kCpo6XbbeakgR6T5Sw1VQJTiiMtDF1YDqdYm1tLXBuTdGNLLKH2VKpFKrVKkajkavrBCCAg+oEEg/0hw6eRpeB8xtUqXKIz+dgMEA6nXaLfXXMFDPU7FzA32HzgpIVFm81+MU+Woy2aiftr+KaNXtNVSBpIFH78SDSQuc22wZ+xqo32BfNgCBxMpudpaj2er3A/MFxoMOpDqg6lSQBeB95rBJMShAsGkMNdLJMEPu0uroaGAcSy5FF9jBbMplEqVRCv993yhiaDTYBi/3gMNPnXzFK8YXPqfrD9ng1i7H2dYvR9hxhSjP9bFi/FL900aslW9gGez3rexPPSYDaVFQgmBas+KR4S8GHZu9pGygGsf6mZlaMx+NAHW5LSGsb9PyWTF9kSh7w3iqW8z36r1SRxeNxjMfjc3UK1V+PLLKH1RisppDH+iPAedW5fV9Ng1VhJCafNWBeWor7pQyHw3MBtB/HM4RhqrWwTFnOA3qdsGvpGFhf0HIPNmAU1gf2O5vNBhSlqjLltTQzi2Olojwezwwt20/bF36OOKuCA/28ql61bVbEpRjN70jYe3YciLnqh/v+mRCQWQi+72NlZSVwz9n+yP7iLCJjxexCiwQqX1OniL+t4pTOFeXqNnLj+z5KpZKLXsTjcfR6PaeQzefzKBQKbiMnRkg0GqXX1qgRH8bpdBpIg+d1bE0Z3/cDi1gFOB6jD7jWYeTxOlb8HDBXiAJzolYnDzqWAAIqAUbzuCCm0kGJicFggH6/7xRfhUIB1WoVuVzuXG1cJSK0JAP7wn5YJ1P7Zhf36lgq2W6/E/xf1Sa8L3RUU6kUstmsu1YqlcJoNMJ//I//EYeHhwCAK1eu4G//7b8dWLSoMxtZZA+zJZNJFItF9yzp5k8aidbao7pQnU6nrtay1ndmQEkVA1Th0zkaDodot9vuc+VyGdls1uEvjc+e4oIlBoltNqtAHWxLGnPOoYqLOGudcmKUlnFQotEG4JQE0ewCz/OcMoCZCTon6PzCc7PNWsNc8VLnIL5HbLdEjM5puVwOlUoFw+EQzWbTzSGcAyzZwGvyXsZiZ+lgPCf7z8UOcZdzAmtoxWIxV56AP7xmvV53u/BevnwZf+Wv/JVzi44IdyN72I0El1XHay059fmIVWphi2d7DfqcwDz4xb/1OFsTXM0Gz3QRrMSDtmXRAtkGnOyC1pIiGuDXeUdVuoo9PIddPDMzLJvNYjQaOf92Nps5hRTx3OILMZjnsDuUA3OyQBVTxD+ej5sm0kfVzBMe96CAIk19eyVHFPN5DV5vPB4jlUq5VGH64aPRCLdv38bR0REAYGtrKxRzI2Igsofd6G/SP1GfDoAjTzUARLOBd7s+Jeegvhc3suL/5BwYUNINvzWIbYVVi+xBwS/iWViQygrM1N/lbxVOaFAnLGClIg1t02g0cutt9u3g4ACNRsOdh1wA+8OfTCaDpaUl5PN5NJvNwMZaarbsQDabRTKZdHXBT09PMZ1O0Wq1HO5x7tV6s9onHUMNrLGN9IH1Hur9UCEKeQ9ifyqVwmQywcnJCU5PTwEAly5dwq/+6q9Gfu7P2CIyVsx++S0wAcG0fSBYkNuqCxRs6RwB81RQJeNI2vq+7yLX+XzeOcsWeBZFtPXBtQ7mgxQFuljn6xrF5muL1J5hIBXmFBP4CRCcGKwijEBMZ46fIbAqKaOAqveRf6vTzgWIVaK2220XCVPVsPZdyYZF42n/Dvve9Pt9Rwyz37xur9dDu93GeDxGPB7HpUuXsL297ciHyCJ71MxGeml8Zq06CZgHSFgqhAtPVdUSe7WWNACXBsagCJ9XOlzEHnUGVaWgASptZ1ggR0laPV4JW1taQFM7OSbaX/bZEgnqTCp5rKoCvWaYs2VJXXt+zgE6Lz5o8a4BOW0/F+f6oxusKTGgagFaGGmsRIK2h8dz0zAle7VGppK329vb2NzcdOnFkUX2KBmJVw2SA3NMs0Ft4Mf7OGqKOTawxOPDxAVhBK8lWBeRrdYPf5Afxr9tFoRVQGUymcAmhFrfn8bxsSSlNcUyEiM8jn6wttOqXC2BrX2086f2jb64Eu3siwb4beDMEtdh19R2KVlEs2nFOjadTgfdbtcR/+vr61hdXXXzcmSRPUrGZ9DWRqXZZ92WKlQLW4NqkBmYl04EgpsDMiiiGwuqL2nXmYsyCvR1y3/wXPq3ciVhPrKKv3Q/AJ7f4rMVI4QRmePxGK1WywWzSEjys1pDVjPHKABR394GqxQnleSOxc7K0th6rRa7OV9wrWLvpfZD+8WxtxyMPVavyX4wE4b8wtbWFtbX1139+Mh+dhaRsWLT6dRFl+wDEAYmNh1IHzY+hDalczAYOFKRClnPmytBp9OzXfsKhYKrYUjl7I9zNO0DagFDHWL2jyQgHSCCIoHLggLHiH/rAl0/R1IjTPFAB1DT5EjC8qdUKiGdTrv04aOjI1dGotVqIRaLBVJdtf6Ynew0Sp/P5+F5nksXSSQSGAwGaLVayOfzyGazyOVyjihWIFNymv2nLXJGtb9U0h4fHyOVSmF9fd19tlgsolAo4NVXX8Xu7i5GoxFKpRJ+4zd+w7U5ssgeVeOzy4WiBlnUWeLzRxyiIpa4TfzShad1toC5+nI8HrsAD9N1ueu0xT+L9bqoVwJUP088pBpUSUB7jJ7H8+a1CblwpqqCi1d7XcVsTePX3Wxp8XgcuVzOKWz5WeKM7QsA59BqDUPbX95LnRtJio/HY/R6PZcNQqwtFAooFApoNBoBxS+xXOtA8jvA11RNxmtwzDk+nK/6/b7b2RuAI1oSiQTeffdd1Ot1TKdTFItF/Mqv/EqEu5E9sqap7nyuNRCUTqdd8ASYK5qsEEAtDDsW4Qj9RfUh9Rz82xKfGiBTU0LUqoksiRsmoCCOMH2XG/gVCgVHxnJRr8pUYrMNWGlAjUZigJvJUHSgm8dOp1N0u92AcIB1rW2JLPW77Rjbe8j7PRwO0ev1XJ/Ybvq7SkpbwseueXhftO2WKO52u4jH4259obVv7927h6OjIwyHQ+TzeXz5y1+OiNjIHlmjsp3rY5oKB/gM8tmif2IFRhqkB+BERfSjudZkORqSsMxCm0wmSCaTqNfrzo+2PAdNeQX7t/IRi7CVuKublykHwWO1Vi7baslPnWN0fuJ1eG5ar9dDo9FAqVTC0tISisWiwyuOL/tIgVixWESxWHTEueVTbE1uxVzyOGtra24doxsSa/uYDcx7rvtd6Nxh5zPN0LBiQfIoei2d9w4ODlCv1zEajVAsFvFLv/RLLmMssp+tRWSsGJ0ouwELI9bqsHLBy4fRKilpSg7yAWMKrC6kuUhk3UTgjEhgSiZBgJEsBTzrAKlyVZ0k9k8VQUx7omOmn1Hig/3QCL461XQ+2S86d+qME1TYXkaoCoXCuTHjZ5iyDMydWY4fI05a8FpT0fQ+8rPpdBqz2cxtgMD7q4DHiH0+nz9HVNvvilUfWOfYRsI8zwsoLXSC0kmHx4ZFHCOL7FE0pm/qs2wX6Uo06v+qAOD/utCmCkgxl2Sklhvh80ZSb1HE2AbDrGPK1/i/dU41WMfXtKxCLDbPlCDJ2O/33d/ab2JTmHMWNjdZ51adNb6uQUS+bz9vx2NRlJ6ONh1y4Gzzgl6vh263i2QyiaWlJbTbbXfvOX6q0NIxZ8qYKj4sPpOg53eAimLibiaTQaVSQT6fR6PRcEFBfo8i3I3sUTX1X9SHIR4R/+wzxd9hrys+WHxUXzksYK4qJ80aULMBOSCYtmrVqhqco99lMXpR+/R4Xpd4ZJVYShorFmv7SSwwjTWZTDoRAVVYnKdsO+3COmxsbH8saW7vF4N7xE3OKRq00/sYRgDpPdPfajp+KrZQ9RY/t4jkjyyyR8VsQB/AOcKP7/N/y0fwubSZQvw8911Qf4tEI3GKmEMS8PT0NJD2bp9HJUEtEat9s4E9rm+JebYMJMlK+nDKMwBzTNFAoB0/xX6bQUUC3PM8dLvdwKauVLAS/3SMdEyVa9D7Q8zW+cKWTaNIYDAYuP5zDqCggFyAjq2qZcnhhH0XwjDXBi855jyPJYYjzL0YFpGxYup0acoQIxZ0VJRo5MNiHRIFBfs3F4pMV8rn8y5ixQ1Eut2uK7ZNaT2VUcBcUaAprqoM5W9Gy2iMxmezWaRSKdRqtXPRE/5PMCCA6TgQGHktOpBhERk60LrQ5gKdEbLxeOzAie8DcH0myLXbbdcvG3kCgtErktocd713rOlCkkbLSrBGDOtt6eRiJ1Mt72BTEazjr4q9WCzm2q9qW+tURxbZ42C+f6aK7XQ67lnVgAkwV+AQwxT/+KMp6OogKd5YUpZOKHecVWzhTtdaD08dGn1elWCw0XRV2ALB6LZ1tNVpJpaRqOa1tfafVXhZcoTn1f5aMkGVtIr5JIjtIl1/a1/DCBRVi7GWerfbRSqVQrvdRjqdRi6Xw7179wI4rtkgmkrn+77b9IsBMzr9uvi3tckYxOO8ms/nsby8jKWlJRweHjpFWmSRPerG55HkGJ97DXzrAtli1I9bwFlssFlZ6ivaBbQldJVYVJU/sVgVSupn6pyQSCSQy+UAzBfXitea5cCADc+n6lyrwFVy2JKxqloD5uVx6EtSfcv5jL46+6S/dVzs2oPjwHbaOUXbxR/duDKRSJwjYzUNWE0JAl3TqDhFr0nilSpsXk/HLLLIHhcLey4tsal/hwl0lIgN8z+J4yzRpcFnpuDT51K1/GAwcEFuYh/xTdulbVjUFvIW5DY0CMXPsixUt9t1gXW7huY4aXmCsLlI1wn0bzm/sU43g/0kSZkdRj5Ga6tq/8gFhAkOeP90z5jJZOL4G25Q2+/3XTmuSqXishZ4PO+f9aHDiF4NEtqgHTklmpZ70E3kI7t4FpGxYuPx2G1qQlPHUSMLAM5FKWhKZNrUAgU3ph8xKkKnmA8pNw0gsGSzWWxsbATSMdWokqIzyjQF1i5h2xgR0rqt6mCpIximDghzjFWJpmMVFrGzaidLuPBadFiZ1qrt1wLWdCwJqGyzEghW6bW8vIzRaITBYBAoIM57oWlmXKTrpl/q+Op9V0Kc/eH3iH3juHGCmk6nuH37NtrtNl577TUcHh6i3+875z2yyB5lI+6ytIkuHpVMpYNIXIzH44Fasfosak1pTY2y2QxAMEgSFhBRJ5e4ZvGRxnNbwlIj/FbxaU0VshokU5zUOUb7osEpLvJJ3FolAfFMzxtm2k6muyYSicA8ow60khg8N+crjjMV0O1226WPra2toVQquTItixQAnDNUPazjzMW+OrtKCAyHQxQKBeTzeRweHuLOnTv4xje+gXv37qHX66FYLD5wPCKL7GG3yWTinkESmPSj+NzRT9EAP03JUrtwBhbXlFX/Tc+jn7fPvSoqNb2fpIMGxfUcFhtUaGGzBXiMzi1UUzFDjWVjwvpEckEDQXa8iEHaPlvTnIt/9l3XBXptVaUyQEnMVSEJMM8AUQKYYgXiM9ugu7yznSomCCNf7BzHvy0RzvYdHR3h5OQEb775Jo6PjzEcDqPNYiJ75I0lphhw5zNtg09aroBBFpr6ahZjVejE9yeTidvsutfruaC1+sIk8IijGtzi+TRgDwT3ylHSlj8UO/H5V1GTDeiwZGC32w0Qq8TuMNwjjmvJADXfn2/+zYwyxX32i742ORKbTaX3IUxsYa9L7G02m07MlclkXCCQe8JQKBaLxZDL5dy1yEnoNXTsOXb8vPXdOR9wrmH7W60Wms0m7t2758RmkV0si8hYsel0Gohg2MiwLsbtonuR82kX9xrF1lqACmgKcuoIMbLMOlJWOk9ilQvfyWSCXC4XWFwSPNQ50n7p5jDq8NlIDEFAQZjnV+JTwTKMiFAiwV7LOvZWZUHnmkBJINRIniWIeY5CoeDUALwnJHDomBLIWEfM7mLLY8MWKZxktS06CTJayPt0dHSEe/fu4f3330e9Xg+McWSRPco2mUzQ6XScY0QLi/oCCASRNPhEvFGylL+tMojn5992Ma9RcSBIohJrwohbdc5sdgAQJIk1aKPXBuZp8krKhqmIFqkm1DG3Tj/bxt/aDmsWg9knvR9h8yRJBD1e1Q/D4RDD4dDtMptOp1Eul1EoFHB8fByaFqbja5VqdE51QUAiifeQigf9f29vD/fv38d7772Hvb09AA/e2T2yyB4F4yKdvigxQHfk5qJOF38f1ugrh+GvLn7V9DlmaScql0jm0ofjzuHqp6q63woMNGAEnN/shLjJsmAUQGhgjtfj/9p+i8HqL6v/q6StZgFoO2zQX00X6jrXaFt0g1sS2Z1Ox60hVK1sA2v2WpYc0Xul95W+rqbm0jqdDu7fv+92Nue4RBbZo2wMdmWzWafWtGYJVRX16PNl/U4bmFefj9fxPM+VYlJBlZ5fg282mMVgNj9PHy9s42uKqOhXs/SWKvfZD887U6b2+/3AnKMci/q51s8PU5QCCChaeU27MRivR1GUBtR4L9QP1d9hQUQrCuEYkHxnBjBJaBLWNlvDri00iMlr2ICixW0l1judDo6Pj9FoNNDpdNz7kV0ci+6GWL1ex+HhIXZ2drCzsxMADToqBDkCi74GBKPE6mwSJO1DDeCcc6YbpNAJrVQqDtgsEci26A6A0+nUbZaizhpNF9X8n4tlG4XRKDdBihF0dWB5HoKakieaNqqKKgUvVcRyvLl5mZYqIEHBaxL4VX2l46nEsjp9iUTCFepmmth4PMbp6akDZe2XkiFaW0fNfg/4N9vs+z7a7TYAoN1uo9frodls4vbt29jf38dnPvMZbGxsAIArTxFZZI+yHR0d4e2338a1a9dw5coVV5rELt6tApKEATDftZkbgGnJEwABR9EG0BQf1XmbzWaOsFDHWR1APY8q7O08oY4rVVR6HY3aE3uY2UBMz2Qy55xAXlPnDY4NSYywRb6SqTR18DQ92Tr+PF77SezksdpnthMIpgHzuNPTU3S7XWxsbCCTyWBlZQXdbhf7+/sBJUAYYcx5StN09Z4DcPNiJpNx88H+/j4ODw/xzjvvYG9vD8899xw+//nPA4AjfSKL7FG1RqOB/f19bG5uYnNz0z2/wHzXb+KrxTnr29AsIWcXqorfYeewWU2lUgmFQgHVatV9hqm1hUIBnuc51RODO/1+311L6wMq5oYF00i60n8sFovO7+TieZF4gEpTuzBWhSzf12M1uAcg0C7+73nBcl66iayOqyUJ2DZeB4CbP/L5vCO57f2gglYzz2hUWtGHpopYy5jpXKX432630Wq1HObu7u7ixo0bWF5edvc+8nUje5SN6zzlF/iMqY+pz60GhvgefSv15WgaOFO/Vv0oNfpGuVxu4QZb/F/FRDwnsUKzKGjENi0tQGLWYiVrqHLPHCueUp9b+6FZUMRTxXkdD/rFmiFh/Ua2lQpm+uOK07wfGuzT/inJzGyO1dVVVwqCx2o2Mf1TtrPX6wXKLPL8ynvQj+Yxur8F+9DpdHBycoL9/X2cnp7ihRdewNramjtXhLkXxyIyVmwwGODo6AhLS0vnFuz6twUFK5PXhzFMlm8Xt/bcuvueRqBUvq9/q4KAn6VjZ6P9bJ9VJejrPLd1mAkuBDPbZzX7nraB71myN4wMpkOs7dQJCZirVAmwth2qiLDKOy441JnUsQ2LQPK8Vk2hZqN5bDf7zgVEs9l09Qp938fy8jKeeOIJRBbZ42L9fh97e3vY2NgIFLMPw5Qw3OJ7SkjqZxVTwsy+rnhNZ5mOkQblFAtoNtNAnVcudMOuzWP4v55TMxSsqWPG45R0tsdYZ9kqV/U4S9YuUgdowEvnjge1V8eP5Xq0bpc68RqstONmx0v7pIQwsZ9zSr/fR7vdRqPRQL/fR61Ww87Ozrn2RhbZo2iDwQCHh4eo1WqBgA/N4gItjJCl2df0f8WMsNfVJ9VFLFM8iQHpdNqlfmrwSjeVVV9aydgHbYRj6+kphnLxrH2ypKslOsLmqEXjpziqtQcBBAJb2i6L+3peFQRY/5N94sYy6sfyPFZQoISv9pvnU59bvy86V9PX3dvbc1kw1WrVCQ8ii+xRt9FohHq9juXl5YCPaf1JNT5zYX5bGIbYZ9kSsdaP0mc3FpuXcgrz3fQ9nk9LyCjpuijTitfR83MNTiWnCh+sz2n/tsG/RZ/XucUqb5WP4PjSVw/z9zW7gSSx5Yt4H2azmfNBNbOC59T1AIltYC4qsfdMSXheW9ccyrGMx2P0ej2cnp667N7l5WVsb28vvDeR/ewsImPFut0u7ty5g0uXLiGXy2EwGLiSAGGqIb6mqk/rbCkpQJKVJQLsgpUPtQIbnTGqpXTXcF6TERqWO2AEnA+qtkdrSwHn0xsY/SYIWZk/AOTzeZdSoMfT4QXgZPlMfaPKKyzipp9l3y0hzHFherJu4sMIPwlN4GziyOfz59puF+1UgJRKJXS7XRe56vf77nx0/OnAkiC295t94VjwfnM8ubgolUqo1+v43ve+h+PjY+zu7uJzn/scnn/+eeTz+Y/vCx1ZZA+BdbtdvPfee1hfX8f29jYymcy5HUA10KJOEJVR3NxKazEx8suNC8JqZeki3RJ/VCtoWRiShOoA0XnUCDyNThOVr0rqqnPImoTWOabjp5sL2N2odW5STCOG24W8miVk9drAvNarOrp0EklgcOfY0WjkyBM66Hyv0+lgMpm4uYljwLnI8zwcHh4imUyiWq26jQ7oTCpR4fu+u59aeoD3h9kkHFf+9n0frVYL9+/fx+npKe7fv49PfepT+OpXvxrhbmSPlXU6Hdy+fRvr6+vwPM9tVMisBAZGwsrGqOmzrMID/bweZwPxQFBpT/+Vm8xMJhNXV3Q8HqNSqSCTyeDo6Ajx+Nk+CtPpFJ1Ox2E1N6clPnW7XTc3sH4pN5JqNBrodrsO94kjzWbT+Xz0sYlT9EGJvczKAuaLaF00K6nL+rMsHaCBwtFo5H44nvR19TfHj/NT2NgD5wkR9X1JahOfbZ1Grj3S6bTbhbzf77t2UMnKz6tfzHtOH7zb7eLdd9/F7u4u3nrrLTz33HN4+eWXA6ULIovsUbfhcIjj42Osr68HSoQooaeEHBDM2gKC5aLo69nNq4HwzVVpSkZqWRJeP51Oo1QquWPo366uriIejztfm+Ilrt+1vbym9d2BYJaatml5edlhtZaFYSA9Fou5mubKrywitDWbQP+nKlX9fG4cTB+ctbw5VhRuMUt5MBig3W67OYP3g343cZDYqtm8xNx+v+9UvKlUCtls1pXb4T2s1+tOKUx/Xvvu+36AvNasjv39fRwdHeH999/Hiy++iBs3bkSYe4EtImPFxuMxOp2OS1kCzm8cosBoF8EaldZjrBpAFbIWQOjs8HULqhq91xQHbSujILyWEpuWLAhTM9Cx0xQIjbrxHJqGpVEjkqo2CqTAaScJuzC372kNRU2ZsO1XVZROMra/lgzRtDwCIidMPdZeT++HXbxYRQEJGTr5xWIRzWbT7e5dqVQQWWSPm7F4f6fTQbfbDaQ82R8lUEkAaiF/4patZ2VrbulvDSrRLF7xh0GYsBp3dJTUwWWbwzbRUiy2KiRtg61nq9jGY3WM6PxpxoGdg9TClFR2vBUHrQLZLhCUKOaY8b7wOpxbtG4VyVY6t5r+a8eGFkYuK2Guc0c6nUY2m0WlUnHzfDKZRLlcDh2XyCJ7VI0BEqZD8vnURfkivFhkih/62qLfijV6LVsmizjPzRqJi8QGu3BnH7hJraaT8vM2oKY/AJxPRjWY+uQqwtBAlj3HjxsrxX/FWn1Px0nHSF+zZE1YVoMex2uokEHfA86XrNGgIz/POYEBQyWdieG8JyRESIhHmyRG9rjZbDZzG01TOEUj5mppD76ufpNVrdMUf/R6VuEahr88Hphv0EpRkQbPmEGhcwT9a/W9bfss1mu/FDNSqZQjSpVXCCOUNfi0CBctP2B9Zl6XeEkfX8tE2rlQ26zt43wQ1hbLY9BHVvUs/+Z3gHsDWaxlG6wvrfMb75kSw5Gfe/EtImPFBoMB9vb2cHJyglardU6lqVEJfdj14VAQUeeMYEaSgQ+8Ol6aLkbn0i5utX4JQV13mvU8D61Wy6kGVEGqJIU6ferYsk90nqwCikQir8dIFQGAIK7kBR06Ko37/T4ABOoZAueBTgFeVa7sr5182F9V9xLkbL8IwurwUm3FfnABMB6Pz02Q/JyWUSDwcZJi2/idaLfb7n7k83l88YtfxCuvvIJbt2593F/lyCJ7aGwwGODg4AB7e3vY29tzEWgNOqlqic8kf6tDplFokm9UASlpyM9qMEXPQwKAzzcDM1QiKQ4y2szPKJYSc4m7vLZuNsjPsA2azqT4DZyvSa71+zRgZskCdeLVYVPTxTPJbVt/i21XNQJxlfUcdXdcqgDsJmScE1Q9wd/NZtMpYz3Pw8nJybn2hs2dvGc8J/GcSjY6pFeuXMG3v/1tvPHGGz/5lzWyyB4BY5mCo6MjHB0dud2vSdCpup24p2YX8rrQVSKPprilPldYOiv9ZK3VqgHvVCqFXq/nCGWeF5jvCRCLxbCxsYF4PI47d+6g0+mg0+k4kpA1+tgG3VgROMMXzcJKpVJot9sOXxKJhFvccrHLeYbpoark5zipaEMFDcQ/9p+kiE2D5Xk45qxtreSAEgz2Xqmal8dTXcVj2Q5ekxsan5ycOIUyCdhkMolMJuPmNmaDce7kvbx27RoGgwFee+21H//ljCyyR9DG4zFarRYajQbq9TqKxeK5TbG4riYu0Xe0gWgNVFklqQaz6LfRX1X/T31FXb9ns1nk83mHPZoJTEIZmG8MqAEd4igAl+WgJKGSnOw7z8vgWL/fdzhixV4qtqLwgJjO/R2A4Bqd/3PcgKBwisE5zgP8TWMWBfvA2uTEeGYasLwWf3gd67vz/lIFTDxlf6hMzmQyaLfbmM1m6PV67n7ze8K5SceXfncymcTly5cxHo/xzjvvfAzf3sh+2vZjyVjP8/5fAH4dwKHv+8/96LUagP8PgMsAbgP4G77v172zb9v/A8CvAugB+N/6vv+dn07TP34rl8t45plnMB6P8corr2B7exvlcjlAGtqFrIIaTQnbMNWVLpr5P0FJyV9dyKujpBF0qxBVYpSkgJKJJF41WsNjVFGlbeL/WsCfbdA6MdYsyaznUlVUGMHN/mWz2cB1dAJRgkAX+joGAM6REnbsw1QNmi7CtijIkjDiGHOxoptGqKJCr817y3S6yCKz9jjhbrFYxNNPPw0AeOONNxwulEol52yQGNTotUb+NS1Ug07qDNEUq/mMapRbF7FKyPLaqqiyC12aYpU6eRoMAuCcXb4WNlfoYl2dY7bDRvvtj7ZnUdv1M/zRucYu7MPmHv2fP6qksAsBva4SFcRUjnUul4Pvn9Ud5Nxo0+F433VThDAyQq8XWWRqjxvmPvnkk5hMJnj99dexsbHh8JalXfRZtQqkRaaYo6+FkbdAMPOMuE0fNcxI9BG3GYxTvwqAWzRTYT+dTpHNZgP9YTuIHdpGFSHYeYWYzMAc+8g2AXAkhNaAVdJayQlL2qqww46Z4q/+5mfsHBZ2b8LukQ1McgzVF1eRAa/DOYnv2fqxTPclcRNZZNYeF9wtFov4xCc+4QJExFxNYbdBLGBx5hZ/P8iX0yCNbhILzDGLvnIqlUIul0Mul0M2m3UEryovLa4QS2zpLC1PoL6digQUr2jKeXCNr+v1MOyyx1p+xbbXjpO+pv65xWt+Tvuk/q3ioR17vSdKtNM4n9CH5XGZTMaVNCBBbIl68iD077nOsJm6kV1s+yCz4z8D8A8B/At57X8E8J983/+fPM/7H3/0//8ZwF8B8NSPfj4L4H/+0e+HwnZ2drC9vY3/8l/+C37nd34Hv/RLv4Tr169jZ2fHOae+7wei61bhSlDV92jqKHGhaclRHkuVEc/HB5wOJq9HMk/BTUlD/k0Hmw90WBkGJYItgNKpVWDxfT+geLKTgoK0RnEymUwgSqZKCgU8z/NQLpcDUS6Ol9ZsVdUXgECUn6nKPL7X6wWUAZzMrPNPIpgOpKpDCJIc+9FohGaziclkgsFgEHDIOWa6ezjHWmtjRhaZsX+GxwR3t7e3sb29ja9//ev4/d//fQBnz/lzzz2HTCbjFE2sU63EJNWPGigiHlHRYyPT6nxZFbvuIO37vtvh1WYAqNLVEg12Uc+ovefNd8bmXMI5QlOz9FyaYUBnzeK0YrUG8vi+mhKUYUEpzjOqFON4W4c4zBG1hCyVW9oOqh54Ll6D95CKOCowarUafN9Hp9MJ1FO3RAGJHP1+qNPKvnMsI4vM2D/DY4K5a2trWF1dxXe+8x187Wtfw0svvYQnnngCTz75pFuMT6dTt9mS+mgWUzSIbjE2bEFsg+oAAgtb3cna4jOxoVQqAYBTDPFa6oM2Gg0AZ3scpFIpjEYj9Pt9tFqtgEJUz8/jNcjOdtCvY2CNmU65XC7gjzM7gBimwUOqoIhhzLogfuqY6U7ZHDdLYtugls5dYfdA7xXXKMB8l25iLjBfO2hWWSw2r3tuF//MJlNfWhV3ERkb2QL7Z3gMcHdjYwMbGxv4xje+gW9+85t4/vnnsbW1he3tbbdOVH+KzxZNyUsl+vicLgo6a9BHeQpiGtXtuVwOlUoF+XwepVLJiRD4TLPuPxAMsBFDSQBOp1NXh1uzackjMDDDttvAGM+jJWY4NygPYH1lfoaYpWUBVX2r2GkDcuQzOD+Rt1C1MUUhOid4PxIRMDtOjw8jagEERBjj8RjtdhsAXHmu8XiMQqGAZDKJ4XDo1iEqKFPyWX/7vh8qRIns4tqPnR193/+vnuddNi//JoBf+NHf/xzAf8EZUP4mgH/hnz0l3/A8r+J53obv+3sfW4t/yuZ5Hq5evYp4PI7t7W1HyFEWTsdDSdSwaLMqR/Xc+jklI4HggpXpnQACzpuCGI0LYF6LabPWkdPaitYJ1o1QtA+23fwMJwu9rqpVbRqpfi4saqROJYlfKg2Y7kDTCYIkqVU7KBGsbVZS1C4i9Hgdcy1Todfh/ZjNZgEQZxt5Dl6TY0DCdjweO1Xg0tLSB/+SRvbI2+OGuwBw6dIleJ6HtbW10KAQsUtT/JWYBM6rifj8qpIyjCDQ69GUjFSc5nX4mtY1JaYoQaDnVfzXcyo+ss+6oCam6oYAinWWnFXy1GYbKNbZH2Kq9k8dX0ug6FjawBswJ171HqnTq4FGfoY42ul0AgsG7qqun7XqLXU+lQDhPeL/uVwO165di+p0R+bsccNcz/OwubnpMDebzbofm5pqlacf9PwPMkuiEhN00at+N9uQyWRQLBYdNih+kBRQdWm320UsFkOtVnMbPhYKBZRKJVfDkcEZzeDi8SyzYtuu6i9d5FtTUYMG98PGwo6LYpvaItX/gzLVlCCw12E7lbhQv58kiq57dH2iSmJVfTGAOBwOkcvl8OSTT0aYG1nAHhfc5XO2s7ODWCyGtbU1FItFtzGr+pn2xwb59bnUIDRwXhBg19Y24MX1NUnYfD6PYrHonmeqMjUwpf2hb0VM0OvR9+N7ljfwPM+tzW3flGyk6Wet76q+t5KzSr5aP13H0KqGeR6+zuN5PxQr6S8zUKfEL/1PvUfaR15rMBi4El8Ua3HOKBaLjhNSDNZAn/JKw+EQ7XYbsdhZuZ5CofDhv7iR/YXYhw1Vrgn47QNY+9HfWwDel8/d+9FrFx4o1Z544gk88cQTAILqTv5ojRb7UFmSUR00Jf5isViARASCMnaNotMR5HEKdjyvErJctKrDxweYdVWoQmL7GJkL25lRF996Xks8aCTOgp6mEKgqVclYBTuV2ScSCeTzeXee0Wjk6uXq5gy2DqPK/ZW4ZeSM7eNGDZZw4ILEllPQCZKf4z3StDQq2vh5ravFthcKBdy4ccOpv37cAiayx9oeadzd2dnBzs4OgGCBfuKF1sdiZN8SrWERYq0ZqwtPVXtpAEdx3JIReg6+blX1YVkCYQ6yYqkuhIk7aup46Q66bK8GjLT/+hltn20Hr8GAnWK5nQvUgSVRwjYoEQvMsxSA4KJfyVgu5u1c0uv1kEwmUSwWHRnb6/XcjuRUP9A0vU37xw0tuWgZDAbIZDIRMRDZB7FHGnOp1gLmNf4YCOei0i4iH/Sbf9vXFxGElni0ZQCI/1Rp6cJXyU1iORWYGqzhIndjYwOZTAa9Xg+FQgHFYhGtVgu9Xu9cUEsDgPQj+boGvJQ01mBUWAkXzh3qq7KNHINFawdr6tuHrT8WjbMG5tRPVwxWX9wSyeyvlmlQxRfnaGaNsazEcDhENpvF1atXUS6XI183sh9njyzuMhOMRv+Mz57uC2NVr4qrmu6vzzWfbRVi0Yg1GtRmndpcLodisYhcLodCoeB8v2QyicFg4Opz0+j3KWegZfr4Gb6nvjvXzFwPW8xSXOI1ODb6vu2f4iCN42AFAZrZxbYoP6PzH6+j4guOt+Kfir1slpgVEqg/TFWtqm7ZJvIfnueh2+264J/659PpvNwZ/fdWq+VIf+65Y8cusotjHzlvxPd93/O8n7gAm+d5fx/A3wdwoXd5m06n+P73v4/ZbIarV6+60gF2sa2f14eUde7srtFcJNIp0YU5P68OkhKiNL6upKXneS6CrQtpVQhNp1PntLIvrVYL2WwWhUIhQLYq6FCCr44g/9dIDfuiZC3boot1VQnY6BbJFiVddMKxExWvwXYTuNWhTCQSLn1MneHpdBpIz9LPa6qeqoat01wqlRxI8r7bvnHsdXzS6TQ++clP4tVXX8Uf//Ef47Of/Sw2Nzd/4u9pZI+XfRy4y1TPi2iTyQR/9md/hkQigaeeegrD4RD37t1zKVVU6mhZAWCuXp9MJkilUshms24hr06v/bwunC12huE8cdemXqozBwSdSTVii1UNqMPI63IO4HsM0ikxoEomVXXxh5jItvP8lhQOi+ATj20fw66nQTYGsnRs2Ue2n3ipgTKeazgcBtRXlUrF3dtFNSX1OmyHnoP9X1tbw5NPPonvf//7+MY3voHPfOYzWF9fDz1nZJEBHw/mXmSFymw2ww9/+EMkk0lcuXLlHPbpwpIW9rxZC1s4h5Fx6msqOUFfiurWYrHosLxYLDp/kGQCz0uVEbGftaeZMkv8ZVkBvsY20P/M5XLI5/MuEESj70zsZTuAIAlhfxNbtQwA+xc2FmHnsWOrBIgda71/ehyDeuq323lPRQc8PzcWs4RPWNCN97DZbCIWi+GJJ57A22+/jVdeeQU3b97EysoKIovsQfZhcFcxt1gs/lTa9XHYbDbDu+++C9/3cfnyZbfeZeanFV4BQVVnGObqelPX1fa6SpDyf/XN+PwPh0NXbsBm+qoPqeUDgfnadzKZoN1uO46C16WYTQVM2hbFFs2MsGv9MGwFgnhHrsOSvJplRgy16lP+aM1WHUNyKzoWbDPLCrA9Or/x+ixtw9KUOneo/81SkwxwqRhC1bEaIIvH41hZWcGdO3fw2muv4ed+7uewtraGyC6efVgy9sD7UWqA53kbAA5/9PougB353PaPXjtnvu//IwD/CAA2Nzcv7G4as9kMh4eHmEwmWFpaQj6fRz6fB3A+IsPX9GHVdFF1MBVg+Bpr0QIIgK0SmRZsVB2mzrJG7pUAtERrIpFwtbQUrLRfvK5dqLOPqlbQ9trx0PMxGqWLZe03+7OosHmY8x92TevoKoDpmOqEpccwakUiVSckdT5Z54WKV73vbI9Noe50Oshms1hbW8NgMMC7776Lp59+GsvLy6H3IrLH3j5W3F1fX7+wuOv7Pvb2zsQO6+vrmEwm6Ha7rkasOnx6jKamMzCljpySCYpdijGLyFdVC6naX9WtPO+iqLhijC7i1ZFSDLfvKa6HYZ/ONTQer7it57YBLcV4xWMeZ6+hc5ptkx1THq+ZHOzbcDgMOLhaBiiXywV27bZjrvffvq7zBNucz+exsbGBV155Bffu3cPTTz+NWq0W4W5k1j5WzF1ZWbmwmDubzXB8fOyCFarg0Uwi/a2mz78uONUP0s8uOg+N73FRGY/HXSotcYUKTN/3kc/nneqSP8PhMKAw0kwK4iJfU4xmn0nIplIpp7C3fqJmcfw4XLbrAxsgC5t/LJli/ekwgjuMHLUkur0v6lNrG/U9jiPXDj/O2K/BYODuT7/fx+7uLq5cuYJKpXIucyOyyPARcVcxd21t7cJiru/7aDQamE6n2N7edqShLWMSti7n8WF4YdfpYRiia3W7BtZzK3GqxCX9Pn5WeQaLIRRHEGe5Plb/16qAbVuVpFR/nrZoHc+/NYOOc5tmUj0I9/T6+ln9CeM91J/WuUfLL1DUoZm8Yf63ij+UqNWgF/vE8yQSCWSzWdy5cwe3b9/G1atXUa1WIz/3AtqHJWP/fwD+OwD/049+/568/n/wPO93cVZUu+k/BLVcHmTxeBzXrl3DaDTC22+/jWKxiOeff96BzyKFEh86Tbfn3wpAvu+j3+8HyhGoEQRUvcUHkw8hVa6ZTMa9x4iWOqU2ZZ4RuOFwiF6vF6jPp5ExtgOYO25KUFpQB85H7O2CnmOrDrVG/Khe0MU6yyxoWoSCehi5qwQKSVhtq5LTGiUD4BR4+XzeRQftDu38LHfq5bjwXrI9/FGCOR6PYzgc4vj4GC+//DJefvll/If/8B/w7W9/G7/2a7+GWq320b/AkT1K9tjgLlU0o9EIr7/+OtLpNNbX1zEejzEYDNwmAHwW6aRoepEtvA/MF6/T6RStVsspMBmdpsOo2Ql0IJVgVSeIWKdKf13QK/axjXoOYrWWHNBjlYDkeTn3LMIjJTyt2on4qIoEdbz5ty7YdQwtsa3H2tesc2kJgbAgm/5QIRKPx125iUKhgNPTU6eG04WJ1oZVrOccSGu1WojH4/jqV7+K3/7t38bv/M7v4N/9u3+Hr371q1HpgsjUHivMXVpawng8xne/+10UCgU89dRT55RKlsTTYJM1S9CqH6nYosEp4Mw3zGazSKVSgRTLlZUVVCoVh8vZbNb5Uul0OoAHx8fHrt4hVUr9ft+VP9AF66K9GHK5nMMdLcOi+E9sJp7q2Nhx0EU21wOLPqvnsaSBxdMwEndRYE6z1oiRxO1CoYDRaIRutxu4v+w3S0ZoOTT6tVR5AXPVbzqdDsyhJycneO6553Dz5k1861vfwg9/+EN84QtfuNBZOpH9TOyxwN1YLIZr165hMBjghz/8IYrFIm7evOl8UfpAWnbFBnsUe21gCJjjK0sTep7nVJ4872AwQK/XQ7vddtkEfG57vR663W7Az7VYyTYQa4kDVN9TzdnpdLCyshLY2FUzo4bDocNs62dqEN7iY5h/qYIui6P6w+O15rX+rX6kXksFGcA8aGiFa/p55R7YVt7ffD7vxoD3RzkhYi7LWPCc5Hu4yTDblMlk4Ptn5b4uX76MK1eu4J133sFrr72GX/zFX4z83AtmP5aM9Tzv/42zQtrLnufdA/APcAaQ/4vnef8DgDsA/saPPv7vAfwqgLcB9AD89z+FNv+FGh2UXq/nIlhUsD5IHaCO66JFLT9PotVGmmz0ShfISgTYCFNYtExVtHptFuYmALMmoV3chy3Q7cJZ+7cokqf/hxEVusujjrECof7Yay+6H2xT2NjYfmpkjQ4mwS7MlPggeCoZoMS0XoukSq/Xw/r6OqrVKvr9Pvb39xem4Ub2eNjjjrvAWTDE8zycnp4il8thc3PTYauNRFuiwGYL0DQIw7QqLWdi6/7R0VFnVzGYphhsr6ftCsNsnQPCStM8CNfsdfQ1XsNG+YHzpRgsYcv2hGG8nQvs+/qZsPfCiIOwvrBNvFdaJxdAgFSwwUA9PowQYjbI1atXcfnyZYzHYxwcHARK0UT2eFmEufPU+9PT08CCWHHrx1mY/xeGDTT1XTUY5XlnGUfckBYAcrkccrncuTJWJBuUcOBilNlNLLPFgI2WLVFM1sB8mJ+v2G/9+w9qlii1+K3jw0CgPdaOn/Vv7Rhbs+sKIEj6hrVZ261zGo1zqCrcPO9MKMG9EsrlMkqlkhMiRL7u422PM+563lnWDwDU63VHWtpsrTCjb8Tf9rP04YDgvjI8VtemvG6/33e+IYNF5AcsdxDmX1k8VLLTBqPoX+r6WJW1Dxoz62NbrkD5Be2zEqh6DNul5KsKzawqln61+rJh6wG+blXKYX3SjeE5T2kbGRDT0ja6TiGfoCpc9iefzyObzeK1116LMPeC2o8lY33f/1sL3vpLIZ/1AfzvP2qjLqJNJhPcu3cP+XweTzzxBCqVClZWVtyDQ+PDqaojgo7uJqim8nlgHqUB5mm4VuHJa8VisUBNF7aVD64lAbhRl++fKYUajQaGwyG63a6LvBQKBeRyuXPFuJWEsIv4MAfOkg4EWS2WrSQxozmsBalOpY7ncDh0dVwVuNkmTaXga+wLFbfqcGsEjFE99jEWi7k6ZM1m0xHWCuqqqGVEkNErqrpILKkygZ/jBhCMikUWWYS7ZzYej3Hnzh1ks1msrKwgl8uhVCq56LySqFqChMpZTW0H5hs6jcdj9Ho95/RwYU/MYBCmWCy6RSRwvkailn5Rh846ZpqWz2sRl3g+foavUZmvc4ON6CtO6vXsAlkJZbsAp6lDrY43ccw63pyjNIqvBHAYMcDXlFDnQoBt5XyhrzEQpooRm8pHkpVzMucZLUnDMY/H4xgMBjg6OoLneej1eojs8bYIc89sPB5jd3cXxWIRly5dQqFQQKlUCmAdcF6FRFWQPusWD9Ts/6y7R4I1m806MjaRSCCVSmF1dRXVatWVCmu32wGBhPpmhUIB2WwW/X4f/X4fjUbDqT55TouDPJZtAOBIXN3NWokFSzIA5xVsen7bd0sIWNJDzxlGgIYFynTxb33zsMCgYr8lSviaDX7p5ylW4JzVaDQCuE4fmnMm597IIotw98yXOjg4wOnpKQqFAqrVKtbW1twzqBuhAnOs1MxO/ra4Q39Tg1q8JnCGK91uFwBw//59h7Wq+les0xr8+kN/GzhT2oYRmNwoTDcISyQSATVuIpFwJassOavkI9tus9+UHFWs0wCbYqoSoFT2Ks6zr3ZcVdRhyd9sNuvGSv1PJYI1E1mzppPJJE5OThyHwL4x4Egflm3k8cRazXwj2c97HGHuxbWPvIHX42KxWAzLy8vO2ZhOp+j3++5h0mLb1rkBgmofdb70IQ2LoNiIU1hkh06iddq03qoCkSpLqTgiuCuw8ThtI9ulDqy+Z0kI7Tvf07ZrBGlRxEhtUUROP6/jr2PMNliyggCvRIEli8OcaL1fdlLUz3PsmUZA8kevT3Ion89jaWlpoTohssgeJ4vH46hWq26DFTqgGsCyC1stKxCm5FGlJbEyzIklhvIYey0guOjWzygeKNYp2agR9zD1g13g0+wiWucMvq8LaTXFT+1nWN/C+qk/bDvrien1F/UFgLs/fF1TuiyZwPtDZVvY/MLAI4+ZTCYOa/V1vQc8N9PzSqUSVldXQ0sFRRbZ42SJRAKVSsWVXmIwiov0RRvLLLIwP23R54gDumAlOcpaiqPRyD3DXPTTB1Z/i+1lII39GA6HrrSXEgLsq5rij/bBYqH1M4HzG5dZX9f662FzwIPOF2Zh85he/4NY2HW0n/beq9iC90pxl+slAC49eTQaIZPJoFQqBXzhyCJ7HC2RSLh1H3GOAilmZn4Q3LWEJBBeDoqf4XOrylgKBehL634w/M1n3q7teT0tq0A8ZmAtk8kEhA9sF0sahOEUfVqLFYsCSvrbjoOSscptKDfDz4fhr/Zb+zCbzQIlbPg5vhd2L+yagMeqkM7yF8rpAHOimJirJDvXQMRtbhQW8QsXzyIy9gNaPp/HV77yFQBnX/7RaITd3V1sbW2hVquhXq9jNBo5dZB1sHQTAjomBDl1HD3PC6iweLzWB2SUi1GXZrOJTqcTcFxVLcYdyBWMGFmxqbqMnrHuKWtEeZ7nJgUFY55TazcquGn9Fa2Vqn3TzymRAJzf+Vs3btF6gpYQpiKZn6diVVPTCKhKqGu6h50c7P1QUNV7pucbjUau3lij0QiQ3rPZzE1QVCdfunQJq6urTvkRWWSPs2WzWXz+858HcPbMDodDnJ6eolarOaxSYlCfb1XAq8Oji3Kq1nmcDdqoSl4dHVXhUnWvO23zHJwPFI+Bs8UoiUDiBa9P54q4q9kMSigSQ6wTyfMoPuliWkljGxBTTLYON3/rJmnsPwA3fwHzTQn0fHydASlLxPJvzovEcyrySLhwQa91gtPptAtM9vt9nJycoFwuB+Y9/b4AcAGwZrOJp59+Gtvb2xHuRvbYWyaTwac//WkAc1V6v993qtRWqxVQrQOLN+IKIwfs5zWYpVlDSqqSjG00Gmi1WufwvNVqIZ1OI5/Puxqvm5ubyGQyrg5ivV7H6ekp6vW6U/t2u92Aopa7X6v/Siy0mKiiBvbDiie0j0oa6I/9XBiZyv5y8R8WxLMBP6uCCssk0PfsAl2JFwDnNqbVaxK/qY6j8phzWa/XQyaTQaFQQLfbxWAwwMbGBsrlssPzyCJ7XC2bzeJLX/oSgLNnqtVq4c6dO9jc3MTq6mrAZ5lOp4HN83S9bU15CCVWFWs9z3O+ZqPRcOVdtBYpszZ9f565oAEt5TnoNwNwgZnpdIpareYEbfQD6QuSj6Dfp+SqErC6gRWvp0EtfkZxVtf0nEeURFWyWf1qS5pqbVjN6NKgk83C45hz3c9x1Uw+xfZUKoV0Oo1WqxXY0Es5DXI6GvDyfd/hqM2mZhsmkwlqtZrLLIzsYll0Rz6gkWSjESxTqRTK5bIDJxud4d8Eh7DUJo06KcHJYzX6Q2LBHkOnVCMmNvKl5+W1WWA/mUyiUCgEIlph7dBjgeBufwQj2zY6tQSfsOiRpgqwoDU/q2RGOp124EaiQx1+64zaEhBh0UELuDZqpfeURA6vzzEoFAqOfLEpDiS9lfzlQoPvM02D0cnXX38d+XweTz31VASckT3WpovEdruNe/fuIR6Po1gsOnxQp9NiJs1Gs+m06AYkVlGwSIVvF9PWIbbXJ3nJc7AOF8+lgSDFROKYxTAb4Q/DtUXkge0L26kOMs+xiMy1ZKxited55zJFrNKVgTKeg+ezChBeT+cwKhCSyaQjYUmSk3DlRhCz2QzZbDZQV4v3R518bg5569YtJJNJPPnkkxHuRvbYmmJup9PB0dGRK2MF4NyO35aIXGT6mUW+stbtSyQSKBaLDiMZ0Oa5+PnBYOA2NykUCsjn8wEfLB6Po1QqYTY7S9dMJpPodruB0gj8HPtn5xJdwKs/rP0KG79F/Q973b6nWKw/YVkTbJu+DwT99UVtsYE7/YyWp+BaIYxYBua+rraN5Azxl0QN54D33nsP6XQaly5dijA3ssfSlOwDgH6/j729PRfYAM7W6poBC8zJSRv0pqnIQDPLVBTF40j6qZCB71n/lYIinpdlBZjBxt/T6RTZbBb5fB6VSgVLS0vufCy9oOIC4qxiNzDHEuUg2FcN3ms2BdtpxRS27Yp7VqigYxi2PlBBgPIGOt70VVWdasuW8Xi+lk6nkU6nXYkctskG+6wC1/q39l4CZxj9zjvvIJ/P4+rVqxHmXhCL7sKHtKOjI3zrW99CoVDA+vo6MpkM0uk0ut2ui4gscopU8cP/bbQ7jFhQ1at9QAlujP6oowTM1UzqWAJnil+mQjCazbbZlE2bIqATCNtFJ10jQVxIs12M6uj5tN4J08i0tqwCBolnLsKZkspx11pgVn2gY673RCNsluDgMclkEu12GycnJ+5zJInX1tbcbr5WUUHylv8nk8lzu2Xyp91uo16v49atWygUCnjiiScisIwssh9ZvV7Hq6++inw+j/X1dYcRth73g0qhWMdMA136vzqjWjeLr+niXEsjqGNII4FAXFQni+fhZ+x8oDtuq8OmTueDyFi23fZDx0SVvvo5DUZxzDRopmnDxHTeD1vrSwNxzDpgdgMDUorHOnaDwcApY1OpFCqVCjKZDDKZjCNYYrGz+t6xWMzVgB2NRoEFgs4l+ne328Xp6Sn+7M/+LCIGIotM7OTkBK+++iqSySSKxSIKhUJAXUSzz3oYKWtJyzASkv4jlZbVahXtdhvtdhvvv/8+Tk5OzuG953kolUpYWlrCysrKuYBQMpnE2toa8vk8EokE+v0+6vW6wyOmztIn1d2+H0TG2kBgGIEABDewsUSuDZAtwvJFY2gJGLvOsAS4Eip8XQUc9rNaL9ySsbxfPE5L1nCdEo+f1f/l6yQaqES+desWMpkMNjc3I8yNLDIAzWYTb7/9thMgraysIJvNurW0llPhc2VLrKivpSUKgSBJC8yfe2KgJWrp5/FvxUpgXvqL6/ZcLheogZrNZlEqlVAqldBsNtHr9ZwPrBkQFDzZLAm2Vf1Rto04T/9OM4PZXvqQw+EQnucFlPvAfC2vKl8as+N0nDgPqMKVY67zgwrP6OdaUQCVsvTrmV1AcRbXAspj6HXo21tCWMtU8jUqnb/xjW+gUChgZ2cnwtwLYtFd+JC2urqKl19+GY1GA1/72tfw5S9/Gdvb24FIBTCPpmgECJgvgIFgHVNdGKtRBcTFJxfCullXWCFpnoeft0QtVQFUdNKZUsdSUw/UMaTDS1OCU3cEVBUYwWg2m7kUUo6NRo84RqruItnLSJMqsyaTCXq9HuLxOCqVSoCUZZsUxBlh1LQH9ptj1Ol0MBgM0G63MRgMAspV/s8aOKx9lclkzikJ6OzyWrpj+3Q6dSquwWCAg4MDHB8fR7t6RxZZiC0vL+NTn/oUms0m/uAP/gCf/exnsbm56XCOZUs0+qxBFqvksYpPdXoUJ+nsqPIfmCte1UFW/AXmjiqxjfjI44kRindsmyVf6bgRAy3poe2lE0f8UcJVx0EzCHg99sEu/NlmbWen03GOHsvtWNzVuUixT0ldkrEW/5VIphO/traGcrmMS5cuYTgc4gc/+IE7hjhsVRTMAonH4y7o1+l00G638d5772F3d9cFAiOLLLIzq1aruHnzJhqNBv7oj/4IP//zP4+NjY3A7s9AOL7S1M9b9L8uLunTUiTQaDScH6b1vu01eCz9y2w26zZD4aZRlmRgW9l+krMMkClO6ucXkdH6vlXIap+tosmSsmHYq+IGtvlBxHfY/GBJDgABlZoSx7PZfMNLphcDcJl0FDHYupLlchme56FerwcCYZ7nubTng4MD3L9/3507ssgiO7PV1VV85jOfQavVwve+9z288MILrqa9Zq9q4EUVoMDiPVhssExJwFQqFSitpZ/lNQqFgiOJeRyD99lsNhCw4w9JWuIW/T+KtOhH05fWrDXrb6o/rUIDvsd20e/lXEOhmPqctq9M6ddykhwTtp+bndnyBnq9MFPSVec/S6JqaUnyA6enp/B9P7BxGe9L2GZjWiaMJDM3seTmipFdLIvI2A9py8vLWF5exh/8wR/gW9/6Fm7evIlLly65Lz6dOj7U+vDo+0AwTYiAaglRPlypVMot+glGGoGhWcBlzRfWR7SOn0aQrINHsLHRNX3fOpU8FxWfmmLKcg5WfQAEo/mWnCBRauuy8DrNZtNF4VRZq+dj+5Us5XnYfoIYidd2u32uvm6j0UC/30e1WnVKN6qjSUATWHld3QVcx0rrRx4fH+Pw8PBj/KZGFtmjY9VqFdVqFX/4h3+I73znO/jEJz6Bzc1N9z6DLHRabWRYCVZL1tFB0QUz/+fxdDz5vxKFqua0pWKs+pN4o8E6uwi30XclZG16k+K4VcTalCwlF+xYqOOtTq51MJVgZZ1FAM6xZttsG3iszovqiFs1rSVkOfclEgnk83ns7Oyg2+3ijTfecPMQlQU0vs5sBG7qM5vN0Gq1XOmLe/fu/WRfxsgiewysXC6jXC7jT//0T/GDH/wAN27ccCpG4qZixiIcsxiyiJDl31yUptNpzGYzF4ihQktxkzjBGv3E/WQyidPTU1deKplMOqJQyUNtm+d5rhxW2A/7FEY6Wxy32WSLiGo7JvY8+joAp1alL0zc1XNYU9xXvPd9P0DG2nWB1lfn8ZlMBsVi0a0leH5iOst2NZtN1z4lUyJfN7LIFlutVkOtVsMf//Ef49atW3jiiSdQLpeRy+UCvpP6ojaLCThfcgSYE7f2WCXx+LqutenDUVWqfijbwrrQuVwuoJBNpVKORNWgD/11kor0A5VctdlcxCTtP03nH+KNJVg5TyhhyzHRYD3Px/bncjk3j2hwzo6hzms6pykRq/fDEuTkObhHj+KmzbhWPsOKN7Rv7HOr1UKr1frwX8zIfmoWkbEfg/m+j/feew+JRAJPPfWUK1KvYKGOqjqPNKsq4IOlAKbkJMmGZDLp1AJaz0udUe6AG7YZlapNqbZSMpagA8wjUoxsU/E6nU4d0NCp43tMRWDfCbSJRAKDwcD1X6NcwDw1gEo3jfLopMKJgJux8FyZTMZt5kBilmpaktdWocC+HBwcuE0hGKXTVFmWVCAhUCwWsbKygnw+78ZNlWhcQHQ6HcxmM+fEcizi8Tja7TYODg4CYxJZZJEtNt/3sb+/j0KhgEuXLrmyKkqsquKTzzudHE0dAuZpleroaJF8+2xrqqwuunlcKpVygSj+ts5bmBOnxs8kEgkXZOLGCQAC5C77rSQD+6lqVxIN6gyq06aKLqs+U8eVxOby8jKGw6FTngEIKAmUuOV908XEcDjEYDBwNRwZrKI6jfeHRPh0OsXu7i7a7TY6nU6AsO12u/A8D8ViMbBJmJ43mUy6+Wl/fx/37t1Dp9P5aF/GyCJ7DMz3fdy7dw/pdBpPP/000uk0Go2Ge/6s2tMG7Gnqx4URBtPpFO12G/1+HwCwsrKCdDrt/Mt+v39OEUT/lUqi0Wjk/EzWhM7lcsjn867ESb/fd5/zPA/5fB6xWAz5fN6ltU6n08Df6otb0tT235IF/Jy+bgnaMNKav6lSbbVaTthAlZrOJ2GKXJolc8IIYvr53W7X+bs8N4mWB7W71WoF+k+yZTqdYn9/H7u7uxHmRhbZBzDf93Hnzh0Mh0Ncv37d+YEk2PgZXR+H8QnA3NdTclKD4tZ31Mxbrflsa56SmM3n865mdzKZdGti+trEYr0OsZWB8W6363xCYgavzZKE2mbNQPN9H/V6PbCfAP16zaggqRlWB5Y+J8ft9PQUsVgMy8vLSCQSKJVKjg+wSl49j/r4fC0sk4HcQ6/Xc+VzWGOb4jmqfVUMZ78jGqjUYFs8HndEbKSIvbgWkbEf0bhIbjQa2N/fx7Vr1wLRJFVBhjlu9m81G6FWAlfBkAtfXlPrlGi0yyqp1HHTBbmWT7ALcy07wN8ERSUUFtUBJGhohMwCv7aHEaywiBIQrHlDRx2Y12bU8dI6OVYVx9QFOrr9ft9tAAMgsKBXMGcqXC6Xc3WxLOiyHb1ez6kGeG22nbt6EyxJuljHPLLIIoNzVLi5zObmplPca8aA4hfxRtOFAAQcO7vAtkQmXwOCEW27AFaHS9vEdum5LHFhz83za7oUz6uEYxgZq+dS5zTsGnp9xUfNWlByl7hM0rTZbJ4LMOq8pWSzKs1YMoDOLcdICW7bvn6/75xy3SCBZXa4SYyms+l3x/POapO1220cHx+7RQ1LAUUWWWRBI+nXbrdxdHSE69evO//SblIKnC89AAQV9fxff9OofmIQK5VKoVqtolgsotPpOD9JF5fEI+JCv993PiTPx4AOd62mEkuFCQymsSQNMUrLA7DNi8hY/Yz92xK2i/x/6/spCay+LkkFxWR7XNj1db6w7SGWDodD1xYdF1VoKTbz2jxO5yD6/s1mE0dHRw5zGdSLLLLIgkY/t9vt4ujoyIkO6MtRjMS/gXm9fVVV2gwA60+FZRro862CKGKM1i0lVlL8pSUKFOM0gKY4w+xQYj7V+Pys8hY6l6ifTv+dcwaP0zlBCVztnyVT9dysM0shVy6Xc9ezpGqYDw8ERQxsr447MC8VM51OUSqVnOCO48prWNNx1Jrd5GM492kGG+e/yC6ORWTsR7TPfOYzuHHjBr7+9a/jrbfeQqVSwcbGBtbW1lxE3S7mCSoalbZRZBodHzo3rLOqhN/S0hIqlYr7TC6Xc+QkwbNSqaBcLjunksQgr6dpsLoZFwCn8GJqPoFSlQKWMFXQU5KY/dTJgcBmlbsKJkpaMBrF4wuFAlKpFI6OjtDv9119Fd/3XdFwfoZpE3TK+/0+er2e2yCi1+sF7okSy1RRUN2ayWTw5JNPolKpoFKpOIKHY8KJp9frodvtot1uAzgD4mw2i/X1dRweHuLOnTvY39/H/fv3XRH03/iN33BqkMgiiyxoP/dzP4fr16/jm9/8Jt566y2USiWsrq6iVqs57KGTx+eSm7eQbNOah1pWRRe0XOBysahkIz+Xy+UCWK7Oo9aKVoeZpgE1HqMOE1O9GOzR+YDRdLafASUguEDneQeDgVMr8No0rcPFvik28xxURBHH9bqNRsONaz6fd1jLY0myKhEwGAzQaDQCO5un02mkUinXzvF4jE6ng2636+rBlkolp9BSp3c4HCIWO9vIq16vuxrlwNm8WCqVUKlU0Gq1cHBwgL29PVe3MJVK4S//5b+MlZWViJCNLDJjzz77LK5cuYLvf//7uHPnDkqlEtbW1rC2tuZUVkr4aSCHuGdLGiwKBPm+j263i3v37mEymWBzcxNra2u4fPkyVlZW8M477wSU+HqeWq2Gcrns/DXdJJBkYKlUcruU01fTQBX3C+j1ei4byqqKLNGqC/hFilglE5QoUIIhzHgsFVIsV8bah6x/SHy2gUEaFV0afNRMEt6j8XjsfP1CoeA24GEWnc5TKnLgHgqtVgvj8djNf1RQv/POO3jvvfdw584djMdjJJNJfOlLX0KtVosI2cgiM/b888/jySefxJ/+6Z/i9u3bSCaTWFpawo0bNxzxqYQkcc7ijw3Aa/k8xSwSoKqwZbAkm80iFosFau8TuxSXlAvgGlpLFlrRA327k5MTdDodtyZnMI3B9Vgs5koEaPDIiivoA3NjWfr5Oia8PsViStgq1nue51S+3OSRfnk2m3WirV6v5/BO5zOte84xI2bTd6fPSqykz18oFByO5nI5VzaR84YKychljEYjt04ol8s4PT3F7u4u7t+/j8PDQ+fn/vIv/zKWlpYifuECWUTGfkRjfZRcLucenH6/79IldVFrLSyqbs06qgpCCkZacJpRKiU7SaxaclgjXRpZD1NsUTlKMlRBFQiqqPT6Yf3m5/m+pgcDCCgfLNlrI2NWFaBtY5SNJCkwd4JZjLvX66HVarm/tQ4X26btSyQSLhWjWCw6kifsHrMPBGAdD6aB1et1AGe12QC43TOXl5dDzxdZZI+78fkj7vIZ6/V6zpnRGrBaB4p4qZseAgg4NjbCTWctTPnF557nClO1styBHq+YFUZIkKRlyhIXwarWUixW5ZZio8Vhq1ZS1YA69mqa+cA5Q0sS2LFk20gQKAltSQntMwDn9HqeF1hczGazgOoirAQFz2NT7gA4FUm320Wr1cLx8bELrAFz3K1Wqwu+dZFF9vgaA0LMSBgOh+h2u86n5PMYpjLSRf+DzGISg+ssL8BUWAZqWNZJ/WymyvLZt6m0GmQnrlBRxGvbzWQ+aHonz6F917/Vdw1LN/1xxnEmlmnWmdZuXJTKqrW5tX1WMcY5Q+cgVbqpElbnOY4rCQ3Oe/Sxj4+PAyXVUqmUI88jiyyyoOXzeWSzWZTLZXS7XbeO7Xa7SKfTLhtTyTkl6wAE/CDLHwDhmaKayWSVljy/+rRhClT9m1iqa3O+R9UmfzQDVeeVMC5B/VErdlsU4FLf1o6N8jEW42zwSecWy5ewPSRXU6lUQECn7dLxtMSyllFbZDq+unYhedtoNOD7vsvITafTWFpaivzcC2YRGfsxWCwWwy/+4i+6CNFoNMI3v/lN1Go1fOYznzlXr8QWpFalEMGH7xMoGJ2yBAA/o85Xr9cLlC9gGilwRkQwGuT7vgMLRr2UIGA7Gf1pt9sYDocuRVSBS2vBsl/8rREybSfPoam1NpWASjXWjuWOiEAwkjeZTAJOI8FNSejBYIC9vT2MRiO0221H3qgql1G4eDyOUqkUuB++7yObzSKbzWJra8s5kVoiQtM6ODkNBgM0m014nufKD3S7Xbz55pvY29vDO++8gy984Qv4zd/8TTdmUcQqssgebJ7n4Ytf/KIj6cbjMW7duoVisYjr168DOFMCabAqkUigUqm4Rb3neS6jgASiOn8kZ+l46iJWnSkGfgAEcJrYwJp+nAMY7KE6S500/k/igzvTss70eDx2tfw0lUvnAcVWu3EAFQOakaCbLFoiGpiTscRgjkUsdlZLq16v4+2333bYxcyMtbU1lMtlFyBkrUeN8FcqFeTzeefULy8vYzabYXd3F91uF41Gw7U/m8263XzVQSb+sh6sltMhnk+nUxweHuKHP/wh7t69i9deew2f+9zn8Eu/9EuurxHuRhbZYvM8D5/+9Kfh+7571l5//XVUKhVcv37dKS/Vj1OCVv0k/k8soTJSfUFu2tXr9bC7u4vLly+jXC5jdXUVvV4Pd+7cAQBXRzGTyaBWq6FYLLo2crGay+WQTqdRLpcxGo1wfHzsFv7NZjOgEGPJKNYtVH/dEp26kNcFt84RlkhQ4tS+Z4Nn6kvzfWKaDUipOpk+OdVoSoSocov3QbPjSF5zoy7W2A2rscj+JBIJNJvNgIIrn8+j1+vh7bffxu7uLm7duoVPfvKT+PznP++Op48eWWSRnbdYLIaXX37ZBVz6/T7+/M//HOVyGTdv3nTBcOUKYrGYyzRioGkwGCCTyQTUtCT6GACnT8eMMlW9A3CZSSzPSCUpca/f77u/VSxAlT39P/pnw+HQKWKbzWagNreSmYpTdmyIPUoMW65B1+TEcMVZnZ8sv6JKWd/3sbGx4frEc9tjWMubilju26NcEMtNcE+bTCbjcHoymaDRaKDVarksZIvfvB7Ff/1+H9PpFLVazc3LJycnuHfvHj7zmc/g+eefd+2NshAunkVk7MdkuoibTCbY3d1Fv9/Hzs6OqytKsw4bECzqH6awtDVfeYz+VlPFrD64dOJ4nDpei9rEyBXTDGw0iddTRy3MoQyLzgHB3cy1JiuAQBqBljnQNhCcstlswAHn5KVRo36/79TLNlLH8bUqAF6fk1AymQwoYsPUunxN6yEyhazZbDqyl+C8vLzs1AKRRRbZBzN1KobDIXZ3d1GpVLC2tuaeYa0fq4EXrbWtkXxikFU5aVDJqoOYjkQc1IAWENxUkabOrp6fxGsmk3FOnRKnLFGg2AUsrqWtxIedZ5TE1ZRf3eBQVW3WEWR7M5kMqtWqC6wxMLa1tYW1tTU0m03nVFrT4KMliQG4Gl3xeNyRLmHKXV1gKCHEFLpGo+F2Wy+Xy9jZ2UGtVgvMzZFFFtmDzWLuvXv30O/3sbGx4bDQZkZZzFzkH1rikc81FbAnJycu02k2O6u/H4/HUSwWXTo8A2wkWklAkKTQAJQqt9gmlqyx+BqmzLK2yCcP89HVfw4z6+fruKXT6YASKow8YP8VuwEE5hwgXN3Kz3HhbgN61ojPnJ84r3CjsWQyiUKhgI2NDVSr1agMTGSR/QSmmMvSTuPxGAcHByiXy6hWq+751PW9ZhlYLFPMsApPHq9+MH/IAbDUHwUMwLy+qvWBGdTStTgDbSxTYFX72h47X1j/zyp+w3BVA07sgwbgrLpVfUqOOzcBVyxlVhjnKHIFYWsHe35gnlnmed65vWm0xE6YctfzPMdnEOs5P1JM5nlnpSwjP/diW0TG/hSs3+/ju9/9LgqFAqbTKa5cuYIbN264h4lRHrtxzHg8BnA+OkNlFZ1JVZkSLBVAKXXPZDIBgCZIKKhZsOYDTVJiPB6j2Wyi1+u5nav12qq4JTDwePaBbeB7JEPoOHue5wp40wEmuFNtxkiSRvht7dlareaKnVNNQbVxq9VytbKUoOC4zGYzFx0sl8suWjWbzdBut5FIJFAsFlEqlVCtVgPvKzHMtrG/3Jir2WyiVqthOp3ivffew9LSEl566aUAqRFZZJF9eOt2u/jmN7+JpaUlFAoFVKtVLC8vu2cyn88jnU4HdnolXmo6vCpHgbl6i38DwQUyHWWt583P0Pkk4agbC6hTxoCP53kolUqBdjJiriVwlEhQs0EutsNiqta3pgPI+Ye4TRWsYhMDXJoWRnLz+vXrbnMWlpB4+eWXcf36dfzJn/wJjo6OnAJNS0ZwPiA5zY1juOHOyspKIFBmyQS2S4mbZrPpMjioLH7zzTeRyWRw6dIlXLp0CS+++GKEu5FF9hGs1+vhlVdewfLyMkqlEtbX17G1teUyjiwWaZ1BNVXPqnKLviTFAPV6HZ7nYWtrC/l8Htvb2069Sb+4Xq/j+PjYPf+rq6uOOBgMBjg9PQ0osIiB6gtz89Yw3F+02Fe8tWQHTZXAivt2LHi8DbTx3LqjN3GR8wv3PtBNCbmGoJhAiRa9LtvCYCAFDtoO3kMtRxGPx92GaN1u182t7733HrLZLK5evYr19XU8++yzEeZGFtlHsPF4jN3dXcRiMXS7XVy/fh1LS0sON4ifVGMSB1k2imtiFQuQMyDHwGdcM8XoszHbTIMuDHi12230+320Wq2Af6t8ADG3Xq+7DXhZd5XZTJqyr/XIVTHL/9kG8io0vT591mw26wQatqwX+RGOjQrBgDm+NxoNR6AyUPX+++/j5OQkIE4A5v4yMA9ucS1AP5wiDpb9abf//+29a5Bc53nf+X97uqfv3XOfwYAAxgRAgCQEEjAp0aKti8PoZm1UrsoH26lKvHGVamtdleyWq7xRXLVb+yEfUtlKNqnNelebeFnZ8jrxKllbVmnXtrQqxZfYoilSFCWKIkQKQwADYO59n57L2Q8z/3eefucMBiSAngPg/6uamunb6afPzPzP8z63t94TkA2L33ibxSStVgtLS0v+c9+8eROjo6N46qmnFF+4j1Aw9h6Qz+dx4cIFtNttvPPOO0ilUj6Qx1mGtqI0zExTaMIKWlttEFYr2X9Ym02xVVt0xCi2+zliPBaF0VashqJAMQ8zN6GAEft6mwGjSNksmK1QY+aK4snPZTP/FOlyuewdT2srBZKf39rECgrO4eFFCYB39NlWwHNos3V0nHkRY5sBFwUA/IYzjUajJ/AuhLhzcrkczp8/j42NDbzxxhuYmprC5uYmisViTwtQs9n0m/lFUeSTQrY6lgHU/QIHcVn6sJoL2E32UKfsApvY9n8ualmdbyvv2ZkQOmpW++PsC6tarSba51i77KxuvtZqFYPYUbS90Q4dcmopRyxcvXoVa2trmJ2d9QFSts3xM7MtzrbccgMCtnjZ66P9bq87tsKX+spxFJlMBj/+8Y97NmaQgyrEnZHNZnHmzBlEUYQ33ngDjUbD/8+Vy2Uf1AwrlmwFbKg9YeV+nJ5xwQqgZxxVFEW+4spWgVHHWLlJLbVFCNa/DTUa2PVl7XFDn5ef1fqy9jPZz0/dj9Njq+dhhRVhgDUsLqCfWigU4JzrCTCEa4c4eL5tp1uo/eHvjjrMtQIDQp1Op6fVWZorxJ2Ry+Xw5JNPotFoYHFxETdu3MDly5f9uCe7gReDmWECPUwy3QoeyxYc8YvHY7EU/Tj61AyQ2sQWE//0B5los/GAsFvX+nk2RhAWC4SEiST7uRnktZ+R72N10t7e3NzE4uKi7/zi56rX6+h0OhgdHfWBaT6fG3jz/DHIS92s1Wr+/nAkwa18eo6OZBCb8Q3O3d0vISiSiYKx94BSqYSPf/zjeOutt/C7v/u72NjYQD6fx8mTJ1EsFv1i2g5qDquRuHi3omAX33aOqXUmGUhgJRVFo1AoIJfL+SADs+V2jAH/8SnmFIdwExWgdxOWra0tX3XKIKkt07eLZCsw4bysuNlR1slkNotCbWeM2UqrkZERrK2tIZfLeeFniT6Fnp+fAd5ut+vPUfh7sUFde1EJfz/8Tod/YWEB77zzjg/GspXu3XffjW3XFUK8f0qlEn7mZ34Gly5dwu///u9jZmYGzjkcP34chULBa2K9XveBzyiKfPWPbT+y+hsSBhYYcOVi3P5v2023mL23Wk7N5MI63LXVJrVYzcUZgHaxzve0ia1wMR9Wp1nCgK4NxlobbICEnQGsgmDVAT9HJpPBpUuX8IMf/MDv5GqDqoODgz5QysAKsFt5YTcwtJVcqVT85oq22sEGY9mutbq62vM+Qog7o1Ao4Nlnn8Xs7Cz+6I/+CPV6HblcDmfOnEG1WvVjrqx/GRYi8P/WLrythtmALI+zurqKWq2G5eVlr93hwpUazwCxnTvIeYV87zCpFReMDTUyrNC31axhZStfT//b2sdARKjTPKb9bkmlUigUCn4MFgMvTF5VKhWvx8DuNc1WtIafjz55mITb7zpIm3lNYjUur6k2GCuEuHOKxSKef/55vPPOO/jKV77i/w/Pnz+P8fFx32HKIiYGRu34Av4/hzNh45I0dt0bJl5s4NQGfTnX345XoF/GgOPKykrPCEQ7PsxuZMVrgi0I4Ho7DMaGnWvUNGoq4xKMn9BvtDZav9cm6BiXuXHjhvfTeexarYZWq4WZmRmUSiUfGGU3m+0qtj5sFEVYWlry2hnaS+jv8rNubm6iXq/7r263i0qlAuccFhcXvQ6L+wcFY+8hExMT+PSnP43FxUW8/PLLPRU7wO4inm2UNkDAf9i4MnMu+O2MFZtdpxDZHacpeGzzXF9f9wtivg9fa0WT/9QMslK8+B4UVn4Wu3jnxYDwfRicZECUF5PQSbSttXbTMQqTrVCwImyry0Jh48/2GLxo2PmR1gm1rQyhnfZiwEVBu93G4uIilpeXsbq66jNlb731FtbX17G8vLynpUIIcXeYmJjACy+8gKWlJbz22ms9wcPBwUEMDw9jYGDABzDpqNpAqW3l4uI+XBiH322ixj7fLuLDGd42GEu9DmdlcfNBZsDthoc2823f01Zi2YV1aE8YGLHHpn5b5zRcmLPyCoC/FtmNKJnIYzDcOvu2fYs2x50/WwlrW+Z4rbKBWG4e0263/biHP/uzP0Or1cKVK1cwMTGx57wJIe6MkZERPP/882g0GnjppZcA7PpWURT5xSg7tKwm2QqqsJr0VtU99Lf4Ot7HAgLbmmpHrlA/7MZcoe8cdnYxkWf1Ogy0hn5mXGWrTdjb19nPTcLrTvg5oyjyyT4GPWwQmLOyrT9uq5Tte9LeMLBhX2vPkb3m8drE3d4zmQxee+01tFot3Lx502/IKM0V4u4xOjqKj370o1hcXMTs7Cycc5ifn/fVmdavpO9kZ2cDuxt42crZuI6ruCCtDRDS/7L6GmouAO/PsuPWVqJSy6zPTU1lFWroB+6XLAqr/61dnU7Hrwfoq9qqXxt7sYVw3KiMx2k0Gt6WSqWCUqnkn2evGzxmqN3W32VBWHgdsusBai3HQNy4cQMLCwsAtos+5ubm/GhEdv2J+wdFhO4h1WoVFy9exLe+9S289NJLGB8f9zu6WqEMnSbr8IXYSljbsspMEV9PkeB9dCRt+639boODFEkKJtDb6hU6qAzs2mCsdVYpLlZUAPQIng1k2PdgBiucD0snn9jKiTAobM8N29f4evv57Dm2x46rWAgdU14YOKNndXUV9XodzWYThUIBg4ODuHr1KlqtFgAocyXEPaJSqeCpp57Ct7/9bbzyyiuYmppCtVr12lupVHxCK9zk0FYP2YW51WIbKCXWeQV6q2fDRb5dPFObrNNlbaKd1Pr9NmWx+hfXvWC1MC5IYKvOqMU2mRYGke3r7I7l4Rx03mbANpzRFQambQA2TtNtMNZWxfJ6xfbjTqeDoaEhpNNpvPnmm1hZWQGwXT0thLi7lEolPP744/je976H1157DdPT06hUKhgbG0M6nUan0+nxw+KqXi1Wt+wiNm5BS22gnnHBHY49scECWxEVp+WE1wSb8GHlqU1khcHMuMpaezwbBAg/N49J4qpSrX9sK95s1ZftyrDnPrxW0O6DgsH7nf/19XU0Gg0sLy9jeHgYg4ODmJ2dleYKcQ+pVCo4f/48Xn31VXzve99DKpVCvV4HsB17yOfzPRWmXOfboCf//0OdtT4csVoC9I44sBWkAHxMgMFY60PTj7XJMPq9fB/rr25tbflqWVuoFuff8lihrob+PWMulri4BF8D7CYHWSHLDi5gu1qZNtiRC2EBgrXB2mgL9azO2msYE42dTgfNZtOPQaxWq8hkMn6TXGDXzxb3DwrG9oEzZ85gfHwcP/jBD/D1r38d586d87s4r62toV6v+6xVuVxGuVzGxMREz6YEdnfYcPA0nUXrbNnFc5gdB+BFm4LDuVt2HEBcppz3sQLUOefnrFLs45xrYLcSlsP9bSUYnWpWfoXYiwUD0aFdHCjO51hnm9gqYp6/uIWAPTYF14o/Awf8me3PXPjPzc3588OB3x/72MdQrVYBbLdLqH1LiHvH6dOnMTo6ikuXLuEb3/gGTp8+jeHhYT8fulgs+hEuYWaf2EAgsLvwjmv1tNn1MIBrtcPqsnWU+Rp2L9BBY0UsX7+xsdHTFsbgpLWX1wW+t12c2wV4nOPL88DnWIfdvifPE6sb7KYGtoKB581el+w5CAO3bDm2wWEmCFl1wFa8brfrq165aWOj0fDfM5kMnnvuORSLRQDSXSHuJSdOnEC1WsW1a9fw1a9+FU888QRGRkZ8C+Xy8jKA3Xmn7BqyPhawOx6K94VffA6DsFaPOSaFz2X3Fmeo2oV8mIQPsYke2kStDu2JS27ZxBhhkIJ+PPXSBlN5DPv5LTboGwZure7GfSb72jib7eLfVtLyeJub2xucXblyxXdtUMNbrRZSqRQuXLjQs7GlNFeIe8Ojjz6KkZERfP/738eVK1fgnEOpVEK5XO4p9spkMpicnMSRI0d6Nty2vhd9ubCjFdibFOKa3fqy1BT6q1wD2wCwTaLz+FbTqZvsqOJGZDY4bH112mfHGgC7hVzEaqUdr2CDwqFehwUBxF57GFi2BXDhfjX2fNoCCkJ/1xYo2GsC/dkf/ehHaLVaWFlZ8bpbq9WQyWTw4Q9/GOVyGYA0935Ewdg+UK1WUa1W8fbbb6PRaKBWq2FwcNDvvs22fVYibW1t+UouYLcC1VYE2eHZNhhrxclWC9hqLgqIbUHl3D7rxIaOWuiQcadF66BaAQirqMLAhrU3LutuK7Nslp7vHzqVvKhQzMOZtPs5ziG3qhKw99vsIdu0GAjguWF77sDAAI4ePYrx8fF931cIcfdgYuvHP/4xarUaVlZW4Nz2LD3qSjhDNHToiK3YDLF6GVdNED6H+hRm4O2XDfpaR9RWxdrFs9VdvtZm4eN0K3Q4aacNNtsRAnEL9/Ac0fEOq974XLvY57XAnltrm+1esIsFG5xloKXdbvvqYdpN/Z+YmOiZPyuEuDeUSiWUSiVcu3YNS0tLmJ+fx+bm9iaK6XS6Z941tdB2OVnNiNPO0C/jbbuwtwl+6gfQu4C/lf9H7MLc6nWYsLM6an+Os9MGNqzW2e/hIvpWwVh7bPvYfp8vrmosDrtWCK997DxgktDOq2Vl1tjYGCqVyoHvI4S4MyqVCiqVCmZnZwEAtVrN+0GDg4N+U610Oo1SqYS1tbU9flrY9WkDhvtpmvWTrU9HLWPRFO8LH7eviyvgYmyCQV4eIy6RxmOFNsV1ivG11t+22OeHx7DvZWMRYWVx3LHsdY/nL3yf0C9m4LrdbqPVavl9I1jwkc/n/TVjamoKo6Oj8X8kIvEoGNtHPvShD+H8+fP42te+hnfeeQe/9Eu/hEqlgtXVVTQaDSwsLPiMx+rqKiYnJ33F6ubmZs9MWbs4Z+t9oVDwC/BwQUxnyYrB5uamD1Dw2Mza23kvNkvDFjBW6mazWb9BgHVa7fszoGBbWBkQ5ntyYc3XM1i8srKCgYEBVKvVnuCzHVtgs3ubm9u7F7LSOM6hJ3ZBEAq5/TzdbhfpdLpnvAQreFutFlqtFl5++WWk02k8/fTTXkTD4AWrs4QQ/ePZZ5/Fk08+iW9+85t49913MTk5iYGBASwsLHi9y+VyyOVyKJfLPVnlcNMpOmbhQhrYDVzagGxctjusZOKMa+oFK/wZXGRlFqtBqbt2o8G4wGhoHz8LZ2Pbtii+hra12+09AVRei2znQujI8/PwOGGA2SbQ+HxqPb/X63VEUeQrq/gYxxDw/bvdLt5++21kMhlcuHChxyarvUxqCiH6wwc+8AGcPn0aL730EmZnZ3Hs2DG/2RST191uF41GA+Pj4ygWi14TWKQQFwSgH8rqebvpom3JtTprK7aA/RPxYULK3m/HyIQabitlbcUXfVnbiWDfxy64AexJEIbaae2P82ND4u6znysMMtjPHX7+VCrlNwB+4403AGx3/NmCD/uZOZZGCNEfLl68iMcffxzf/OY3MTc350fEcM3e7XaxvLyMN998E2fOnMHU1JTXHHYhsOPUFneFQU8GRe3oF2pZqEXUMh6XSXbqqU2a83EGNxl/uNVeAhYb1I1by4dFDPS7bXdWXPA09N0Zr7B+v/V5gd0xAeF1jH45XxeuDfg8xjJY3PX2228jm83iwoULsR0Xii/c/ygY20e4y97Q0BA2NjawsrLiK3zsgpuB2a2tLRSLReTzeTSbTeRyOR/4BOI3sAorChgItY4ig6r84nPDylYu2sPFNAUqnMtKQuGz9lrB5nuH1WT2dXS2w0xdWIkWtiPYWVrhe4fHCbN71obQfgZZWYlVr9d9ZfPg4KCf0yOESAaFQgG5XA6VSgXr6+s9s5UYnGSFJUcYFAqFnoU9E0fctGA/qCehRsU5iWHVKF9vZxTabHqYUX8vhPpsq18tNlgaBpXpJIfHtMFZ2+VgA7K2MsJ+RnZ98Ppnr1fWFrtBFwOynH/OURPv57wIIe4++Xwe2WwWxWLR+7pckNvZzs5tj7niLFa74ct+FVn7VSuRsA2UmhundwcRBgFC/zq0K7Q3riU1ZL9gauiz2vfc79wwiBL3GQ7CBmN5jDBAYWf0SnOFSA7cE2F4eNgHYPn/ymIntrbbQgQm2W2gEOidSw3EV/tbjYrze8PEFrCrx/SlbQzBdsLeSltupadxAWT7mWyCzM6pjXtNGEil9jFeYgu6wtfZ68WtknH2tfR1WXywurrqi8EGBwdRLBY1fuABRVGjPpNOp/HCCy+gXq/jS1/6Era2tnxVD8Wy2+367PPx48cxPDyMbDaLcrmM0dHRnswSS9XjskFbW1t+jh5FJazq4n2tVqtnuDfFcnBw0Lc15HI5L775fN6XyNvMlp39yuOwKtYKNm2zg7kpbsDuiAU7RzAuOBo6snyOrTaLot1dzO2OuECvcxs67zYAzSrYer2O1dVVrKysoF6vY35+HqlUys8lVCBWiOSRSqXwsY99DM1mE7/3e7+HbreLkydP7qn+zGQyyGQymJmZQbVaxeTkJKJoe2ZTJpNBPp/v0Ye4aiWrceEim68NdxWnRgK7IwHshlgMRrLqyAYjQifawsdshh+Ar/i3gQ3rbNpqWgY8qdnhZ4qrOgidVuuYh/N57cZk4eKfttJB5caI1N2LFy/634kQIjmkUil86EMfQqvVwte+9jVEUYTz58/7ZDZnPl+7dg2ZTAbnz5/H0NBQz2gU25VFjQLgx2mFVUK22CBc7NqFd1wANEx4Mflju8TC5/N7mHgKnxf6rrQ17DIAejsswmBGGIRluyo1O6zcDQMc1sYwyGo/v32s0Wig3W6j2WwC2J4LbLsyhBDJIJVK4fnnn0er1cIf/MEfoNPpYHp6GsCuj7m+vo6XX34ZAwMDOHHiBIaGhjA9PY18Pu9njtr9BsLAItfrVjPifF0bawgLx0Jfk1rP9bmNb/BYXMeHVfv8bm21vrl9nMdixS73urExEj4/PA5tYuewtSsukMs4hNXV8FwSXrM46nBlZQWtVgvLy8tIpVJ49tlnezYJEw8eihwdAplMBoVCASdOnMDm5iaGh4cBwFcHsXJga2sLw8PDKBQKfiMXBgSq1eoe8bEO2H5f4aDuOBG1G3jR4Y2rVLBBACsudArDRTptY9vYfjsO0ukeGBhAPp/391txtNVmYQbP7kYeCl6YibJiG1Y9sFKOM2BrtZqflbW8vIxms4lyuYxisejbhoUQyYTaefz4cayvr2N8fHyPc0Pt4IgTLnbpLLXbbV/BBeyd3WedrTDjze9Ww+zcbhJm0nmf7VKw78efrT1hNZUdm2IdaBsoDQOqNlgQV50VXm/CzD/fI0wAhhswhpXAtNXOreXr2+026vW6Hych3RUiuXC805EjRxBFkW+l5KYsm5ubfpFLvSiVSnDOodls7tkfIdRYLtjjuquI1V7aZIkLzFq9ChfS4XPjKmLt8/erpAX27kh+O5WuccQdO7SP9oSFGFan2TFiA7uNRgONRgP5fN4nEaW5QiQPJvqjKMLx48fR7XYxMjICoLdTlgwNDSGXy6HVavnxhixI2E9TbDwhfO+4eMN+2hqnm7YA6lbVr+H7WuI2tQ2vDSwGsNeEMEm3XyEF77dxDNuZbJ9vN0S3umtjL/Sz2+22T3rVajU0m01UKhXfZaKK2AcbXVEPiVwuh0984hO39dwoivzw5mazibGxMYyOjvZUU1kRCBfetlKL4tFut3uCk3ZX7yiKfCsogxNWUMIgQVzm37nduSd218IoilAqlQDsOp82u0T7uKP46OhoTxUXbQF2Z2pZUaMNdBZtm7ENhKyvr/vPzfNCEacNq6urfqfuRqOBxcVF/zk52/dTn/oUpqambvvCIYQ4PLLZLD7+8Y8DuHWrE7BbHUWHjTMOS6WSr4K3jp6t/g+7FMKRLHwNuwPiAq/WKbXB2LAKINS+MLBqnWMbgOVzbbCTn5uOHythOb81TuttYixMytnAKm1glwEfZ0CF55rXnlqthnQ67asXBgYGsLy8jPn5eXz4wx/WplxC3Adks1k899xzAOKTRYRV8hMTE3DO4ebNm34mPwOANnnFNltqSlzFpw2oWn/WJvbDwEIYoLSJtf2STmGhwH6BDAYBbFKN2CqwuNdYbMUVbQyvAyH2vNjX8jGuC1j4kc1mkcvlsLCwgOXlZVy4cMEXjgghkks2m8VHPvKR23ru+vo6ZmdnvX5WKhUUCgWvI1w7h1ppdY/+Yrfb7dFbVqCG473C6lb6oKy8tfNU3yvs4rJzW0Ot5WfiPjAclRPqt/3Zjrthhxo/H4O5YVctu4RtoQHPHYu9ut0u2u02lpeX/fiepaUldDodnDt3zl8PxYONgrGHyO3+g62treHVV19FNpvFBz7wAbRaLVy+fNlvOhNXBRpXsWUfozPHRXbocALwWTQ7xysUNi6cQ2eVx7DZHNrBdidWPETR7rzacKg1B2XbkQbWBiuW+2XTbHCA7bC0mxtK8DNwQcBgbLvdxuLiop8paT+3Db4IIe4Pbvf/tdPp4LXXXkMul8O5c+d6Fv9x406sgxnqEp/DnVD5Wm7OyOSQdWypZ+HCPq5KlT/bQEH4uk6n07NpQXgMaz+Do2z3tQFXviZ0zG1QwB6bjjl1dn19vafqzTq5m5ub6HQ6/hzbjcu2trawsrKCdru9bxWcECJ5hJq7nwa3Wi189atfRalUwuOPP+7n8HOjWqC30ilcZO+XkGLF2H4dZCHUUhYShG2vce95q+pc+9z9uhdu9Zr9qmPDLgT7ucJEGIMB3PwM2G1dttrLz2E3SRNC3F/crp+7ubmJy5cv70l45XI5r5l2g6o4fzFMgAHwQc79igtCO+nj7fe80MfmfXbjQxZV3Y4O87MD2wFcO7YrnC9OH5afy35mG68IxyqmUil0Oh2f5KLPy+/0h7vdLmq1Ws99cYV24sFFwdj7gLW1Nbz88suoVCo4d+6c3+BrfHwcIyMjXjCt4BErnDbLz+dyQzCbseF3LsRt5VZYUcDAJjf7spVTYVk/7WBVLp+7ubnp57DYQeI2a0+HkOI+MDDgK21pV1hxYG2hkHNhT4FcWFjw4sfnrq2t+bEErBKw9jM4fDvOtxDi/qTT6eBP//RPMTw8jLNnz/osOgA/S9AGT62m2Sy4XUgzwMgqLet8Mtlk34fHJaHehAk0YP+Ztd1u1zvIccFU+36264DOo+2csLZYZzpuNAPPJateu90ums2mnxkZOuPcEJELAz6/1Wqh0Wj07GQrhHhwaDab+PKXv4ypqSmcPXvWj/RqNBp+fvXtECaW6DOGCa64BT7vT6VSflyNvX+/YGqouazgpT23CqrSTmt7mFyzXQ68L25EA39mcMIGY7vdLjqdjj8+iwq42Y/1+RkAt362EOLBYn19HZcvX8bAwACGh4e9j1qpVFAul31nUuiT0s8NEz5WK7hep45Z/zHsMGBxlg38huv5/Sr/w7EDFmtPGCNhgDWXy3nND9/DxjqszTYYy+MymFqv1/3x2NFMv5+FYNRnvoaxCavL0t2HBwVj7yOy2Symp6cRRdtD9bvdLmZnZ7G0tIRsNouJiQm/EziAnsw4AN+WTxhwjWvZYgUrF/C2CszOmwXgB3rb4CfQ234Qvt6K9q12uWWGiOLH6i67+U0oWLZdjBtFLC0t+Yqqra0tNBoNtFot3Lx504sh52HZIEM6nUapVPKvW11dxfLyck9bnBDiwYU6QA1iBT+dRmofK4m4AObCll8A/HPa7XZPCz41lMcP9ThsnaXzZxNWtnrLLuhpO4OctIuP8zXpdLpnlqtNqIVZ+v2CGOHGD/y50+n4DbjW1tZQr9d91Ss/kz2fhE5qo9Hwr6HTK4R4MFlfX8fCwkJPV5b1w5ggsvpH3bULeSa52PFksSOu7GuoRVZ/bbLfjlwJK7xCnzCsngLQY7d9Lb/og8Zt7sXvYeCWugjs+ue26pVFBWtra/46YK8f9H25Rshms7h58ybm5ub8/hWqkBXiwYVayfUy/bNKpYJsNuuLBDhC0K7tqSUcR0ANokbSvwXi908AekcLAr3xCTsmgbaScHSXTb7RPnt/XKCV9sZV1e7XFWGTa41Go8evtd1b1GAWecUVcvG9W60WVldXfXzhVrER8WBxYDDWOfdbAD4L4GYURed27vsnAP4zAF0APwLwn0dRtLLz2BcA/AqATQB/L4qiP7w3pj9cZDIZ5HI5DA0NefGbn5/3i9vBwUEUi0U453wwNnRQmcGhw0gnDejdYZUCxexMsVjcI0Z2ILUVwv3a0WwFmA0qkLB9zDqbNlNlRxvYGa9hhSwrqlgJsLKygmaz6T9nvV5Hs9nEysqKF+JsNusrdPn+PF82+8d2ObUPiHuFdPfwsdWq1pGzbVu2ip9fXLxyxAuwdzHOiiPbDRC2TlnCIC87EcKFv3UcrXNqj2u12H5W+0WsxsYFe8Ofeds6zlzIdzod1Go1rK2todFooNPp9ARj2eFBp5xJMs6NZBBBGxmIe4E0NxkwedRsNv0GfYODg76VFNg7DxbYHatlNYSLcmL9RPv6cHEP9I6asTpqiQvIcl8DvoctTthvLmzcc0J7+JnDjjPbhQHs6qbdrJeBgrW1NXQ6nT0az2Asz18qlcLa2hqWlpb8c1R4IO420txkwBEF1C5qBDuibOEVfVquk23SiNoNoCehZAukSFhJa6tm+Xjoy4bJqLg1eOirhxocBmOt707tC5P9ce9j72PBQK1W83EHm3RzpuOWwVjaYj+3Hcej+MLDxe1Uxr4I4H8C8G/MfX8M4AtRFG045/4xgC8A+G+cc08A+AUATwKYBvA159xjURQpvH8HFItF/PzP/3xPywCHbFerVZ+V+dGPfgQAPnOVy+WQz+dRKpX8jnxWHK1AMXNlnVgKU7jbta3SCgMStrrJirEN2FKM7DwZVoeRsErAirmdrWgvFMxwsaWNjifL/wFgdXUV3W4Xi4uL/j4bbOGGMsBuBW+r1fKVBSdPnsQLL7zgbRwfH7+nv3vx0PIipLuHSj6fx2c/+1lks1k/u7BcLqPdbmNtbc0HXW3Si5ug0Mmy+sjgbiaTQT6f31N5yoQP59KyAtTqbtgWFm50YI/J41If6fSxOjdc+NNGBjOsjtuKVzrutqXKdmAAu7MIWY01Pz+Pdrvtk1/UXgY50uk0CoWCPy4rYqnjp06dwunTpwFsX7u0eZe4B7wIae6hksvl8LM/+7PI5XKoVCo9Pl673fZ+Hzdc4UaAVs/CPRSsxtkgJm9bHbSaR8JAgP05xL5n6P8y6Nntdv2McGA36BsGXW03mV2YM+Fnb9u5r1z0c6SLTQ4C8AUH4QgyJszo646NjeEzn/mMf/+hoaHb/0UKcXu8CGnuoVIsFvHJT36yx88sFAo+KHv9+nX/vEwm40cjDg0NxXbbRtHuXjRMStkYgS38snEBG2jlbVu0ZXU6HEVoH2fBVi6X27PJefhe9GE7nY6/dtg4hk3k2UpX7nvA+ALXBK1WCxsbG2g2m3uK2/h6xhXsber28ePH/cZrzjmMjIzc1d+1SC4HBmOjKPqPzrmZ4L4/Mjf/AsDf3Pn5cwD+bRRFawDecc5dAvBBAP/p7pj7cJJOp3HkyBHkcjm0Wi0UCgW/ozeFgUEA/lNzVgkdMQotF+qchRUu3q24hcFYK5ZhdVXonMZlsQjFyM7jiqvMsuMR7EyauOoCBmLpUHKHQm68RQeYj3FuFoA9AWRbJWADEbwAHTt27O78YoXYB+nu4ZNOpzE5OYlUKuXHCvCLHQXUB7tQD6tILdTa/WYHhs6iPabVypCwOstW9Ie6TZvjnGi+3gZluci3zjH10m5EYM8D52PVajXfmdDpdNBqtXzgIZVK+Qpft9PRkc1mvV5zrm4qlcLIyAgeeeSR2//lCfEekeYePul0GmNjY15j2RZLPQg1CsAev5DEVbzGtaDeChu8Db/HPTfuZ/ueoXbHabldwIfVsHa0Ap9LvzecPcjvDNzaAgfq7+DgIDKZjJ9faDfwyefzmJycvOX5EeJOkOYePgMDA5icnESn08HNmzcBoEdz7WZS1AtuHM6fWVVLzQ2rXAHsWduH3VNxWK0LjxPXlQDAV/PyKw57HbGFBPRHwxFkhLEQxhDYsWULB2wCLOwABtBTgcyCCl7bhoaGMD09vd+vSjzA3I2ZsX8XwL/b+fkotsWTXNm5T9wFlpaW8Oqrr+LUqVP4yEc+gmKxiFQq5Ste+Q++srKCer2Od999F++++y4ajQZGR0dRqVQwOTmJYrGI8fHxnoUwgJ6FNLAbOOBtBnErlYovxafYMetOG2x1GEWTzuTa2pqvAKPwsTLWVqnyuLZil/Z1u12f+WdV7/Lysl/4W2GMosi3aFEkmbnj50yn07519uLFi5iamvLnxFbe2tcJcYhId/vEysoKXn/9dUxPT+P8+fMolUoolUoA4DXGJpdsBaqdxQf0tlfZqoIwCRYGEcLOBUKnMawUsDMPGTxlgqnb7fa0m4VObWgD35+OJRNZDACsrq56TeZ4GDqo1GTONMxmsygUChgfH0e5XMbIyAiuX7+O5eVlnDx5EsPDwz0OOrVXuisSgDS3TzQaDbz11ls4duwYLly44BfWdiwXg4kMSLIiibuA2wR7WAEL7K2SBXpnstpOgbgkWbjQt8GGUJO54GdXQviYDfTyOfx5a2vLV7NyBBnbYbmvA/dGoP4yiALsdspx5M7g4CBqtRparRaOHDmCoaGhPVW/DMwKcchIc/vE2toaLl++jEqlgkceecSvzakn/Pny5cs+CT80NISxsTGMjY31dDLQf7QawmPYzlobbLU+KB+36+8wTsFqW8Ln09fM5XI9r7FFAGHXhNXKwcFBZLPZPcHobreLtbU1X1jATWg5SosBWhvcZbCVxXHr6+t47rnnMDk5GZvko08uHj7u6GrrnPsNABsAfvt9vPbzAD4PANVq9U7MeKhg6yqdKbZu0TEFdttNK5WKryoF4AdLA/DzUZmlsbNegFtXAdgxATZLRbHjd1uhZUXRZvcZOLDOKT8DhdUSRZEfP9BsNrG2tubFb2FhwTuovFjQLhs04WdgYKFSqWB0dBS5XA7dbtePdRAiidwt3a1UKnfZsgcTJqFSqRTW19dRq9XQaDR6ZvJR5xjktI91Oh3fDkrnlHobVxkVOnNh+y3vp22hbtOBDcfKEFvNu5+jGwYebCsstZTtsLVazQd57Y6xDM4C8CMISqWSrwTgbf4dsjJWiKRxtzSXSRxxMG5nzEqr1UKn0/FJfFv1ZMdYAb17ChDeZ8eohGMJLHHzXLnwt8+N6wrbrxrXaq19bqix9jFbWcWAAF/DYCzbYu2mMDwOF/bcKbzT6aBUKvkWZBZyKOgqksjd0txyuXyXLXswYXdSPp9HJpPxPhzQO8qPvisDlc1m0/u82WzWJ372ixGEHV5xVa6hXSSuGjYuqBlXMRsWhQG7FbphhSwAHzxtt9u+q3Ztbc13e3GTLusT2/FbjF8Ui0WMjIz45yi+IOJ431dh59wvY3vw9l+Ldj2KqwBsD/cjO/ftIYqiLwL4IgBMT0/H9/2IHvL5PE6fPg3nHL7//e9jZWUFtVoNJ06cwNDQkJ9JdfToUWxubmJqagqtVgv1eh1LS0tYXV1FrVZDrVbDjRs3kE6nUa1Wkc1mUalU/DxDVnPZsn8LHTnONmT2B9h1XCk2VhS5UM/n836WzMDAAIrFIoDd8QU2u8+AKsWfYwbW1tawvLyMRqOB+fl5dDodLCwsANgdRm6dTP7MgHUqlUKr1cLc3BzGxsZw8eJF/1xtECOSyt3U3ampKenubVAul3Hx4kU459DpdDA7O4tr167hyJEjKJfLGBoaQjabRT6f98kmLpSZMWeQlnO3qDFxwU8O8LcOIx1bW0llK6zsMcK2VRugjaLtGa/cNdcm64D4zbiox3RIuRkiZ2mzQoCOu7W7VCr5MTvFYhHDw8NYWlrCSy+9hHw+j0qlgnK5vOezCJEU7qbmjo+PS3Nvg2w2i+PHjyOVSuGdd95Bs9lEs9nEzMwMqtWq18NcLufnyWazWeRyuZ62WqC3a4s+J+cCAr0JLbuYDqunbOLKLujtxoh8bfi8cFZi3ExaHsfOm+WxeB2p1+teg3kc+7lYETY4OIhCoeDnky8vL+Ott97CzMwMTp06hdHRUW+jEEnjbmru5OSkNPc2KJfLePrpp71uLS0t4dq1axgfH0exWPQB2eHhYWxtbWFtbQ3tdhvLy8tYXFzE1tYWhoeHkcvlvN/HEQbcH8ZWzgK71a2hHoZdDKG22pEzVj8ZLGaHLrCdlKIfy2ParmCb+GJ8odvt+oK3mzdvot1u+/nk1GW7lw6hng4MDPgq2iNHjuCDH/ygt13xBRHH+wrGOuc+BeDXAXw0iqKWeejLAP5P59w/xfaA7dMAvnXHVgoAvU4j24jojDKwmEqlcOzYMb/gHxwcxMjICFKpFHK5nM/O1Ot179w1m03U63VkMhm/WU06nUY+n/dCGpb2h9UAFCj7mK1CtW253HyBm2XxNo9hN2+xIkknlJWxrA7gc+Pm1NhAxsDAAEZHR32VW6fTQblcxvT0tKoDROKR7h4etnI/n89jdHQUhUIBAwMDuHLlCgBgcnJyz+yssEKgXq8D2N2Jlh0K1KTQSQ2rCGxQ1VZbWYcw7C4Iq6z4XI6ZCUcd8Bhs+7WbFbTbbb9BYlw7K/Wc7V65XA6ZTAZDQ0P+dqVSwczMjN+Ea78KYSEOG2nu4RD6utRKatHq6iqcczhy5IhfmLPDK5vN9lSTsoLJVtDSH6RvarsO+P5xxFXBsios3IHbBhlsJxiDBnGdCcRqL31cbqhjN4DhZ2aXG7sNONuRHXPlchnHjh3D8PCw9FYkGmnu4UAdJKVSCRMTE75Q68aNG9jY2EC1Wu3ZALBcLvu1uw1oUhutT8iEWTgike9vv4edtrzPJrmsf8tgMX1pwjnYJNz0lvETO/6QRWvtdtt3ftnCBmJ99IGBAYyPj/sRORsbGxgbG8Pk5KQCsOJADoxAOed+B8DHAIw5564A+O+wvbthFsAf7/wh/kUURf9FFEXfc879LoDvY7u94Fcj7XR4zygUCigUCr5t6Zvf/CY2NjbwzDPPYGhoCEePHkWlUsHIyAiq1aoPZjLL02q1/MK62Wx6sWSAYHx8HIVCARMTE97ZoxDaEQRbW1t+NABbo8LWLDujkK9pNBo+c89WK851bTQaPljQ7XYxPz+PlZUVXLlyxdvPzJTNwoWtBlac0+k0HnvsMbVni8Qj3U0unJEFbAc0/+RP/gTNZhMnTpzw87iLxaJ3Yhmc3djYwNzcnG93ovZVq1UMDw/vmS9LbOVTOH4F2J0zCOzd2MYGbxmIZdUssKvLrFygNrP1anFx0XdW1Ot11Ot178iyIpjB2I2NDeRyOVSrVZTLZZTLZeTzeR8wILlczp8/IZKCNDe50C9dW1tDs9nEd7/7XQDAM888g3w+j2KxiHw+j1Kp5DWQ+wTUajU/UoU+I7vAqM/UXZs8I2FA1W6oxUADu7XCKtqw6tW+1gZoAfi54Ay6UnvpC9sNDZn0o3ZzLZDP51Eul32BA+GcbiGShDQ3uYR+7re+9S00Gg0cO3YMhUIBY2NjKBQKviBsa2sLCwsLaDQaqNfrPV2t2WzWj6QaGRnxem1HzgDYE2TlfWGRVdzYFwaIub6v1Wr+vUP/mNq9vr6O9fV1rK6u+gKvmzdv4vr16/66Qb1mkDUs8OLtTCaDs2fP7okvKPElbocDg7FRFP1izN3/+hbP/0cA/tGdGCXeO845nD59Go1GA6+//roPpjIoEFfJmsvl/CBptqF2u10vhPV63W/EEs7pspkxtn3RweR7AL0tXnSEOYOlXq9jdXUV8/PzPTsUcpdCzhzkc+2AbI4boNhSFDlMmz/zs5w5cwYTExPI5XJ9/s0I8d6R7t4fpNNpnDlzBvV6HXNzc6jVan4DAT7OBTOr8VOpFMbHx3tmZa+uru7J2PO2DRQA8XO8bRcCF+HsPrDjCezc8E6n4ytYG42Gnyu4vr7uAwB0qjnSgO1q6XTaV7sWi0Wsr6/j5s2bXn+bzSbm5uZw6tQpDA8P39tfghB3AWnu/UEqlcLRo0fR6XS8r8txW6y6YuKKX6lUCtVqtWc2YDhTm4/RPyZ27iFv24ooO8vWHs/O5rbfeQxWuLIDjP4uixKovalUCvl83s90TKfTKBQK2NjYwPLysh9Ls7i4iCiKMDMzo41gxH2BNPf+YGBgAGfOnMHq6iquXr0K5xxqtZov0mISi523THDZ2ahbW1t+Hc9CBPqSNrDJwgDebzsXwkIF6wtTozkjnMkt+r02IUft5b4zi4uLfiNE3scxONYPj6LI672d4X3q1CmMjY31bBomxHtBvdkPCOl0GjMzM1haWsKf//mf+3YCzo5i1WulUkE2m8XQ0BDy+bxv8wK2dw1fWlrqWZBHUYSVlRX/Phx9EFbRFovFnjEBdqMFAN7p3NjYQKPR8BUL3HCLTmyj0fDBWm7ewIAssNtKYTcfsyMQ+N6sQKBjOzQ0hOPHj/fxNyKEeNBJpVI4deoUVldX8cYbb/iZUtQ1jiCoVCp+llY+n8fIyEjPHGyOLyDWGWXFrB1lENfSBey2xlq9pdPIKoB2u+2fz7ZePsbNCZaXl/19mUzGj2SwowdYgTU2NoZWq4WFhQXvrK6urqLRaGB6ehojIyN9/I0IIR5kUqkUJiYmUKvV8MorrwAARkZGevxSVr2m02kUi0UUCgWUy2XvL9pFt01kMYhKjYx29kOwhQdc9NvFOLA7WsYu1Km//GL7Ll/PMQr0tzc2NvxYBdrEEWPZbBblchmDg4OoVqt+XiP949XVVbRaLUxOTqr7Swhx1xgYGMCjjz6KlZUVvP76675Ays7uZpeUcw7FYtGPSLRjBlk1y3FfDNhaP5fxCvrO1F+gdw637Qyz2GAsE1Wcb8siL8YgVlZWvL/L5xK7fw4/J4sNAPgE2sbGBkZHRzEzM9OfX4Z4IFEw9gGjVCrhhRdewNbWll+826yTnauSTqf9zuCzs7N+ob24uOjbbvP5vK9Gdc5hbm4O3/3ud3Hu3Dk88cQTuHTpks8qsXWVcwEJnU7bJsbRA6EA2qyVnadlRw3wi5/BHrdareLJJ5/0TjPfe2pqqu+/CyHEw0GhUMBP//RPA0DPDGzbwmRnaKXTaSwvL+PNN9/0etZut7GxsYGzZ8+iUCjg+vXriKIImUwGKysruHz5Ms6ePYuzZ89idnYWtVrNayQdYW4iZscRMGDL+1i9yw23GEC1O3bTmR0ZGUE+n/djFFjly8oGJuJKpVJPay+rablRjBBC3E3y+TyeffZZAOhJVNkiAPq+6XTajy1YXl72viTHdh09etQXFPDr2rVreOWVV3Dy5Ek8+uijmJ+fR7vd7tHGXC6H4eHhngAsgJ6EGDeDYdKNlbMM1NoNu1KpFMrlMjKZDEqlkq/25aZc1g/udDr+XKRSKa/f1Wq1/78MIcQDT6FQwEc+8hE/r9ruB2OLBFgpm8lk/CbbAHp0mJp7/fp1vwcBk/pnz57FY489hh/+8IdYWVnx/u3Q0BCKxSJGR0d7xiSy2IDFDWtra7hx44bvsrWdCYw/2L1u2GFrK3nD4ge3szniuXPneorMoijCxMRE/38Z4oFCwdgHjMHBQTz66KO3/Xw6itevX0e1WvVOXrvd9hVRFDDOWLl69SpmZmYwMDCAxcVFXL58Gc1mE8D2JjbFYtE7irZSgM4iM0xLS0s+s2QdaGA322UrEfgcBo25UyM3N1hbW0OhUMDx48e1IZcQom9kMpn3XHm/vr6O5eVlP/OPG7awVYuaubW1heXlZVy+fBlHjhzx3Qo3btzwHQDcObtUKu0JxtrEFFuzOCecQQAGY6mxlUrF20GtZYUZk2AcvcDgbLixoxBC3CsymQymp6dv+/nsVlhaWvKaxpEq4T4D6XQa3W4X169fx9TUFFKplH8tgwwclcXEm93U0FbCrq+v+z0alpaW9oxJsCMFGHTI5XLI5/OoVCq+A4GJPGB3A18bABZCiHvJ4OAgTpw48Z5ew9EFHGfADoJisYhyuYy5uTmfvF9ZWcHc3ByOHj0KAFhcXMTVq1d95e36+jo6nU7P+AC7cRgrdpvNJmZnZ9FsNtFoNGI3qAV6N9INi734ZalUKjh27Jg25BJ3HUWsBIaGhvD000/7CoKhoSFsbm76OVXFYtE/9+zZs5ienkapVAKwPSR7aWkJIyMjvi1sY2MDi4uLezb7YiWAnbHFY4TBWGau+N4M2na7XVy4cAFPP/10z4YzwO7mCBJKIUTSGRkZwfPPP79nQxdWaJ04ccJn7aenp/Hkk0+iUCgAAObn53H58mVUq1VkMhlfAWsX+0Dv5gHWsdzc3PQbHDC4wA1yOJswlUphYWHBBzGeeuopXLhwwdvL9+EMWgUFhBBJplAo4NixY173SqWS7yJjdRU1c2JiAj/3cz/nN8m9efMmrl275itir169inQ6jStXrvgxBvR3mUSzgVlW5XLUFn1eFjIMDw/77jNq7uOPP45z5871dDiQcGdvIYRIGuVyGefOnQOwW8Fv/dyTJ09635c6yXX/6uoqbty44TfpZlfDG2+8AWC3WAtAT0ft1taW3wyce8jYKld243LGKxNy6+vrePLJJ3H+/Pk982ltQZgQdxsFYwXS6bQPrgLYM/zfLuI50wXYdgZZMcX5tBy+TWFl1at1HCm8wLaYlsvlnoHbwG4wlu/FCoOBgQGfURNCiPsVbjyzH3azwVwu16N57A7gqABbpRWOSAB2Z9AC27pNzWfyym7AAMCPmaFjyyCDvRbsN7NLCCGSCKugSFj5ZP3UwcFBv5s4NZNJK477YgDXtu0CvQv3gYEBtNttP1uR93Em+Nramvd1eUwbWLDJNSGEuJ8IK0ztKIC42ySKImSzWRSLReTz+Z5gKitpmaSyM7xt/IFjX8JgLMcT8FrALrJut+s3PReinygYK943zjl88IMfxE/+5E/2zFUh169fx6VLl3p29Aa2nVzOdpmYmMCHPvSh2GwTHVwAvhWBrxdCiIeVZ599FhcvXtyjucB2p8H8/LwPGPArn89jfX0dly9fxtjYGD760Y/uuxGYbYdlgEI7dAshHkacczh37hwef/zxntmIfIxjWwg1lyO7vvOd76BQKODxxx/fV3PpA1NvGQAWQoiHDedcj58bPjY3N4dLly55HzjcSHFlZQXj4+N49tln9/jJ9rnhfdJccRjor07cEdYBDXHO+UorC2fPcmdDVgQIIYQ4mFslpOhMMnnFRBbnbdVqNb8BV+jkCiGE2Esmk9k3IRVFkd/olnNgGbRtt9tYXl5GsVj0bbFCCCFuza3iC5lMxo8mtMVaAwMDfl43xy0qviCSjoKx4p6xvr6ORqPhWwI434U7zN64ccOPIRBCCHHnbGxs+M256KAC205qs9nED3/4Qxw7dgzPPPOMAgNCCHGHcMdwau7g4KDvLmg0Gnj99dcxNTWF06dPS3OFEOIOYYEBsWMKuDmuHb8oRJJRMFbcMyqVih/OTVKplJ9pOD09jdHRUTmnQghxl8jn85iYmNizwYtzDt1uF8DuZjFCCCHujEwmg0ql4se6DAwMYGBgwI8aOH36NIaGhqS5QghxFyiVSpiZmfG3bZxhc3MT4+PjGB4ePgTLhHjvKBgr7hljY2N+AwQhhBD3nlKpdMuKgMcee6yP1gghxINNLpfr2XAx5MiRI320RgghHmxGRkYwMjJy2GYIcVfQIA0hhBBCCCGEEEIIIYToAwrGCiGEEEIIIYQQQgghRB9QMFYIIYQQQgghhBBCCCH6gIKxQgghhBBCCCGEEEII0QcUjBVCCCGEEEIIIYQQQog+oGCsEEIIIYQQQgghhBBC9AEFY4UQQgghhBBCCCGEEKIPKBgrhBBCCCGEEEIIIYQQfUDBWCGEEEIIIYQQQgghhOgDCsYKIYQQQgghhBBCCCFEH1AwVgghhBBCCCGEEEIIIfqAgrFCCCGEEEIIIYQQQgjRBxSMFUIIIYQQQgghhBBCiD6gYKwQQgghhBBCCCGEEEL0AQVjhRBCCCGEEEIIIYQQog8oGCuEEEIIIYQQQgghhBB9QMFYIYQQQgghhBBCCCGE6AMKxgohhBBCCCGEEEIIIUQfODAY65z7LefcTefc6zGP/ZpzLnLOje3cds65f+Gcu+Sce805d/FeGC2EEA8y0l0hhOgf0lwhhOgv0l0hxMPO7VTGvgjgU+GdzrljAD4BYNbc/WkAp3e+Pg/gN+/cRCGEeOh4EdJdIYToFy9CmiuEEP3kRUh3hRAPMQcGY6Mo+o8AlmIe+mcAfh1AZO77HIB/E23zFwCGnHNH7oqlQgjxkCDdFUKI/iHNFUKI/iLdFUI87LyvmbHOuc8BuBpF0XeCh44CeNfcvrJzX9wxPu+c+yvn3F+1Wq33Y4YQQjw03G3dbbfb98hSIYS4/7nbmtvpdO6RpUII8WBwp7orP1cIcT+Rfq8vcM4VAPxDbLcPvG+iKPoigC8CwPT0dHTA04UQ4qHlXuju1NSUdFcIIWK4F5o7Pj4uzRVCiH24G7prNXdyclKaK4RINO85GAvgJICfAPAd5xwAPALg2865DwK4CuCYee4jO/cJIYR4/0h3hRCif0hzhRCiv0h3hRAPFe95TEEURd+NomgiiqKZKIpmsN0mcDGKousAvgzgb+/sePgcgNUoiuburslCCPFwId0VQoj+Ic0VQoj+It0VQjxsHBiMdc79DoD/BOCMc+6Kc+5XbvH0rwJ4G8AlAP8bgP/yrlgphBAPEdJdIYToH9JcIYToL9JdIcTDzoFjCqIo+sUDHp8xP0cAfvXOzRJCiIcX6a4QQvQPaa4QQvQX6a4Q4mHnPY8pEEIIIYQQQgghhBBCCPHeUTBWCCGEEEIIIYQQQggh+oCCsUIIIYQQQgghhBBCCNEHFIwVQgghhBBCCCGEEEKIPqBgrBBCCCGEEEIIIYQQQvQBBWOFEEIIIYQQQgghhBCiDygYK4QQQgghhBBCCCGEEH1AwVghhBBCCCGEEEIIIYToAwrGCiGEEEIIIYQQQgghRB9QMFYIIYQQQgghhBBCCCH6gIKxQgghhBBCCCGEEEII0QcUjBVCCCGEEEIIIYQQQog+oGCsEEIIIYQQQgghhBBC9AEFY4UQQgghhBBCCCGEEKIPKBgrhBBCCCGEEEIIIYQQfUDBWCGEEEIIIYQQQgghhOgDCsYKIYQQQgghhBBCCCFEH1AwVgghhBBCCCGEEEIIIfqAgrFCCCGEEEIIIYQQQgjRBxSMFUIIIYQQQgghhBBCiD6gYKwQQgghhBBCCCGEEEL0AQVjhRBCCCGEEEIIIYQQog8oGCuEEEIIIYQQQgghhBB9QMFYIYQQQgghhBBCCCGE6AMKxgohhBBCCCGEEEIIIUQfUDBWCCGEEEIIIYQQQggh+oCCsUIIIYQQQgghhBBCCNEHFIwVQgghhBBCCCGEEEKIPqBgrBBCCCGEEEIIIYQQQvQBBWOFEEIIIYQQQgghhBCiD7goig7bBjjn5gE0ASwcti0BY0iWTbLnYJJmk+w5mCTZdCKKovHDNqIfOOfqAN48bDsCkvS3ACTPHiB5Nsmeg0maTUmz56HQ3YT6ukn7WwCSZ5PsOZik2ZQ0e4Bk2STNPVyS9LcAyJ7bIWk2Jc0eIHk2JcmefTU3EcFYAHDO/VUURc8cth2WpNkkew4maTbJnoNJok0PA0k870mzKWn2AMmzSfYcTNJsSpo9DxNJO/dJswdInk2y52CSZlPS7AGSadPDQBLPe9Jskj0HkzSbkmYPkDybkmbPfmhMgRBCCCGEEEIIIYQQQvQBBWOFEEIIIYQQQgghhBCiDyQpGPvFwzYghqTZJHsOJmk2yZ6DSaJNDwNJPO9Jsylp9gDJs0n2HEzSbEqaPQ8TSTv3SbMHSJ5NsudgkmZT0uwBkmnTw0ASz3vSbJI9B5M0m5JmD5A8m5JmTyyJmRkrhBBCCCGEEEIIIYQQDzJJqowVQgghhBBCCCGEEEKIB5ZEBGOdc59yzr3pnLvknPsHh/D+x5xz33DOfd859z3n3N/fuX/EOffHzrm3dr4P99muAefcK865r+zc/gnn3F/unKd/55wb7LM9Q865LznnfuCce8M591OHeY6cc//1zu/rdefc7zjncv0+R86533LO3XTOvW7uiz0nbpt/sWPba865i32y55/s/M5ec8793865IfPYF3bsedM598m7bc9+NpnHfs05FznnxnZu3/NzJKS5t7BLmntre6S5t2ePNFf0cNiau2ODdPdgWxKluTs2HaruJk1zb2HToemuNDeZHLbuSnNv255E6e5ha+6ODYnS3aRp7n42mcfuG9099GCsc24AwL8E8GkATwD4RefcE302YwPAr0VR9ASA5wD86o4N/wDA16MoOg3g6zu3+8nfB/CGuf2PAfyzKIpOAVgG8Ct9tuefA/h/oyg6C+CpHdsO5Rw5544C+HsAnomi6ByAAQC/gP6foxcBfCq4b79z8mkAp3e+Pg/gN/tkzx8DOBdF0XkAPwTwBQDY+Rv/BQBP7rzmf975f+yHTXDOHQPwCQCz5u5+nKOHGmnuLZHm7oM09z3ZI80VnoRoLiDdvR0So7lAYnT3RSRLc/ez6TB1N84eae4hkhDdlebeHonR3YRoLpA83Y2zR77u3SCKokP9AvBTAP7Q3P4CgC8csk2/D+CvA3gTwJGd+44AeLOPNjyC7X+0nwXwFQAOwAKAdNx564M9VQDvYGfOsLn/UM4RgKMA3gUwAiC9c44+eRjnCMAMgNcPOicA/lcAvxj3vHtpT/DYzwP47Z2fe/7XAPwhgJ/qxznaue9L2L7o/hjAWD/P0cP8Jc3d1wZp7q3tkebepj3BY9Lch/wriZq7Y4d0t9eWRGnuzvslQneTprlxNgWP9V13pbnJ+kqi7kpzY+1JlO4mRXN33idRups0zd3PpvtNdw+9Mha7f/Tkys59h4JzbgbABQB/CWAyiqK5nYeuA5jsoyn/I4BfB7C1c3sUwEoURRs7t/t9nn4CwDyA/32nteFfOeeKOKRzFEXRVQD/A7azHnMAVgG8jMM9R2S/c5KEv/W/C+D/OWx7nHOfA3A1iqLvBA8l4Rw96CTqHEtz90Wae/tIcw9AmnuoJO4cS3djSZTmAonW3SRrLpAA3ZXmHjqJOs/S3H1JlO4mWHOBZOvuoWsucH/qbhKCsYnBOVcC8O8B/FdRFNXsY9F2GD3qkx2fBXAziqKX+/F+t0kawEUAvxlF0QUATQQtA30+R8MAPodtEZ8GUERMqfph089zchDOud/AdsvMbx+yHQUA/xDAf3uYdojDR5p7S6S57wNpbqwd0lzhke7uS6I0F7g/dDdJmgskQ3elucIizb0lidLd+0FzgWTpbhI0d8eO+1J3kxCMvQrgmLn9yM59fcU5l8G2UP52FEX/YefuG865IzuPHwFws0/mPA/gbzjnfgzg32K7leCfAxhyzqV3ntPv83QFwJUoiv5y5/aXsC2eh3WOXgDwThRF81EUrQP4D9g+b4d5jsh+5+TQ/tadc78M4LMA/taOgB+mPSexfZH7zs7f+CMAvu2cmzpEmx4mEnGOpbkHIs29faS5t0aae7gk5hxLd29J0jQXSK7uJk5zd2z5ZSRDd6W5h08izrM090CSprtJ1VwggbqbIM0F7lPdTUIw9iUAp932LnWD2B74++V+GuCccwD+NYA3oij6p+ahLwP4Ozs//x1sz3q550RR9IUoih6JomgG2+fj/4ui6G8B+AaAv9lve3Zsug7gXefcmZ27/hqA7+OQzhG22weec84Vdn5/tOfQzpFhv3PyZQB/223zHIBV025wz3DOfQrbLSl/I4qiVmDnLzjnss65n8D2UOtv3Wt7oij6bhRFE1EUzez8jV8BcHHnb+xQztFDhjQ3QJp7W0hzbxNprgg4dM0FpLu3YU/SNBdIru4mSnOBZOmuNDcRHLruSnNvy6ak6W5SNRdImO4mSXOB+1h3owQMrgXwGWzvwvYjAL9xCO//09gu9X4NwKs7X5/B9hyVrwN4C8DXAIwcgm0fA/CVnZ8fxfYf8yUA/xeAbJ9teRrAX+2cp98DMHyY5wjAfw/gBwBeB/B/AMj2+xwB+B1sz5RZx/Y//a/sd06wPST9X+78nX8X2zs19sOeS9iek8K/7f/FPP83dux5E8Cn+3WOgsd/jN0B2/f8HOlLmnuAbdLc/e2R5t6ePdJcfYW/k0PV3B0bpLsH25Eozd2x6VB1N2maewubDk13pbnJ/Dps3ZXm3rYtidLdw9bcHRsSpbtJ09z9bAoevy901+0YKIQQQgghhBBCCCGEEOIekoQxBUIIIYQQQgghhBBCCPHAo2CsEEIIIYQQQgghhBBC9AEFY4UQQgghhBBCCCGEEKIPKBgrhBBCCCGEEEIIIYQQfUDBWCGEEEIIIYQQQgghhOgDCsYKIYQQQgghhBBCCCFEH1AwVgghhBBCCCGEEEIIIfqAgrFCCCGEEEIIIYQQQgjRB/5/fEGhxrJ+oRcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    val_outputs = sliding_window_inference(\n",
    "        inputs=val_inputs, roi_size=(240, 240, 160), sw_batch_size=1, predictor=model, overlap=0.5\n",
    "    )\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_org_transforms,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    num_workers=4,\n",
    "    cache_num=0,\n",
    ")\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "post_transforms = Compose([\n",
    "    EnsureTyped(keys=\"pred\"),\n",
    "    Invertd(\n",
    "        keys=\"pred\",\n",
    "        transform=val_org_transforms,\n",
    "        orig_keys=\"image\",\n",
    "        meta_keys=\"pred_meta_dict\",\n",
    "        orig_meta_keys=\"image_meta_dict\",\n",
    "        meta_key_postfix=\"meta_dict\",\n",
    "        nearest_interp=False,\n",
    "        to_tensor=True,\n",
    "    ),\n",
    "    Activationsd(keys=\"pred\", sigmoid=True),\n",
    "    AsDiscreted(keys=\"pred\", threshold_values=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric on original image spacing:  0.7536337971687317\n",
      "metric_tc: 0.8062\n",
      "metric_wt: 0.8956\n",
      "metric_et: 0.5592\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        val_data[\"pred\"] = sliding_window_inference(\n",
    "            inputs=val_inputs, roi_size=(240, 240, 160), sw_batch_size=1, predictor=model, overlap=0.5\n",
    "        )\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    metric_batch_org = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = metric_batch[0].item(), metric_batch[1].item(), metric_batch[2].item()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
