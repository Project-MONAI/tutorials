{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDS Integration\n",
    "\n",
    "This notebook introduces how to integrate GDS into MONAI. It mainly includes several parts as shown below.\n",
    "- What is GPUDirect Storage(GDS)?\n",
    "\n",
    "    GDS is the newest addition to the GPUDirect family. Like GPUDirect peer to peer (https://developer.nvidia.com/gpudirect) that enables a direct memory access (DMA) path between the memory of two graphics processing units (GPUs) and GPUDirect RDMA that enables a direct DMA path to a network interface card (NIC), GDS enables a direct DMA data path between GPU memory and storage, thus avoiding a bounce buffer through the CPU. This direct path can increase system bandwidth while decreasing latency and utilization load on the CPU and GPU.\n",
    "\n",
    "- GDS hardware and software requirements and how to enable GDS.\n",
    "\n",
    "    1. GDS has been tested on following NVIDIA GPUs: T10x, T4, A10, Quadro P6000, A100, and V100. For a full list of GPUs that GDS works with, refer to the [Known Limitations](https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-limitations) section. For more requirments, you can refer to the 3 and 4 in this [link](https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#mofed-fs-req).\n",
    "\n",
    "    2. To enable GDS on bare metal, follow the detailed steps provided in this [section](https://docs.nvidia.com/dgx/dgx-os-6-user-guide/additional_software.html#installing-gpudirect-storage-support). To verify successful GDS installation, run the following command:\n",
    "        \n",
    "        ```/usr/local/cuda-<x>.<y>/gds/tools/gdscheck.py -p``` \n",
    "        \n",
    "        (Replace X with the major version of the CUDA toolkit, and Y with the minor version.)\n",
    "\n",
    "- `GDSDataset` inherited from `PersistentDataset`.\n",
    "\n",
    "    In this tutorial, we have implemented a `GDSDataset` that inherits from `PersistentDataset`. We have re-implemented the `_cachecheck` method to create and save cache using GDS.\n",
    "\n",
    "- A simple demo comparing the time taken with and without GDS.\n",
    "\n",
    "   In this tutorial, we are creating a conda environment to install `kvikio`, which provides a Python API for GDS. To install `kvikio` using other methods, refer to https://github.com/rapidsai/kvikio#install.\n",
    "\n",
    "    ```conda create -n gds_env -c rapidsai-nightly -c conda-forge python=3.10 cuda-version=11.8 kvikio```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cupy\n",
    "import torch\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from collections.abc import Callable, Sequence\n",
    "from kvikio.numpy import fromfile, tofile\n",
    "\n",
    "import monai\n",
    "import monai.transforms as mt\n",
    "from monai.config import print_config\n",
    "from monai.data.utils import pickle_hashing\n",
    "from monai.utils import convert_to_tensor, convert_to_cupy, set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified, a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmppwi1_sx5\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDSDataset(monai.data.PersistentDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Sequence,\n",
    "        transform: Sequence[Callable] | Callable,\n",
    "        cache_dir: Path | str | None,\n",
    "        hash_func: Callable[..., bytes] = pickle_hashing,\n",
    "        hash_transform: Callable[..., bytes] | None = None,\n",
    "        reset_ops_id: bool = True,\n",
    "        device : int = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data=data,\n",
    "            transform=transform,\n",
    "            cache_dir=cache_dir,\n",
    "            hash_func = hash_func,\n",
    "            hash_transform = hash_transform,\n",
    "            reset_ops_id = reset_ops_id,\n",
    "            **kwargs\n",
    "            )\n",
    "        self.device = device\n",
    "        self.shape_dict = {}\n",
    "\n",
    "    def _cachecheck(self, item_transformed):\n",
    "        \"\"\"given the input dictionary ``item_transformed``, return a transformed version of it\"\"\"\n",
    "        hashfile = None\n",
    "        # compute a cache id\n",
    "        if self.cache_dir is not None:\n",
    "            data_item_md5 = self.hash_func(item_transformed).decode(\"utf-8\")\n",
    "            data_item_md5 += self.transform_hash\n",
    "            hashfile = self.cache_dir / f\"{data_item_md5}.pt\"\n",
    "\n",
    "        if hashfile is not None and hashfile.is_file():  # cache hit\n",
    "            with cupy.cuda.Device(self.device):\n",
    "                item = {}\n",
    "                for k in item_transformed:\n",
    "                    if f\"{hashfile}-{k}\" in self.shape_dict:\n",
    "                        shape_k = self.shape_dict[f\"{hashfile}-{k}\"]\n",
    "                    else:\n",
    "                        shape_k = fromfile(f\"{hashfile}-{k}-shape\", dtype=int)\n",
    "                        self.shape_dict[f\"{hashfile}-{k}\"] = shape_k\n",
    "                    item[k] = fromfile(f\"{hashfile}-{k}\", dtype=np.float32, like=cupy.empty(()))\n",
    "                    item[k] = convert_to_tensor(item[k].reshape(shape_k), device=f\"cuda:{self.device}\")\n",
    "                return item\n",
    "\n",
    "        # create new cache\n",
    "        _item_transformed = self._pre_transform(deepcopy(item_transformed))  # keep the original hashed\n",
    "        if hashfile is None:\n",
    "            return _item_transformed\n",
    "\n",
    "        for k in _item_transformed:  # {'image': ..., 'label': ...}\n",
    "            item_k_shape = convert_to_cupy(_item_transformed[k].shape, wrap_sequence=True)\n",
    "            tofile(item_k_shape, f\"{hashfile}-{k}-shape\")\n",
    "            item_k = _item_transformed[k]\n",
    "            tofile(item_k, f\"{hashfile}-{k}\")\n",
    "        open(hashfile, \"a\").close()  # store cacheid\n",
    "        return _item_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple demo to show how to use the GDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset and set dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 22:35:29,985 - INFO - Expected md5 is None, skip md5 check for file samples.zip.\n",
      "2023-07-10 22:35:29,987 - INFO - File exists: samples.zip, skipped downloading.\n",
      "2023-07-10 22:35:29,988 - INFO - Writing into directory: /tmp/tmppwi1_sx5.\n"
     ]
    }
   ],
   "source": [
    "sample_url = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases\"\n",
    "sample_url += \"/download/0.8.1/totalSegmentator_mergedLabel_samples.zip\"\n",
    "monai.apps.download_and_extract(\n",
    "    sample_url, output_dir=root_dir, filepath=\"samples.zip\"\n",
    ")\n",
    "\n",
    "base_name = os.path.join(root_dir, \"totalSegmentator_mergedLabel_samples\")\n",
    "input_data = []\n",
    "for filename in os.listdir(os.path.join(base_name, \"imagesTr\")):\n",
    "    input_data.append(\n",
    "        {\n",
    "            \"image\": os.path.join(base_name, \"imagesTr\", filename),\n",
    "            \"label\": os.path.join(base_name, \"labelsTr\", filename),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set deterministic for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "transform = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImageD(keys=(\"image\", \"label\"), image_only=True, ensure_channel_first=True),\n",
    "        mt.EnsureTypeD(keys=(\"image\", \"label\"), device=device),\n",
    "        mt.SpacingD(keys=(\"image\", \"label\"), pixdim=1.5),\n",
    "        mt.RandRotateD(\n",
    "            keys=(\"image\", \"label\"), prob=1.0, range_x=0.1, range_y=0.1, range_z=0.3, mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        mt.RandZoomD(keys=(\"image\", \"label\"), prob=1.0, min_zoom=0.8, max_zoom=1.2, mode=(\"trilinear\", \"nearest\")),\n",
    "        mt.ResizeWithPadOrCropD(keys=(\"image\", \"label\"), spatial_size=(200, 210, 220)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GDSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(root_dir, \"gds_cache_dir\")\n",
    "dataset = GDSDataset(data=input_data, transform=transform, cache_dir=cache_dir, device=0)\n",
    "\n",
    "data_loader = monai.data.ThreadDataLoader(dataset, batch_size=1)\n",
    "\n",
    "s = time.time()\n",
    "for x in range(1):\n",
    "    e = time.time()\n",
    "    for x in data_loader:\n",
    "        print(x[\"image\"].dtype, x[\"image\"].device, x[\"image\"].shape)\n",
    "    print(\"epoch time\", time.time() - e)\n",
    "print(\"total time\", time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PersistentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir_per = os.path.join(root_dir, \"persistent_cache_dir\")\n",
    "dataset = monai.data.PersistentDataset(data=input_data, transform=transform, cache_dir=cache_dir_per)\n",
    "data_loader = monai.data.ThreadDataLoader(dataset, batch_size=1)\n",
    "\n",
    "s = time.time()\n",
    "for x in range(1):\n",
    "    e = time.time()\n",
    "    for x in data_loader:\n",
    "        print(x[\"image\"].dtype, x[\"image\"].device, x[\"image\"].shape)\n",
    "    print(\"epoch time\", time.time() - e)\n",
    "print(\"total time\", time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
