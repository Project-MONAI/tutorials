{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Pipeline for MONAI\n",
    "[Hugging Face](https://huggingface.co/) is a popular open-source platform for natural language processing (NLP) and other machine learning applications. Their tools and libraries are designed to make it easy to use state-of-the-art models for a wide range of tasks. It provides pre-trained models and a user-friendly API, making cutting-edge machine learning accessible to developers and researchers alike.\n",
    "\n",
    "The [Transformers](https://huggingface.co/docs/transformers/index) library by Hugging Face is a comprehensive collection of pre-trained models for NLP and other tasks. It supports models for text classification, information extraction, question answering, summarization, translation, and more. The library is designed to be user-friendly, allowing users to easily integrate powerful models into their applications with just a few lines of code.\n",
    "\n",
    "The [Hugging Face Hub](https://huggingface.co/docs/hub/index) is an online platform where users can share and discover pre-trained models. It hosts models for various languages and tasks, enabling collaboration and reuse of models within the machine learning community. The Hub also provides tools for versioning, managing, and deploying models, making it a valuable resource for both developers and researchers.\n",
    "\n",
    "In this tutorial, a pretrained spleen segmentation model will be wrapped into a pipeline provided by the Transformers library. Following this tutorial, users can easily build their own discriminative MONAI models (classification, detection, segmentation) into a transformer pipeline and let others use it as a hugging face native Transformers pipeline transparently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment \n",
    "MONAI installation has the optional dependencies needed for this tutorial. Therefore installing MONAI with necessary optional dependiencies is recommanded for this turtorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q 'monai[transformers,skimage,nibabel]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.5.0a0+872d972e41.nv24.08\n",
      "MONAI flags: HAS_EXT = True, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /opt/monai/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.3.1\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.0\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.16.2\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.20.0a0\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: 1.5.1\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.40.2\n",
      "mlflow version: 2.17.0\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.5rc2\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "\n",
    "from monai.utils.enums import CommonKeys as Keys\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.data import decollate_batch, list_data_collate\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import tempfile\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from transformers import PretrainedConfig, PreTrainedModel, Pipeline\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable. This allows you to save results and reuse downloads. If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Downloads and extracts the dataset. The dataset comes from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name} for image_name in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model Configuration\n",
    "\n",
    "A PretrainedConfig in Hugging Face's Transformers library is a class that holds the configuration parameters for a model. This configuration includes information such as model architecture details, hyperparameters, and other settings that define how the model behaves. The PretrainedConfig is used to initialize a model with a specific configuration or to load a model's configuration from a pre-trained checkpoint.\n",
    "\n",
    "In this section, a MONAI UNet configuration will be defined for the UNet initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MONAIUNetConfig(PretrainedConfig):\n",
    "    model_type = \"monai_unet\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        norm=\"batch\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.num_res_units = num_res_units\n",
    "        self.norm = norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Hugging Face Pretrained Model\n",
    "\n",
    "A PreTrainedModel in Hugging Face's Transformers library is a base class for all model classes. It provides common functionalities for loading, saving, and configuring models. This class is designed to work seamlessly with PretrainedConfig, allowing users to load pre-trained models from the Hugging Face Hub or from local files. The PreTrainedModel class ensures consistency and ease of use across different model architectures.\n",
    "\n",
    "In this section, we will define a pretrained MONAI UNet class which will load the configuration defined before for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MONAIUNet(PreTrainedModel):\n",
    "    config_class = MONAIUNetConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.unet = UNet(\n",
    "            spatial_dims=config.spatial_dims,\n",
    "            in_channels=config.in_channels,\n",
    "            out_channels=config.out_channels,\n",
    "            channels=config.channels,\n",
    "            strides=config.strides,\n",
    "            num_res_units=config.num_res_units,\n",
    "            norm=config.norm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Hugging Face Pipeline\n",
    "\n",
    "A Hugging Face pipeline is a high-level API that simplifies the process of using pre-trained models for various tasks such as text classification, image classification, object detection, and more. It abstracts the complexity of loading models and tokenizers, making it easy to perform inference with just a few lines of code. The pipeline handles the preprocessing of inputs, passing data through the model, and post-processing of outputs.\n",
    "\n",
    "This section will illustrate how to build a hugging face pipeline based on the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpleenCTSegmentationPipeline(Pipeline):\n",
    "    PREPROCESSING_EXTRA_ARGS = [\n",
    "        \"image_key\",\n",
    "        \"load_image\",\n",
    "    ]\n",
    "    INFERENCE_EXTRA_ARGS = [\n",
    "        \"mode\",\n",
    "        \"amp\",\n",
    "        \"hyper_kwargs\",\n",
    "        \"roi_size\",\n",
    "        \"overlap\",\n",
    "        \"sw_batch_size\",\n",
    "        \"use_point_window\",\n",
    "    ]\n",
    "    POSTPROCESSING_EXTRA_ARGS = [\n",
    "        \"pred_key\",\n",
    "        \"image_key\",\n",
    "        \"output_dir\",\n",
    "        \"output_ext\",\n",
    "        \"output_postfix\",\n",
    "        \"separate_folder\",\n",
    "        \"save_output\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, model, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "        self.model = model\n",
    "        self.preprocessing_transforms = self._init_preprocessing_transforms(**self._preprocess_params)\n",
    "        self.inferer = self._init_inferer(**self._forward_params)\n",
    "        self.postprocessing_transforms = self._init_postprocessing_transforms(**self._postprocess_params)\n",
    "\n",
    "    def _init_preprocessing_transforms(self, image_key=Keys.IMAGE, load_image=True):\n",
    "        transform_list = [LoadImaged(keys=image_key)] if load_image else []\n",
    "        transform_list.extend(\n",
    "            [\n",
    "                EnsureChannelFirstd(keys=image_key),\n",
    "                Orientationd(keys=image_key, axcodes=\"RAS\"),\n",
    "                Spacingd(keys=image_key, pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "                ScaleIntensityRanged(keys=image_key, a_min=-57, a_max=164, b_min=0, b_max=1, clip=True),\n",
    "                EnsureTyped(keys=image_key),\n",
    "            ]\n",
    "        )\n",
    "        preprocessing_transforms = Compose(transform_list)\n",
    "        return preprocessing_transforms\n",
    "\n",
    "    def _init_postprocessing_transforms(\n",
    "        self,\n",
    "        pred_key: str = Keys.PRED,\n",
    "        image_key: str = Keys.IMAGE,\n",
    "        output_dir: str = \"output_directory\",\n",
    "        output_ext: str = \".nii.gz\",\n",
    "        output_dtype: torch.dtype = torch.float32,\n",
    "        output_postfix: str = \"seg\",\n",
    "        separate_folder: bool = True,\n",
    "        save_output: bool = True,\n",
    "    ):\n",
    "        transform_list = [\n",
    "            Activationsd(keys=pred_key, softmax=True),\n",
    "            Invertd(\n",
    "                keys=pred_key,\n",
    "                transform=self.preprocessing_transforms,\n",
    "                orig_keys=image_key,\n",
    "                nearest_interp=False,\n",
    "                to_tensor=True,\n",
    "            ),\n",
    "            AsDiscreted(keys=pred_key, argmax=True),\n",
    "        ]\n",
    "        \n",
    "        transform_list.append(\n",
    "            SaveImaged(\n",
    "                keys=pred_key,\n",
    "                output_dir=output_dir,\n",
    "                output_ext=output_ext,\n",
    "                output_dtype=output_dtype,\n",
    "                output_postfix=output_postfix,\n",
    "                separate_folder=separate_folder,\n",
    "            )\n",
    "        )if save_output else transform_list\n",
    "        postprocessing_transforms = Compose(transform_list)\n",
    "        return postprocessing_transforms\n",
    "\n",
    "    def _init_inferer(\n",
    "        self,\n",
    "        roi_size=(96, 96, 96),\n",
    "        overlap=0.5,\n",
    "        sw_batch_size=4,\n",
    "    ):\n",
    "        return SlidingWindowInferer(roi_size=roi_size, sw_batch_size=sw_batch_size, overlap=overlap)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocessing_kwargs = {}\n",
    "        infer_kwargs = {}\n",
    "        postprocessing_kwargs = {}\n",
    "        for key in self.INFERENCE_EXTRA_ARGS:\n",
    "            if key in kwargs:\n",
    "                infer_kwargs[key] = kwargs[key]\n",
    "\n",
    "        for key in self.PREPROCESSING_EXTRA_ARGS:\n",
    "            if key in kwargs:\n",
    "                preprocessing_kwargs[key] = kwargs[key]\n",
    "\n",
    "        for key in self.POSTPROCESSING_EXTRA_ARGS:\n",
    "            if key in kwargs:\n",
    "                postprocessing_kwargs[key] = kwargs[key]\n",
    "\n",
    "        return (preprocessing_kwargs, infer_kwargs, postprocessing_kwargs)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        inputs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self._preprocess_params and value != self._preprocess_params[key]:\n",
    "                logging.warning(f\"Please set the parameter {key} during initialization.\")\n",
    "\n",
    "            if key not in self.PREPROCESSING_EXTRA_ARGS:\n",
    "                logging.warning(f\"Cannot set parameter {key} for preprocessing.\")\n",
    "        inputs = self.preprocessing_transforms(inputs)\n",
    "        inputs = list_data_collate([inputs])\n",
    "        return inputs\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        inputs,\n",
    "        amp: bool = True,\n",
    "    ):\n",
    "        inputs[Keys.IMAGE].to(self.device)\n",
    "        self.model.unet.to(self.device)\n",
    "        mode = eval_mode\n",
    "        with mode(self.model):\n",
    "            if amp:\n",
    "                with torch.autocast(\"cuda\"):\n",
    "                    inputs[Keys.PRED] = self.inferer(inputs[Keys.IMAGE], self.model)\n",
    "            else:\n",
    "                inputs[Keys.PRED] = self.inferer(inputs[Keys.IMAGE], self.model)\n",
    "        return inputs\n",
    "\n",
    "    def postprocess(self, outputs, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if key not in self.POSTPROCESSING_EXTRA_ARGS:\n",
    "                logging.warning(f\"Cannot set parameter {key} for postprocessing.\")\n",
    "            if (\n",
    "                key in self._postprocess_params\n",
    "                and value != self._postprocess_params[key]\n",
    "            ) or (key not in self._postprocess_params):\n",
    "                self._postprocess_params.update(kwargs)\n",
    "                self.postprocessing_transforms = self._init_postprocessing_transforms(\n",
    "                    **self._postprocess_params\n",
    "                )\n",
    "        outputs = decollate_batch(outputs)\n",
    "        outputs = self.postprocessing_transforms(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "In this section, we will instantiate the pipeline and run inference with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 16:34:41,094 INFO image_writer.py:197 - writing: output_directory/spleen_1/spleen_1_seg.nii.gz\n",
      "2025-02-28 16:34:47,260 INFO image_writer.py:197 - writing: output_directory/spleen_11/spleen_11_seg.nii.gz\n"
     ]
    }
   ],
   "source": [
    "config = MONAIUNetConfig()\n",
    "monai_unet = MONAIUNet(config)\n",
    "pipeline = SpleenCTSegmentationPipeline(model=monai_unet, device=torch.device(\"cuda:0\"))\n",
    "output = pipeline(data_dicts[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
