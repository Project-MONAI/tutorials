{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fc7b5c",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b7dcb",
   "metadata": {},
   "source": [
    "# Training a 3D Diffusion Model for Generating 3D Images with Various Sizes and Spacings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecfb90",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cbde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import xformers\" || pip install -q xformers --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655b95c",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bf0346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.1+27.g8cfbcbab\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.3.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8cfbcbabd1529ef4090fb6f7ffbeef47d6b70cc2\n",
      "MONAI __file__: /localhome/<username>/miniconda3/envs/monai-dev/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.13.1\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: 2.17.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.18.1+cu121\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.14.1\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.2\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import subprocess\n",
    "\n",
    "from monai.data import create_test_image_3d\n",
    "from monai.config import print_config\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e29c23",
   "metadata": {},
   "source": [
    "## Simulate a special dataset\n",
    "\n",
    "It is well known that AI takes time to train. We will simulate a small dataset and run training only for multiple epochs. Due to the nature of AI, the performance shouldn't be highly expected, but the entire pipeline will be completed within minutes!\n",
    "\n",
    "`sim_datalist` provides the information of the simulated datasets. It lists 2 training images. The size of the dimension is defined by the `sim_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc32a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_datalist = {\"training\": [{\"image\": \"tr_image_001.nii.gz\"}, {\"image\": \"tr_image_002.nii.gz\"}]}\n",
    "\n",
    "sim_dim = (128, 160, 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac7677",
   "metadata": {},
   "source": [
    "## Generate images\n",
    "\n",
    "Now we can use MONAI `create_test_image_3d` and `nib.Nifti1Image` functions to generate the 3D simulated images under the work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b199078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated simulated images.\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"./helloworld_work_dir\"\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "dataroot_dir = os.path.join(work_dir, \"sim_dataroot\")\n",
    "if not os.path.isdir(dataroot_dir):\n",
    "    os.makedirs(dataroot_dir)\n",
    "\n",
    "datalist_file = os.path.join(work_dir, \"sim_datalist.json\")\n",
    "with open(datalist_file, \"w\") as f:\n",
    "    json.dump(sim_datalist, f)\n",
    "\n",
    "for d in sim_datalist[\"training\"]:\n",
    "    im, _ = create_test_image_3d(\n",
    "        sim_dim[0], sim_dim[1], sim_dim[2], rad_max=10, num_seg_classes=1, random_state=np.random.RandomState(42)\n",
    "    )\n",
    "    image_fpath = os.path.join(dataroot_dir, d[\"image\"])\n",
    "    nib.save(nib.Nifti1Image(im, affine=np.eye(4)), image_fpath)\n",
    "\n",
    "print(\"Generated simulated images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2389853",
   "metadata": {},
   "source": [
    "## Set up directories and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7b434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['models', 'sim_datalist.json', 'embeddings', 'config_maisi.json', 'predictions', 'environment_maisi_diff_model.json', 'sim_dataroot', 'config_maisi_diff_model.json']\n"
     ]
    }
   ],
   "source": [
    "env_config_path = \"./configs/environment_maisi_diff_model.json\"\n",
    "model_config_path = \"./configs/config_maisi_diff_model.json\"\n",
    "model_def_path = \"./configs/config_maisi.json\"\n",
    "\n",
    "# Load environment configuration, model configuration and model definition\n",
    "with open(env_config_path, \"r\") as f:\n",
    "    env_config = json.load(f)\n",
    "\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "with open(model_def_path, \"r\") as f:\n",
    "    model_def = json.load(f)\n",
    "\n",
    "env_config_out = copy.deepcopy(env_config)\n",
    "model_config_out = copy.deepcopy(model_config)\n",
    "model_def_out = copy.deepcopy(model_def)\n",
    "\n",
    "# Set up directories based on configurations\n",
    "env_config_out[\"data_base_dir\"] = dataroot_dir\n",
    "env_config_out[\"embedding_base_dir\"] = os.path.join(work_dir, env_config_out[\"embedding_base_dir\"])\n",
    "env_config_out[\"json_data_list\"] = datalist_file\n",
    "env_config_out[\"model_dir\"] = os.path.join(work_dir, env_config_out[\"model_dir\"])\n",
    "env_config_out[\"output_dir\"] = os.path.join(work_dir, env_config_out[\"output_dir\"])\n",
    "env_config_out[\"trained_autoencoder_path\"] = None\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(env_config_out[\"embedding_base_dir\"], exist_ok=True)\n",
    "os.makedirs(env_config_out[\"model_dir\"], exist_ok=True)\n",
    "os.makedirs(env_config_out[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "env_config_filepath = os.path.join(work_dir, \"environment_maisi_diff_model.json\")\n",
    "with open(env_config_filepath, \"w\") as f:\n",
    "    json.dump(env_config_out, f, sort_keys=True, indent=4)\n",
    "\n",
    "# Update model configuration for demo\n",
    "max_epochs = 2\n",
    "model_config_out[\"diffusion_unet_train\"][\"n_epochs\"] = max_epochs\n",
    "\n",
    "model_config_filepath = os.path.join(work_dir, \"config_maisi_diff_model.json\")\n",
    "with open(model_config_filepath, \"w\") as f:\n",
    "    json.dump(model_config_out, f, sort_keys=True, indent=4)\n",
    "\n",
    "# Update model definition for demo\n",
    "model_def_out[\"autoencoder_def\"][\"num_splits\"] = 4\n",
    "model_def_filepath = os.path.join(work_dir, \"config_maisi.json\")\n",
    "with open(model_def_filepath, \"w\") as f:\n",
    "    json.dump(model_def_out, f, sort_keys=True, indent=4)\n",
    "\n",
    "# Print files and folders under work_dir\n",
    "print(os.listdir(work_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ea6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_torchrun(module, module_args, num_gpus=1):\n",
    "    # Define the arguments for torchrun\n",
    "    num_nodes = 1\n",
    "\n",
    "    # Build the torchrun command\n",
    "    torchrun_command = [\n",
    "        \"torchrun\",\n",
    "        \"--nproc_per_node\",\n",
    "        str(num_gpus),\n",
    "        \"--nnodes\",\n",
    "        str(num_nodes),\n",
    "        \"-m\",\n",
    "        module,\n",
    "    ] + module_args\n",
    "\n",
    "    # Set the OMP_NUM_THREADS environment variable\n",
    "    env = os.environ.copy()\n",
    "    env[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "    # Execute the command\n",
    "    process = subprocess.Popen(torchrun_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)\n",
    "\n",
    "    # Print the output in real-time\n",
    "    try:\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == \"\" and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Capture and print any remaining output\n",
    "        stdout, stderr = process.communicate()\n",
    "        print(stdout)\n",
    "        if stderr:\n",
    "            print(stderr)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c904f52",
   "metadata": {},
   "source": [
    "## Step 1: Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45ea863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data...\n",
      "\n",
      "The torch.distributed is either unavailable and uninitiated when RankFilter is instantiated.\n",
      "If torch.distributed is used, please ensure that the RankFilter() is called\n",
      "after torch.distributed.init_process_group() in the script.\n",
      "\n",
      "[2024-07-13 22:31:42.847][ INFO](diff_model_create_training_data) - Using device cuda:0\n",
      "[2024-07-13 22:31:43.720][ERROR](diff_model_create_training_data) - The trained_autoencoder_path does not exist!\n",
      "[2024-07-13 22:31:43.721][ INFO](diff_model_create_training_data) - filenames_raw: ['tr_image_001.nii.gz', 'tr_image_002.nii.gz']\n",
      "[2024-07-13 22:31:43.839][ INFO](diff_model_create_training_data) - old dim: [128, 160, 96], old spacing: [1.0, 1.0, 1.0]\n",
      "[2024-07-13 22:31:43.895][ INFO](diff_model_create_training_data) - new dim: (128, 128, 128), new affine: [[ 1.     0.     0.     0.   ]\n",
      " [ 0.     1.25   0.     0.125]\n",
      " [ 0.     0.     0.75  -0.125]\n",
      " [ 0.     0.     0.     1.   ]]\n",
      "[2024-07-13 22:31:43.895][ INFO](diff_model_create_training_data) - out_filename: ./helloworld_work_dir/./embeddings/tr_image_001_emb.nii.gz\n",
      "[2024-07-13 22:31:44.777][ INFO](diff_model_create_training_data) - z: torch.Size([1, 4, 32, 32, 32]), torch.float32\n",
      "[2024-07-13 22:31:44.892][ INFO](diff_model_create_training_data) - old dim: [128, 160, 96], old spacing: [1.0, 1.0, 1.0]\n",
      "[2024-07-13 22:31:44.947][ INFO](diff_model_create_training_data) - new dim: (128, 128, 128), new affine: [[ 1.     0.     0.     0.   ]\n",
      " [ 0.     1.25   0.     0.125]\n",
      " [ 0.     0.     0.75  -0.125]\n",
      " [ 0.     0.     0.     1.   ]]\n",
      "[2024-07-13 22:31:44.947][ INFO](diff_model_create_training_data) - out_filename: ./helloworld_work_dir/./embeddings/tr_image_002_emb.nii.gz\n",
      "[2024-07-13 22:31:45.505][ INFO](diff_model_create_training_data) - z: torch.Size([1, 4, 32, 32, 32]), torch.float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training data...\")\n",
    "\n",
    "# Define the arguments for torchrun\n",
    "num_gpus = 1  # Adjust based on the number of GPUs you want to use\n",
    "module = \"scripts.diff_model_create_training_data\"  # Replace with your script\n",
    "module_args = [\n",
    "    \"--env_config\", env_config_filepath,\n",
    "    \"--model_config\", model_config_filepath,\n",
    "    \"--model_def\", model_def_filepath,\n",
    "]\n",
    "\n",
    "run_torchrun(module, module_args, num_gpus=num_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c0c4a",
   "metadata": {},
   "source": [
    "## Create .json files for embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0221a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (32, 32, 32), 'spacing': [1.0, 1.25, 0.75], 'top_region_index': [0, 1, 0, 0], 'bottom_region_index': [0, 0, 1, 0]}\n",
      "{'dim': (32, 32, 32), 'spacing': [1.0, 1.25, 0.75], 'top_region_index': [0, 1, 0, 0], 'bottom_region_index': [0, 0, 1, 0]}\n",
      "Completed creating .json files for all embedding files.\n"
     ]
    }
   ],
   "source": [
    "def list_gz_files(folder_path):\n",
    "    \"\"\"List all .gz files in the folder and its subfolders.\"\"\"\n",
    "    gz_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".gz\"):\n",
    "                gz_files.append(os.path.join(root, file))\n",
    "    return gz_files\n",
    "\n",
    "\n",
    "def create_json_files(gz_files):\n",
    "    \"\"\"Create .json files for each .gz file with the specified keys and values.\"\"\"\n",
    "    for gz_file in gz_files:\n",
    "        # Load the NIfTI image\n",
    "        img = nib.load(gz_file)\n",
    "\n",
    "        # Get the dimensions and spacing\n",
    "        dimensions = img.shape\n",
    "        dimensions = dimensions[:3]\n",
    "        spacing = img.header.get_zooms()[:3]\n",
    "        spacing = spacing[:3]\n",
    "        spacing = [float(_item) for _item in spacing]\n",
    "\n",
    "        # Create the dictionary with the specified keys and values\n",
    "        # The region can be selected from one of four regions from top to bottom.\n",
    "        # [1,0,0,0] is the head and neck, [0,1,0,0] is the chest region, [0,0,1,0]\n",
    "        # is the abdomen region, and [0,0,0,1] is the lower body region.\n",
    "        data = {\n",
    "            \"dim\": dimensions,\n",
    "            \"spacing\": spacing,\n",
    "            \"top_region_index\": [0, 1, 0, 0],  # chest region\n",
    "            \"bottom_region_index\": [0, 0, 1, 0],  # abdomen region\n",
    "        }\n",
    "        print(f\"{data}\")\n",
    "\n",
    "        # Create the .json filename\n",
    "        json_filename = gz_file + \".json\"\n",
    "\n",
    "        # Write the dictionary to the .json file\n",
    "        with open(json_filename, \"w\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "folder_path = env_config_out[\"embedding_base_dir\"]\n",
    "gz_files = list_gz_files(folder_path)\n",
    "create_json_files(gz_files)\n",
    "\n",
    "print(\"Completed creating .json files for all embedding files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a9e48",
   "metadata": {},
   "source": [
    "## Step 2: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade6389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - Using cuda:0 of 1\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] ckpt_folder -> ./helloworld_work_dir/./models.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] data_root -> ./helloworld_work_dir/./embeddings.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] data_list -> ./helloworld_work_dir/sim_datalist.json.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] lr -> 0.0001.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] num_epochs -> 2.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - [config] num_train_timesteps -> 1000.\n",
      "[2024-07-13 22:40:11.548][ INFO](diff_model_create_training_data) - num_files_train: 2\n",
      "[2024-07-13 22:40:14.730][ INFO](diff_model_create_training_data) - Training from scratch.\n",
      "[2024-07-13 22:40:15.056][ INFO](diff_model_create_training_data) - Scaling factor set to 0.8944792747497559.\n",
      "[2024-07-13 22:40:15.058][ INFO](diff_model_create_training_data) - scale_factor -> 0.8944792747497559.\n",
      "[2024-07-13 22:40:15.063][ INFO](diff_model_create_training_data) - torch.set_float32_matmul_precision -> highest.\n",
      "[2024-07-13 22:40:15.063][ INFO](diff_model_create_training_data) - Epoch 1, lr 0.0001.\n",
      "[2024-07-13 22:40:16.354][ INFO](diff_model_create_training_data) - [2024-07-13 22:40:16] epoch 1, iter 1/2, loss: 0.7978, lr: 0.000100000000.\n",
      "[2024-07-13 22:40:16.499][ INFO](diff_model_create_training_data) - [2024-07-13 22:40:16] epoch 1, iter 2/2, loss: 0.7950, lr: 0.000056250000.\n",
      "[2024-07-13 22:40:16.501][ INFO](diff_model_create_training_data) - epoch 1 average loss: 0.7964.\n",
      "[2024-07-13 22:40:18.541][ INFO](diff_model_create_training_data) - Epoch 2, lr 2.5e-05.\n",
      "[2024-07-13 22:40:18.973][ INFO](diff_model_create_training_data) - [2024-07-13 22:40:18] epoch 2, iter 1/2, loss: 0.7891, lr: 0.000025000000.\n",
      "[2024-07-13 22:40:19.111][ INFO](diff_model_create_training_data) - [2024-07-13 22:40:19] epoch 2, iter 2/2, loss: 0.7898, lr: 0.000006250000.\n",
      "[2024-07-13 22:40:19.113][ INFO](diff_model_create_training_data) - epoch 2 average loss: 0.7894.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model...\")\n",
    "\n",
    "# Define the arguments for torchrun\n",
    "num_gpus = 1  # Adjust based on the number of GPUs you want to use\n",
    "module = \"scripts.diff_model_train\"  # Replace with your script\n",
    "module_args = [\n",
    "    \"--env_config\", env_config_filepath,\n",
    "    \"--model_config\", model_config_filepath,\n",
    "    \"--model_def\", model_def_filepath,\n",
    "]\n",
    "\n",
    "run_torchrun(module, module_args, num_gpus=num_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf7b17",
   "metadata": {},
   "source": [
    "## Step 3: Infer using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1626526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - Using cuda:0 of 1 with random seed: 66343\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - [config] ckpt_filepath -> ./helloworld_work_dir/./models/diff_unet_ckpt.pt.\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - [config] random_seed -> 66343.\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - [config] output_prefix -> unet_3d.\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - [config] output_size -> (128, 128, 128).\n",
      "[2024-07-13 22:42:57.610][ INFO](diff_model_create_training_data) - [config] out_spacing -> (1.0, 1.25, 0.75).\n",
      "[2024-07-13 22:42:58.434][ERROR](diff_model_create_training_data) - The trained_autoencoder_path does not exist!\n",
      "[2024-07-13 22:43:01.346][ INFO](diff_model_create_training_data) - checkpoints ./helloworld_work_dir/./models/diff_unet_ckpt.pt loaded.\n",
      "[2024-07-13 22:43:01.349][ INFO](diff_model_create_training_data) - scale_factor -> 0.8944792747497559.\n",
      "[2024-07-13 22:43:01.349][ INFO](diff_model_create_training_data) - num_downsample_level -> 4, divisor -> 4.\n",
      "[2024-07-13 22:43:01.362][ INFO](diff_model_create_training_data) - noise: cuda:0, torch.float32, <class 'torch.Tensor'>\n",
      "\n",
      "  0%|                                                                                  | 0/10 [00:00<?, ?it/s]\n",
      " 10%|███████▍                                                                  | 1/10 [00:00<00:04,  1.90it/s]\n",
      " 50%|█████████████████████████████████████                                     | 5/10 [00:00<00:00, 10.02it/s]\n",
      " 90%|██████████████████████████████████████████████████████████████████▌       | 9/10 [00:00<00:00, 16.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.30it/s]\n",
      "[2024-07-13 22:43:05.684][ INFO](diff_model_create_training_data) - Saved ./helloworld_work_dir/./predictions/unet_3d_seed66343_size128x128x128_spacing1.00x1.25x0.75_20240713224305.nii.gz.\n",
      "\n",
      "Completed all steps.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running inference...\")\n",
    "\n",
    "# Define the arguments for torchrun\n",
    "num_gpus = 1  # Adjust based on the number of GPUs you want to use\n",
    "module = \"scripts.diff_model_infer\"  # Replace with your script\n",
    "module_args = [\n",
    "    \"--env_config\", env_config_filepath,\n",
    "    \"--model_config\", model_config_filepath,\n",
    "    \"--model_def\", model_def_filepath,\n",
    "]\n",
    "\n",
    "run_torchrun(module, module_args, num_gpus=num_gpus)\n",
    "\n",
    "print(\"Completed all steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c8158-08f6-4e15-ba03-7aa0173178d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
