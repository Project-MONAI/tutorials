{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONAI Auto3DSeg Hyper-parameter Optimization with NNI\n",
    "\n",
    "**Auto3DSeg** supports hyper parameter optimization (HPO) with `NNI` and `Optuna` packages.\n",
    "Please check the [Optuna Notebook](hpo_optuna.ipynb) if you want to use **Auto3DSeg** with `Optuna` HPO.\n",
    "\n",
    "This notebook provides an example to perform HPO on learning rate with grid search method for hippocampus segmentation using NNI.\n",
    "To run this notebook, please install `nni` via `pip install` if want to execute HPO with NNI in this tutorial\n",
    "\n",
    "Note: if you have used other notebooks under `auto3dseg`, for examples: \n",
    "- `auto_runner.ipynb`\n",
    "- `auto3dseg_autorunner_ref_api.ipynb`\n",
    "- `auto3dseg_hello_world.ipynb`\n",
    "- `hpo_optuna.ipynb`\n",
    "\n",
    "You may have already generated the algorithm templates in MONAI bundle formats (hint: find them in the working directory). \n",
    "\n",
    "Please feel free to skip step 1-5 if the bundles are already generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, nni, tqdm, cucim, yaml, optuna]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries for HPO and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.apps.auto3dseg import BundleGen, DataAnalyzer, NNIGen\n",
    "from monai.apps.auto3dseg.utils import export_bundle_algo_history, import_bundle_algo_history\n",
    "from monai.bundle.config_parser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)\n",
    "\n",
    "msd_task = \"Task04_Hippocampus\"\n",
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/\" + msd_task + \".tar\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, msd_task + \".tar\")\n",
    "dataroot = os.path.join(root_dir, msd_task)\n",
    "if not os.path.exists(dataroot):\n",
    "    download_and_extract(resource, compressed_file, root_dir)\n",
    "\n",
    "datalist_file = os.path.join(\"..\", \"tasks\", \"msd\", msd_task, \"msd_\" + msd_task.lower() + \"_folds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define experiment file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User created files\n",
    "nni_yaml = './nni_config.yaml'\n",
    "\n",
    "# Experiment setup\n",
    "test_path = \"./\"\n",
    "work_dir = os.path.join(test_path, \"hpo_nni_work_dir\")\n",
    "datastats_file = os.path.join(work_dir, \"datastats.yaml\")\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare an input yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cfg = {\n",
    "    \"name\": msd_task,  # optional, it is only for your own record\n",
    "    \"task\": \"segmentation\",  # optional, it is only for your own record\n",
    "    \"modality\": \"MRI\",  # required\n",
    "    \"datalist\": datalist_file,  # required\n",
    "    \"dataroot\": dataroot,  # required\n",
    "}\n",
    "input = './input.yaml'\n",
    "ConfigParser.export_config_file(input_cfg, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bundle Generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(datastats_file):\n",
    "    da = DataAnalyzer(datalist_file, dataroot, output_path=datastats_file)\n",
    "    da.get_all_case_stats()\n",
    "\n",
    "# algorithm generation\n",
    "bundle_generator = BundleGen(\n",
    "    algo_path=work_dir,\n",
    "    data_stats_filename=datastats_file,\n",
    "    data_src_cfg_name=input,\n",
    ")\n",
    "\n",
    "bundle_generator.generate(work_dir, num_fold=1)\n",
    "history = bundle_generator.get_history()\n",
    "export_bundle_algo_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Algo object from bundle_generator history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm selected to do HPO. Refer to bundle history for the mapping between\n",
    "# algorithm name and index, 0 is SegResNet2D\n",
    "selected_algorithm_index = 0\n",
    "\n",
    "# you can get history from bundle_generator. It can also be acquired by reading bundles saved on disk\n",
    "try:\n",
    "    history = bundle_generator.get_history()\n",
    "    assert len(history) > 0\n",
    "except Exception:\n",
    "    history = import_bundle_algo_history(work_dir, only_trained=False)\n",
    "\n",
    "algo_dict = history[selected_algorithm_index]\n",
    "algo_name = list(algo_dict.keys())[selected_algorithm_index]\n",
    "algo = algo_dict[algo_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"override_params\" is used to update algorithm hyperparameters \n",
    "# like num_epochs, which are not in the HPO search space. We set num_epochs=2\n",
    "# to shorten the training time as an example\n",
    "\n",
    "max_epochs = 2\n",
    "\n",
    "# safeguard to ensure max_epochs is greater or equal to 2\n",
    "max_epochs = max(max_epochs, 2)\n",
    "\n",
    "num_gpus = 1 if \"multigpu\" in input_cfg and not input_cfg[\"multigpu\"] else torch.cuda.device_count()\n",
    "\n",
    "num_epoch = max_epochs\n",
    "num_images_per_batch = 2\n",
    "n_data = 24  # total is 30 images, hold out one set (6 images) for cross fold val.\n",
    "n_iter = int(num_epoch * n_data / num_images_per_batch / num_gpus)\n",
    "n_iter_val = int(n_iter / 2)\n",
    "\n",
    "override_param = {\n",
    "    \"num_iterations\": n_iter,\n",
    "    \"num_iterations_per_validation\": n_iter_val,\n",
    "    \"num_images_per_batch\": num_images_per_batch,\n",
    "    \"num_epochs\": num_epoch,\n",
    "    \"num_warmup_iterations\": n_iter_val,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nni_gen = NNIGen(algo=algo, params=override_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create your NNI configs. Refer to [NNI](https://nni.readthedocs.io/en/stable/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nni_config = {\n",
    "    \"experimentName\": msd_task + \"_lr\",\n",
    "    \"searchSpace\": {\n",
    "        \"learning_rate\": {\n",
    "            \"_type\": \"choice\",\n",
    "            \"_value\": [0.0001, 0.001, 0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"trialCommand\": None,\n",
    "    \"trialCodeDirectory\": \".\",\n",
    "    \"trialGpuNumber\": 1,\n",
    "    \"trialConcurrency\": 2,\n",
    "    \"maxTrialNumber\": 10,\n",
    "    \"maxExperimentDuration\": \"1h\",\n",
    "    \"tuner\": {\"name\": \"GridSearch\"},\n",
    "    \"trainingService\": {\n",
    "        \"platform\": \"local\", \"useActiveGpu\": True}\n",
    "}\n",
    "with open(nni_yaml, 'w') as f:\n",
    "    yaml.dump(nni_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run NNI from terminal\n",
    "### Step 1: copy the trialCommand print out info, e.g.\n",
    "```\n",
    "python -m monai.apps.auto3dseg NNIGen run_algo  ./workdir/segresnet2d_0/algo_object.pkl {result_dir}\n",
    "```\n",
    "Replace {result_dir} with a folder path to save HPO experiments.\n",
    "### Step 2: copy the above trialCommand to replace the trialCommand in nni_config.yaml\n",
    "### Step 3: run NNI experiemtns from a terminal with \n",
    "```\n",
    "nnictl create --config ./nni_config.yaml\n",
    "```\n",
    "\n",
    "Use the print out trialCommand from NNIGen initialization to replace the trialCommand in nni_config and run NNI from terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example Results\n",
    "We changed override_param to {'num_iterations':6000, 'num_iterations_per_validation':600}, to run the experiments for longer time.\n",
    "Here is the results shown in NNI webui. The optimal learning rate for SegResNet2D (selected_algorithm_index=0) is 0.1, which achieves Dice score of 0.735.\n",
    "\n",
    "![](../figures/nni_image0.png)\n",
    "![](../figures/nni_image1.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
