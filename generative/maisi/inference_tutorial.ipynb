{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa57bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "# MAISI Inference Tutorial\n",
    "\n",
    "This tutorial illustrates how to use trained MAISI model and codebase to generate synthetic 3D images and paired masks.\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import nibabel\" || pip install -q \"nibabel\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e2019e-1556-41a6-95e8-5d1a65f8b3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.1+25.g64ea76d8\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.3.0+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 64ea76d83a92b7cf7f13c8f93498d50037c3324c\n",
      "MONAI __file__: /mnt/drive3/wenao/anaconda3/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.11.4\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.18.1+cu121\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.1.4\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: 4.41.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "import argparse\n",
    "from monai.utils import set_determinism\n",
    "from scripts.utils import define_instance, load_autoencoder_ckpt\n",
    "from scripts.sample import check_input, LDMSampler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Read in environment setting, including data directory, model directory, and output directory\n",
    "\n",
    "The information for data directory, model directory, and output directory are saved in ./configs/environment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38b4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been read in.\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "environment_file = './configs/environment.json'\n",
    "env_dict = json.load(open(environment_file, \"r\"))\n",
    "for k, v in env_dict.items():\n",
    "    setattr(args, k, v)\n",
    "print(\"Environment variables have been read in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98233f-6492-40ed-9ba5-7ab4ae0f8ffd",
   "metadata": {},
   "source": [
    "## Read in configuration setting, including network definition, body region and anatomy to generate, etc.\n",
    "\n",
    "The information for the configuration that is not related to working directory, including network definition, body region and anatomy to generate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533414f3-bef5-49f7-b082-f803b5e494bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controllable_anatomy_size is not empty. We will ignore body_region and anatomy_list and synthesize based on controllable_anatomy_size ([['hepatic tumor', 0.3], ['liver', 0.5]]).\n",
      "The generate results will have voxel size to be [1.5, 1.5, 2.0]mm, and volume size to be [256, 256, 256].\n",
      "Configuration variables have been read in.\n"
     ]
    }
   ],
   "source": [
    "config_file = './configs/config_maisi.json'\n",
    "config_dict = json.load(open(config_file, \"r\"))\n",
    "for k, v in config_dict.items():\n",
    "    setattr(args, k, v)\n",
    "    \n",
    "# check the format of inference inputs\n",
    "check_input(args.body_region,args.anatomy_list,args.label_dict_json,args.output_size,args.spacing,args.controllable_anatomy_size)\n",
    "latent_shape = [args.latent_channels, args.output_size[0]//4, args.output_size[1]//4, args.output_size[2]//4]\n",
    "print(\"Configuration variables have been read in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "## Define networks and noise scheduler, load network weights.\n",
    "\n",
    "The networks and noise scheduler are defined in config_file. We will read them in and load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-07 06:31:28,641 - INFO - 'dst' model updated: 158 of 206 variables.\n",
      "All the trained model weights have been loaded.\n"
     ]
    }
   ],
   "source": [
    "noise_scheduler = define_instance(args, \"noise_scheduler\")\n",
    "mask_generation_noise_scheduler = define_instance(args, \"mask_generation_noise_scheduler\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "autoencoder = define_instance(args, \"autoencoder_def\").to(device)\n",
    "checkpoint_autoencoder = load_autoencoder_ckpt(args.trained_autoencoder_path)\n",
    "autoencoder.load_state_dict(checkpoint_autoencoder)\n",
    "\n",
    "diffusion_unet = define_instance(args, \"diffusion_unet_def\").to(device)\n",
    "checkpoint_diffusion_unet = torch.load(args.trained_diffusion_path)\n",
    "diffusion_unet.load_state_dict(checkpoint_diffusion_unet['unet_state_dict'])\n",
    "scale_factor = checkpoint_diffusion_unet['scale_factor'].to(device)\n",
    "\n",
    "controlnet = define_instance(args, \"controlnet_def\").to(device)\n",
    "checkpoint_controlnet = torch.load(args.trained_controlnet_path)\n",
    "monai.networks.utils.copy_model_state(controlnet, diffusion_unet.state_dict())\n",
    "controlnet.load_state_dict(checkpoint_controlnet['controlnet_state_dict'], strict=True)\n",
    "\n",
    "mask_generation_autoencoder = define_instance(args, \"mask_generation_autoencoder_def\").to(device)\n",
    "checkpoint_mask_generation_autoencoder = torch.load(args.trained_mask_generation_autoencoder_path)\n",
    "mask_generation_autoencoder.load_state_dict(checkpoint_mask_generation_autoencoder)\n",
    "\n",
    "mask_generation_diffusion_unet = define_instance(args, \"mask_generation_diffusion_def\").to(device)\n",
    "checkpoint_mask_generation_diffusion_unet = torch.load(args.trained_mask_generation_diffusion_path)\n",
    "mask_generation_diffusion_unet.load_state_dict(checkpoint_mask_generation_diffusion_unet, strict=True)\n",
    "mask_generation_scale_factor = args.mask_generation_scale_factor\n",
    "\n",
    "print(\"All the trained model weights have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "## Define the LDM Sampler, which contains functions that will perform the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controllable_anatomy_size is given, mask generation is triggered!\n",
      "LDM sampler initialized.\n"
     ]
    }
   ],
   "source": [
    "ldm_sampler = LDMSampler(\n",
    "    args.body_region,\n",
    "    args.anatomy_list,\n",
    "    args.all_mask_files_json,\n",
    "    args.all_anatomy_size_condtions_json,\n",
    "    args.all_mask_files_base_dir,\n",
    "    args.label_dict_json,\n",
    "    args.label_dict_remap_json,\n",
    "    autoencoder,\n",
    "    diffusion_unet,\n",
    "    controlnet,\n",
    "    noise_scheduler,\n",
    "    scale_factor,\n",
    "    mask_generation_autoencoder,\n",
    "    mask_generation_diffusion_unet,\n",
    "    mask_generation_scale_factor,\n",
    "    mask_generation_noise_scheduler,\n",
    "    device,\n",
    "    latent_shape,\n",
    "    args.mask_generation_latent_shape,\n",
    "    args.output_size,\n",
    "    args.output_dir,\n",
    "    args.controllable_anatomy_size,\n",
    "    image_output_ext = args.image_output_ext,\n",
    "    label_output_ext = args.label_output_ext,\n",
    "    spacing=args.spacing,\n",
    "    num_inference_steps=1000,\n",
    "    mask_generation_num_inference_steps=1000,\n",
    "    random_seed = args.random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff93fa3-da0c-46ff-8c77-ab53f39f26f5",
   "metadata": {},
   "source": [
    "## Perform the inference. \n",
    "It will take around 3min to generate a pair of [256,256,256] image/mask on one 80G A100. The time cost per image pair is roughly linear to the output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271f91bf-1c55-46e2-ae56-8677cd8eb81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated image/mask pairs will be saved in ./output.\n",
      "controllable_anatomy_size: [['hepatic tumor', 0.3], ['liver', 0.5]]\n",
      "provide_anatomy_size: [None, 0.5, None, None, None, None, None, 0.3, None, None]\n",
      "candidate_condition: [-1.0, 0.463429, 0.486525, 0.287951, 0.278651, -1.0, -1.0, 0.310093, -1.0, -1.0]\n",
      "final candidate_condition: [-1.0, 0.5, 0.486525, 0.287951, 0.278651, -1.0, -1.0, 0.3, -1.0, -1.0]\n",
      "Prepare mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [01:03<00:00, 15.75it/s]\n",
      "/mnt/drive3/wenao/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_tumor_label for postprocess: 26\n",
      "Current model does not support hepatic vessel by size control, so we treat generated hepatic vessel as part of liver for better visiaulization.\n",
      "Mask preparation time: 87.0064799785614 seconds.\n",
      "Start generating latent features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [01:15<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent features generation time: 75.73426008224487 seconds\n",
      "Start decoding latent features into images...\n",
      "Image decoding time: 10.641048669815063 seconds\n",
      "2024-07-07 06:34:25,599 INFO image_writer.py:197 - writing: output/sample_20240707_063425_584977_image.nii.gz\n",
      "2024-07-07 06:34:26,892 INFO image_writer.py:197 - writing: output/sample_20240707_063425_584977_label.nii.gz\n",
      "MAISI image/mask generation finished\n"
     ]
    }
   ],
   "source": [
    "print(f\"The generated image/mask pairs will be saved in {args.output_dir}.\")\n",
    "ldm_sampler.sample_multiple_images(args.num_output_samples)\n",
    "print(f\"MAISI image/mask generation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc619b6-f01f-4991-b5b0-141e00f551f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
