{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2bc6c7-f54c-436f-ab66-86a631fb75d8",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f51a451-566f-4501-aeb8-f3cd5d1f7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.apps import MedNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682936a-09ed-4703-af06-c59f755395ee",
   "metadata": {},
   "source": [
    "# MedNIST Classification Bundle\n",
    "\n",
    "In this tutorial we'll revisit the bundle replicating [MONAI 101 notebook](https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/monai_101.ipynb) and add more features representing best practice concepts. This will include evaluation and checkpoint saving techniques.\n",
    "\n",
    "We'll first create a bundle very much like in the previous tutorial with the same metadata and common script file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9dc6ec-13da-4a37-8afa-28e2766b9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/tree\n",
      "\u001b[01;34mMedNISTClassifier_v2\u001b[00m\n",
      "├── \u001b[01;34mconfigs\u001b[00m\n",
      "│   ├── inference.json\n",
      "│   └── metadata.json\n",
      "├── \u001b[01;34mdocs\u001b[00m\n",
      "│   └── README.md\n",
      "├── LICENSE\n",
      "└── \u001b[01;34mmodels\u001b[00m\n",
      "\n",
      "3 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python -m monai.bundle init_bundle MedNISTClassifier_v2\n",
    "which tree && tree MedNISTClassifier_v2 || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29f053b-cf16-4ffc-bbe7-d9433fdfa872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MedNISTClassifier_v2/configs/metadata.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile MedNISTClassifier_v2/configs/metadata.json\n",
    "\n",
    "{\n",
    "    \"version\": \"0.0.1\",\n",
    "    \"changelog\": {\n",
    "        \"0.0.1\": \"Initial version\"\n",
    "    },\n",
    "    \"monai_version\": \"1.2.0\",\n",
    "    \"pytorch_version\": \"2.0.0\",\n",
    "    \"numpy_version\": \"1.23.5\",\n",
    "    \"optional_packages_version\": {},\n",
    "    \"name\": \"MedNISTClassifier\",\n",
    "    \"task\": \"MedNIST Classification Network\",\n",
    "    \"description\": \"This is a demo network for classifying MedNIST images by type/modality\",\n",
    "    \"authors\": \"Your Name Here\",\n",
    "    \"copyright\": \"Copyright (c) Your Name Here\",\n",
    "    \"data_source\": \"MedNIST dataset kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic)\",\n",
    "    \"data_type\": \"jpeg\",\n",
    "    \"intended_use\": \"This is suitable for demonstration only\",\n",
    "    \"network_data_format\": {\n",
    "        \"inputs\": {\n",
    "            \"image\": {\n",
    "                \"type\": \"image\",\n",
    "                \"format\": \"magnitude\",\n",
    "                \"modality\": \"any\",\n",
    "                \"num_channels\": 1,\n",
    "                \"spatial_shape\": [64, 64],\n",
    "                \"dtype\": \"float32\",\n",
    "                \"value_range\": [0, 1],\n",
    "                \"is_patch_data\": false,\n",
    "                \"channel_def\": {\n",
    "                    \"0\": \"image\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"pred\": {\n",
    "                \"type\": \"probabilities\",\n",
    "                \"format\": \"classes\",\n",
    "                \"num_channels\": 6,\n",
    "                \"spatial_shape\": [6],\n",
    "                \"dtype\": \"float32\",\n",
    "                \"value_range\": [0, 1],\n",
    "                \"is_patch_data\": false,\n",
    "                \"channel_def\": {\n",
    "                    \"0\": \"AbdomenCT\",\n",
    "                    \"1\": \"BreastMRI\",\n",
    "                    \"2\": \"CXR\",\n",
    "                    \"3\": \"ChestCT\",\n",
    "                    \"4\": \"Hand\",\n",
    "                    \"5\": \"HeadCT\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04826c73-7c26-4c5e-8d2a-8968c3954b5a",
   "metadata": {},
   "source": [
    "As you've likely seen in outputs, there should be a `logging.conf` file in the `configs` directory to set up the Python logger appropriately. This will improve the output we get in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb1b023-d192-4ad7-b2eb-c4a2c6b42b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MedNISTClassifier_v2/configs/logging.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile MedNISTClassifier_v2/configs/logging.conf\n",
    "\n",
    "[loggers]\n",
    "keys=root\n",
    "\n",
    "[handlers]\n",
    "keys=consoleHandler\n",
    "\n",
    "[formatters]\n",
    "keys=fullFormatter\n",
    "\n",
    "[logger_root]\n",
    "level=INFO\n",
    "handlers=consoleHandler\n",
    "\n",
    "[handler_consoleHandler]\n",
    "class=StreamHandler\n",
    "level=INFO\n",
    "formatter=fullFormatter\n",
    "args=(sys.stdout,)\n",
    "\n",
    "[formatter_fullFormatter]\n",
    "format=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306ff33-c39b-4822-b6d4-346987cfe87b",
   "metadata": {},
   "source": [
    "We'll change the common file slightly by adding some extra symbols, specifically `bundle_root` which should always be present in bundles. We'll keep `root_dir` since it's used to determine where MedNIST is downloaded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11681af-3210-4b2b-b7bd-8ad8dedfe230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MedNISTClassifier_v2/configs/common.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile MedNISTClassifier_v2/configs/common.yaml\n",
    "\n",
    "# added a few more imports\n",
    "imports: \n",
    "- $import torch\n",
    "- $import datetime\n",
    "- $import os\n",
    "\n",
    "root_dir: .\n",
    "\n",
    "# use constants from MONAI instead of hard-coding names\n",
    "image: $monai.utils.CommonKeys.IMAGE\n",
    "label: $monai.utils.CommonKeys.LABEL\n",
    "pred: $monai.utils.CommonKeys.PRED\n",
    "\n",
    "# these are added definitions\n",
    "bundle_root: .\n",
    "ckpt_path: $@bundle_root + '/models/model.pt'\n",
    "\n",
    "# define a device for the network\n",
    "device: '$torch.device(''cuda:0'')'\n",
    "\n",
    "# store the class names for inference later\n",
    "class_names: [AbdomenCT, BreastMRI, CXR, ChestCT, Hand, HeadCT]\n",
    "\n",
    "# define the network separately, don't need to refer to MONAI types by name or import MONAI\n",
    "network_def:\n",
    "  _target_: densenet121\n",
    "  spatial_dims: 2\n",
    "  in_channels: 1\n",
    "  out_channels: 6\n",
    "\n",
    "# define the network to be the given definition moved to the device\n",
    "net: '$@network_def.to(@device)'\n",
    "\n",
    "# define a transform sequence as a list of transform objects instead of using Compose here\n",
    "train_transforms:\n",
    "- _target_: LoadImaged\n",
    "  keys: '@image'\n",
    "  image_only: true\n",
    "- _target_: EnsureChannelFirstd\n",
    "  keys: '@image'\n",
    "- _target_: ScaleIntensityd\n",
    "  keys: '@image'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf81ea7-9ea3-4548-a32e-992f0b9bc0ab",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "For training we have the same elements again but we'll add a `SupervisedEvaluator` object to track model progress with handlers to save checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfd052e-abe7-473a-bbf4-25674a3b20ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MedNISTClassifier_v2/configs/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile MedNISTClassifier_v2/configs/train.yaml\n",
    "\n",
    "max_epochs: 25\n",
    "learning_rate: 0.00001  # learning rate, again artificially slow\n",
    "val_interval: 1  # run validation every n'th epoch\n",
    "save_interval: 1 # save the model weights every n'th epoch\n",
    "\n",
    "# choose a unique output subdirectory every time training is started, \n",
    "output_dir: '$datetime.datetime.now().strftime(@root_dir+''/output/output_%y%m%d_%H%M%S'')'\n",
    "\n",
    "train_dataset:\n",
    "  _target_: MedNISTDataset\n",
    "  root_dir: '@root_dir'\n",
    "  transform: \n",
    "    _target_: Compose\n",
    "    transforms: '@train_transforms'\n",
    "  section: training\n",
    "  download: true\n",
    "\n",
    "train_dl:\n",
    "  _target_: DataLoader\n",
    "  dataset: '@train_dataset'\n",
    "  batch_size: 512\n",
    "  shuffle: true\n",
    "  num_workers: 4\n",
    "\n",
    "# separate dataset taking from the \"validation\" section\n",
    "eval_dataset:\n",
    "  _target_: MedNISTDataset\n",
    "  root_dir: '@root_dir'\n",
    "  transform: \n",
    "    _target_: Compose\n",
    "    transforms: '$@train_transforms'\n",
    "  section: validation\n",
    "  download: true\n",
    "\n",
    "# separate dataloader for evaluation\n",
    "eval_dl:\n",
    "  _target_: DataLoader\n",
    "  dataset: '@eval_dataset'\n",
    "  batch_size: 512\n",
    "  shuffle: false\n",
    "  num_workers: 4\n",
    "\n",
    "# transforms applied to network output, in this case applying activation, argmax, and one-hot-encoding\n",
    "post_transform:\n",
    "  _target_: Compose\n",
    "  transforms:\n",
    "  - _target_: Activationsd\n",
    "    keys: '@pred'\n",
    "    softmax: true  # apply softmax to the prediction to emphasize the most likely value\n",
    "  - _target_: AsDiscreted\n",
    "    keys: ['@label','@pred']\n",
    "    argmax: [false, true]  # apply argmax to the prediction only to get a class index number\n",
    "    to_onehot: 6  # convert both prediction and label to one-hot format so that both have shape (6,)\n",
    "\n",
    "# separating out loss, inferer, and optimizer definitions\n",
    "\n",
    "loss_function:\n",
    "  _target_: torch.nn.CrossEntropyLoss\n",
    "\n",
    "inferer: \n",
    "  _target_: SimpleInferer\n",
    "\n",
    "optimizer: \n",
    "  _target_: torch.optim.Adam\n",
    "  params: '$@net.parameters()'\n",
    "  lr: '@learning_rate'\n",
    "\n",
    "# Handlers to load the checkpoint if present, run validation at the chosen interval, save the checkpoint\n",
    "# at the chosen interval, log stats, and write the log to a file in the output directory.\n",
    "handlers:\n",
    "- _target_: CheckpointLoader\n",
    "  _disabled_: '$not os.path.exists(@ckpt_path)'\n",
    "  load_path: '@ckpt_path'\n",
    "  load_dict:\n",
    "    model: '@net'\n",
    "- _target_: ValidationHandler\n",
    "  validator: '@evaluator'\n",
    "  epoch_level: true\n",
    "  interval: '@val_interval'\n",
    "- _target_: CheckpointSaver\n",
    "  save_dir: '@output_dir'\n",
    "  save_dict:\n",
    "    model: '@net'\n",
    "  save_interval: '@save_interval'\n",
    "  save_final: true  # save the final weights, either when the run finishes or is interrupted somehow\n",
    "- _target_: StatsHandler\n",
    "  name: train_loss\n",
    "  tag_name: train_loss\n",
    "  output_transform: '$monai.handlers.from_engine([''loss''], first=True)'  # print per-iteration loss\n",
    "- _target_: LogfileHandler\n",
    "  output_dir: '@output_dir'\n",
    "\n",
    "trainer:\n",
    "  _target_: SupervisedTrainer\n",
    "  device: '@device'\n",
    "  max_epochs: '@max_epochs'\n",
    "  train_data_loader: '@train_dl'\n",
    "  network: '@net'\n",
    "  optimizer: '@optimizer'\n",
    "  loss_function: '@loss_function'\n",
    "  inferer: '@inferer'\n",
    "  train_handlers: '@handlers'\n",
    "\n",
    "# validation handlers which log stats and direct the log to a file\n",
    "val_handlers:\n",
    "- _target_: StatsHandler\n",
    "  name: val_stats\n",
    "  output_transform: '$lambda x: None'\n",
    "- _target_: LogfileHandler\n",
    "  output_dir: '@output_dir'\n",
    "    \n",
    "# Metrics to assess validation results, you can have more than one here but may \n",
    "# need to adapt the format of pred and label.\n",
    "metrics:\n",
    "  accuracy:\n",
    "    _target_: 'ignite.metrics.Accuracy'\n",
    "    output_transform: '$monai.handlers.from_engine([@pred, @label])'\n",
    "\n",
    "# runs the evaluation process, invoked by trainer via the ValidationHandler object\n",
    "evaluator:\n",
    "  _target_: SupervisedEvaluator\n",
    "  device: '@device'\n",
    "  val_data_loader: '@eval_dl'\n",
    "  network: '@net'\n",
    "  inferer: '@inferer'\n",
    "  postprocessing: '@post_transform'\n",
    "  key_val_metric: '@metrics'\n",
    "  val_handlers: '@val_handlers'\n",
    "\n",
    "train:\n",
    "- '$@trainer.run()'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de752181-80b1-4221-9e4a-315e5f7f22a6",
   "metadata": {},
   "source": [
    "We can now train as normal, specifying the logging config file and a maximum number of epochs you probably will want to set higher for a good result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8357670d-fe69-4789-9b9a-77c0d8144b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:44:56,163 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2023-09-11 16:44:56,163 - INFO - > run_id: 'train'\n",
      "2023-09-11 16:44:56,163 - INFO - > meta_file: './MedNISTClassifier_v2/configs/metadata.json'\n",
      "2023-09-11 16:44:56,164 - INFO - > config_file: ['./MedNISTClassifier_v2/configs/common.yaml',\n",
      " './MedNISTClassifier_v2/configs/train.yaml']\n",
      "2023-09-11 16:44:56,164 - INFO - > logging_file: './MedNISTClassifier_v2/configs/logging.conf'\n",
      "2023-09-11 16:44:56,164 - INFO - > bundle_root: './MedNISTClassifier_v2'\n",
      "2023-09-11 16:44:56,164 - INFO - > max_epochs: 2\n",
      "2023-09-11 16:44:56,164 - INFO - ---\n",
      "\n",
      "\n",
      "2023-09-11 16:44:56,164 - INFO - Setting logging properties based on config: ./MedNISTClassifier_v2/configs/logging.conf.\n",
      "2023-09-11 16:44:56,297 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n",
      "2023-09-11 16:44:56,297 - INFO - File exists: MedNIST.tar.gz, skipped downloading.\n",
      "2023-09-11 16:44:56,297 - INFO - Non-empty folder exists in MedNIST, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 47164/47164 [00:43<00:00, 1085.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:45:40,487 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n",
      "2023-09-11 16:45:40,487 - INFO - File exists: MedNIST.tar.gz, skipped downloading.\n",
      "2023-09-11 16:45:40,487 - INFO - Non-empty folder exists in MedNIST, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 5895/5895 [00:06<00:00, 894.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:45:47,217 - ignite.engine.engine.SupervisedTrainer - INFO - Engine run resuming from iteration 0, epoch 0 until 2 epochs\n",
      "2023-09-11 16:45:48,671 - INFO - Epoch: 1/2, Iter: 1/93 -- train_loss: 1.8460 \n",
      "2023-09-11 16:45:49,046 - INFO - Epoch: 1/2, Iter: 2/93 -- train_loss: 1.8131 \n",
      "2023-09-11 16:45:49,436 - INFO - Epoch: 1/2, Iter: 3/93 -- train_loss: 1.7636 \n",
      "2023-09-11 16:45:49,803 - INFO - Epoch: 1/2, Iter: 4/93 -- train_loss: 1.7511 \n",
      "2023-09-11 16:45:50,174 - INFO - Epoch: 1/2, Iter: 5/93 -- train_loss: 1.7146 \n",
      "2023-09-11 16:45:50,544 - INFO - Epoch: 1/2, Iter: 6/93 -- train_loss: 1.6879 \n",
      "2023-09-11 16:45:50,919 - INFO - Epoch: 1/2, Iter: 7/93 -- train_loss: 1.6394 \n",
      "2023-09-11 16:45:51,288 - INFO - Epoch: 1/2, Iter: 8/93 -- train_loss: 1.6172 \n",
      "2023-09-11 16:45:51,665 - INFO - Epoch: 1/2, Iter: 9/93 -- train_loss: 1.5972 \n",
      "2023-09-11 16:45:52,034 - INFO - Epoch: 1/2, Iter: 10/93 -- train_loss: 1.5663 \n",
      "2023-09-11 16:45:52,413 - INFO - Epoch: 1/2, Iter: 11/93 -- train_loss: 1.5483 \n",
      "2023-09-11 16:45:52,787 - INFO - Epoch: 1/2, Iter: 12/93 -- train_loss: 1.4914 \n",
      "2023-09-11 16:45:53,162 - INFO - Epoch: 1/2, Iter: 13/93 -- train_loss: 1.4504 \n",
      "2023-09-11 16:45:53,530 - INFO - Epoch: 1/2, Iter: 14/93 -- train_loss: 1.4477 \n",
      "2023-09-11 16:45:53,904 - INFO - Epoch: 1/2, Iter: 15/93 -- train_loss: 1.4099 \n",
      "2023-09-11 16:45:54,275 - INFO - Epoch: 1/2, Iter: 16/93 -- train_loss: 1.3985 \n",
      "2023-09-11 16:45:54,648 - INFO - Epoch: 1/2, Iter: 17/93 -- train_loss: 1.3849 \n",
      "2023-09-11 16:45:55,015 - INFO - Epoch: 1/2, Iter: 18/93 -- train_loss: 1.3735 \n",
      "2023-09-11 16:45:55,394 - INFO - Epoch: 1/2, Iter: 19/93 -- train_loss: 1.3040 \n",
      "2023-09-11 16:45:55,762 - INFO - Epoch: 1/2, Iter: 20/93 -- train_loss: 1.3018 \n",
      "2023-09-11 16:45:56,137 - INFO - Epoch: 1/2, Iter: 21/93 -- train_loss: 1.2656 \n",
      "2023-09-11 16:45:56,509 - INFO - Epoch: 1/2, Iter: 22/93 -- train_loss: 1.2451 \n",
      "2023-09-11 16:45:56,883 - INFO - Epoch: 1/2, Iter: 23/93 -- train_loss: 1.2429 \n",
      "2023-09-11 16:45:57,252 - INFO - Epoch: 1/2, Iter: 24/93 -- train_loss: 1.2009 \n",
      "2023-09-11 16:45:57,631 - INFO - Epoch: 1/2, Iter: 25/93 -- train_loss: 1.1890 \n",
      "2023-09-11 16:45:58,000 - INFO - Epoch: 1/2, Iter: 26/93 -- train_loss: 1.1832 \n",
      "2023-09-11 16:45:58,373 - INFO - Epoch: 1/2, Iter: 27/93 -- train_loss: 1.1359 \n",
      "2023-09-11 16:45:58,745 - INFO - Epoch: 1/2, Iter: 28/93 -- train_loss: 1.1588 \n",
      "2023-09-11 16:45:59,122 - INFO - Epoch: 1/2, Iter: 29/93 -- train_loss: 1.1134 \n",
      "2023-09-11 16:45:59,489 - INFO - Epoch: 1/2, Iter: 30/93 -- train_loss: 1.0843 \n",
      "2023-09-11 16:45:59,863 - INFO - Epoch: 1/2, Iter: 31/93 -- train_loss: 1.0956 \n",
      "2023-09-11 16:46:00,235 - INFO - Epoch: 1/2, Iter: 32/93 -- train_loss: 1.0651 \n",
      "2023-09-11 16:46:00,611 - INFO - Epoch: 1/2, Iter: 33/93 -- train_loss: 1.0697 \n",
      "2023-09-11 16:46:00,978 - INFO - Epoch: 1/2, Iter: 34/93 -- train_loss: 1.0189 \n",
      "2023-09-11 16:46:01,631 - INFO - Epoch: 1/2, Iter: 35/93 -- train_loss: 0.9943 \n",
      "2023-09-11 16:46:01,998 - INFO - Epoch: 1/2, Iter: 36/93 -- train_loss: 1.0024 \n",
      "2023-09-11 16:46:02,372 - INFO - Epoch: 1/2, Iter: 37/93 -- train_loss: 0.9881 \n",
      "2023-09-11 16:46:02,739 - INFO - Epoch: 1/2, Iter: 38/93 -- train_loss: 1.0021 \n",
      "2023-09-11 16:46:03,114 - INFO - Epoch: 1/2, Iter: 39/93 -- train_loss: 0.9297 \n",
      "2023-09-11 16:46:03,482 - INFO - Epoch: 1/2, Iter: 40/93 -- train_loss: 0.9498 \n",
      "2023-09-11 16:46:03,868 - INFO - Epoch: 1/2, Iter: 41/93 -- train_loss: 0.9560 \n",
      "2023-09-11 16:46:04,239 - INFO - Epoch: 1/2, Iter: 42/93 -- train_loss: 0.9241 \n",
      "2023-09-11 16:46:04,621 - INFO - Epoch: 1/2, Iter: 43/93 -- train_loss: 0.8911 \n",
      "2023-09-11 16:46:04,990 - INFO - Epoch: 1/2, Iter: 44/93 -- train_loss: 0.8677 \n",
      "2023-09-11 16:46:05,370 - INFO - Epoch: 1/2, Iter: 45/93 -- train_loss: 0.8857 \n",
      "2023-09-11 16:46:05,738 - INFO - Epoch: 1/2, Iter: 46/93 -- train_loss: 0.8587 \n",
      "2023-09-11 16:46:06,114 - INFO - Epoch: 1/2, Iter: 47/93 -- train_loss: 0.8366 \n",
      "2023-09-11 16:46:06,481 - INFO - Epoch: 1/2, Iter: 48/93 -- train_loss: 0.8365 \n",
      "2023-09-11 16:46:06,858 - INFO - Epoch: 1/2, Iter: 49/93 -- train_loss: 0.8071 \n",
      "2023-09-11 16:46:07,228 - INFO - Epoch: 1/2, Iter: 50/93 -- train_loss: 0.7914 \n",
      "2023-09-11 16:46:07,603 - INFO - Epoch: 1/2, Iter: 51/93 -- train_loss: 0.7689 \n",
      "2023-09-11 16:46:07,971 - INFO - Epoch: 1/2, Iter: 52/93 -- train_loss: 0.7649 \n",
      "2023-09-11 16:46:08,351 - INFO - Epoch: 1/2, Iter: 53/93 -- train_loss: 0.7562 \n",
      "2023-09-11 16:46:08,721 - INFO - Epoch: 1/2, Iter: 54/93 -- train_loss: 0.7854 \n",
      "2023-09-11 16:46:09,098 - INFO - Epoch: 1/2, Iter: 55/93 -- train_loss: 0.7297 \n",
      "2023-09-11 16:46:09,466 - INFO - Epoch: 1/2, Iter: 56/93 -- train_loss: 0.7237 \n",
      "2023-09-11 16:46:09,841 - INFO - Epoch: 1/2, Iter: 57/93 -- train_loss: 0.7184 \n",
      "2023-09-11 16:46:10,209 - INFO - Epoch: 1/2, Iter: 58/93 -- train_loss: 0.7446 \n",
      "2023-09-11 16:46:10,585 - INFO - Epoch: 1/2, Iter: 59/93 -- train_loss: 0.7179 \n",
      "2023-09-11 16:46:10,954 - INFO - Epoch: 1/2, Iter: 60/93 -- train_loss: 0.6467 \n",
      "2023-09-11 16:46:11,332 - INFO - Epoch: 1/2, Iter: 61/93 -- train_loss: 0.6886 \n",
      "2023-09-11 16:46:11,701 - INFO - Epoch: 1/2, Iter: 62/93 -- train_loss: 0.6816 \n",
      "2023-09-11 16:46:12,082 - INFO - Epoch: 1/2, Iter: 63/93 -- train_loss: 0.6509 \n",
      "2023-09-11 16:46:12,451 - INFO - Epoch: 1/2, Iter: 64/93 -- train_loss: 0.6453 \n",
      "2023-09-11 16:46:12,833 - INFO - Epoch: 1/2, Iter: 65/93 -- train_loss: 0.6316 \n",
      "2023-09-11 16:46:13,203 - INFO - Epoch: 1/2, Iter: 66/93 -- train_loss: 0.6317 \n",
      "2023-09-11 16:46:13,581 - INFO - Epoch: 1/2, Iter: 67/93 -- train_loss: 0.5938 \n",
      "2023-09-11 16:46:13,957 - INFO - Epoch: 1/2, Iter: 68/93 -- train_loss: 0.6120 \n",
      "2023-09-11 16:46:14,335 - INFO - Epoch: 1/2, Iter: 69/93 -- train_loss: 0.5958 \n",
      "2023-09-11 16:46:14,704 - INFO - Epoch: 1/2, Iter: 70/93 -- train_loss: 0.5930 \n",
      "2023-09-11 16:46:15,079 - INFO - Epoch: 1/2, Iter: 71/93 -- train_loss: 0.5662 \n",
      "2023-09-11 16:46:15,448 - INFO - Epoch: 1/2, Iter: 72/93 -- train_loss: 0.5763 \n",
      "2023-09-11 16:46:16,041 - INFO - Epoch: 1/2, Iter: 73/93 -- train_loss: 0.5695 \n",
      "2023-09-11 16:46:16,410 - INFO - Epoch: 1/2, Iter: 74/93 -- train_loss: 0.5743 \n",
      "2023-09-11 16:46:16,789 - INFO - Epoch: 1/2, Iter: 75/93 -- train_loss: 0.5466 \n",
      "2023-09-11 16:46:17,157 - INFO - Epoch: 1/2, Iter: 76/93 -- train_loss: 0.5320 \n",
      "2023-09-11 16:46:17,540 - INFO - Epoch: 1/2, Iter: 77/93 -- train_loss: 0.5176 \n",
      "2023-09-11 16:46:17,911 - INFO - Epoch: 1/2, Iter: 78/93 -- train_loss: 0.5000 \n",
      "2023-09-11 16:46:18,287 - INFO - Epoch: 1/2, Iter: 79/93 -- train_loss: 0.5113 \n",
      "2023-09-11 16:46:18,658 - INFO - Epoch: 1/2, Iter: 80/93 -- train_loss: 0.4966 \n",
      "2023-09-11 16:46:19,035 - INFO - Epoch: 1/2, Iter: 81/93 -- train_loss: 0.5185 \n",
      "2023-09-11 16:46:19,404 - INFO - Epoch: 1/2, Iter: 82/93 -- train_loss: 0.4719 \n",
      "2023-09-11 16:46:19,783 - INFO - Epoch: 1/2, Iter: 83/93 -- train_loss: 0.4695 \n",
      "2023-09-11 16:46:20,154 - INFO - Epoch: 1/2, Iter: 84/93 -- train_loss: 0.4637 \n",
      "2023-09-11 16:46:20,535 - INFO - Epoch: 1/2, Iter: 85/93 -- train_loss: 0.4910 \n",
      "2023-09-11 16:46:20,906 - INFO - Epoch: 1/2, Iter: 86/93 -- train_loss: 0.4873 \n",
      "2023-09-11 16:46:21,284 - INFO - Epoch: 1/2, Iter: 87/93 -- train_loss: 0.4566 \n",
      "2023-09-11 16:46:21,654 - INFO - Epoch: 1/2, Iter: 88/93 -- train_loss: 0.4357 \n",
      "2023-09-11 16:46:22,047 - INFO - Epoch: 1/2, Iter: 89/93 -- train_loss: 0.4304 \n",
      "2023-09-11 16:46:22,419 - INFO - Epoch: 1/2, Iter: 90/93 -- train_loss: 0.4286 \n",
      "2023-09-11 16:46:22,796 - INFO - Epoch: 1/2, Iter: 91/93 -- train_loss: 0.4116 \n",
      "2023-09-11 16:46:23,165 - INFO - Epoch: 1/2, Iter: 92/93 -- train_loss: 0.4424 \n",
      "2023-09-11 16:46:23,422 - INFO - Epoch: 1/2, Iter: 93/93 -- train_loss: 0.5651 \n",
      "2023-09-11 16:46:23,423 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2023-09-11 16:46:32,635 - ignite.engine.engine.SupervisedEvaluator - INFO - Got new best metric of accuracy: 0.9867684478371501\n",
      "2023-09-11 16:46:32,635 - INFO - Epoch[1] Metrics -- accuracy: 0.9868 \n",
      "2023-09-11 16:46:32,635 - INFO - Key metric: accuracy best value: 0.9867684478371501 at epoch: 1\n",
      "2023-09-11 16:46:32,635 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:09.072\n",
      "2023-09-11 16:46:32,635 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:09.213\n",
      "2023-09-11 16:46:32,765 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 1\n",
      "2023-09-11 16:46:32,765 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[1] Complete. Time taken: 00:00:45.405\n",
      "2023-09-11 16:46:33,378 - INFO - Epoch: 2/2, Iter: 1/93 -- train_loss: 0.4218 \n",
      "2023-09-11 16:46:33,760 - INFO - Epoch: 2/2, Iter: 2/93 -- train_loss: 0.4012 \n",
      "2023-09-11 16:46:34,130 - INFO - Epoch: 2/2, Iter: 3/93 -- train_loss: 0.3729 \n",
      "2023-09-11 16:46:34,507 - INFO - Epoch: 2/2, Iter: 4/93 -- train_loss: 0.3895 \n",
      "2023-09-11 16:46:34,889 - INFO - Epoch: 2/2, Iter: 5/93 -- train_loss: 0.3915 \n",
      "2023-09-11 16:46:35,260 - INFO - Epoch: 2/2, Iter: 6/93 -- train_loss: 0.4068 \n",
      "2023-09-11 16:46:35,630 - INFO - Epoch: 2/2, Iter: 7/93 -- train_loss: 0.3784 \n",
      "2023-09-11 16:46:36,002 - INFO - Epoch: 2/2, Iter: 8/93 -- train_loss: 0.3559 \n",
      "2023-09-11 16:46:36,379 - INFO - Epoch: 2/2, Iter: 9/93 -- train_loss: 0.3693 \n",
      "2023-09-11 16:46:36,749 - INFO - Epoch: 2/2, Iter: 10/93 -- train_loss: 0.3890 \n",
      "2023-09-11 16:46:37,118 - INFO - Epoch: 2/2, Iter: 11/93 -- train_loss: 0.3663 \n",
      "2023-09-11 16:46:37,491 - INFO - Epoch: 2/2, Iter: 12/93 -- train_loss: 0.3512 \n",
      "2023-09-11 16:46:37,863 - INFO - Epoch: 2/2, Iter: 13/93 -- train_loss: 0.3410 \n",
      "2023-09-11 16:46:38,236 - INFO - Epoch: 2/2, Iter: 14/93 -- train_loss: 0.3644 \n",
      "2023-09-11 16:46:38,608 - INFO - Epoch: 2/2, Iter: 15/93 -- train_loss: 0.3316 \n",
      "2023-09-11 16:46:38,982 - INFO - Epoch: 2/2, Iter: 16/93 -- train_loss: 0.3547 \n",
      "2023-09-11 16:46:39,353 - INFO - Epoch: 2/2, Iter: 17/93 -- train_loss: 0.3406 \n",
      "2023-09-11 16:46:39,729 - INFO - Epoch: 2/2, Iter: 18/93 -- train_loss: 0.3200 \n",
      "2023-09-11 16:46:40,101 - INFO - Epoch: 2/2, Iter: 19/93 -- train_loss: 0.3069 \n",
      "2023-09-11 16:46:40,475 - INFO - Epoch: 2/2, Iter: 20/93 -- train_loss: 0.3044 \n",
      "2023-09-11 16:46:40,850 - INFO - Epoch: 2/2, Iter: 21/93 -- train_loss: 0.2921 \n",
      "2023-09-11 16:46:41,502 - INFO - Epoch: 2/2, Iter: 22/93 -- train_loss: 0.2953 \n",
      "2023-09-11 16:46:41,875 - INFO - Epoch: 2/2, Iter: 23/93 -- train_loss: 0.3098 \n",
      "2023-09-11 16:46:42,248 - INFO - Epoch: 2/2, Iter: 24/93 -- train_loss: 0.3126 \n",
      "2023-09-11 16:46:42,622 - INFO - Epoch: 2/2, Iter: 25/93 -- train_loss: 0.2839 \n",
      "2023-09-11 16:46:42,995 - INFO - Epoch: 2/2, Iter: 26/93 -- train_loss: 0.2934 \n",
      "2023-09-11 16:46:43,373 - INFO - Epoch: 2/2, Iter: 27/93 -- train_loss: 0.2862 \n",
      "2023-09-11 16:46:43,753 - INFO - Epoch: 2/2, Iter: 28/93 -- train_loss: 0.2911 \n",
      "2023-09-11 16:46:44,126 - INFO - Epoch: 2/2, Iter: 29/93 -- train_loss: 0.2814 \n",
      "2023-09-11 16:46:44,500 - INFO - Epoch: 2/2, Iter: 30/93 -- train_loss: 0.2819 \n",
      "2023-09-11 16:46:44,873 - INFO - Epoch: 2/2, Iter: 31/93 -- train_loss: 0.2679 \n",
      "2023-09-11 16:46:45,246 - INFO - Epoch: 2/2, Iter: 32/93 -- train_loss: 0.2932 \n",
      "2023-09-11 16:46:45,617 - INFO - Epoch: 2/2, Iter: 33/93 -- train_loss: 0.2752 \n",
      "2023-09-11 16:46:45,994 - INFO - Epoch: 2/2, Iter: 34/93 -- train_loss: 0.2591 \n",
      "2023-09-11 16:46:46,371 - INFO - Epoch: 2/2, Iter: 35/93 -- train_loss: 0.2724 \n",
      "2023-09-11 16:46:46,748 - INFO - Epoch: 2/2, Iter: 36/93 -- train_loss: 0.2638 \n",
      "2023-09-11 16:46:47,120 - INFO - Epoch: 2/2, Iter: 37/93 -- train_loss: 0.2707 \n",
      "2023-09-11 16:46:47,495 - INFO - Epoch: 2/2, Iter: 38/93 -- train_loss: 0.2540 \n",
      "2023-09-11 16:46:47,867 - INFO - Epoch: 2/2, Iter: 39/93 -- train_loss: 0.2716 \n",
      "2023-09-11 16:46:48,241 - INFO - Epoch: 2/2, Iter: 40/93 -- train_loss: 0.2449 \n",
      "2023-09-11 16:46:48,613 - INFO - Epoch: 2/2, Iter: 41/93 -- train_loss: 0.2530 \n",
      "2023-09-11 16:46:48,987 - INFO - Epoch: 2/2, Iter: 42/93 -- train_loss: 0.2429 \n",
      "2023-09-11 16:46:49,364 - INFO - Epoch: 2/2, Iter: 43/93 -- train_loss: 0.2279 \n",
      "2023-09-11 16:46:49,740 - INFO - Epoch: 2/2, Iter: 44/93 -- train_loss: 0.2243 \n",
      "2023-09-11 16:46:50,113 - INFO - Epoch: 2/2, Iter: 45/93 -- train_loss: 0.2431 \n",
      "2023-09-11 16:46:50,492 - INFO - Epoch: 2/2, Iter: 46/93 -- train_loss: 0.2439 \n",
      "2023-09-11 16:46:50,864 - INFO - Epoch: 2/2, Iter: 47/93 -- train_loss: 0.2279 \n",
      "2023-09-11 16:46:51,238 - INFO - Epoch: 2/2, Iter: 48/93 -- train_loss: 0.2097 \n",
      "2023-09-11 16:46:51,616 - INFO - Epoch: 2/2, Iter: 49/93 -- train_loss: 0.2345 \n",
      "2023-09-11 16:46:51,992 - INFO - Epoch: 2/2, Iter: 50/93 -- train_loss: 0.2191 \n",
      "2023-09-11 16:46:52,447 - INFO - Epoch: 2/2, Iter: 51/93 -- train_loss: 0.2042 \n",
      "2023-09-11 16:46:52,821 - INFO - Epoch: 2/2, Iter: 52/93 -- train_loss: 0.2438 \n",
      "2023-09-11 16:46:53,193 - INFO - Epoch: 2/2, Iter: 53/93 -- train_loss: 0.2154 \n",
      "2023-09-11 16:46:53,566 - INFO - Epoch: 2/2, Iter: 54/93 -- train_loss: 0.2276 \n",
      "2023-09-11 16:46:53,939 - INFO - Epoch: 2/2, Iter: 55/93 -- train_loss: 0.2033 \n",
      "2023-09-11 16:46:54,313 - INFO - Epoch: 2/2, Iter: 56/93 -- train_loss: 0.2054 \n",
      "2023-09-11 16:46:54,692 - INFO - Epoch: 2/2, Iter: 57/93 -- train_loss: 0.2188 \n",
      "2023-09-11 16:46:55,065 - INFO - Epoch: 2/2, Iter: 58/93 -- train_loss: 0.1989 \n",
      "2023-09-11 16:46:55,438 - INFO - Epoch: 2/2, Iter: 59/93 -- train_loss: 0.1964 \n",
      "2023-09-11 16:46:55,815 - INFO - Epoch: 2/2, Iter: 60/93 -- train_loss: 0.2212 \n",
      "2023-09-11 16:46:56,200 - INFO - Epoch: 2/2, Iter: 61/93 -- train_loss: 0.2041 \n",
      "2023-09-11 16:46:56,577 - INFO - Epoch: 2/2, Iter: 62/93 -- train_loss: 0.1918 \n",
      "2023-09-11 16:46:56,958 - INFO - Epoch: 2/2, Iter: 63/93 -- train_loss: 0.2110 \n",
      "2023-09-11 16:46:57,333 - INFO - Epoch: 2/2, Iter: 64/93 -- train_loss: 0.1816 \n",
      "2023-09-11 16:46:57,706 - INFO - Epoch: 2/2, Iter: 65/93 -- train_loss: 0.1850 \n",
      "2023-09-11 16:46:58,079 - INFO - Epoch: 2/2, Iter: 66/93 -- train_loss: 0.2006 \n",
      "2023-09-11 16:46:58,459 - INFO - Epoch: 2/2, Iter: 67/93 -- train_loss: 0.1794 \n",
      "2023-09-11 16:46:58,835 - INFO - Epoch: 2/2, Iter: 68/93 -- train_loss: 0.1977 \n",
      "2023-09-11 16:46:59,208 - INFO - Epoch: 2/2, Iter: 69/93 -- train_loss: 0.2084 \n",
      "2023-09-11 16:46:59,582 - INFO - Epoch: 2/2, Iter: 70/93 -- train_loss: 0.1948 \n",
      "2023-09-11 16:46:59,955 - INFO - Epoch: 2/2, Iter: 71/93 -- train_loss: 0.1848 \n",
      "2023-09-11 16:47:00,328 - INFO - Epoch: 2/2, Iter: 72/93 -- train_loss: 0.1792 \n",
      "2023-09-11 16:47:00,701 - INFO - Epoch: 2/2, Iter: 73/93 -- train_loss: 0.1613 \n",
      "2023-09-11 16:47:01,076 - INFO - Epoch: 2/2, Iter: 74/93 -- train_loss: 0.1810 \n",
      "2023-09-11 16:47:01,451 - INFO - Epoch: 2/2, Iter: 75/93 -- train_loss: 0.1802 \n",
      "2023-09-11 16:47:01,830 - INFO - Epoch: 2/2, Iter: 76/93 -- train_loss: 0.1606 \n",
      "2023-09-11 16:47:02,205 - INFO - Epoch: 2/2, Iter: 77/93 -- train_loss: 0.1644 \n",
      "2023-09-11 16:47:02,586 - INFO - Epoch: 2/2, Iter: 78/93 -- train_loss: 0.1597 \n",
      "2023-09-11 16:47:02,961 - INFO - Epoch: 2/2, Iter: 79/93 -- train_loss: 0.1742 \n",
      "2023-09-11 16:47:03,336 - INFO - Epoch: 2/2, Iter: 80/93 -- train_loss: 0.1581 \n",
      "2023-09-11 16:47:03,718 - INFO - Epoch: 2/2, Iter: 81/93 -- train_loss: 0.1650 \n",
      "2023-09-11 16:47:04,098 - INFO - Epoch: 2/2, Iter: 82/93 -- train_loss: 0.1644 \n",
      "2023-09-11 16:47:04,473 - INFO - Epoch: 2/2, Iter: 83/93 -- train_loss: 0.1667 \n",
      "2023-09-11 16:47:04,849 - INFO - Epoch: 2/2, Iter: 84/93 -- train_loss: 0.1704 \n",
      "2023-09-11 16:47:05,228 - INFO - Epoch: 2/2, Iter: 85/93 -- train_loss: 0.1650 \n",
      "2023-09-11 16:47:05,602 - INFO - Epoch: 2/2, Iter: 86/93 -- train_loss: 0.1483 \n",
      "2023-09-11 16:47:05,975 - INFO - Epoch: 2/2, Iter: 87/93 -- train_loss: 0.1452 \n",
      "2023-09-11 16:47:06,353 - INFO - Epoch: 2/2, Iter: 88/93 -- train_loss: 0.1462 \n",
      "2023-09-11 16:47:06,727 - INFO - Epoch: 2/2, Iter: 89/93 -- train_loss: 0.1543 \n",
      "2023-09-11 16:47:07,101 - INFO - Epoch: 2/2, Iter: 90/93 -- train_loss: 0.1516 \n",
      "2023-09-11 16:47:07,486 - INFO - Epoch: 2/2, Iter: 91/93 -- train_loss: 0.1564 \n",
      "2023-09-11 16:47:07,879 - INFO - Epoch: 2/2, Iter: 92/93 -- train_loss: 0.1535 \n",
      "2023-09-11 16:47:07,995 - INFO - Epoch: 2/2, Iter: 93/93 -- train_loss: 0.2525 \n",
      "2023-09-11 16:47:07,995 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 1 until 2 epochs\n",
      "2023-09-11 16:47:17,163 - ignite.engine.engine.SupervisedEvaluator - INFO - Got new best metric of accuracy: 0.9939496748657054\n",
      "2023-09-11 16:47:17,163 - INFO - Epoch[2] Metrics -- accuracy: 0.9939 \n",
      "2023-09-11 16:47:17,163 - INFO - Key metric: accuracy best value: 0.9939496748657054 at epoch: 2\n",
      "2023-09-11 16:47:17,163 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[2] Complete. Time taken: 00:00:09.038\n",
      "2023-09-11 16:47:17,163 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:09.168\n",
      "2023-09-11 16:47:17,301 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 2\n",
      "2023-09-11 16:47:17,302 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[2] Complete. Time taken: 00:00:44.536\n",
      "2023-09-11 16:47:17,387 - ignite.engine.engine.SupervisedTrainer - INFO - Train completed, saved final checkpoint: output/output_230911_164547/model_final_iteration=186.pt\n",
      "2023-09-11 16:47:17,387 - ignite.engine.engine.SupervisedTrainer - INFO - Engine run complete. Time taken: 00:01:30.170\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./MedNISTClassifier_v2\"\n",
    "\n",
    "python -m monai.bundle run train \\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --logging_file \"$BUNDLE/configs/logging.conf\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --config_file \"['$BUNDLE/configs/common.yaml','$BUNDLE/configs/train.yaml']\" \\\n",
    "    --max_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bf8a5-1524-425f-93f8-28e217f2adec",
   "metadata": {},
   "source": [
    "Results and logs get put into unique timestamped directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c84e2c-1709-4136-8612-87142026ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/tree\n",
      "\u001b[01;34moutput/output_230911_164547\u001b[00m\n",
      "├── log.txt\n",
      "├── model_epoch=1.pt\n",
      "├── model_epoch=2.pt\n",
      "└── model_final_iteration=186.pt\n",
      "\n",
      "0 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!which tree && tree output/* || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705ff79-fe58-410a-bb93-80b4f3fa2ea2",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "What is also needed is an inference script which will apply a loaded network to every image in a given directory and write a result to a file or to the log output. For segmentation networks this should save generated segmentations to know locations, but for this classification network we'll stick to just printing results to the log. \n",
    "\n",
    "First thing to do is create a test directory with only a few test images so we can demonstrate inference quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a957503-39e4-4f73-a989-ce6e4e2d3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 5895/5895 [00:03<00:00, 1671.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedNIST/AbdomenCT/001990.jpeg Label: 0\n",
      "MedNIST/BreastMRI/007676.jpeg Label: 1\n",
      "MedNIST/ChestCT/006763.jpeg Label: 3\n",
      "MedNIST/CXR/001214.jpeg Label: 2\n",
      "MedNIST/Hand/004427.jpeg Label: 4\n",
      "MedNIST/HeadCT/003806.jpeg Label: 5\n",
      "MedNIST/HeadCT/004638.jpeg Label: 5\n",
      "MedNIST/CXR/005013.jpeg Label: 2\n",
      "MedNIST/ChestCT/008275.jpeg Label: 3\n",
      "MedNIST/BreastMRI/000630.jpeg Label: 1\n",
      "MedNIST/BreastMRI/007547.jpeg Label: 1\n",
      "MedNIST/BreastMRI/008425.jpeg Label: 1\n",
      "MedNIST/AbdomenCT/003981.jpeg Label: 0\n",
      "MedNIST/Hand/001130.jpeg Label: 4\n",
      "MedNIST/BreastMRI/005118.jpeg Label: 1\n",
      "MedNIST/CXR/006505.jpeg Label: 2\n",
      "MedNIST/ChestCT/008218.jpeg Label: 3\n",
      "MedNIST/HeadCT/005305.jpeg Label: 5\n",
      "MedNIST/AbdomenCT/007871.jpeg Label: 0\n",
      "MedNIST/Hand/007065.jpeg Label: 4\n"
     ]
    }
   ],
   "source": [
    "root_dir = \".\"  # assuming MedNIST was downloaded to the current directory\n",
    "num_images = 20\n",
    "dataset = MedNISTDataset(root_dir=root_dir, section=\"test\", download=False)\n",
    "\n",
    "!mkdir -p test_images\n",
    "\n",
    "for i in range(num_images):\n",
    "    filename = dataset[i][\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "    print(filename, \"Label:\", dataset[i][\"label\"])\n",
    "    !cp {root_dir}/{filename} test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044efdc-6c5e-479c-880b-acd9e7ab4fea",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next remove the existing example inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f800520-f29f-4b80-9af4-5e069f97824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm \"MedNISTClassifier_v2/configs/inference.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85014c-d1eb-4a93-911b-f405eac74094",
   "metadata": {},
   "source": [
    "Next we'll create the inference script which will apply the network to all the files in the given directory (thus assuming all are images) and save the results to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5556db-2e63-484c-9358-977b4c35d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MedNISTClassifier_v2/configs/inference.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile MedNISTClassifier_v2/configs/inference.yaml\n",
    "\n",
    "imports:\n",
    "- $import glob\n",
    "\n",
    "input_dir: 'input'\n",
    "# dataset is a list of dictionaries to work with dictionary transforms\n",
    "input_files: '$[{@image: f} for f in sorted(glob.glob(@input_dir+''/*.*''))]'\n",
    "\n",
    "infer_dataset:\n",
    "  _target_: Dataset\n",
    "  data: '@input_files'\n",
    "  transform: \n",
    "    _target_: Compose\n",
    "    transforms: '@train_transforms'\n",
    "\n",
    "infer_dl:\n",
    "  _target_: DataLoader\n",
    "  dataset: '@infer_dataset'\n",
    "  batch_size: 1\n",
    "  shuffle: false\n",
    "  num_workers: 0\n",
    "\n",
    "# transforms applied to network output, same as those in training except \"label\" isn't present\n",
    "post_transform:\n",
    "  _target_: Compose\n",
    "  transforms:\n",
    "  - _target_: Activationsd\n",
    "    keys: '@pred'\n",
    "    softmax: true \n",
    "  - _target_: AsDiscreted\n",
    "    keys: ['@pred']\n",
    "    argmax: true \n",
    "\n",
    "# handlers to load the checkpoint file (and fail if a file isn't found), and save classification results to a csv file\n",
    "handlers:\n",
    "- _target_: CheckpointLoader\n",
    "  load_path: '@ckpt_path'\n",
    "  load_dict:\n",
    "    model: '@net'\n",
    "- _target_: ClassificationSaver\n",
    "  batch_transform: '$lambda batch: batch[0][@image].meta'\n",
    "  output_transform: '$monai.handlers.from_engine([''pred''])'\n",
    "\n",
    "inferer: \n",
    "  _target_: SimpleInferer\n",
    "\n",
    "evaluator:\n",
    "  _target_: SupervisedEvaluator\n",
    "  device: '@device'\n",
    "  val_data_loader: '@infer_dl'\n",
    "  network: '@net'\n",
    "  inferer: '@inferer'\n",
    "  postprocessing: '@post_transform'\n",
    "  val_handlers: '@handlers'\n",
    "\n",
    "inference:\n",
    "- '$@evaluator.run()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a706a-b135-4943-8245-0da8d5dad415",
   "metadata": {},
   "source": [
    "Inference can now be run, specifying the checkpoint file to load as being one from our training run and the input directory as \"test_images\" which was created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdcc111-f259-4701-8b1d-31fcf74398bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:54:49,564 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2023-09-11 16:54:49,564 - INFO - > run_id: 'inference'\n",
      "2023-09-11 16:54:49,564 - INFO - > meta_file: './MedNISTClassifier_v2/configs/metadata.json'\n",
      "2023-09-11 16:54:49,564 - INFO - > config_file: ['./MedNISTClassifier_v2/configs/common.yaml',\n",
      " './MedNISTClassifier_v2/configs/inference.yaml']\n",
      "2023-09-11 16:54:49,564 - INFO - > logging_file: './MedNISTClassifier_v2/configs/logging.conf'\n",
      "2023-09-11 16:54:49,565 - INFO - > bundle_root: './MedNISTClassifier_v2'\n",
      "2023-09-11 16:54:49,565 - INFO - > ckpt_path: 'output/output_230911_164547/model_final_iteration=186.pt'\n",
      "2023-09-11 16:54:49,565 - INFO - > input_dir: 'test_images'\n",
      "2023-09-11 16:54:49,565 - INFO - ---\n",
      "\n",
      "\n",
      "2023-09-11 16:54:49,565 - INFO - Setting logging properties based on config: ./MedNISTClassifier_v2/configs/logging.conf.\n",
      "2023-09-11 16:54:49,924 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2023-09-11 16:54:50,035 - ignite.engine.engine.SupervisedEvaluator - INFO - Restored all variables from output/output_230911_164547/model_final_iteration=186.pt\n",
      "2023-09-11 16:54:50,936 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:00.901\n",
      "2023-09-11 16:54:50,936 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.012\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./MedNISTClassifier_v2\"\n",
    "# need to capture name since it'll be different for you\n",
    "ckpt=$(find output -name 'model_final_iteration=186.pt'|sort|tail -1)\n",
    "\n",
    "python -m monai.bundle run inference \\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --logging_file \"$BUNDLE/configs/logging.conf\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --config_file \"['$BUNDLE/configs/common.yaml','$BUNDLE/configs/inference.yaml']\" \\\n",
    "    --ckpt_path \"$ckpt\" \\\n",
    "    --input_dir test_images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955faa08-0552-4bff-ba84-238e9a404f62",
   "metadata": {},
   "source": [
    "This will save the results of the inference to \"predictions.csv\" by default. You can change what the output filename is with an argument like `'--handlers#1#filename' pred.csv` which will directly change the `filename` parameter of the appropriate handler. Note the single quotes around the argument name since the hash sigil is interpreted by Bash as a comment otherwise.\n",
    "\n",
    "Looking at the output, the results aren't terribly legible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a695039-7a53-4f9a-9754-769a9f8ebac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images/000630.jpeg,1.0\n",
      "test_images/001130.jpeg,4.0\n",
      "test_images/001214.jpeg,2.0\n",
      "test_images/001990.jpeg,0.0\n",
      "test_images/003806.jpeg,5.0\n",
      "test_images/003981.jpeg,0.0\n",
      "test_images/004427.jpeg,4.0\n",
      "test_images/004638.jpeg,5.0\n",
      "test_images/005013.jpeg,2.0\n",
      "test_images/005118.jpeg,1.0\n",
      "test_images/005305.jpeg,5.0\n",
      "test_images/006505.jpeg,2.0\n",
      "test_images/006763.jpeg,3.0\n",
      "test_images/007065.jpeg,4.0\n",
      "test_images/007547.jpeg,1.0\n",
      "test_images/007676.jpeg,1.0\n",
      "test_images/007871.jpeg,0.0\n",
      "test_images/008218.jpeg,3.0\n",
      "test_images/008275.jpeg,3.0\n",
      "test_images/008425.jpeg,1.0\n"
     ]
    }
   ],
   "source": [
    "!cat predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c937-9ced-4a6d-b01c-3bc9a128fd62",
   "metadata": {},
   "source": [
    "The second column is the predicted class which we can use as an index into our list of class names to get something more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1065f928-3f66-47af-aed4-be2f0443cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images/000630.jpeg BreastMRI\n",
      "test_images/001130.jpeg Hand\n",
      "test_images/001214.jpeg CXR\n",
      "test_images/001990.jpeg AbdomenCT\n",
      "test_images/003806.jpeg HeadCT\n",
      "test_images/003981.jpeg AbdomenCT\n",
      "test_images/004427.jpeg Hand\n",
      "test_images/004638.jpeg HeadCT\n",
      "test_images/005013.jpeg CXR\n",
      "test_images/005118.jpeg BreastMRI\n",
      "test_images/005305.jpeg HeadCT\n",
      "test_images/006505.jpeg CXR\n",
      "test_images/006763.jpeg ChestCT\n",
      "test_images/007065.jpeg Hand\n",
      "test_images/007547.jpeg BreastMRI\n",
      "test_images/007676.jpeg BreastMRI\n",
      "test_images/007871.jpeg AbdomenCT\n",
      "test_images/008218.jpeg ChestCT\n",
      "test_images/008275.jpeg ChestCT\n",
      "test_images/008425.jpeg BreastMRI\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"AbdomenCT\", \"BreastMRI\", \"CXR\", \"ChestCT\", \"Hand\", \"HeadCT\"]\n",
    "\n",
    "for fn, idx in np.loadtxt(\"predictions.csv\", delimiter=\",\", dtype=str):\n",
    "    print(fn, class_names[int(float(idx))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e90b9-9209-4a58-885d-042ab55c9c18",
   "metadata": {},
   "source": [
    "## Putting the Bundle Together\n",
    "\n",
    "We have a checkpoint for our network which produces good results that we can now make the \"official\" shared weights for the bundle. We need to copy the checkpoint into the `models` directory and optionally produce a Torchscript version of the network. \n",
    "\n",
    "For the Torchscript convertion MONAI provides the `ckpt_export` program in the bundles submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6672caa-fd51-4dde-a31d-5c4de8c3cc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:57:08,807 - INFO - --- input summary of monai.bundle.scripts.ckpt_export ---\n",
      "2023-09-11 16:57:08,807 - INFO - > net_id: 'network_def'\n",
      "2023-09-11 16:57:08,807 - INFO - > filepath: './MedNISTClassifier_v2/models/model.ts'\n",
      "2023-09-11 16:57:08,807 - INFO - > meta_file: './MedNISTClassifier_v2/configs/metadata.json'\n",
      "2023-09-11 16:57:08,807 - INFO - > config_file: ['./MedNISTClassifier_v2/configs/common.yaml',\n",
      " './MedNISTClassifier_v2/configs/inference.yaml']\n",
      "2023-09-11 16:57:08,807 - INFO - > ckpt_file: './MedNISTClassifier_v2/models/model.pt'\n",
      "2023-09-11 16:57:08,807 - INFO - > key_in_ckpt: 'model'\n",
      "2023-09-11 16:57:08,807 - INFO - > bundle_root: './MedNISTClassifier_v2'\n",
      "2023-09-11 16:57:08,807 - INFO - ---\n",
      "\n",
      "\n",
      "2023-09-11 16:57:12,519 - INFO - exported to file: ./MedNISTClassifier_v2/models/model.ts.\n",
      "/usr/bin/tree\n",
      "\u001b[01;34m./MedNISTClassifier_v2\u001b[00m\n",
      "├── \u001b[01;34mconfigs\u001b[00m\n",
      "│   ├── common.yaml\n",
      "│   ├── inference.yaml\n",
      "│   ├── logging.conf\n",
      "│   ├── metadata.json\n",
      "│   └── train.yaml\n",
      "├── \u001b[01;34mdocs\u001b[00m\n",
      "│   └── README.md\n",
      "├── LICENSE\n",
      "└── \u001b[01;34mmodels\u001b[00m\n",
      "    ├── model.pt\n",
      "    └── model.ts\n",
      "\n",
      "3 directories, 9 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./MedNISTClassifier_v2\"\n",
    "\n",
    "ckpt=$(find output -name 'model_final_iteration=186.pt'|sort|tail -1)\n",
    "cp \"$ckpt\" \"$BUNDLE/models/model.pt\"\n",
    "\n",
    "python -m monai.bundle ckpt_export \\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --config_file \"['$BUNDLE/configs/common.yaml','$BUNDLE/configs/inference.yaml']\" \\\n",
    "    --net_id network_def \\\n",
    "    --key_in_ckpt model \\\n",
    "    --ckpt_file \"$BUNDLE/models/model.pt\" \\\n",
    "    --filepath \"$BUNDLE/models/model.ts\" \n",
    "\n",
    "which tree && tree \"$BUNDLE\" || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def15f8-d0dc-4ed0-8bf7-669e0720ac81",
   "metadata": {},
   "source": [
    "This will have produced the `model.ts` file in `models` as shown here which can be loaded in Python without the bundle config scripts like any other Torchscript object.\n",
    "\n",
    "The arguments for the `ckpt_export` command specify the components to use in the config files and the checkpoint:\n",
    "* `bundle_root`, `meta_file`, and `config_file` are as in previous usages.\n",
    "* `net_id` specifies the object in the config files which represents the network definition, ie. the instantiated network object.\n",
    "* `key_in_ckpt` names the key under which the weights for the model are found in the checkpoint, this assumes the checkpoint is a dictionary which is what `CheckpointSaver` produces, if this file isn't a dictionary omit this argument.\n",
    "* `ckpt_file` the name of the checkpoint file itself\n",
    "* `filepath` the output filename to store the Torchscript object to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a62139-8a21-4bb9-96d4-e86d61298c40",
   "metadata": {},
   "source": [
    "## Summary and Next\n",
    "\n",
    "This tutorial has covered MONAI Bundle best practices:\n",
    "  * Separate common definition config files which are combined with specific application files\n",
    "  * Separating out definitions in config files for easier reading and changes\n",
    "  * Using Engine based classes for traning and validation\n",
    "  * Simple training run management with uniquely-created results directories\n",
    "  * Inference script to generate a results csv file containing predictions\n",
    "  \n",
    "The next tutorial will discuss creating bundles to wrap pre-existing Pytorch code so that you can get code into the bundle ecosystem without rewriting the world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:monai1]",
   "language": "python",
   "name": "conda-env-monai1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
