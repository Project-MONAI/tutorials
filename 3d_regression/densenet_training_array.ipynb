{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# 3D regression example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D regression task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to predict the ages of subjects from MR imagee.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_regression/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:55:06.730943Z",
     "start_time": "2024-06-22T20:55:00.616335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paulreiners/Library/CloudStorage/Dropbox/projects/3d_regression/tutorials/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-22T20:55:06.733916Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n",
    "images = [\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI314-IOP-0889-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI249-Guys-1072-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI609-HH-2600-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI173-HH-1590-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI020-Guys-0700-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI342-Guys-0909-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI134-Guys-0780-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI577-HH-2661-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI066-Guys-0731-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI130-HH-1528-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI607-Guys-1097-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI175-HH-1570-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI385-HH-2078-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI344-Guys-0905-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI409-Guys-0960-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI584-Guys-1129-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI253-HH-1694-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI092-HH-1436-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI574-IOP-1156-T1.nii.gz\"]),\n",
    "    os.sep.join([root_dir, \"ixi\", \"IXI585-Guys-1130-T1.nii.gz\"]),\n",
    "]\n",
    "\n",
    "# ages of subjects\n",
    "ages = np.array([45.86, 68.27, 29.0, 29.57, 39.47, 48.68, 47.35, 64.19, 46.17, 38.77, 83.81, 72.27, 64.65, 62.09, 70.95, 41.33, 24.0, 33.24, 50.57, 28.12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ixi.tar:  97%|█████████▋| 4.35G/4.51G [10:30<00:18, 9.03MB/s]"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(images[0]):\n",
    "    resource = \"http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar\"\n",
    "    md5 = \"34901a0593b41dd19c1a1f746eac2d58\"\n",
    "\n",
    "    dataset_dir = os.path.join(root_dir, \"ixi\")\n",
    "    tarfile_name = f\"{dataset_dir}.tar\"\n",
    "\n",
    "    download_and_extract(resource, tarfile_name, dataset_dir, md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96)), RandRotate90()])\n",
    "\n",
    "val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=images, labels=ages, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=images[:10], labels=ages[:10], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=images[-10:], labels=ages[-10:], transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Create DenseNet121, MSELoss, and Adam optimizer\n",
    "# Note that we only have one out channel for the age prediction.\n",
    "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "# It is important that we use nn.MSELoss for regression.\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "max_epochs = 5\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "\n",
    "        num_correct = 0.0\n",
    "        metric_count = 0\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_images)\n",
    "                value = torch.eq(val_outputs.argmax(dim=1), val_labels.argmax(dim=1))\n",
    "                metric_count += len(value)\n",
    "                num_correct += value.sum().item()\n",
    "\n",
    "        metric = num_correct / metric_count\n",
    "        metric_values.append(metric)\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"best_metric_model_classification3d_array.pth\")\n",
    "            print(\"saved new best metric model\")\n",
    "\n",
    "        print(f\"Current epoch: {epoch+1} current accuracy: {metric:.4f} \")\n",
    "        print(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "        writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion sensitivity\n",
    "One method for trying to visualise why the network made a given prediction is occlusion sensitivity. We occlude part of the image, and see how the probability of a given prediction changes. We then iterate over the image, moving the occluded portion as we go, and in doing so we build up a sensitivity map detailing which areas were the most important in making the decision.\n",
    "\n",
    "#### Bounds\n",
    "If we were to test the occlusion centred on all voxels in our image, we would have to do `torch.prod(im.shape) = 96^3 = ~1e6` predictions. We can use the bounding box to only to the estimations in a region of interest, for example over one slice.\n",
    "\n",
    "To do this, we simply give the bounding box as `(minC,maxC,minD,maxD,minH,maxH,minW,maxW)`. We can use `-1` for any value to use its full extent (`0` and `im.shape-1` for min's and max's, respectively).\n",
    "\n",
    "#### Output\n",
    "The output image in this example will look fairly bad, since our network hasn't been trained for very long. Training for longer should improve the quality of the occlusion map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# create a validation data loader\n",
    "test_ds = ImageDataset(image_files=images[-10:], labels=ages[-10:], transform=val_transforms)\n",
    "test_loader = DataLoader(val_ds, batch_size=1, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "itera = iter(test_loader)\n",
    "\n",
    "\n",
    "def get_next_im():\n",
    "    test_data = next(itera)\n",
    "    return test_data[0].to(device), test_data[1].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def plot_occlusion_heatmap(im, heatmap):\n",
    "    plt.subplots(1, 2)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.squeeze(im.cpu()))\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Get a random image and its corresponding label\n",
    "img, label = get_next_im()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the occlusion sensitivity map\n",
    "occ_sens = monai.visualize.OcclusionSensitivity(nn_module=model, mask_size=12, n_batch=10)\n",
    "# Only get a single slice to save time.\n",
    "# For the other dimensions (channel, width, height), use\n",
    "# -1 to use 0 and img.shape[x]-1 for min and max, respectively\n",
    "depth_slice = img.shape[2] // 2\n",
    "occ_sens_b_box = [depth_slice - 1, depth_slice, -1, -1, -1, -1]\n",
    "\n",
    "occ_result, _ = occ_sens(x=img, b_box=occ_sens_b_box)\n",
    "occ_result = occ_result[0, label.argmax().item()][None]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 15), facecolor=\"white\")\n",
    "\n",
    "for i, im in enumerate([img[:, :, depth_slice, ...], occ_result]):\n",
    "    cmap = \"gray\" if i == 0 else \"jet\"\n",
    "    ax = axes[i]\n",
    "    im_show = ax.imshow(np.squeeze(im[0][0].detach().cpu()), cmap=cmap)\n",
    "    ax.axis(\"off\")\n",
    "    fig.colorbar(im_show, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
