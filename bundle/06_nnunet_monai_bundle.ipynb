{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnUNet MONAI Bundle\n",
    "\n",
    "This notebook demonstrates how to create a MONAI Bundle for a trained nnUNet and use it for inference. This is needed when some other application from the MONAI EcoSystem require a MONAI Bundle (MONAI Label, MonaiAlgo for Federated Learning, etc).\n",
    "\n",
    "This notebook cover the steps to convert a trained nnUNet model to a consumable MONAI Bundle. The nnUNet training is here perfomed using the `nnUNetV2Runner`.\n",
    "\n",
    "Optionally, the notebook also demonstrates how to use the same nnUNet MONAI Bundle for training a new model. This might be needed in some applications where the nnUNet training needs to be performed through a MONAI Bundle (i.e., Active Learning in MONAI Label, MonaiAlgo for Federated Learning, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import nnunetv2\" || pip install -q nnunetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.config import print_config\n",
    "import os\n",
    "import tempfile\n",
    "from monai.bundle.config_parser import ConfigParser\n",
    "from monai.apps.nnunet import nnUNetV2Runner\n",
    "#from monai.bundle.nnunet import convert_nnunet_to_monai_bundle\n",
    "import nnunetv2\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Decathlon Spleen Dataset and Generate Data List\n",
    "\n",
    "To get the Decathlon Spleen dataset and generate the corresponding data list, you can follow the instructions in the [MSD Datalist Generator Notebook](../auto3dseg/notebooks/msd_datalist_generator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the notebook, remember to copy the generated `msd_task09_spleen_folds.json` file to the `<root_dir>/Task09_Spleen` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnUNet Experiment with nnUNetV2Runner\n",
    "\n",
    "In the following sections, we will use the nnUNetV2Runner to train a model on the spleen dataset from the Medical Segmentation Decathlon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create the Config file for the nnUNetV2Runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet_root_dir = os.path.join(root_dir, \"nnUNet\")\n",
    "\n",
    "os.makedirs(nnunet_root_dir, exist_ok=True)\n",
    "\n",
    "data_src_cfg = os.path.join(nnunet_root_dir, \"data_src_cfg.yaml\")\n",
    "data_src = {\"modality\": \"CT\", \"datalist\": os.path.join(root_dir,\"Task09_Spleen/msd_task09_spleen_folds.json\"), \"dataroot\": os.path.join(root_dir,\"Task09_Spleen\")}\n",
    "\n",
    "ConfigParser.export_config_file(data_src, data_src_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = nnUNetV2Runner(input_config=data_src_cfg, trainer_class_name=\"nnUNetTrainer_10epochs\", work_dir=nnunet_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.plan_and_process(npfp=2,n_proc=[2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(configs=\"3d_fullres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.run(run_train=True, run_find_best_configuration=False, run_predict_ensemble_postprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnUNet MONAI Bundle for Inference\n",
    "\n",
    "This section is the relevant part of the nnUNet MONAI Bundle for Inference, showing how to use the trained model to perform inference on new data through the use of a MONAI Bundle, wrapping the native nnUNet model and its pre- and post-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create the MONAI Bundle for the nnUNet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm nnUNetBundle/configs/inference.json\n",
    "python -m monai.bundle init_bundle nnUNetBundle\n",
    "\n",
    "mkdir -p nnUNetBundle/src\n",
    "touch nnUNetBundle/src/__init__.py\n",
    "which tree && tree nnUNetBundle || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then populate the MONAI Bundle with the configuration for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nnUNetBundle/configs/inference.yaml\n",
    "\n",
    "imports: \n",
    "  - $import json\n",
    "  - $from pathlib import Path\n",
    "  - $import os\n",
    "  - $import monai.bundle.nnunet\n",
    "  - $from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "  - $import shutil\n",
    "\n",
    "\n",
    "output_dir: \".\"\n",
    "bundle_root: \".\"\n",
    "data_list_file : \".\"\n",
    "data_dir: \".\"\n",
    "\n",
    "prediction_suffix: \"prediction\"\n",
    "\n",
    "test_data_list: \"$monai.data.load_decathlon_datalist(@data_list_file, is_segmentation=True, data_list_key='testing', base_dir=@data_dir)\"\n",
    "image_modality_keys: \"$list(@modality_conf.keys())\"\n",
    "image_key: \"image\"\n",
    "image_suffix: \"@image_key\"\n",
    "\n",
    "preprocessing:\n",
    "  _target_: Compose\n",
    "  transforms:\n",
    "  - _target_: LoadImaged\n",
    "    keys: \"image\"\n",
    "    ensure_channel_first: True\n",
    "    image_only: False\n",
    "\n",
    "test_dataset:\n",
    "  _target_: Dataset\n",
    "  data: \"$@test_data_list\"\n",
    "  transform: \"@preprocessing\"\n",
    "\n",
    "test_loader:\n",
    "  _target_: DataLoader\n",
    "  dataset: \"@test_dataset\"\n",
    "  batch_size: 1\n",
    "\n",
    "\n",
    "device: \"$torch.device('cuda')\"\n",
    "\n",
    "nnunet_config:\n",
    "  model_folder: \"$@bundle_root + '/models'\"\n",
    "\n",
    "network_def: \"$monai.bundle.nnunet.get_nnunet_monai_predictor(**@nnunet_config)\"\n",
    "\n",
    "postprocessing:\n",
    "  _target_: \"Compose\"\n",
    "  transforms:\n",
    "    - _target_: Transposed\n",
    "      keys: \"pred\"\n",
    "      indices:\n",
    "      - 0\n",
    "      - 3\n",
    "      - 2\n",
    "      - 1\n",
    "    - _target_: SaveImaged\n",
    "      keys: \"pred\"\n",
    "      resample: False\n",
    "      output_postfix: \"@prediction_suffix\"\n",
    "      output_dir: \"@output_dir\"\n",
    "      meta_keys: \"image_meta_dict\"\n",
    "\n",
    "\n",
    "testing:\n",
    "  dataloader: \"$@test_loader\"\n",
    "  pbar:\n",
    "    _target_: \"ignite.contrib.handlers.tqdm_logger.ProgressBar\"\n",
    "  test_inferer: \"$@inferer\"\n",
    "\n",
    "inferer: \n",
    "  _target_: \"SimpleInferer\"\n",
    "\n",
    "validator:\n",
    "  _target_: \"SupervisedEvaluator\"\n",
    "  postprocessing: \"$@postprocessing\"\n",
    "  device: \"$@device\"\n",
    "  inferer: \"$@testing#test_inferer\"\n",
    "  val_data_loader: \"$@testing#dataloader\"\n",
    "  network: \"@network_def\"\n",
    "  #prepare_batch: \"$src.inferer.prepare_nnunet_inference_batch\"\n",
    "  val_handlers:\n",
    "  - _target_: \"CheckpointLoader\"\n",
    "    load_path: \"$@bundle_root+'/models/model.pt'\"\n",
    "    load_dict:\n",
    "      network_weights: '$@network_def.network_weights'\n",
    "run:\n",
    "  - \"$@testing#pbar.attach(@validator)\"\n",
    "  - \"$@validator.run()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nnUnet to MONAI Bundle Conversion\n",
    "\n",
    "Finally, we convert the nnUNet Trained Model to a Bundle-compatible format using the `convert_nnunet_to_monai_bundle` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet_config = {\n",
    "                \"dataset_name_or_id\": \"001\",\n",
    "                \"nnunet_trainer\": \"nnUNetTrainer_1epoch\",\n",
    "}\n",
    "\n",
    "bundle_root = \"nnUNetBundle\"\n",
    "\n",
    "convert_nnunet_to_monai_bundle(nnunet_config, bundle_root, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then inspect the content of the `models` folder to verify that the model has been converted to the MONAI Bundle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "which tree && tree nnUNetBundle/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the MONAI Bundle for Inference\n",
    "\n",
    "The MONAI Bundle for Inference is now ready to be used for inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m monai.bundle run \\\n",
    "    --config-file nnUNetBundle/configs/inference.yaml \\\n",
    "    --bundle-root nnUNetBundle \\\n",
    "    --data_list_file  $MONAI_DATA_DIRECTORY/Task09_Spleen/msd_task09_spleen_folds.json \\\n",
    "    --output-dir nnUNetBundle/pred_output \\\n",
    "    --data_dir /home/maia-user/Tutorials/MONAI/data/Task09_Spleen \\\n",
    "    --logging-file nnUNetBundle/configs/logging.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Training nnUNet from the MONAI Bundle\n",
    "\n",
    "In some cases, you may want to train the nnUNet model from the MONAI Bundle (i.e., without using the nnUNetV2Runner).\n",
    "This is usually the case when the specific training logic is designed to be used with the MONAI Bundle, such as the Active Learning in MONAI Label or Federated Learning in NVFLare using the MONAI Algo implementation.\n",
    "\n",
    "This can be done by following the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nnUNetBundle/configs/train.yaml\n",
    "\n",
    "imports:\n",
    "  - $import json\n",
    "  - $import os\n",
    "  - $import nnunetv2\n",
    "  - $import src\n",
    "  - $import src.nnunet_batch_preparation\n",
    "  - $import monai.bundle.nnunet\n",
    "  - $import shutil\n",
    "  - $import pathlib\n",
    "\n",
    "\n",
    "pymaia_config_dict: \"$json.load(open(@pymaia_config_file))\"\n",
    "bundle_root: .\n",
    "ckpt_dir: \"$@bundle_root + '/models'\"\n",
    "num_classes: 2\n",
    "\n",
    "nnunet_configuration: \"3d_fullres\"\n",
    "dataset_name_or_id: \"001\"\n",
    "fold: \"0\"\n",
    "trainer_class_name: \"nnUNetTrainer\"\n",
    "plans_identifier: \"nnUNetPlans\"\n",
    "\n",
    "dataset_name: \"$nnunetv2.utilities.dataset_name_id_conversion.maybe_convert_to_dataset_name(@dataset_name_or_id)\"\n",
    "nnunet_model_folder: \"$os.path.join(os.environ['nnUNet_results'], @dataset_name, @trainer_class_name+'__'+@plans_identifier+'__'+@nnunet_configuration)\"\n",
    "\n",
    "nnunet_config:\n",
    "  dataset_name_or_id: \"@dataset_name_or_id\"\n",
    "  configuration: \"@nnunet_configuration\"\n",
    "  trainer_class_name: \"@trainer_class_name\"\n",
    "  plans_identifier: \"@plans_identifier\"\n",
    "  fold: \"@fold\"\n",
    "\n",
    "\n",
    "nnunet_trainer: \"$monai.bundle.nnunet.get_nnunet_trainer(**@nnunet_config)\"\n",
    "\n",
    "iterations: $@nnunet_trainer.num_iterations_per_epoch\n",
    "device: $@nnunet_trainer.device\n",
    "epochs: $@nnunet_trainer.num_epochs\n",
    "\n",
    "loss: $@nnunet_trainer.loss\n",
    "lr_scheduler: $@nnunet_trainer.lr_scheduler\n",
    "\n",
    "network_def: $@nnunet_trainer_def.network\n",
    "network: $@nnunet_trainer.network\n",
    "\n",
    "optimizer: $@nnunet_trainer.optimizer\n",
    "\n",
    "\n",
    "checkpoint:\n",
    "  init_args: '$@nnunet_trainer.my_init_kwargs'\n",
    "  trainer_name: '$@nnunet_trainer.__class__.__name__'\n",
    "  inference_allowed_mirroring_axes: '$@nnunet_trainer.inference_allowed_mirroring_axes'\n",
    "\n",
    "checkpoint_filename: \"$@bundle_root+'/models/nnunet_checkpoint.pth'\"\n",
    "output_dir: $@bundle_root + '/logs'\n",
    "\n",
    "train:\n",
    "  pbar:\n",
    "    _target_: \"ignite.contrib.handlers.tqdm_logger.ProgressBar\"\n",
    "  dataloader: $@nnunet_trainer.dataloader_train\n",
    "  train_data: \"$[{'case_identifier':k} for k in @nnunet_trainer.dataloader_train.generator._data.dataset.keys()]\"\n",
    "  train_dataset:\n",
    "    _target_: Dataset\n",
    "    data: \"@train#train_data\"\n",
    "  handlers:\n",
    "  - _target_: LrScheduleHandler\n",
    "    lr_scheduler: '@lr_scheduler'\n",
    "    print_lr: true\n",
    "  - _target_: ValidationHandler\n",
    "    epoch_level: true\n",
    "    interval: '@val_interval'\n",
    "    validator: '@validate#evaluator'\n",
    "  #- _target_: StatsHandler\n",
    "  #  output_transform: $monai.handlers.from_engine(['loss'], first=True)\n",
    "  #  tag_name: train_loss\n",
    "  - _target_: TensorBoardStatsHandler\n",
    "    log_dir: '@output_dir'\n",
    "    output_transform: $monai.handlers.from_engine(['loss'], first=True)\n",
    "    tag_name: train_loss\n",
    "  inferer:\n",
    "    _target_: SimpleInferer\n",
    "  key_metric:\n",
    "    Train_Dice:\n",
    "      _target_: \"MeanDice\"\n",
    "      include_background: False\n",
    "      output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "      reduction: \"mean\"\n",
    "  additional_metrics:\n",
    "    Train_Dice_per_class:\n",
    "      _target_: \"MeanDice\"\n",
    "      include_background: False\n",
    "      output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "      reduction: \"mean_batch\"\n",
    "  postprocessing:\n",
    "    _target_: \"Compose\"\n",
    "    transforms:\n",
    "    - _target_: Lambdad\n",
    "      keys:\n",
    "        - \"pred\"\n",
    "        - \"label\"\n",
    "      func: \"$lambda x: x[0]\"\n",
    "    - _target_: Activationsd\n",
    "      keys:\n",
    "        - \"pred\"\n",
    "      softmax: True\n",
    "    - _target_: AsDiscreted\n",
    "      keys:\n",
    "       - \"pred\"\n",
    "      threshold: 0.5\n",
    "    - _target_: AsDiscreted\n",
    "      keys:\n",
    "        - \"label\"\n",
    "      to_onehot: \"@num_classes\"\n",
    "  postprocessing_region_based:\n",
    "    _target_: \"Compose\"\n",
    "    transforms:\n",
    "    - _target_: Lambdad\n",
    "      keys:\n",
    "        - \"pred\"\n",
    "        - \"label\"\n",
    "      func: \"$lambda x: x[0]\"\n",
    "    - _target_: Activationsd\n",
    "      keys:\n",
    "        - \"pred\"\n",
    "      sigmoid: True\n",
    "    - _target_: AsDiscreted\n",
    "      keys:\n",
    "       - \"pred\"\n",
    "      threshold: 0.5\n",
    "  trainer:\n",
    "    _target_: SupervisedTrainer\n",
    "    amp: true\n",
    "    device: '@device'\n",
    "    additional_metrics: \"@train#additional_metrics\"\n",
    "    epoch_length: \"@iterations\"\n",
    "    inferer: '@train#inferer'\n",
    "    key_train_metric: '@train#key_metric'\n",
    "    loss_function: '@loss'\n",
    "    max_epochs: '@epochs'\n",
    "    network: '@network'\n",
    "    prepare_batch: \"$src.nnunet_batch_preparation.prepare_nnunet_batch\"\n",
    "    optimizer: '@optimizer'\n",
    "    postprocessing: '@train#postprocessing'\n",
    "    train_data_loader: '@train#dataloader'\n",
    "    train_handlers: '@train#handlers'\n",
    "\n",
    "val_interval: 1\n",
    "validate:\n",
    "  pbar:\n",
    "    _target_: \"ignite.contrib.handlers.tqdm_logger.ProgressBar\"\n",
    "  key_metric:\n",
    "    Val_Dice:\n",
    "      _target_: \"MeanDice\"\n",
    "      output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "      reduction: \"mean\"\n",
    "      include_background: False\n",
    "  additional_metrics:\n",
    "    Val_Dice_per_class:\n",
    "      _target_: \"MeanDice\"\n",
    "      output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "      reduction: \"mean_batch\"\n",
    "      include_background: False\n",
    "  dataloader: $@nnunet_trainer.dataloader_val\n",
    "  evaluator:\n",
    "    _target_: SupervisedEvaluator\n",
    "    additional_metrics: '@validate#additional_metrics'\n",
    "    amp: true\n",
    "    epoch_length: $@nnunet_trainer.num_val_iterations_per_epoch\n",
    "    device: '@device'\n",
    "    inferer: '@validate#inferer'\n",
    "    key_val_metric: '@validate#key_metric'\n",
    "    network: '@network'\n",
    "    postprocessing: '@validate#postprocessing'\n",
    "    val_data_loader: '@validate#dataloader'\n",
    "    val_handlers: '@validate#handlers'\n",
    "    prepare_batch: \"$src.nnunet_batch_preparation.prepare_nnunet_batch\"\n",
    "  handlers:\n",
    "  - _target_: StatsHandler\n",
    "    iteration_log: false\n",
    "  - _target_: TensorBoardStatsHandler\n",
    "    iteration_log: false\n",
    "    log_dir: '@output_dir'\n",
    "  - _target_: \"CheckpointSaver\"\n",
    "    save_dir: \"$str(@bundle_root)+'/models'\"\n",
    "    save_interval: 1\n",
    "    n_saved: 1\n",
    "    save_key_metric: true\n",
    "    save_dict:\n",
    "      network_weights: '$@nnunet_trainer.network._orig_mod'\n",
    "      optimizer_state: '$@nnunet_trainer.optimizer'\n",
    "      scheduler: '$@nnunet_trainer.lr_scheduler'\n",
    "  inferer:\n",
    "    _target_: SimpleInferer\n",
    "  postprocessing: '%train#postprocessing'\n",
    "\n",
    "run:\n",
    "- \"$torch.save(@checkpoint,@checkpoint_filename)\"\n",
    "- \"$shutil.copy(pathlib.Path(@nnunet_model_folder).joinpath('dataset.json'), @bundle_root+'/models/dataset.json')\"\n",
    "- \"$shutil.copy(pathlib.Path(@nnunet_model_folder).joinpath('plans.json'), @bundle_root+'/models/plans.json')\"\n",
    "- \"$@train#pbar.attach(@train#trainer,output_transform=lambda x: {'loss': x[0]['loss']})\"\n",
    "- \"$@validate#pbar.attach(@validate#evaluator,metric_names=['Val_Dice'])\"\n",
    "- $@train#trainer.run()\n",
    "\n",
    "initialize:\n",
    "- $monai.utils.set_determinism(seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we create the Python function to prepare the batch from the nnUNet DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nnUNetBundle/src/nnunet_batch_preparation.py\n",
    "\n",
    "def prepare_nnunet_batch(batch, device, non_blocking):\n",
    "    data = batch[\"data\"].to(device, non_blocking=non_blocking)\n",
    "    if isinstance(batch[\"target\"], list):\n",
    "        target = [i.to(device, non_blocking=non_blocking) for i in batch[\"target\"]]\n",
    "    else:\n",
    "        target = batch[\"target\"].to(device, non_blocking=non_blocking)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the original nnUNet Scheduler implementation is not compatible with a MONAI Bundle training, we will create a custom PolyLRScheduler class that can be used in the nnUNet training, overriding the original implementation.\n",
    "\n",
    "The incompatibility is derived from the missing `get_last_lr` method in the original implementation, which is used to log the learning rate in the MONAI Bundle training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnunetv2\n",
    "print(nnunetv2.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite the original PolyLRScheduler class with the custom implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile </path/to/nnunetv2>/training/lr_scheduler/polylr.py\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "class PolyLRScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr: float, max_steps: int, exponent: float = 0.9, current_step: int = None):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_steps = max_steps\n",
    "        self.exponent = exponent\n",
    "        self.ctr = 0\n",
    "        super().__init__(optimizer, current_step if current_step is not None else -1, False)\n",
    "\n",
    "    def step(self, current_step=None):\n",
    "        if current_step is None or current_step == -1:\n",
    "            current_step = self.ctr\n",
    "            self.ctr += 1\n",
    "\n",
    "        new_lr = self.initial_lr * (1 - current_step / self.max_steps) ** self.exponent\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        return self._last_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the nnUNet model using the MONAI Bundle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export nnUNet_raw=$MONAI_DATA_DIRECTORY\"/nnUNet/nnUNet_raw_data_base\"\n",
    "export nnUNet_preprocessed=$MONAI_DATA_DIRECTORY\"/nnUNet/nnUNet_preprocessed\"\n",
    "export nnUNet_results=$MONAI_DATA_DIRECTORY\"/nnUNet/nnUNet_trained_models\"\n",
    "\n",
    "export BUNDLE=nnUNetBundle\n",
    "export PYTHONPATH=$BUNDLE\n",
    "\n",
    "#export nnUNet_def_n_proc=2\n",
    "#export nnUNet_n_proc_DA=2\n",
    "\n",
    "python -m monai.bundle run \\\n",
    "--bundle-root nnUNetBundle \\\n",
    "--config-file nnUNetBundle/configs/train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the training progress with TensorBoard by running the following command in a new terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir nnUNetBundle/logs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MONAI",
   "language": "python",
   "name": "monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
