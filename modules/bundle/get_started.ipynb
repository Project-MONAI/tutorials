{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started to MONAI bundle\n",
    "\n",
    "A MONAI bundle usually includes the stored weights of a model, TorchScript model, JSON files which include configs and metadata about the model, information for constructing training, inference, and post-processing transform sequences, plain-text description, legal information, and other data the model creator wishes to include.\n",
    "\n",
    "For more information about MONAI bundles read the description: https://docs.monai.io/en/latest/bundle_intro.html.\n",
    "\n",
    "This notebook is a step-by-step tutorial to help get started to develop a bundle package, which contains a config file to construct the training pipeline and also has a `metadata.json` file to define the metadata information.\n",
    "\n",
    "This notebook mainly contains the below sections:\n",
    "- Define a training config with `JSON` or `YAML` format\n",
    "- Execute training based on bundle scripts and configs\n",
    "- Hybrid programming with config and python code\n",
    "\n",
    "You can find the usage examples of MONAI bundle key features and syntax in this tutorial, like:\n",
    "- Instantiate a python object from a dictionary config with `_target_` indicating class or function name or module path.\n",
    "- Execute python expression from a string config with the `$` syntax.\n",
    "- Refer to other python object with the `@` syntax.\n",
    "- Macro text replacement with the `%` syntax to simplify the config content.\n",
    "- Leverage the `_disabled_` syntax to tune or debug different components.\n",
    "- Override config content at runtime.\n",
    "- Hybrid programming with config and python code.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/modules/bundles/get_started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.bundle import ConfigParser\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "Here specify a directory with the `MONAI_DATA_DIRECTORY` environment variable to save downloaded dataset and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir is: /workspace/data/medical/\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "print(f\"root dir is: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "Downloads and extracts the dataset.  \n",
    "The dataset comes from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Set imports and input / output environments\n",
    "\n",
    "Now let's start to define the config file for a regular training task. MONAI bundles support both `JSON` and `YAML` format, here we use `JSON` as the example.\n",
    "\n",
    "According to the predefined syntax of MONAI bundle, `$` indicates an expression to evaluate, `@` refers to another object in the config content. For more details about the syntax in bundle config, please check: https://docs.monai.io/en/latest/config_syntax.html.\n",
    "\n",
    "Please note that a MONAI bundle doesn't require any hard-coded logic in the config, so users can define the config content in any structure.\n",
    "\n",
    "For the first step, import `os` and `glob` to use in the expressions (start with `$`), then define input / output environments and enable `cudnn.benchmark` for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"imports\": [\n",
    "        \"$import glob\",\n",
    "        \"$import os\",\n",
    "        \"$import ignite\"\n",
    "    ],\n",
    "    \"device\": \"$torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\",\n",
    "    \"ckpt_path\": \"/workspace/data/models/model.pt\",\n",
    "    \"dataset_dir\": \"/workspace/data/Task09_Spleen\",\n",
    "    \"images\": \"$list(sorted(glob.glob(@dataset_dir + '/imagesTr/*.nii.gz')))\",\n",
    "    \"labels\": \"$list(sorted(glob.glob(@dataset_dir + '/labelsTr/*.nii.gz')))\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define network, optimizer, loss function\n",
    "\n",
    "Define `UNet` of MONAI as the training network, and use the `Adam` optimizer of PyTorch, `DiceCELoss` of MONAI.\n",
    "\n",
    "An instantiable config component uses `_target_` keyword to define the class / function name or module path, other keys are args for the component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"network_def\": {\n",
    "    \"_target_\": \"UNet\",\n",
    "    \"spatial_dims\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 2,\n",
    "    \"channels\": [16, 32, 64, 128, 256],\n",
    "    \"strides\": [2, 2, 2, 2],\n",
    "    \"num_res_units\": 2,\n",
    "    \"norm\": \"batch\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the network to the expected `device`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"network\": \"$@network_def.to(@device)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and loss function, for MONAI classes, we can use the class name directly, other classes should provide the module path (like `Adam`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"loss\": {\n",
    "    \"_target_\": \"DiceCELoss\",\n",
    "    \"to_onehot_y\": true,\n",
    "    \"softmax\": true,\n",
    "    \"squared_pred\": true,\n",
    "    \"batch\": true\n",
    "},\n",
    "\"optimizer\": {\n",
    "    \"_target_\": \"torch.optim.Adam\",\n",
    "    \"params\": \"$@network.parameters()\",\n",
    "    \"lr\": 1e-4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define data loading and preprocessing logic\n",
    "\n",
    "Define `transforms` and `dataset`, `dataloader` to generate training data for network.\n",
    "\n",
    "To make the config stucture clear, here we split the `train` and `validate` related components into 2 sections:\n",
    "```json\n",
    "\"train\": {...},\n",
    "\"validate\": {...}\n",
    "```\n",
    "The composed transforms are for preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"train\": {\n",
    "    \"preprocessing\": {\n",
    "        \"_target_\": \"Compose\",\n",
    "        \"transforms\": [\n",
    "            {\n",
    "                \"_target_\": \"LoadImaged\",\n",
    "                \"keys\": [\"image\", \"label\"]\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"EnsureChannelFirstd\",\n",
    "                \"keys\": [\"image\", \"label\"]\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"Orientationd\",\n",
    "                \"keys\": [\"image\", \"label\"],\n",
    "                \"axcodes\": \"RAS\"\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"Spacingd\",\n",
    "                \"keys\": [\"image\", \"label\"],\n",
    "                \"pixdim\": [1.5, 1.5, 2.0],\n",
    "                \"mode\": [\"bilinear\", \"nearest\"]\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"ScaleIntensityRanged\",\n",
    "                \"keys\": \"image\",\n",
    "                \"a_min\": -57,\n",
    "                \"a_max\": 164,\n",
    "                \"b_min\": 0,\n",
    "                \"b_max\": 1,\n",
    "                \"clip\": true\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"RandCropByPosNegLabeld\",\n",
    "                \"keys\": [\"image\", \"label\"],\n",
    "                \"label_key\": \"label\",\n",
    "                \"spatial_size\": [96, 96, 96],\n",
    "                \"pos\": 1,\n",
    "                \"neg\": 1,\n",
    "                \"num_samples\": 4,\n",
    "                \"image_key\": \"image\",\n",
    "                \"image_threshold\": 0\n",
    "            },\n",
    "            {\n",
    "                \"_target_\": \"EnsureTyped\",\n",
    "                \"keys\": [\"image\", \"label\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and validation image file names are organized into a list of dictionaries.\n",
    "\n",
    "Here we use `dataset` instance as 1 argument of `dataloader` by the `@` syntax, and please note that `\"#\"` in the reference id are interpreted as special characters to go one level further into the nested config structures. For example: `\"dataset\": \"@train#dataset\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"dataset\": {\n",
    "    \"_target_\": \"CacheDataset\",\n",
    "    \"data\": \"$[{'image': i, 'label': l} for i, l in zip(@images[:-9], @labels[:-9])]\",\n",
    "    \"transform\": \"@train#preprocessing\",\n",
    "    \"cache_rate\": 1.0,\n",
    "    \"num_workers\": 4\n",
    "},\n",
    "\"dataloader\": {\n",
    "    \"_target_\": \"DataLoader\",\n",
    "    \"dataset\": \"@train#dataset\",\n",
    "    \"batch_size\": 2,\n",
    "    \"shuffle\": false,\n",
    "    \"num_workers\": 4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define inference method, post-processing and event-handlers\n",
    "\n",
    "Here we use `SimpleInferer` to execute `forward()` computation for the network and add post-processing methods like `activation`, `argmax`, `one-hot`, etc. And logging into stdout and TensorBoard based on event handlers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"inferer\": {\n",
    "    \"_target_\": \"SimpleInferer\"\n",
    "},\n",
    "\"postprocessing\": {\n",
    "    \"_target_\": \"Compose\",\n",
    "    \"transforms\": [\n",
    "        {\n",
    "            \"_target_\": \"Activationsd\",\n",
    "            \"keys\": \"pred\",\n",
    "            \"softmax\": true\n",
    "        },\n",
    "        {\n",
    "            \"_target_\": \"AsDiscreted\",\n",
    "            \"keys\": [\"pred\", \"label\"],\n",
    "            \"argmax\": [true, false],\n",
    "            \"to_onehot\": 2\n",
    "        }\n",
    "    ]\n",
    "},\n",
    "\"handlers\": [\n",
    "    {\n",
    "        \"_target_\": \"StatsHandler\",\n",
    "        \"tag_name\": \"train_loss\",\n",
    "        \"output_transform\": \"$monai.handlers.from_engine(['loss'], first=True)\"\n",
    "    },\n",
    "    {\n",
    "        \"_target_\": \"TensorBoardStatsHandler\",\n",
    "        \"log_dir\": \"eval\",\n",
    "        \"tag_name\": \"train_loss\",\n",
    "        \"output_transform\": \"$monai.handlers.from_engine(['loss'], first=True)\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define Accuracy metric for training data to avoid over-fitting\n",
    "\n",
    "Here we define the `Accuracy` metric to compute on training data to help check whether the converge is expected and avoid over-fitting. Note that it's not validation step during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"key_metric\": {\n",
    "    \"train_accuracy\": {\n",
    "        \"_target_\": \"ignite.metrics.Accuracy\",\n",
    "        \"output_transform\": \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define the trainer\n",
    "\n",
    "Here we use MONAI engine `SupervisedTrainer` to execute a regular training.\n",
    "\n",
    "If users have customized logic, then can put the logic in the `iteration_update` arg or implement their own `trainer` in python code and set `_target_` to the class directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"trainer\": {\n",
    "    \"_target_\": \"SupervisedTrainer\",\n",
    "    \"max_epochs\": 100,\n",
    "    \"device\": \"@device\",\n",
    "    \"train_data_loader\": \"@train#dataloader\",\n",
    "    \"network\": \"@network\",\n",
    "    \"loss_function\": \"@loss\",\n",
    "    \"optimizer\": \"@optimizer\",\n",
    "    \"inferer\": \"@train#inferer\",\n",
    "    \"postprocessing\": \"@train#postprocessing\",\n",
    "    \"key_train_metric\": \"@train#key_metric\",\n",
    "    \"train_handlers\": \"@train#handlers\",\n",
    "    \"amp\": true\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train config - Define the validation section\n",
    "\n",
    "Usually we need to execute validation for every N epochs during training to verify the model and save the best model.\n",
    "\n",
    "Here we don't define the `validate` section step by step as it's similar to the `train` section. The full config is available:  \n",
    "https://github.com/Project-MONAI/tutorials/blob/master/modules/bundles/spleen_segmentation/configs/train.json\n",
    "\n",
    "Just show an example of `macro text replacement` to simplify the config content and avoid duplicated text. Please note that it's just token text replacement of the config content, not refer to the instantiated python objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"validate\": {\n",
    "    \"preprocessing\": {\n",
    "        \"_target_\": \"Compose\",\n",
    "        \"transforms\": [\n",
    "            \"%train#preprocessing#transforms#0\",\n",
    "            \"%train#preprocessing#transforms#1\",\n",
    "            \"%train#preprocessing#transforms#2\",\n",
    "            \"%train#preprocessing#transforms#3\",\n",
    "            \"%train#preprocessing#transforms#4\",\n",
    "            \"%train#preprocessing#transforms#6\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metadata information\n",
    "\n",
    "We can define a `metadata` file in the bundle, which contains the metadata information relating to the model, including what the shape and format of inputs and outputs are, what the meaning of the outputs are, what type of model is present, and other information. The structure is a dictionary containing a defined set of keys with additional user-specified keys.\n",
    "\n",
    "A typical `metadata` example is available:  \n",
    "https://github.com/Project-MONAI/tutorials/blob/master/modules/bundles/spleen_segmentation/configs/metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training with bundle script - `run`\n",
    "\n",
    "There are several predefined scripts in MONAI bundle module to help execute `regular training`, `metadata verification base on schema`, `network input / output verification`, `export to TorchScript model`, etc.\n",
    "\n",
    "Here we leverage the `run` script and specify the ID of trainer in the config.\n",
    "\n",
    "Just define the entry point expressions in the config to execute in order, and specify the `runner_id` in CLI script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"training\": [\n",
    "    \"$monai.utils.set_determinism(seed=123)\",\n",
    "    \"$setattr(torch.backends.cudnn, 'benchmark', True)\",\n",
    "    \"$@train#trainer.run()\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -m monai.bundle run training --config_file configs/train.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training with bundle script - Override config at runtime\n",
    "\n",
    "To override some config items at runtime, users can specify the target `id` and `value` at command line, or override the `id` with some content in another config file. Here we set the device to `cuda:1` at runtime.\n",
    "\n",
    "Please note that \"#\" and \"$\" may be meaningful syntax for some `shell` and `CLI` tools, so may need to add escape character or quotes for them in the command line, like: `\"\\$torch.device('cuda:1')\"`. For more details: https://github.com/google/python-fire/blob/v0.4.0/fire/parser.py#L60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -m monai.bundle run training --config_file configs/train.json --device \"\\$torch.device('cuda:1')\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override content from another config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -m monai.bundle run training --config_file configs/train.json --network \"%configs/test.json#network\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid programming with config and python code\n",
    "\n",
    "A MONAI bundle supports flexible customized logic, there are several ways to achieve this:\n",
    "\n",
    "- If defining own components like transform, loss, trainer, etc. in a python file, just use its module path in `_target_` within the config file.\n",
    "- Parse the config in your own python program and do lazy instantiation with customized logic.\n",
    "\n",
    "Here we show an example to parse the config in python code and execute the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read_config(f=\"configs/train.json\")\n",
    "parser.read_meta(f=\"configs/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get`/`set` configuration content, the `set` method should happen before calling `parse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original input channels 1\n",
    "print(parser[\"network_def\"][\"in_channels\"])\n",
    "# change input channels to 4\n",
    "parser[\"network_def\"][\"in_channels\"] = 4\n",
    "print(parser[\"network_def\"][\"in_channels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the config content and instantiate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the structured config content\n",
    "parser.parse()\n",
    "# instantiate the network component and print the network structure\n",
    "net = parser.get_parsed_content(\"network\")\n",
    "print(net)\n",
    "\n",
    "# execute training\n",
    "trainer = parser.get_parsed_content(\"train#trainer\")\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
