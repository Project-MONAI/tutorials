{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# MAISI Inference Tutorial\n",
    "\n",
    "This tutorial illustrates how to use trained MAISI model and codebase to generate synthetic 3D images and paired masks.\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e2019e-1556-41a6-95e8-5d1a65f8b3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0rc1+1.g4877767c\n",
      "Numpy version: 1.26.0\n",
      "Pytorch version: 2.3.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 4877767cf92649a38ffda0fc590f2b92ba59f019\n",
      "MONAI __file__: /localhome/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.23.2\n",
      "scipy version: 1.14.0\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.17.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.18.1+cu121\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: 1.5.1\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.40.2\n",
      "mlflow version: 2.15.1\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.3\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "from monai.apps import download_url\n",
    "from monai.config import print_config\n",
    "from monai.transforms import LoadImage, Orientation\n",
    "from monai.utils import set_determinism\n",
    "from scripts.sample import LDMSampler, check_input\n",
    "from scripts.utils import define_instance, load_autoencoder_ckpt, load_diffusion_ckpt\n",
    "from scripts.utils_plot import find_label_center_loc, get_xyz_plot, show_image\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.\n",
    "This allows you to save results and reuse downloads.\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c12dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-14 17:06:54,446 - INFO - Expected md5 is None, skip md5 check for file models/autoencoder_epoch273.pt.\n",
      "2024-08-14 17:06:54,447 - INFO - File exists: models/autoencoder_epoch273.pt, skipped downloading.\n",
      "2024-08-14 17:06:54,448 - INFO - Expected md5 is None, skip md5 check for file models/input_unet3d_data-all_steps1000size512ddpm_random_current_inputx_v1.pt.\n",
      "2024-08-14 17:06:54,448 - INFO - File exists: models/input_unet3d_data-all_steps1000size512ddpm_random_current_inputx_v1.pt, skipped downloading.\n",
      "2024-08-14 17:06:54,449 - INFO - Expected md5 is None, skip md5 check for file models/controlnet-20datasets-e20wl100fold0bc_noi_dia_fsize_current.pt.\n",
      "2024-08-14 17:06:54,449 - INFO - File exists: models/controlnet-20datasets-e20wl100fold0bc_noi_dia_fsize_current.pt, skipped downloading.\n",
      "2024-08-14 17:06:54,450 - INFO - Expected md5 is None, skip md5 check for file models/mask_generation_autoencoder.pt.\n",
      "2024-08-14 17:06:54,450 - INFO - File exists: models/mask_generation_autoencoder.pt, skipped downloading.\n",
      "2024-08-14 17:06:54,451 - INFO - Expected md5 is None, skip md5 check for file models/mask_generation_diffusion_unet.pt.\n",
      "2024-08-14 17:06:54,452 - INFO - File exists: models/mask_generation_diffusion_unet.pt, skipped downloading.\n",
      "2024-08-14 17:06:54,454 - INFO - Expected md5 is None, skip md5 check for file configs/candidate_masks_flexible_size_and_spacing_3000.json.\n",
      "2024-08-14 17:06:54,454 - INFO - File exists: configs/candidate_masks_flexible_size_and_spacing_3000.json, skipped downloading.\n",
      "2024-08-14 17:06:54,455 - INFO - Expected md5 is None, skip md5 check for file configs/all_anatomy_size_condtions.json.\n",
      "2024-08-14 17:06:54,455 - INFO - File exists: configs/all_anatomy_size_condtions.json, skipped downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=16MKsDKkHvDyF2lEir4dzlxwex_GHStUf\n",
      "From (redirected): https://drive.google.com/uc?id=16MKsDKkHvDyF2lEir4dzlxwex_GHStUf&confirm=t&uuid=90c41b32-cfaf-4274-a0ec-dbd6408cf638\n",
      "To: /tmp/tmpe4zcopul/all_masks_flexible_size_and_spacing_3000.zip\n",
      "100%|██████████████████████████████████████| 9.03G/9.03G [01:54<00:00, 79.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-14 17:08:52,675 - INFO - Downloaded: /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000.zip\n",
      "2024-08-14 17:08:52,675 - INFO - Expected md5 is None, skip md5 check for file /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "\n",
    "# TODO: remove the `files` after the files are uploaded to the NGC\n",
    "files = [\n",
    "    {\n",
    "        \"path\": \"models/autoencoder_epoch273.pt\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1jQefG0yJPzSvTG5rIJVHNqDReBTvVmZ0/view?usp=drive_link\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"models/input_unet3d_data-all_steps1000size512ddpm_random_current_inputx_v1.pt\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1FtOHBGUF5dLZNHtiuhf5EH448EQGGs-_/view?usp=sharing\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"models/controlnet-20datasets-e20wl100fold0bc_noi_dia_fsize_current.pt\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1izr52Whkk56OevNTk2QzI86eJV9TTaLk/view?usp=sharing\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"models/mask_generation_autoencoder.pt\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1FzWrpv6ornYUaPiAWGOOxhRx2P9Wnynm/view?usp=drive_link\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"models/mask_generation_diffusion_unet.pt\",\n",
    "        \"url\": \"https://drive.google.com/file/d/11SA9RUZ6XmCOJr5v6w6UW1kDzr6hlymw/view?usp=drive_link\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"configs/candidate_masks_flexible_size_and_spacing_3000.json\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1yMkH-lrAsn2YUGoTuVKNMpicziUmU-1J/view?usp=sharing\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"configs/all_anatomy_size_condtions.json\",\n",
    "        \"url\": \"https://drive.google.com/file/d/1AJyt1DSoUd2x2AOQOgM7IxeSyo4MXNX0/view?usp=sharing\",\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"datasets/all_masks_flexible_size_and_spacing_3000.zip\",\n",
    "        \"url\": \"https://drive.google.com/file/d/16MKsDKkHvDyF2lEir4dzlxwex_GHStUf/view?usp=sharing\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    file[\"path\"] = file[\"path\"] if \"datasets/\" not in file[\"path\"] else os.path.join(root_dir, file[\"path\"])\n",
    "    download_url(url=file[\"url\"], filepath=file[\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f164998-6c7d-44cd-93ee-e9d36c26ef96",
   "metadata": {},
   "source": [
    "## Read in environment setting, including data directory, model directory, and output directory\n",
    "\n",
    "The information for data directory, model directory, and output directory are saved in ./configs/environment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38b4c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: output\n",
      "trained_autoencoder_path: models/autoencoder_epoch273.pt\n",
      "trained_diffusion_path: models/input_unet3d_data-all_steps1000size512ddpm_random_current_inputx_v1.pt\n",
      "trained_controlnet_path: models/controlnet-20datasets-e20wl100fold0bc_noi_dia_fsize_current.pt\n",
      "trained_mask_generation_autoencoder_path: models/mask_generation_autoencoder.pt\n",
      "trained_mask_generation_diffusion_path: models/mask_generation_diffusion_unet.pt\n",
      "all_mask_files_base_dir: /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000\n",
      "all_mask_files_json: ./configs/candidate_masks_flexible_size_and_spacing_3000.json\n",
      "all_anatomy_size_conditions_json: ./configs/all_anatomy_size_condtions.json\n",
      "label_dict_json: ./configs/label_dict.json\n",
      "label_dict_remap_json: ./configs/label_dict_124_to_132.json\n",
      "Global config variables have been loaded.\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "environment_file = \"./configs/environment.json\"\n",
    "env_dict = json.load(open(environment_file, \"r\"))\n",
    "for k, v in env_dict.items():\n",
    "    # Update the path to the downloaded dataset in MONAI_DATA_DIRECTORY\n",
    "    val = v if \"datasets/\" not in v else os.path.join(root_dir, v)\n",
    "    setattr(args, k, val)\n",
    "    print(f\"{k}: {val}\")\n",
    "print(\"Global config variables have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98233f-6492-40ed-9ba5-7ab4ae0f8ffd",
   "metadata": {},
   "source": [
    "## Read in configuration setting, including network definition, body region and anatomy to generate, etc.\n",
    "\n",
    "The information for the inference input, like body region and anatomy to generate, is stored in \"./configs/config_infer.json\". Please refer to README.md for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533414f3-bef5-49f7-b082-f803b5e494bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_output_samples: 1\n",
      "body_region: ['abdomen']\n",
      "anatomy_list: ['liver', 'hepatic tumor']\n",
      "controllable_anatomy_size: []\n",
      "num_inference_steps: 1000\n",
      "mask_generation_num_inference_steps: 1000\n",
      "output_size: [256, 256, 256]\n",
      "image_output_ext: .nii.gz\n",
      "label_output_ext: .nii.gz\n",
      "spacing: [1.5, 1.5, 2.0]\n",
      "autoencoder_sliding_window_infer_size: [48, 48, 48]\n",
      "autoencoder_sliding_window_infer_overlap: 0.25\n",
      "Network definition and inference inputs have been loaded.\n"
     ]
    }
   ],
   "source": [
    "config_file = \"./configs/config_maisi.json\"\n",
    "config_dict = json.load(open(config_file, \"r\"))\n",
    "for k, v in config_dict.items():\n",
    "    setattr(args, k, v)\n",
    "\n",
    "# check the format of inference inputs\n",
    "config_infer_file = \"./configs/config_infer.json\"\n",
    "config_infer_dict = json.load(open(config_infer_file, \"r\"))\n",
    "for k, v in config_infer_dict.items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "check_input(\n",
    "    args.body_region,\n",
    "    args.anatomy_list,\n",
    "    args.label_dict_json,\n",
    "    args.output_size,\n",
    "    args.spacing,\n",
    "    args.controllable_anatomy_size,\n",
    ")\n",
    "latent_shape = [args.latent_channels, args.output_size[0] // 4, args.output_size[1] // 4, args.output_size[2] // 4]\n",
    "print(\"Network definition and inference inputs have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ba613d-a2f5-4afc-95df-65ad21fafedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "args.random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02476bc7-c980-4b0b-8d7a-c780a5ccc11f",
   "metadata": {},
   "source": [
    "## Initialize networks and noise scheduler, then load the trained model weights.\n",
    "\n",
    "The networks and noise scheduler are defined in `config_maisi.json`. We will read them in and load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-14 17:12:39,907 - INFO - 'dst' model updated: 158 of 206 variables.\n",
      "All the trained model weights have been loaded.\n"
     ]
    }
   ],
   "source": [
    "noise_scheduler = define_instance(args, \"noise_scheduler\")\n",
    "mask_generation_noise_scheduler = define_instance(args, \"mask_generation_noise_scheduler\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "autoencoder = define_instance(args, \"autoencoder_def\").to(device)\n",
    "checkpoint_autoencoder = load_autoencoder_ckpt(args.trained_autoencoder_path)\n",
    "autoencoder.load_state_dict(checkpoint_autoencoder)\n",
    "\n",
    "diffusion_unet = define_instance(args, \"diffusion_unet_def\").to(device)\n",
    "checkpoint_diffusion_unet = torch.load(args.trained_diffusion_path)\n",
    "new_dict = load_diffusion_ckpt(diffusion_unet.state_dict(), checkpoint_diffusion_unet[\"unet_state_dict\"])\n",
    "diffusion_unet.load_state_dict(new_dict, strict=True)\n",
    "scale_factor = checkpoint_diffusion_unet[\"scale_factor\"].to(device)\n",
    "\n",
    "controlnet = define_instance(args, \"controlnet_def\").to(device)\n",
    "checkpoint_controlnet = torch.load(args.trained_controlnet_path)\n",
    "new_dict = load_diffusion_ckpt(controlnet.state_dict(), checkpoint_controlnet[\"controlnet_state_dict\"])\n",
    "monai.networks.utils.copy_model_state(controlnet, diffusion_unet.state_dict())\n",
    "controlnet.load_state_dict(new_dict, strict=True)\n",
    "\n",
    "mask_generation_autoencoder = define_instance(args, \"mask_generation_autoencoder_def\").to(device)\n",
    "checkpoint_mask_generation_autoencoder = load_autoencoder_ckpt(args.trained_mask_generation_autoencoder_path)\n",
    "mask_generation_autoencoder.load_state_dict(checkpoint_mask_generation_autoencoder)\n",
    "\n",
    "mask_generation_diffusion_unet = define_instance(args, \"mask_generation_diffusion_def\").to(device)\n",
    "checkpoint_mask_generation_diffusion_unet = torch.load(args.trained_mask_generation_diffusion_path)\n",
    "mask_generation_diffusion_unet.load_old_state_dict(checkpoint_mask_generation_diffusion_unet)\n",
    "mask_generation_scale_factor = args.mask_generation_scale_factor\n",
    "\n",
    "print(\"All the trained model weights have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "## Define the LDM Sampler, which contains functions that will perform the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldm_sampler = LDMSampler(\n",
    "    args.body_region,\n",
    "    args.anatomy_list,\n",
    "    args.all_mask_files_json,\n",
    "    args.all_anatomy_size_conditions_json,\n",
    "    args.all_mask_files_base_dir,\n",
    "    args.label_dict_json,\n",
    "    args.label_dict_remap_json,\n",
    "    autoencoder,\n",
    "    diffusion_unet,\n",
    "    controlnet,\n",
    "    noise_scheduler,\n",
    "    scale_factor,\n",
    "    mask_generation_autoencoder,\n",
    "    mask_generation_diffusion_unet,\n",
    "    mask_generation_scale_factor,\n",
    "    mask_generation_noise_scheduler,\n",
    "    device,\n",
    "    latent_shape,\n",
    "    args.mask_generation_latent_shape,\n",
    "    args.output_size,\n",
    "    args.output_dir,\n",
    "    args.controllable_anatomy_size,\n",
    "    image_output_ext=args.image_output_ext,\n",
    "    label_output_ext=args.label_output_ext,\n",
    "    spacing=args.spacing,\n",
    "    num_inference_steps=args.num_inference_steps,\n",
    "    mask_generation_num_inference_steps=args.mask_generation_num_inference_steps,\n",
    "    random_seed=args.random_seed,\n",
    "    autoencoder_sliding_window_infer_size=args.autoencoder_sliding_window_infer_size,\n",
    "    autoencoder_sliding_window_infer_overlap=args.autoencoder_sliding_window_infer_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a7fce-e9ae-48a0-b74a-47f880369efa",
   "metadata": {},
   "source": [
    "## Perform the inference\n",
    "**Time cost:** It will take around 80s to generate a [256,256,256] mask, and another 105s to generate the corresponding [256,256,256] image on one A100. The time cost per mask is fixed; time cost per image is roughly linear to the output size. i.e., if we want to generate a [512,512,512] image/mask pair, it will take around 80s to generate a [256,256,256] mask and then resample it to a [512,512,512] mask. The time cost for [512,512,512] image generatation will be 8 times longer, around 8x105s~14min. \n",
    "\n",
    "**GPU memory:** Decoding the latents into image with autoencoder is the step that consumes the largest GPU memory. The mask generation takes 34G. For image generation, to save GPU memory, we use sliding window inference for the autoencoder when the `output_size` is large. So the GPU memory for image generation is not strictly linear to the output_size. For [256,256,256], it takes 34G. For [512,512,512], it takes 67G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f91bf-1c55-46e2-ae56-8677cd8eb81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated image/mask pairs will be saved in output.\n",
      "Extracting /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000.zip to /tmp/tmpr4d6e0ww/datasets\n",
      "2024-08-14 17:12:42,609 - INFO - Writing into directory: /tmp/tmpr4d6e0ww/datasets.\n",
      "Unzipped /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000.zip to /tmp/tmpr4d6e0ww/datasets/all_masks_flexible_size_and_spacing_3000.\n",
      "augmenting liver tumor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████▌                                                 | 293/1000 [00:28<01:07, 10.53it/s]"
     ]
    }
   ],
   "source": [
    "print(f\"The generated image/mask pairs will be saved in {args.output_dir}.\")\n",
    "output_filenames = ldm_sampler.sample_multiple_images(args.num_output_samples)\n",
    "print(\"MAISI image/mask generation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bb8d7-17db-48b5-8d08-d8af61fc763a",
   "metadata": {},
   "source": [
    "## Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0453d9f-1614-4c84-aef1-77b6339d8c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_image_filename = output_filenames[0][0]\n",
    "visualize_mask_filename = output_filenames[0][1]\n",
    "print(f\"Visualizing {visualize_image_filename} and {visualize_mask_filename}...\")\n",
    "\n",
    "# load image/mask pairs\n",
    "loader = LoadImage(image_only=True, ensure_channel_first=True)\n",
    "orientation = Orientation(axcodes=\"RAS\")\n",
    "image_volume = orientation(loader(visualize_image_filename))\n",
    "mask_volume = orientation(loader(visualize_mask_filename)).to(torch.uint8)\n",
    "\n",
    "# visualize for CT HU intensity between [-200, 500]\n",
    "image_volume = torch.clip(image_volume, -200, 500)\n",
    "image_volume = image_volume - torch.min(image_volume)\n",
    "image_volume = image_volume / torch.max(image_volume)\n",
    "\n",
    "# create a random color map for mask visualization\n",
    "colorize = torch.clip(torch.cat([torch.zeros(3, 1, 1, 1), torch.randn(3, 200, 1, 1)], 1), 0, 1)\n",
    "target_class_index = 1\n",
    "\n",
    "# find center voxel location for 2D slice visualization\n",
    "center_loc_axis = find_label_center_loc(torch.flip(mask_volume[0, ...] == target_class_index, [-3, -2, -1]))\n",
    "\n",
    "# visualization\n",
    "vis_mask = get_xyz_plot(\n",
    "    mask_volume, center_loc_axis, mask_bool=True, n_label=201, colorize=colorize, target_class_index=target_class_index\n",
    ")\n",
    "show_image(vis_mask, title=\"mask\")\n",
    "\n",
    "vis_image = get_xyz_plot(image_volume, center_loc_axis, mask_bool=False)\n",
    "show_image(vis_image, title=\"image\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
