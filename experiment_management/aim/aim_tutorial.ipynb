{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spleen 3D segmentation with MONAI\n",
    "\n",
    "This tutorial shows how to integrate MONAI into an existing PyTorch medical DL program.\n",
    "\n",
    "And easily use below features:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Load Nifti image with metadata.\n",
    "1. Add channel dim to the data if no channel dimension.\n",
    "1. Scale medical image intensity with expected range.\n",
    "1. Crop out a batch of balanced images based on positive / negative label ratio.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D UNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "1. Sliding window inference method.\n",
    "1. Deterministic training for reproducibility.\n",
    "\n",
    "The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
    "\n",
    "![spleen](http://medicaldecathlon.com/img/spleen0.png)\n",
    "\n",
    "Target: Spleen  \n",
    "Modality: CT  \n",
    "Size: 61 3D volumes (41 Training + 20 Testing)  \n",
    "Source: Memorial Sloan Kettering Cancer Center  \n",
    "Challenge: Large ranging foreground size\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'plotly'\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting plotly==5.9.0\n",
      "  Downloading plotly-5.9.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.9.0 tenacity-8.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "import monai.transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "\n",
    "import torch\n",
    "from torch.onnx import utils\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import aim\n",
    "from aim.pytorch import track_params_dists, track_gradients_dists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.0\n",
      "Numpy version: 1.19.5\n",
      "Pytorch version: 1.10.2+cu102\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 714d00dffe6653e21260160666c4c201ab66511b\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.8\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.17.2\n",
      "Pillow version: 8.4.0\n",
      "Tensorboard version: 2.6.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.62.3\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.7.0\n",
      "pandas version: 1.0.4\n",
      "einops version: 0.4.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 1.23.1\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Tracking Server\n",
    "\n",
    "\n",
    "The remote tracking server allows to easily track and save the experiments from a remote dedicated server. In order to use this feature simply changing the variable `remote_tracking_server` to the designated `aim://ip:port` will be sufficient. I.E. `remote_tracking_server = 'aim://17.116.226.20:12345'`. To learn more about this feature please visit the [Official Aim Documentation](https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html).\n",
    "\n",
    "If no such server is present the result will be stord and tracked **locally**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remote_tracking_server = None\n",
    "experiment_name = 'spleen_segmentation_3d'\n",
    "if remote_tracking_server is not None:\n",
    "    remote_tracking_server = f'aim://{args.aim_servr}'\n",
    "    aim_run  = aim.Run(experiment = experiment_name, repo = remote_tracking_server)\n",
    "else:\n",
    "    aim_run  = aim.Run(experiment = experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model Meta-param Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_transformations_metadata(transformation_type:str = 'train_transforms', transformations = None) -> bool:\n",
    "    \"\"\"\n",
    "    Tracking Processing Meta-information\n",
    "        Arguments:\n",
    "            transformation_type (str): context tag of transformation\n",
    "        Returns:\n",
    "            ddict: The dictionary to be passed to aim\n",
    "    \"\"\"\n",
    "\n",
    "    tracking_success = False\n",
    "    primitives= (int, str, float, bool, tuple)\n",
    "    \n",
    "    tansformations_dict = defaultdict(lambda: defaultdict(defaultdict))\n",
    "    try:\n",
    "        for transform in tqdm(transformations.transforms):\n",
    "            transform = transform.__dict__\n",
    "            for meta_key,meta_val in transform.items():\n",
    "                if hasattr(meta_val, '__module__'):\n",
    "                    meta_val = meta_val.__dict__\n",
    "                    for key,val in meta_val.items():\n",
    "                        if type(val) in primitives:\n",
    "                            tansformations_dict[transformation_type][meta_key][key] = val\n",
    "                            \n",
    "        tracking_success = True\n",
    "                            \n",
    "    except RuntimeError as e:\n",
    "        print(\"Unable to track transformation params with error: \", e)\n",
    "        return tansformations_dict\n",
    "    return tansformations_dict\n",
    "\n",
    "def track_model_graph(model, input_size) -> dict:\n",
    "    \"\"\"Tracking Model Graph Data Meta-information\n",
    "        Arguments:\n",
    "            model (Torch-model): The model being tacked\n",
    "            input_size (list): the list of input shapes\n",
    "        Returns:\n",
    "            dict: \n",
    "    \"\"\"\n",
    "    model_metadata = {}\n",
    "    try:\n",
    "        graph, params_dict, torch_out = utils._model_to_graph(model,\\\n",
    "                         (torch.zeros(input_size).float().cuda(),),\\\n",
    "                           operator_export_type=torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH)\n",
    "        \n",
    "        op_counter = {}\n",
    "        for node in graph.nodes():\n",
    "            input_types = []\n",
    "            inputs = []\n",
    "            for node_input in node.inputs():\n",
    "                if isinstance(node_input.type(), torch._C.TensorType):\n",
    "                    input_types.append(node_input.type().scalarType())\n",
    "                    inputs.append(node_input.debugName())\n",
    "\n",
    "            attributes_dict = {}\n",
    "            for attr in node.attributeNames():\n",
    "                attributes_dict[attr] = node[attr]\n",
    "\n",
    "            operator = node.kind()[6:]\n",
    "            \n",
    "            if operator not in op_counter:\n",
    "                op_counter[operator] = 0\n",
    "            else:\n",
    "                op_counter[operator] += 1\n",
    "            \n",
    "            outputs = [o.unique() for o in node.outputs()]\n",
    "            scope = node.scopeName()\n",
    "            \n",
    "            \n",
    "            input_name = inputs[0]+ f\"_{operator}_{op_counter[operator]}\"\n",
    "            \n",
    "            model_metadata[input_name] = {'operator': operator, 'attributes': attributes_dict, \\\n",
    "                                      'input_types':input_types, 'inputs': inputs, 'outputs':outputs, \\\n",
    "                                          'scope':scope\n",
    "                                     }\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"Unable to track model graph and hyperparams with error: \",e)\n",
    "        return model_metadata\n",
    "    return model_metadata\n",
    "\n",
    "\n",
    "def bin_ndarray(ndarray, new_shape, operation='mean'):\n",
    "    \"\"\"\n",
    "    Bins an ndarray in all axes based on the target shape, by summing or\n",
    "        averaging.\n",
    "    Number of output dimensions must match number of input dimensions.\n",
    "    Example\n",
    "    -------\n",
    "    >>> m = np.arange(0,100,1).reshape((10,10))\n",
    "    >>> n = bin_ndarray(m, new_shape=(5,5), operation='sum')\n",
    "    >>> print(n)\n",
    "    [[ 22  30  38  46  54]\n",
    "     [102 110 118 126 134]\n",
    "     [182 190 198 206 214]\n",
    "     [262 270 278 286 294]\n",
    "     [342 350 358 366 374]]\n",
    "    \"\"\"\n",
    "    if not operation.lower() in ['sum', 'mean', 'average', 'avg']:\n",
    "        raise ValueError(\"Operation {} not supported.\".format(operation))\n",
    "    if ndarray.ndim != len(new_shape):\n",
    "        raise ValueError(\"Shape mismatch: {} -> {}\".format(ndarray.shape,\n",
    "                                                           new_shape))\n",
    "    compression_pairs = [(d, c//d) for d, c in zip(new_shape,\n",
    "                                                   ndarray.shape)]\n",
    "    flattened = [l for p in compression_pairs for l in p]\n",
    "    ndarray = ndarray.reshape(flattened)\n",
    "    for i in range(len(new_shape)):\n",
    "        if operation.lower() == \"sum\":\n",
    "            ndarray = ndarray.sum(-1*(i+1))\n",
    "        elif operation.lower() in [\"mean\", \"average\", \"avg\"]:\n",
    "            ndarray = ndarray.mean(-1*(i+1))\n",
    "    return ndarray\n",
    "\n",
    "def pad_ndarray(ndarray, new_shape):\n",
    "    result = np.zeros(new_shape)\n",
    "    result[:ndarray.shape[0],:ndarray.shape[1],:ndarray.shape[2]] = ndarray\n",
    "    return result\n",
    "\n",
    "def closest_number_up(n, m) :\n",
    "    # Find the quotient\n",
    "    q = int(n / m)\n",
    "         \n",
    "    if((n * m) > 0) :\n",
    "        closest_up = (m * (q + 1))\n",
    "    else:\n",
    "        closest_up = (m * (q - 1))\n",
    "     \n",
    "    return closest_up\n",
    "\n",
    "def closest_shapes_up(shapes, dims, pad_factor = 10):\n",
    "    new_shape = [shapes[i] for i in range(len(shapes))]\n",
    "    for dim in dims:\n",
    "        new_shape[dim] = closest_number_up(shapes[dim],pad_factor)\n",
    "        \n",
    "    return new_shape\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "Downloads and extracts the dataset.  \n",
    "The dataset comes from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(\"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MSD Spleen dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(train_images, train_labels)\n",
    "]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation\n",
    "\n",
    "Here we use several transforms to augment the dataset:\n",
    "1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n",
    "1. `AddChanneld` as the original data doesn't have channel dim, add 1 dim to construct \"channel first\" shape.\n",
    "1. `Spacingd` adjusts the spacing by `pixdim=(1.5, 1.5, 2.)` based on the affine matrix.\n",
    "1. `Orientationd` unifies the data orientation based on the affine matrix.\n",
    "1. `ScaleIntensityRanged` extracts intensity range [-57, 164] and scales to [0, 1].\n",
    "1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n",
    "1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n",
    "The image centers of negative samples must be in valid body area.\n",
    "1. `RandAffined` efficiently performs `rotate`, `scale`, `shear`, `translate`, etc. together based on PyTorch affine transform.\n",
    "1. `EnsureTyped` converts the numpy array to PyTorch Tensor for further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        # user can also add other random transforms\n",
    "        # RandAffined(\n",
    "        #     keys=['image', 'label'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     prob=1.0, spatial_size=(96, 96, 96),\n",
    "        #     rotate_range=(0, 0, np.pi/15),\n",
    "        #     scale_range=(0.1, 0.1, 0.1)),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tansformations_dict_train = track_transformations_metadata('train_transforms', train_transforms)\n",
    "tansformations_dict_val = track_transformations_metadata('validation_transforms', val_transforms)\n",
    "\n",
    "tansformations_dict_train.update(tansformations_dict_val)\n",
    "aim_run['hparams'] = tansformations_dict_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check transforms in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "# plot the slice [:, :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:, :, 80], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vol = check_data[\"image\"][0][0]\n",
    "\n",
    "def frame_args(duration):\n",
    "    return {\n",
    "            \"frame\": {\"duration\": duration},\n",
    "            \"mode\": \"immediate\",\n",
    "            \"fromcurrent\": True,\n",
    "            \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n",
    "        }\n",
    "\n",
    "def plotly_3d_figure(volume, number_of_frames, sampling_rate = 2, padding_factor = 10):\n",
    "\n",
    "    volume = (vol.T*255).float()\n",
    "    nb_frames = number_of_frames\n",
    "    frame_ratio = nb_frames/10\n",
    "    \n",
    "    original_frames = len(volume)\n",
    "    frame_subsample_rate = math.ceil(original_frames/nb_frames)\n",
    "    \n",
    "    volume = volume[::frame_subsample_rate]\n",
    "\n",
    "    pad_shape = closest_shapes_up(volume.shape, dims=(1,2),pad_factor = padding_factor)\n",
    "    volume = pad_ndarray(volume, pad_shape)\n",
    "    r, c = volume[0].shape\n",
    "    volume = bin_ndarray(volume, (nb_frames,int(r/sampling_rate),int(c/sampling_rate)))\n",
    "    r, c = volume[0].shape\n",
    "\n",
    "\n",
    "    fig = go.Figure(frames=[go.Frame(data=go.Surface(\n",
    "        z=(frame_ratio - k * 0.1) * np.ones((r, c)),\n",
    "        surfacecolor=np.flipud(volume[nb_frames - 1 - k]),\n",
    "        cmin=0, cmax=300\n",
    "        ),\n",
    "        name=str(k) # you need to name the frame for the animation to behave properly\n",
    "        )\n",
    "        for k in range(nb_frames)])\n",
    "\n",
    "    # Add data to be displayed before animation starts\n",
    "    fig.add_trace(go.Surface(\n",
    "        z=frame_ratio * np.ones((r, c)),\n",
    "        surfacecolor=np.flipud(volume[nb_frames-1]),\n",
    "        colorscale='Gray',\n",
    "        cmin=0, cmax=300,\n",
    "        colorbar=dict(thickness=20, ticklen=4)\n",
    "        ))\n",
    "\n",
    "    sliders = [\n",
    "                {\n",
    "                    \"pad\": {\"b\": 10, \"t\": 60},\n",
    "                    \"len\": 0.9,\n",
    "                    \"x\": 0.1,\n",
    "                    \"y\": 0,\n",
    "                    \"steps\": [\n",
    "                        {\n",
    "                            \"args\": [[f.name], frame_args(0)],\n",
    "                            \"label\": str(k),\n",
    "                            \"method\": \"animate\",\n",
    "                        }\n",
    "                        for k, f in enumerate(fig.frames)\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "             title='Slices in volumetric data',\n",
    "             width=500,\n",
    "             height=500,\n",
    "             scene=dict(\n",
    "                        zaxis=dict(range=[-0.1, frame_ratio], autorange=False),\n",
    "                        aspectmode = 'auto',\n",
    "                        aspectratio=dict(x=1, y=1, z=1),\n",
    "                        ),\n",
    "             updatemenus = [\n",
    "                {\n",
    "                    \"buttons\": [\n",
    "                        {\n",
    "                            \"args\": [None, frame_args(1)],\n",
    "                            \"label\": \"&#9654;\", # play symbol\n",
    "                            \"method\": \"animate\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"args\": [[None], frame_args(0)],\n",
    "                            \"label\": \"&#9724;\", # pause symbol\n",
    "                            \"method\": \"animate\",\n",
    "                        },\n",
    "                    ],\n",
    "                    \"direction\": \"left\",\n",
    "                    \"pad\": {\"r\": 10, \"t\": 70},\n",
    "                    \"type\": \"buttons\",\n",
    "                    \"x\": 0.1,\n",
    "                    \"y\": 0,\n",
    "                }\n",
    "             ],\n",
    "             sliders=sliders\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "vol = check_data[\"image\"][0][0]\n",
    "fig = plotly_3d_figure(vol,number_of_frames = 10, sampling_rate = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CacheDataset and DataLoader for training and validation\n",
    "\n",
    "Here we use CacheDataset to accelerate training and validation process, it's 10x faster than the regular Dataset.  \n",
    "To achieve best performance, set `cache_rate=1.0` to cache all the data, if memory is not enough, set lower value.  \n",
    "Users can also set `cache_num` instead of `cache_rate`, will use the minimum value of the 2 settings.  \n",
    "And set `num_workers` to enable multi-threads during caching.  \n",
    "If want to to try the regular Dataset, just change to use the commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(\n",
    "    data=train_files, transform=train_transforms,\n",
    "    cache_rate=1.0, num_workers=4)\n",
    "# train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=0)\n",
    "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet_meatdata = dict(spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = UNet(**UNet_meatdata).to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "loss_type = \"DiceLoss\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "aim_run['UNet_meatdata'] = UNet_meatdata\n",
    "\n",
    "Optimizer_metadata = {}\n",
    "for ind, param_group in enumerate(optimizer.param_groups):\n",
    "    optim_meta_keys = list(param_group.keys())\n",
    "    Optimizer_metadata[f'param_group_{ind}'] = {key: value for (key, value) in param_group.items() if 'params' not in key}\n",
    "\n",
    "aim_run['Optimizer_metadata'] = Optimizer_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_metadata = track_model_graph(model, input_size=[32, 1, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_run['model_graph_metadata'] = model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 600\n",
    "val_interval = 10\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "slice_to_track = 80\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\")\n",
    "        \n",
    "        aim_run.track(loss.item() , name = \"Batch_Loss\", context = {'type':loss_type})\n",
    "        \n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    \n",
    "    aim_run.track(epoch_loss , name = \"Epoch_Loss\", context = {'type':loss_type})\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        \n",
    "        volumetric_scans_tracked = False\n",
    "        \n",
    "        if (epoch + 1) % val_interval*2 == 0:\n",
    "\n",
    "            #THIS SEGMENT TAKES VERY LITTLE\n",
    "            track_params_dists(model,aim_run)\n",
    "            print(\"tracked param dists\")\n",
    "\n",
    "            #THIS SEGMENT TAKES RELATIVELY LONG (Advise Against it)\n",
    "            track_gradients_dists(model, aim_run)\n",
    "            print(\"tracked gradient dists\")\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, val_data in enumerate(val_loader):\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                \n",
    " \n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, roi_size, sw_batch_size, model)\n",
    "                \n",
    "                #Tracking greyscale properly\n",
    "                output = torch.argmax(val_outputs, dim=1)[0, :, :, slice_to_track].float()\n",
    "                \n",
    "                #Alternative Method\n",
    "#                 output = (torch.argmax(val_outputs, dim=1)[0, :, :, 80] * 255).cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "                aim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n",
    "                                        caption=f'Input Image: {index}'), \\\n",
    "                               name = 'Validation',  context = {'type':'Input'})\n",
    "                \n",
    "                aim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n",
    "                                        caption=f'Label Image: {index}'), \\\n",
    "                               name = 'Validation',  context = {'type':'Label'})\n",
    "                aim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n",
    "                               name = 'Predictions',  context = {'type':'labels'}) \n",
    "                        \n",
    "                print(\"tracked images\")\n",
    "         \n",
    "    \n",
    "                if (epoch + 1) % val_interval*2 == 0 and not volumetric_scans_tracked:\n",
    "                    #THIS SEGMENT TAKES NOT SO LONG\n",
    "\n",
    "                    fig_raw = plotly_3d_figure(val_inputs[0][0],number_of_frames = 113, sampling_rate = 5)\n",
    "                    fig_label = plotly_3d_figure(val_labels[0][0],number_of_frames = 113, sampling_rate = 5)\n",
    "\n",
    "                    aim_run.track(aim.Figure(fig_raw), name='Volumetric Validation', context={'Inputs':'Raw'})\n",
    "                    aim_run.track(aim.Figure(fig_label), name='Volumetric Validation', context={'Inputs':'label'})\n",
    "                    aim_run.track(aim.Figure(fig), name='Volumetric Validation', context={'Predictions':'label'})\n",
    "                    print(\"tracked Plotly\")\n",
    "                    \n",
    "                    volumetric_scans_tracked = True\n",
    "\n",
    "            \n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                \n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                \n",
    "\n",
    "            volumetric_scans_tracked = False\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            aim_run.track(metric , name = \"Val_metric\", context = {'type':loss_type})\n",
    "\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                aim_run[\"best_metric\"] = metric\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    root_dir, \"best_metric_model.pth\"))\n",
    "                \n",
    "                best_model_log_message = f\"saved new best metric model at the {epoch+1}th epoch\"\n",
    "                aim_run.track(aim.Text(best_model_log_message), name = f'best_model_log_message', epoch=epoch+1)\n",
    "                print(best_model_log_message)\n",
    "                \n",
    "            message1 = f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "            message2 = f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            message3 = f\"at epoch: {best_metric_epoch}\"\n",
    "    \n",
    "            aim_run.track(aim.Text(message1 +\"\\n\" + message2 + message3), name = f'Epoch Summary', epoch=epoch+1)\n",
    "            print(message1, message2, message3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aim\n",
    "%aim up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_data[\"image\"].to(device), roi_size, sw_batch_size, model\n",
    "        )\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(\n",
    "            val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "        plt.show()\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = Dataset(\n",
    "    data=val_files, transform=val_org_transforms)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose([\n",
    "    EnsureTyped(keys=\"pred\"),\n",
    "    Invertd(\n",
    "        keys=\"pred\",\n",
    "        transform=val_org_transforms,\n",
    "        orig_keys=\"image\",\n",
    "        meta_keys=\"pred_meta_dict\",\n",
    "        orig_meta_keys=\"image_meta_dict\",\n",
    "        meta_key_postfix=\"meta_dict\",\n",
    "        nearest_interp=False,\n",
    "        to_tensor=True,\n",
    "    ),\n",
    "    AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "    AsDiscreted(keys=\"label\", to_onehot=2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_data[\"pred\"] = sliding_window_inference(\n",
    "            val_inputs, roi_size, sw_batch_size, model)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    # aggregate the final mean dice result\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
