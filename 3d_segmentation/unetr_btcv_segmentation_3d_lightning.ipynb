{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Multi-organ Segmentation with UNETR  (BTCV Challenge)\n",
    "# PyTorch Lightning Tutorial\n",
    "\n",
    "\n",
    "Prepared by: **Ali Hatamizadeh** and **Yucheng Tang**.\n",
    "\n",
    "This tutorial demonstrates how to construct UNETR training workflow in PyTorch Lightning framework with MONAI for multi-organ segmentation task using the BTCV challenge dataset.\n",
    "\n",
    "![image](https://lh3.googleusercontent.com/pw/AM-JKLU2eTW17rYtCmiZP3WWC-U1HCPOHwLe6pxOfJXwv2W-00aHfsNy7jeGV1dwUq0PXFOtkqasQ2Vyhcu6xkKsPzy3wx7O6yGOTJ7ZzA01S6LSh8szbjNLfpbuGgMe6ClpiS61KGvqu71xXFnNcyvJNFjN=w1448-h496-no?authuser=0)\n",
    "\n",
    "And it contains the following features:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly adjust intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D UNETR model, Dice loss function, Mean Dice metric for multi-oorgan segmentation task.\n",
    "\n",
    "The dataset comes from https://www.synapse.org/#!Synapse:syn3193805/wiki/217752.  \n",
    "\n",
    "Under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm. \n",
    "\n",
    "Target: 13 abdominal organs including 1. Spleen 2. Right Kidney 3. Left Kideny 4.Gallbladder 5.Esophagus 6. Liver 7. Stomach 8.Aorta 9. IVC 10. Portal and Splenic Veins 11. Pancreas 12 Right adrenal gland 13 Left adrenal gland.\n",
    "\n",
    "Modality: CT\n",
    "Size: 30 3D volumes (24 Training + 6 Testing)  \n",
    "Challenge: BTCV MICCAI Challenge\n",
    "\n",
    "The following figure shows image patches with the organ sub-regions that are annotated in the CT (top left) and the final labels for the whole dataset (right).\n",
    "\n",
    "Data, figures and resources are taken from: \n",
    "\n",
    "\n",
    "1. [UNETR: Transformers for 3D Medical Image Segmentation](https://arxiv.org/abs/2103.10504)\n",
    "\n",
    "2. [High-resolution 3D abdominal segmentation with random patch network fusion (MIA)](https://www.sciencedirect.com/science/article/abs/pii/S1361841520302589)\n",
    "\n",
    "3. [Efficient multi-atlas abdominal segmentation on clinically acquired CT with SIMPLE context learning (MIA)](https://www.sciencedirect.com/science/article/abs/pii/S1361841515000766?via%3Dihub)\n",
    "\n",
    "\n",
    "![image](https://lh3.googleusercontent.com/pw/AM-JKLX0svvlMdcrchGAgiWWNkg40lgXYjSHsAAuRc5Frakmz2pWzSzf87JQCRgYpqFR0qAjJWPzMQLc_mmvzNjfF9QWl_1OHZ8j4c9qrbR6zQaDJWaCLArRFh0uPvk97qAa11HtYbD6HpJ-wwTCUsaPcYvM=w1724-h522-no?authuser=0)\n",
    "\n",
    "\n",
    "\n",
    "The image patches show anatomies of a subject, including: \n",
    "1. large organs: spleen, liver, stomach. \n",
    "2. Smaller organs: gallbladder, esophagus, kidneys, pancreas. \n",
    "3. Vascular tissues: aorta, IVC, P&S Veins. \n",
    "4. Glands: left and right adrenal gland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q monai==0.6.0\n",
    "!pip install -q tqdm==4.59.0\n",
    "!pip install -q einops==0.3.0\n",
    "!pip install -q nibabel==3.1.1\n",
    "!pip install -q pytorch-lightning==1.4.0\n",
    "!pip install -q lightning-bolts==0.3.4\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    list_data_collate,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and format in the folder.\n",
    "    1. Download dataset from here: https://www.synapse.org/#!Synapse:syn3193805/wiki/89480\\n\n",
    "    2. Put images in the ./data/imagesTr\n",
    "    3. Put labels in the ./data/labelsTr\n",
    "    4. make JSON file accordingly: ./data/dataset_0.json\n",
    "    Example of JSON file:\n",
    "     {\n",
    "    \"description\": \"btcv yucheng\",\n",
    "    \"labels\": {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"spleen\",\n",
    "        \"2\": \"rkid\",\n",
    "        \"3\": \"lkid\",\n",
    "        \"4\": \"gall\",\n",
    "        \"5\": \"eso\",\n",
    "        \"6\": \"liver\",\n",
    "        \"7\": \"sto\",\n",
    "        \"8\": \"aorta\",\n",
    "        \"9\": \"IVC\",\n",
    "        \"10\": \"veins\",\n",
    "        \"11\": \"pancreas\",\n",
    "        \"12\": \"rad\",\n",
    "        \"13\": \"lad\"\n",
    "    },\n",
    "    \"licence\": \"yt\",\n",
    "    \"modality\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"name\": \"btcv\",\n",
    "    \"numTest\": 20,\n",
    "    \"numTraining\": 80,\n",
    "    \"reference\": \"Vanderbilt University\",\n",
    "    \"release\": \"1.0 06/08/2015\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"test\": [\n",
    "        \"imagesTs/img0061.nii.gz\",\n",
    "        \"imagesTs/img0062.nii.gz\",\n",
    "        \"imagesTs/img0063.nii.gz\",\n",
    "        \"imagesTs/img0064.nii.gz\",\n",
    "        \"imagesTs/img0065.nii.gz\",\n",
    "        \"imagesTs/img0066.nii.gz\",\n",
    "        \"imagesTs/img0067.nii.gz\",\n",
    "        \"imagesTs/img0068.nii.gz\",\n",
    "        \"imagesTs/img0069.nii.gz\",\n",
    "        \"imagesTs/img0070.nii.gz\",\n",
    "        \"imagesTs/img0071.nii.gz\",\n",
    "        \"imagesTs/img0072.nii.gz\",\n",
    "        \"imagesTs/img0073.nii.gz\",\n",
    "        \"imagesTs/img0074.nii.gz\",\n",
    "        \"imagesTs/img0075.nii.gz\",\n",
    "        \"imagesTs/img0076.nii.gz\",\n",
    "        \"imagesTs/img0077.nii.gz\",\n",
    "        \"imagesTs/img0078.nii.gz\",\n",
    "        \"imagesTs/img0079.nii.gz\",\n",
    "        \"imagesTs/img0080.nii.gz\"\n",
    "    ],\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0001.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0001.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0002.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0002.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0003.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0003.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0004.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0004.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0005.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0005.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0006.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0006.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0007.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0007.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0008.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0008.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0009.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0009.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0010.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0010.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0021.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0021.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0022.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0022.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0023.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0023.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0024.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0024.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0025.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0025.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0026.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0026.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0027.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0027.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0028.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0028.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0029.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0029.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0030.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0030.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0031.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0031.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0032.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0032.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0033.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0033.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0034.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0034.nii.gz\"\n",
    "        }\n",
    "    ],\n",
    "    \"validation\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0035.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0035.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0036.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0036.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0037.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0037.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0038.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0038.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0039.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0039.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0040.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0040.nii.gz\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LightningModule (transform, network)\n",
    "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in spleen_segmentation_3d.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self._model = UNETR(\n",
    "            in_channels=1,\n",
    "            out_channels=14,\n",
    "            img_size=(96, 96, 96),\n",
    "            feature_size=16,\n",
    "            hidden_size=768,\n",
    "            mlp_dim=3072,\n",
    "            num_heads=12,\n",
    "            pos_embed=\"perceptron\",\n",
    "            norm_name=\"instance\",\n",
    "            res_block=True,\n",
    "            conv_block=True,\n",
    "            dropout_rate=0.0,\n",
    "        ).to(device)\n",
    "\n",
    "        self.loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=True, n_classes=14)\n",
    "        self.post_label = AsDiscrete(to_onehot=True, n_classes=14)\n",
    "        self.dice_metric = DiceMetric(\n",
    "            include_background=False, reduction=\"mean\", get_not_nans=False\n",
    "        )\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.max_epochs = 1300\n",
    "        self.check_val = 30\n",
    "        self.warmup_epochs = 20\n",
    "        self.metric_values = []\n",
    "        self.epoch_loss_values = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # prepare data\n",
    "        # data_dir ='/dataset/dataset0/'\n",
    "        data_dir = \"/home/ali/Desktop/data_local/Synapse_Orig/\"\n",
    "        split_JSON = \"dataset_0.json\"\n",
    "        datasets = data_dir + split_JSON\n",
    "        datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "        val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                RandCropByPosNegLabeld(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=(96, 96, 96),\n",
    "                    pos=1,\n",
    "                    neg=1,\n",
    "                    num_samples=4,\n",
    "                    image_key=\"image\",\n",
    "                    image_threshold=0,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[2],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                RandShiftIntensityd(\n",
    "                    keys=[\"image\"],\n",
    "                    offsets=0.10,\n",
    "                    prob=0.50,\n",
    "                ),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_ds = CacheDataset(\n",
    "            data=datalist,\n",
    "            transform=train_transforms,\n",
    "            cache_num=24,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        self.val_ds = CacheDataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "            cache_num=6,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self._model.parameters(), lr=1e-4, weight_decay=1e-5\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.epoch_loss_values.append(avg_loss.detach().cpu().numpy())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        roi_size = (96, 96, 96)\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self.forward\n",
    "        )\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        return {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "        }\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.metric_values.append(mean_val_dice)\n",
    "        return {\"log\": tensorboard_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the LightningModule\n",
    "net = Net()\n",
    "\n",
    "# set up checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=root_dir, filename=\"best_metric_model\")\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    gpus=[0],\n",
    "    max_epochs=net.max_epochs,\n",
    "    check_val_every_n_epoch=net.check_val,\n",
    "    callbacks=checkpoint_callback,\n",
    "    default_root_dir=root_dir,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_num = 250\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(net.epoch_loss_values))]\n",
    "y = net.epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_map = {\n",
    "    \"img0035.nii.gz\": 170,\n",
    "    \"img0036.nii.gz\": 230,\n",
    "    \"img0037.nii.gz\": 204,\n",
    "    \"img0038.nii.gz\": 204,\n",
    "    \"img0039.nii.gz\": 204,\n",
    "    \"img0040.nii.gz\": 180,\n",
    "}\n",
    "case_num = 4\n",
    "net.load_from_checkpoint(os.path.join(root_dir, \"best_metric_model-v1.ckpt\"))\n",
    "net.eval()\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_name = os.path.split(\n",
    "        net.val_ds[case_num][\"image_meta_dict\"][\"filename_or_obj\"]\n",
    "    )[1]\n",
    "    img = net.val_ds[case_num][\"image\"]\n",
    "    label = net.val_ds[case_num][\"label\"]\n",
    "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
    "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
    "    val_outputs = sliding_window_inference(\n",
    "        val_inputs, (96, 96, 96), 4, net, overlap=0.8\n",
    "    )\n",
    "    plt.figure(\"check\", (18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"image\")\n",
    "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_map[img_name]], cmap=\"gray\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"label\")\n",
    "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_map[img_name]])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"output\")\n",
    "    plt.imshow(\n",
    "        torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, slice_map[img_name]]\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
