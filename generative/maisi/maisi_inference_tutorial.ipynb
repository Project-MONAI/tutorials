{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "# MAISI Inference Tutorial\n",
    "\n",
    "This tutorial illustrates how to use trained MAISI model and codebase to generate synthetic 3D images and paired masks.\n",
    "\n",
    "## Setup environment\n",
    "\n",
    "To run this tutorial, please install `xformers` by following the [official guide](https://github.com/facebookresearch/xformers#installing-xformers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import generative\" || pip install monai-generative\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import os; print(os.path.isfile('models/autoencoder_epoch273.pt'))\" || python -m monai.bundle download_large_files --bundle_path .\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2019e-1556-41a6-95e8-5d1a65f8b3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "import argparse\n",
    "from scripts.utils import define_instance, load_autoencoder_ckpt\n",
    "from scripts.sample import check_input, LDMSampler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Read in environment setting, including data directory, model directory, and output directory\n",
    "\n",
    "The information for data directory, model directory, and output directory are saved in ./configs/environment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "environment_file = \"./configs/environment.json\"\n",
    "env_dict = json.load(open(environment_file, \"r\"))\n",
    "for k, v in env_dict.items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"Global config variables have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98233f-6492-40ed-9ba5-7ab4ae0f8ffd",
   "metadata": {},
   "source": [
    "## Read in configuration setting, including network definition, body region and anatomy to generate, etc.\n",
    "\n",
    "The information used for both training and inference, like network definition, is stored in \"./configs/config_maisi.json\". Training and inference should use the same \"./configs/config_maisi.json\".\n",
    "\n",
    "The information for the inference input, like body region and anatomy to generate, is stored in \"./configs/config_infer.json\". Please feel free to play with it.\n",
    "- `\"num_output_samples\"`: int, the number of output image/mask pairs it will generate.\n",
    "- `\"spacing\"`: voxel size of generated images. E.g., if set to `[1.5, 1.5, 2.0]`, it will generate images with a resolution of 1.5x1.5x2.0 mm.\n",
    "- `\"output_size\"`: volume size of generated images. E.g., if set to `[512, 512, 256]`, it will generate images with size of 512x512x256. They need to be divisible by 16. If you have a small GPU memory size, you should adjust it to small numbers.\n",
    "- `\"controllable_anatomy_size\"`: a list of controllable anatomy and its size scale (0--1). E.g., if set to `[[\"liver\", 0.5],[\"hepatic tumor\", 0.3]]`, the generated image will contain liver that have a median size, with size around 50% percentile, and hepatic tumor that is relatively small, with around 30% percentile. The output will contain paired image and segmentation mask for the controllable anatomy.\n",
    "- `\"body_region\"`: If \"controllable_anatomy_size\" is not specified, \"body_region\" will be used to constrain the region of generated images. It needs to be chosen from \"head\", \"chest\", \"thorax\", \"abdomen\", \"pelvis\", \"lower\".\n",
    "- `\"anatomy_list\"`: If \"controllable_anatomy_size\" is not specified, the output will contain paired image and segmentation mask for the anatomy in \"./configs/label_dict.json\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533414f3-bef5-49f7-b082-f803b5e494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./configs/config_maisi.json\"\n",
    "config_dict = json.load(open(config_file, \"r\"))\n",
    "for k, v in config_dict.items():\n",
    "    setattr(args, k, v)\n",
    "\n",
    "# check the format of inference inputs\n",
    "config_infer_file = \"./configs/config_infer.json\"\n",
    "config_infer_dict = json.load(open(config_infer_file, \"r\"))\n",
    "for k, v in config_infer_dict.items():\n",
    "    setattr(args, k, v)\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "check_input(\n",
    "    args.body_region,\n",
    "    args.anatomy_list,\n",
    "    args.label_dict_json,\n",
    "    args.output_size,\n",
    "    args.spacing,\n",
    "    args.controllable_anatomy_size,\n",
    ")\n",
    "latent_shape = [args.latent_channels, args.output_size[0] // 4, args.output_size[1] // 4, args.output_size[2] // 4]\n",
    "print(\"Network definition and inference inputs have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "## Initialize networks and noise scheduler, then load the trained model weights.\n",
    "\n",
    "The networks and noise scheduler are defined in `config_file`. We will read them in and load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "noise_scheduler = define_instance(args, \"noise_scheduler\")\n",
    "mask_generation_noise_scheduler = define_instance(args, \"mask_generation_noise_scheduler\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "autoencoder = define_instance(args, \"autoencoder_def\").to(device)\n",
    "checkpoint_autoencoder = load_autoencoder_ckpt(args.trained_autoencoder_path)\n",
    "autoencoder.load_state_dict(checkpoint_autoencoder)\n",
    "\n",
    "diffusion_unet = define_instance(args, \"diffusion_unet_def\").to(device)\n",
    "checkpoint_diffusion_unet = torch.load(args.trained_diffusion_path)\n",
    "diffusion_unet.load_state_dict(checkpoint_diffusion_unet[\"unet_state_dict\"])\n",
    "scale_factor = checkpoint_diffusion_unet[\"scale_factor\"].to(device)\n",
    "\n",
    "controlnet = define_instance(args, \"controlnet_def\").to(device)\n",
    "checkpoint_controlnet = torch.load(args.trained_controlnet_path)\n",
    "monai.networks.utils.copy_model_state(controlnet, diffusion_unet.state_dict())\n",
    "controlnet.load_state_dict(checkpoint_controlnet[\"controlnet_state_dict\"], strict=True)\n",
    "\n",
    "mask_generation_autoencoder = define_instance(args, \"mask_generation_autoencoder_def\").to(device)\n",
    "checkpoint_mask_generation_autoencoder = torch.load(args.trained_mask_generation_autoencoder_path)\n",
    "mask_generation_autoencoder.load_state_dict(checkpoint_mask_generation_autoencoder)\n",
    "\n",
    "mask_generation_diffusion_unet = define_instance(args, \"mask_generation_diffusion_def\").to(device)\n",
    "checkpoint_mask_generation_diffusion_unet = torch.load(args.trained_mask_generation_diffusion_path)\n",
    "mask_generation_diffusion_unet.load_state_dict(checkpoint_mask_generation_diffusion_unet, strict=True)\n",
    "mask_generation_scale_factor = args.mask_generation_scale_factor\n",
    "\n",
    "print(\"All the trained model weights have been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "## Define the LDM Sampler, which contains functions that will perform the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldm_sampler = LDMSampler(\n",
    "    args.body_region,\n",
    "    args.anatomy_list,\n",
    "    args.all_mask_files_json,\n",
    "    args.all_anatomy_size_condtions_json,\n",
    "    args.all_mask_files_base_dir,\n",
    "    args.label_dict_json,\n",
    "    args.label_dict_remap_json,\n",
    "    autoencoder,\n",
    "    diffusion_unet,\n",
    "    controlnet,\n",
    "    noise_scheduler,\n",
    "    scale_factor,\n",
    "    mask_generation_autoencoder,\n",
    "    mask_generation_diffusion_unet,\n",
    "    mask_generation_scale_factor,\n",
    "    mask_generation_noise_scheduler,\n",
    "    device,\n",
    "    latent_shape,\n",
    "    args.mask_generation_latent_shape,\n",
    "    args.output_size,\n",
    "    args.output_dir,\n",
    "    args.controllable_anatomy_size,\n",
    "    image_output_ext=args.image_output_ext,\n",
    "    label_output_ext=args.label_output_ext,\n",
    "    spacing=args.spacing,\n",
    "    num_inference_steps=args.num_inference_steps,\n",
    "    mask_generation_num_inference_steps=args.mask_generation_num_inference_steps,\n",
    "    random_seed=args.random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff93fa3-da0c-46ff-8c77-ab53f39f26f5",
   "metadata": {},
   "source": [
    "## Perform the inference. \n",
    "Time cost: It will take around 80s to generate a [256,256,256] mask, and another 105s to generate the corresponding [256,256,256] image on one A100 using 34G GPU memory. The time cost per image is roughly linear to the output size. i.e., if we want to generate a [512,512,512] image/mask pair, it will take around 80s to generate a [256,256,256] mask and then resample it to a [512,512,512] mask. The time for [512,512,512] image generatation will be 8 times longer, around 8x105s~14min. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f91bf-1c55-46e2-ae56-8677cd8eb81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"The generated image/mask pairs will be saved in {args.output_dir}.\")\n",
    "ldm_sampler.sample_multiple_images(args.num_output_samples)\n",
    "print(\"MAISI image/mask generation finished\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
