{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a MedNIST Classifier with Ray\n",
    "\n",
    "This notebook demos the process of deploying a network with Ray Serve as a web service. Ray provides various ways of deploying models with existing platforms like AWS or Azure but we'll focus on local deployment here since researchers are more likely to do this. Ray also provides other libraries for tuning, reinforcement learning, and distributed training in addition to deployment. This tutorial will use MedNIST classifier from the BentoML tutorial so please run at least the training component of that notebook first. The documentation on Ray Serve [start here](https://docs.ray.io/en/master/serve/index.html#rayserve), this notebook will be using the Pytorch specific functionality [discussed here](https://docs.ray.io/en/master/serve/tutorials/pytorch.html).\n",
    "\n",
    "To start install the Ray Serve component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray[serve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports for MONAI are the same as for the BentoML tutorial (assuming it's already installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/py37/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.5.1+2.gdde11000\n",
      "Numpy version: 1.20.2\n",
      "Pytorch version: 1.8.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: dde11000a7a10bcc9ccae76570cbb0e92ea23cf9\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.4\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: 0.18.1\n",
      "Pillow version: 8.2.0\n",
      "Tensorboard version: 2.5.0\n",
      "gdown version: 3.12.2\n",
      "TorchVision version: 0.9.1\n",
      "ITK version: 5.1.2\n",
      "tqdm version: 4.60.0\n",
      "lmdb version: 1.2.1\n",
      "psutil version: 5.8.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "from monai.apps import download_url\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"https://drive.google.com/uc?id=1zKRi5FrwEES_J-AUkM7iBJwc__jy6ct6\"\n",
    "dst = os.path.join(\"..\", \"bentoml\", \"classifier.zip\")\n",
    "if not os.path.exists(dst):\n",
    "    download_url(resource, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will represent the service for the model, which accepts an image sent as the body of a POST request and returns the class name in a JSON structure. Note that this class uses asyncio to define the `__call__` to be compatible with the server backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDNIST_CLASSES = [\"AbdomenCT\", \"BreastMRI\", \"CXR\", \"ChestCT\", \"Hand\", \"HeadCT\"]\n",
    "\n",
    "\n",
    "class MedNISTClassifier:\n",
    "    def __init__(self):\n",
    "        # create the transform for normalizing the image data\n",
    "        self.transform = Compose([AddChannel(), ScaleIntensity(), ToTensor()])\n",
    "        # load the network on the CPU for simplicity and in eval mode\n",
    "        self.net = torch.jit.load(\"../bentoml/classifier.zip\", map_location=\"cpu\").eval()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        image_bytes = await request.body()\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = np.array(img)\n",
    "        image_tensor = self.transform(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(image_tensor[None].float())\n",
    "\n",
    "        _, output_classes = outputs.max(dim=1)\n",
    "\n",
    "        return {\"class_index\": MEDNIST_CLASSES[output_classes[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the server is started and the classifier backend is associated with an endpoint, which is the route to the service relate to the server address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:36:24,683\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=25642)\u001b[0m 2021-03-26 14:36:26,404\tINFO http_state.py:67 -- Starting HTTP proxy with name 'oRDGQN:SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-node:10.246.179.34-0' on node 'node:10.246.179.34-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m INFO:     Started server process [25637]\n",
      "\u001b[2m\u001b[36m(pid=25642)\u001b[0m 2021-03-26 14:36:34,799\tINFO controller.py:178 -- Registering route '/image_classify' to endpoint 'classifier' with methods '['POST']'.\n"
     ]
    }
   ],
   "source": [
    "client = serve.start()\n",
    "client.create_backend(\"classifier\", MedNISTClassifier)\n",
    "client.create_endpoint(\"classifier\", backend=\"classifier\", route=\"/image_classify\", methods=[\"POST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the server running in another process we can send it a query with an image and get a response. By default the server will listen on port 8000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m 2021-03-26 14:37:47,281\tINFO router.py:248 -- Endpoint classifier doesn't exist, waiting for registration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_index': 'Hand'}\n"
     ]
    }
   ],
   "source": [
    "image_bytes = open(\"./hand.jpg\", \"rb\").read()\n",
    "\n",
    "resp = requests.post(\"http://localhost:8000/image_classify\", data=image_bytes)\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done on the command line with `curl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"class_index\": \"Hand\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"http://localhost:8000/image_classify\" --data-binary \"@hand.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally shut down the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:39:26,795\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n",
      "2021-03-26 14:39:26,800\tERROR worker.py:1109 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2021-03-26 14:39:26,803\tERROR worker.py:919 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Usage\n",
    "\n",
    "Ray can be started on the command line. Since it operates as a cluster of nodes the first thing to do is create the head node locally then start the serve component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:54:40,757\tINFO scripts.py:537 -- Local node IP: 10.246.179.34\n",
      "2021-03-26 14:54:41,396\tSUCC scripts.py:565 -- --------------------\n",
      "2021-03-26 14:54:41,396\tSUCC scripts.py:566 -- Ray runtime started.\n",
      "2021-03-26 14:54:41,396\tSUCC scripts.py:567 -- --------------------\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:569 -- Next steps\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:570 -- To connect to this Ray runtime from another node, run\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:574 --   ray start --address='10.246.179.34:6379' --redis-password='5241590000000000'\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:579 -- Alternatively, use the following Python code:\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:582 -- import ray\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:583 -- ray.init(address='auto', _redis_password='5241590000000000')\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:591 -- If connection fails, check your firewall settings and network configuration.\n",
      "2021-03-26 14:54:41,396\tINFO scripts.py:596 -- To terminate the Ray runtime, run\n",
      "2021-03-26 14:54:41,397\tINFO scripts.py:597 --   ray stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:54:41,379\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://localhost:8265\u001b[39m\u001b[22m\n",
      "2021-03-26 14:54:41,800\tINFO worker.py:654 -- Connecting to existing Ray cluster at address: 10.246.179.34:6379\n",
      "\u001b[2m\u001b[36m(pid=28386)\u001b[0m 2021-03-26 14:54:42,165\tINFO http_state.py:67 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-node:10.246.179.34-0' on node 'node:10.246.179.34-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(pid=28423)\u001b[0m INFO:     Started server process [28423]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ray start --head\n",
    "serve start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separate script with very similar code can then be used to add or replace the backend. This would be useful in an experimental setting where the server is running constantly in the background to which you can push updates quickly as you edit your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mednist_classifier_start.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mednist_classifier_start.py\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "MEDNIST_CLASSES = [\"AbdomenCT\", \"BreastMRI\", \"CXR\", \"ChestCT\", \"Hand\", \"HeadCT\"]\n",
    "\n",
    "\n",
    "class MedNISTClassifier:\n",
    "    def __init__(self):\n",
    "        self.transform = Compose([AddChannel(), ScaleIntensity(), ToTensor()])\n",
    "        self.net = torch.jit.load(\"../bentoml/classifier.zip\", map_location=\"cpu\").eval()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        image_bytes = await request.body()\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = np.array(img)\n",
    "        image_tensor = self.transform(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(image_tensor[None].float())\n",
    "\n",
    "        _, output_classes = outputs.max(dim=1)\n",
    "\n",
    "        return {\"class_index\": MEDNIST_CLASSES[output_classes[0]]}\n",
    "\n",
    "\n",
    "ray.init(address=\"auto\")\n",
    "client = serve.connect()\n",
    "\n",
    "# remove previous instance of this backend if present\n",
    "if \"classifier\" in client.list_backends():\n",
    "    client.delete_endpoint(\"classifier\")\n",
    "    client.delete_backend(\"classifier\")\n",
    "\n",
    "client.create_backend(\"classifier\", MedNISTClassifier)\n",
    "client.create_endpoint(\"classifier\", backend=\"classifier\", route=\"/image_classify\", methods=[\"POST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint is then added by running the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:58:16,160\tINFO worker.py:654 -- Connecting to existing Ray cluster at address: 10.246.179.34:6379\n",
      "Exception ignored in: <function ActorHandle.__del__ at 0x7ff8c45b1ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/localek10/miniconda3/envs/monai/lib/python3.8/site-packages/ray/actor.py\", line 769, in __del__\n",
      "AttributeError: 'NoneType' object has no attribute 'global_worker'\n"
     ]
    }
   ],
   "source": [
    "!python mednist_classifier_start.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And checked once again for response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"class_index\": \"Hand\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"http://localhost:8000/image_classify\" --data-binary \"@hand.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the service can be stopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStopped all 19 Ray processes.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
