{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import csv\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import monai.transforms as mt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import yaml\n",
    "from monai.apps import get_logger\n",
    "from monai.auto3dseg.utils import datafold_read\n",
    "from monai.bundle import BundleWorkflow, ConfigParser\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from monai.metrics import CumulativeAverage\n",
    "from monai.utils import (\n",
    "    BundleProperty,\n",
    "    ImageMetaKey,\n",
    "    convert_to_dst_type,\n",
    "    ensure_tuple,\n",
    "    look_up_option,\n",
    "    optional_import,\n",
    "    set_determinism,\n",
    ")\n",
    "from monai.inferers import SlidingWindowInfererAdapt\n",
    "#from monai.networks.nets.cell_sam_wrapper\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#if __package__ in (None, \"\"):\n",
    "#    from components import LabelsToFlows, LoadTiffd, LogitsToLabels\n",
    "#    from cell_sam_wrapper import CellSamWrapper\n",
    "#else:\n",
    "from components import LabelsToFlows, LoadTiffd, LogitsToLabels\n",
    "from components import CellLoss, CellAcc\n",
    "\n",
    "import importlib\n",
    "import cell_sam_wrapper\n",
    "importlib.reload(cell_sam_wrapper)\n",
    "from cell_sam_wrapper import CellSamWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cellpose dataset needs to be downloaded from the following link.\n",
    "TODO: Write more text here \n",
    "\n",
    "The SAM weights need to be download as well, put the links etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CellSamWrapper auto_resize_inputs True network_resize_roi [1024, 1024] checkpoint /home/vnath/Downloads/cellpose_dataset/sam_vit_b_01ec64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vnath/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM ViT-B weights loaded succesfully ...\n"
     ]
    }
   ],
   "source": [
    "# Paths of training data, testing data and output log files\n",
    "data_list_path = 'cellpose_toy_datalist.json'\n",
    "data_root = os.path.normpath('/home/vnath/Downloads/cellpose_dataset/')\n",
    "sam_weights_path = os.path.normpath('/home/vnath/Downloads/cellpose_dataset/sam_vit_b_01ec64.pth')\n",
    "\n",
    "# Define the network, load SAM weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CellSamWrapper(checkpoint=sam_weights_path)\n",
    "model.to(device)\n",
    "print('SAM ViT-B weights loaded succesfully ...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended Data Root to Json file list ...\n",
      "Total Training Data: 40\n",
      "Total Validation Data: 41\n",
      "Total Testing Data: 68\n"
     ]
    }
   ],
   "source": [
    "# Create Required Data lists\n",
    "# Append root path to training, validation and testing data list\n",
    "with open(data_list_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "validation_fold = 0\n",
    "training_list = []\n",
    "validation_list = []\n",
    "testing_list = []\n",
    "\n",
    "# Process training data\n",
    "for item in data.get(\"training\", []):\n",
    "    # Append the base path to image and label\n",
    "    item[\"image\"] = os.path.join(data_root, item[\"image\"])\n",
    "    item[\"label\"] = os.path.join(data_root, item[\"label\"])\n",
    "    \n",
    "    if item[\"fold\"] == validation_fold:\n",
    "        validation_list.append(item)\n",
    "    else:\n",
    "        training_list.append(item)\n",
    "\n",
    "# Process testing data\n",
    "for item in data.get(\"testing\", []):\n",
    "    # Append the base path to image and label\n",
    "    item[\"image\"] = os.path.join(data_root, item[\"image\"])\n",
    "    item[\"label\"] = os.path.join(data_root, item[\"label\"])\n",
    "    testing_list.append(item)\n",
    "\n",
    "print('Appended Data Root to Json file list ...')\n",
    "print('Total Training Data: {}'.format(len(training_list)))\n",
    "print('Total Validation Data: {}'.format(len(validation_list)))\n",
    "print('Total Testing Data: {}'.format(len(testing_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vnath/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/transforms/intensity/array.py:996: Warning: Divide by zero (a_min == a_max)\n",
      "  warn(\"Divide by zero (a_min == a_max)\", Warning)\n",
      "/home/vnath/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/transforms/intensity/array.py:996: Warning: Divide by zero (a_min == a_max)\n",
      "  warn(\"Divide by zero (a_min == a_max)\", Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3 0/40 \n",
      "loss: 7.9356 time 1.53s \n",
      "Epoch 0/3 1/40 \n",
      "loss: 6.5465 time 2.03s \n",
      "Epoch 0/3 2/40 \n",
      "loss: 6.6404 time 2.54s \n",
      "Epoch 0/3 3/40 \n",
      "loss: 5.9076 time 3.06s \n",
      "Epoch 0/3 4/40 \n",
      "loss: 5.8856 time 3.57s \n",
      "Epoch 0/3 5/40 \n",
      "loss: 5.8127 time 4.09s \n",
      "Epoch 0/3 6/40 \n",
      "loss: 5.2565 time 4.61s \n",
      "Epoch 0/3 7/40 \n",
      "loss: 5.2298 time 5.12s \n",
      "Epoch 0/3 8/40 \n",
      "loss: 4.8683 time 5.64s \n",
      "Epoch 0/3 9/40 \n",
      "loss: 4.5087 time 6.16s \n",
      "Epoch 0/3 10/40 \n",
      "loss: 4.5209 time 6.66s \n",
      "Epoch 0/3 11/40 \n",
      "loss: 4.3839 time 7.16s \n",
      "Epoch 0/3 12/40 \n",
      "loss: 4.4858 time 7.66s \n",
      "Epoch 0/3 13/40 \n",
      "loss: 4.4320 time 8.16s \n",
      "Epoch 0/3 14/40 \n",
      "loss: 4.3583 time 8.67s \n",
      "Epoch 0/3 15/40 \n",
      "loss: 4.2036 time 9.18s \n",
      "Epoch 0/3 16/40 \n",
      "loss: 4.0403 time 9.70s \n",
      "Epoch 0/3 17/40 \n",
      "loss: 4.1045 time 10.21s \n",
      "Epoch 0/3 18/40 \n",
      "loss: 3.9796 time 10.73s \n",
      "Epoch 0/3 19/40 \n",
      "loss: 3.8721 time 11.24s \n",
      "Epoch 0/3 20/40 \n",
      "loss: 3.7623 time 11.75s \n",
      "Epoch 0/3 21/40 \n",
      "loss: 3.7757 time 12.26s \n",
      "Epoch 0/3 22/40 \n",
      "loss: 3.7683 time 12.77s \n",
      "Epoch 0/3 23/40 \n",
      "loss: 3.7118 time 13.28s \n",
      "Epoch 0/3 24/40 \n",
      "loss: 3.6743 time 13.79s \n",
      "Epoch 0/3 25/40 \n",
      "loss: 3.7472 time 14.29s \n",
      "Epoch 0/3 26/40 \n",
      "loss: 3.7398 time 14.80s \n",
      "Epoch 0/3 27/40 \n",
      "loss: 3.7518 time 15.31s \n",
      "Epoch 0/3 28/40 \n",
      "loss: 3.7671 time 15.81s \n",
      "Epoch 0/3 29/40 \n",
      "loss: 3.8048 time 16.32s \n",
      "Epoch 0/3 30/40 \n",
      "loss: 3.7420 time 16.83s \n",
      "Epoch 0/3 31/40 \n",
      "loss: 3.6527 time 17.33s \n",
      "Epoch 0/3 32/40 \n",
      "loss: 3.6259 time 17.81s \n",
      "Epoch 0/3 33/40 \n",
      "loss: 3.6294 time 18.28s \n",
      "Epoch 0/3 34/40 \n",
      "loss: 3.6432 time 18.74s \n",
      "Epoch 0/3 35/40 \n",
      "loss: 3.5813 time 19.20s \n",
      "Epoch 0/3 36/40 \n",
      "loss: 3.5985 time 19.66s \n",
      "Epoch 0/3 37/40 \n",
      "loss: 3.5573 time 20.12s \n",
      "Epoch 0/3 38/40 \n",
      "loss: 3.5501 time 20.58s \n",
      "Epoch 0/3 39/40 \n",
      "loss: 3.5888 time 21.04s \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vnath/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/transforms/intensity/array.py:996: Warning: Divide by zero (a_min == a_max)\n",
      "  warn(\"Divide by zero (a_min == a_max)\", Warning)\n",
      "/home/vnath/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/transforms/intensity/array.py:996: Warning: Divide by zero (a_min == a_max)\n",
      "  warn(\"Divide by zero (a_min == a_max)\", Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-30 17:17:25,924 - INFO - CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 25.31 MiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 220.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2024-08-30 17:17:25,924 - WARNING - GPU stitching failed, buffer 1 dim -1, image dim torch.Size([1, 3, 512, 512]).\n",
      "2024-08-30 17:17:26,527 - INFO - CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 228.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2024-08-30 17:17:26,527 - WARNING - GPU buffered stitching failed, attempting on CPU, image dim torch.Size([1, 3, 512, 512]).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 224.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 181\u001b[0m\n\u001b[1;32m    178\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43msliding_inferrer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    184\u001b[0m     val_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m.\u001b[39mas_subclass(torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    188\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m loss_function(logits, target)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/inferers/inferer.py:595\u001b[0m, in \u001b[0;36mSlidingWindowInfererAdapt.__call__\u001b[0;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gpu_stitching \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buffered_stitching \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutOfMemoryError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 595\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    597\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(e)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gpu_stitching:  \u001b[38;5;66;03m# if failed on gpu\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/inferers/inferer.py:584\u001b[0m, in \u001b[0;36mSlidingWindowInfererAdapt.__call__\u001b[0;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# at most 10 trials\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgpu_stitching\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbuffer_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuffered_stitching\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbuffer_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gpu_stitching \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buffered_stitching \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutOfMemoryError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/inferers/inferer.py:515\u001b[0m, in \u001b[0;36mSlidingWindowInferer.__call__\u001b[0;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_thresh:\n\u001b[1;32m    513\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# stitch in cpu memory if image is too large\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msliding_window_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msw_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msw_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_weight_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_coord\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/monai/inferers/utils.py:229\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, with_coord, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m predictor(win_data, unravel_slice, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m dict_keys, seg_tuple \u001b[38;5;241m=\u001b[39m _flatten_struct(seg_prob_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/vista_2d_tutorial_notebook_aug_2024/cell_sam_wrapper.py:71\u001b[0m, in \u001b[0;36mCellSamWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_resize_roi, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print(\"CellSamWrapper x1\", x.shape)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (1, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# print(\"CellSamWrapper image_embeddings\", x.shape)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_features:\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:355\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    353\u001b[0m r_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, q_h, q_w, dim)\n\u001b[1;32m    354\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[0;32m--> 355\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbhwc,wkc->bhwk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    358\u001b[0m     attn\u001b[38;5;241m.\u001b[39mview(B, q_h, q_w, k_h, k_w) \u001b[38;5;241m+\u001b[39m rel_h[:, :, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m rel_w[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    359\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m~/anaconda3/envs/py311_vista_2d/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 224.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training & Validation Transforms\n",
    "roi_size = [256, 256]\n",
    "train_transforms = mt.Compose([\n",
    "    LoadTiffd(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureTyped(\n",
    "        keys=[\"image\", \"label\"], data_type=\"tensor\", dtype=torch.float\n",
    "    ),\n",
    "    mt.ScaleIntensityd(keys=\"image\", minv=0, maxv=1, channel_wise=True),\n",
    "    mt.ScaleIntensityRangePercentilesd(\n",
    "        keys=\"image\",\n",
    "        lower=1,\n",
    "        upper=99,\n",
    "        b_min=0.0,\n",
    "        b_max=1.0,\n",
    "        channel_wise=True,\n",
    "        clip=True,\n",
    "    ),\n",
    "    mt.SpatialPadd(keys=[\"image\", \"label\"], spatial_size=roi_size),\n",
    "    mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=roi_size),\n",
    "    mt.RandAffined(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        prob=0.5,\n",
    "        rotate_range=np.pi,\n",
    "        scale_range=[-0.5, 0.5],\n",
    "        mode=[\"bilinear\", \"nearest\"],\n",
    "        spatial_size=roi_size,\n",
    "        cache_grid=True,\n",
    "        padding_mode=\"border\",\n",
    "    ),\n",
    "    mt.RandAxisFlipd(keys=[\"image\", \"label\"], prob=0.5),\n",
    "    mt.RandGaussianNoised(keys=[\"image\"], prob=0.25, mean=0, std=0.1),\n",
    "    mt.RandAdjustContrastd(keys=[\"image\"], prob=0.25, gamma=(1, 2)),\n",
    "    mt.RandGaussianSmoothd(keys=[\"image\"], prob=0.25, sigma_x=(1, 2)),\n",
    "    mt.RandHistogramShiftd(keys=[\"image\"], prob=0.25, num_control_points=3),\n",
    "    mt.RandGaussianSharpend(keys=[\"image\"], prob=0.25),\n",
    "    LabelsToFlows(keys=\"label\", flow_key=\"flow\")\n",
    "])\n",
    "\n",
    "val_transforms = mt.Compose([\n",
    "    LoadTiffd(keys=[\"image\", \"label\"], allow_missing_keys=True),\n",
    "    mt.EnsureTyped(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                data_type=\"tensor\",\n",
    "                dtype=torch.float,\n",
    "                allow_missing_keys=True,\n",
    "            ),\n",
    "    mt.ScaleIntensityRangePercentilesd(\n",
    "                keys=\"image\",\n",
    "                lower=1,\n",
    "                upper=99,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                channel_wise=True,\n",
    "                clip=True,\n",
    "            ),\n",
    "    LabelsToFlows(keys=\"label\", flow_key=\"flow\", allow_missing_keys=True)\n",
    "])\n",
    "\n",
    "# Datasets & Dataloaders for training, validation and testing\n",
    "train_dataset = Dataset(\n",
    "                        data=training_list, \n",
    "                        transform=train_transforms\n",
    "                    )\n",
    "train_loader = DataLoader(\n",
    "                        train_dataset,\n",
    "                        batch_size = 1,\n",
    "                        shuffle=True,\n",
    "                        num_workers=2\n",
    "                    )\n",
    "\n",
    "val_dataset = Dataset(\n",
    "                        data=validation_list, \n",
    "                        transform=val_transforms\n",
    "                    )\n",
    "val_loader = DataLoader(\n",
    "                        val_dataset,\n",
    "                        batch_size = 1,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2\n",
    "                    )\n",
    "\n",
    "# Training loop with validation\n",
    "loss_function = CellLoss()\n",
    "acc_function = CellAcc\n",
    "\n",
    "# Define the Sliding Window Inferer\n",
    "sliding_inferrer = SlidingWindowInfererAdapt(\n",
    "    roi_size=[256, 256],\n",
    "    sw_batch_size=1,\n",
    "    overlap=0.25,\n",
    "    #mode=\"gaussian\",\n",
    "    cache_roi_weight_map=True,\n",
    "    progress=False)\n",
    "\n",
    "# TODO Just remove and hardset it to being True in the training loop\n",
    "channels_last = True\n",
    "\n",
    "# TODO This path need to be defined above\n",
    "ckpt_path = os.path.join(data_root, 'sanity_model')\n",
    "if os.path.exists(ckpt_path) == False:\n",
    "    os.mkdir(ckpt_path)\n",
    "num_epochs = 3\n",
    "\n",
    "# TODO The validation is yet to be defined\n",
    "num_epochs_per_validation = 1\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.SGD(\n",
    "                        params=model.parameters(),\n",
    "                        momentum=0.9,\n",
    "                        lr=0.01,\n",
    "                        weight_decay=1e-5\n",
    "                    )\n",
    "\n",
    "best_ckpt_path = os.path.join(ckpt_path, \"model.pt\")\n",
    "intermediate_ckpt_path = os.path.join(ckpt_path, \"model_final.pt\")\n",
    "\n",
    "best_metric = -1\n",
    "start_epoch = 0\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "val_epoch_loss_value = []\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    memory_format = torch.channels_last if channels_last else torch.preserve_format\n",
    "    run_loss = CumulativeAverage()\n",
    "    avg_loss = avg_acc = 0\n",
    "\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        data = (\n",
    "            batch_data[\"image\"]\n",
    "            .as_subclass(torch.Tensor)\n",
    "            .to(memory_format=memory_format, device=device)\n",
    "        )\n",
    "\n",
    "        target = (\n",
    "            batch_data[\"flow\"]\n",
    "            .as_subclass(torch.Tensor)\n",
    "            .to(memory_format=memory_format, device=device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(data)\n",
    "\n",
    "        # print('logits', logits.shape, logits.dtype)\n",
    "        loss = loss_function(logits.float(), target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = data.shape[0]\n",
    "        run_loss.append(loss, count=batch_size)\n",
    "        avg_loss = run_loss.aggregate() \n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} {idx}/{len(train_loader)} \")\n",
    "        print(f\"loss: {avg_loss:.4f} time {time.time() - start_time:.2f}s \")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss_values.append(avg_loss)\n",
    "    # Model Saving & Checkpointing\n",
    "    if avg_loss < best_metric:\n",
    "        best_metric = avg_loss\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save({\"state_dict\": state_dict}, best_ckpt_path)\n",
    "\n",
    "\n",
    "# Write the loss plot visualization here\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot 1: Loss\n",
    "axs[0].plot(range(0, num_epochs), epoch_loss_values, marker='o')\n",
    "axs[0].set_title('Training Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss Value')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_xticks(range(0, num_epochs))\n",
    "#axs[0].set_yticks(sorted(set(loss_values)))\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "# TODO Update epoch_loss_values with val_loss_values variable, remove this eventually\n",
    "#axs[1].plot(range(0, num_epochs), epoch_loss_values, marker='o', color='orange')\n",
    "#axs[1].set_title('Validation Loss')\n",
    "#axs[1].set_xlabel('Epoch')\n",
    "#axs[1].set_ylabel('Val Loss Value')\n",
    "#axs[1].grid(True)\n",
    "#axs[1].set_xticks(epoch_values)\n",
    "#axs[1].set_yticks(sorted(set(val_loss_values)))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#print(run_loss)\n",
    "#print(avg_loss)\n",
    "# Testing\n",
    "\n",
    "# Visualization of the dataset  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_vista_2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
